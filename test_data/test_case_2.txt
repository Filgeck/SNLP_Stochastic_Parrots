You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Add helpers to convert between different types of uncertainties
Currently there no easy way to convert from an arbitrary uncertainty class to a different uncertainty class. This would be useful to be able to pass NDData objects to external libraries/tools which assume, for example, that uncertainties will always stored as variances. Here's some really scrappy code I bunged together quickly for my purposes (probably buggy, I need to properly test it), but what are peoples opinions on what's the best API/design/framework for such a system?



```python

from astropy.nddata import (

    VarianceUncertainty, StdDevUncertainty, InverseVariance,

)



def std_to_var(obj):

    return VarianceUncertainty(obj.array ** 2, unit=obj.unit ** 2)





def var_to_invvar(obj):

    return InverseVariance(obj.array ** -1, unit=obj.unit ** -1)





def invvar_to_var(obj):

    return VarianceUncertainty(obj.array ** -1, unit=obj.unit ** -1)





def var_to_std(obj):

    return VarianceUncertainty(obj.array ** 1/2, unit=obj.unit ** 1/2)





FUNC_MAP = {

    (StdDevUncertainty, VarianceUncertainty): std_to_var,

    (StdDevUncertainty, InverseVariance): lambda x: var_to_invvar(

        std_to_var(x)

    ),

    (VarianceUncertainty, StdDevUncertainty): var_to_std,

    (VarianceUncertainty, InverseVariance): var_to_invvar,

    (InverseVariance, StdDevUncertainty): lambda x: var_to_std(

        invvar_to_var(x)

    ),

    (InverseVariance, VarianceUncertainty): invvar_to_var,

    (StdDevUncertainty, StdDevUncertainty): lambda x: x,

    (VarianceUncertainty, VarianceUncertainty): lambda x: x,

    (InverseVariance, InverseVariance): lambda x: x,

}





def convert_uncertainties(obj, new_class):

    return FUNC_MAP[(type(obj), new_class)](obj)

```

</issue>
<code>
[start of README.rst]
1 =======
2 Astropy
3 =======
4 
5 |Actions Status| |CircleCI Status| |Azure Status| |Coverage Status| |PyPI Status| |Documentation Status| |Zenodo|
6 
7 The Astropy Project (http://astropy.org/) is a community effort to develop a
8 single core package for Astronomy in Python and foster interoperability between
9 Python astronomy packages. This repository contains the core package which is
10 intended to contain much of the core functionality and some common tools needed
11 for performing astronomy and astrophysics with Python.
12 
13 Releases are `registered on PyPI <https://pypi.org/project/astropy>`_,
14 and development is occurring at the
15 `project's GitHub page <http://github.com/astropy/astropy>`_.
16 
17 For installation instructions, see the `online documentation <https://docs.astropy.org/>`_
18 or  `docs/install.rst <docs/install.rst>`_ in this source distribution.
19 
20 Contributing Code, Documentation, or Feedback
21 ---------------------------------------------
22 
23 The Astropy Project is made both by and for its users, so we welcome and
24 encourage contributions of many kinds. Our goal is to keep this a positive,
25 inclusive, successful, and growing community by abiding with the
26 `Astropy Community Code of Conduct <http://www.astropy.org/about.html#codeofconduct>`_.
27 
28 More detailed information on contributing to the project or submitting feedback
29 can be found on the `contributions <http://www.astropy.org/contribute.html>`_
30 page. A `summary of contribution guidelines <CONTRIBUTING.md>`_ can also be
31 used as a quick reference when you are ready to start writing or validating
32 code for submission.
33 
34 Supporting the Project
35 ----------------------
36 
37 |NumFOCUS| |Donate|
38 
39 The Astropy Project is sponsored by NumFOCUS, a 501(c)(3) nonprofit in the
40 United States. You can donate to the project by using the link above, and this
41 donation will support our mission to promote sustainable, high-level code base
42 for the astronomy community, open code development, educational materials, and
43 reproducible scientific research.
44 
45 License
46 -------
47 
48 Astropy is licensed under a 3-clause BSD style license - see the
49 `LICENSE.rst <LICENSE.rst>`_ file.
50 
51 .. |Actions Status| image:: https://github.com/astropy/astropy/workflows/CI/badge.svg
52     :target: https://github.com/astropy/astropy/actions
53     :alt: Astropy's GitHub Actions CI Status
54 
55 .. |CircleCI Status| image::  https://img.shields.io/circleci/build/github/astropy/astropy/main?logo=circleci&label=CircleCI
56     :target: https://circleci.com/gh/astropy/astropy
57     :alt: Astropy's CircleCI Status
58 
59 .. |Azure Status| image:: https://dev.azure.com/astropy-project/astropy/_apis/build/status/astropy.astropy?repoName=astropy%2Fastropy&branchName=main
60     :target: https://dev.azure.com/astropy-project/astropy
61     :alt: Astropy's Azure Pipelines Status
62 
63 .. |Coverage Status| image:: https://codecov.io/gh/astropy/astropy/branch/main/graph/badge.svg
64     :target: https://codecov.io/gh/astropy/astropy
65     :alt: Astropy's Coverage Status
66 
67 .. |PyPI Status| image:: https://img.shields.io/pypi/v/astropy.svg
68     :target: https://pypi.org/project/astropy
69     :alt: Astropy's PyPI Status
70 
71 .. |Zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.4670728.svg
72    :target: https://doi.org/10.5281/zenodo.4670728
73    :alt: Zenodo DOI
74 
75 .. |Documentation Status| image:: https://img.shields.io/readthedocs/astropy/latest.svg?logo=read%20the%20docs&logoColor=white&label=Docs&version=stable
76     :target: https://docs.astropy.org/en/stable/?badge=stable
77     :alt: Documentation Status
78 
79 .. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
80     :target: http://numfocus.org
81     :alt: Powered by NumFOCUS
82 
83 .. |Donate| image:: https://img.shields.io/badge/Donate-to%20Astropy-brightgreen.svg
84     :target: https://numfocus.salsalabs.org/donate-to-astropy/index.html
85 
86 
87 If you locally cloned this repo before 7 Apr 2021
88 -------------------------------------------------
89 
90 The primary branch for this repo has been transitioned from ``master`` to
91 ``main``.  If you have a local clone of this repository and want to keep your
92 local branch in sync with this repo, you'll need to do the following in your
93 local clone from your terminal::
94 
95    git fetch --all --prune
96    # you can stop here if you don't use your local "master"/"main" branch
97    git branch -m master main
98    git branch -u origin/main main
99 
100 If you are using a GUI to manage your repos you'll have to find the equivalent
101 commands as it's different for different programs. Alternatively, you can just
102 delete your local clone and re-clone!
103 
[end of README.rst]
[start of astropy/io/fits/fitsrec.py]
1 # Licensed under a 3-clause BSD style license - see PYFITS.rst
2 
3 import copy
4 import operator
5 import warnings
6 import weakref
7 
8 from contextlib import suppress
9 from functools import reduce
10 
11 import numpy as np
12 
13 from numpy import char as chararray
14 
15 from .column import (ASCIITNULL, FITS2NUMPY, ASCII2NUMPY, ASCII2STR, ColDefs,
16                      _AsciiColDefs, _FormatX, _FormatP, _VLF, _get_index,
17                      _wrapx, _unwrapx, _makep, Delayed)
18 from .util import decode_ascii, encode_ascii, _rstrip_inplace
19 from astropy.utils import lazyproperty
20 
21 
22 class FITS_record:
23     """
24     FITS record class.
25 
26     `FITS_record` is used to access records of the `FITS_rec` object.
27     This will allow us to deal with scaled columns.  It also handles
28     conversion/scaling of columns in ASCII tables.  The `FITS_record`
29     class expects a `FITS_rec` object as input.
30     """
31 
32     def __init__(self, input, row=0, start=None, end=None, step=None,
33                  base=None, **kwargs):
34         """
35         Parameters
36         ----------
37         input : array
38             The array to wrap.
39         row : int, optional
40             The starting logical row of the array.
41         start : int, optional
42             The starting column in the row associated with this object.
43             Used for subsetting the columns of the `FITS_rec` object.
44         end : int, optional
45             The ending column in the row associated with this object.
46             Used for subsetting the columns of the `FITS_rec` object.
47         """
48 
49         self.array = input
50         self.row = row
51         if base:
52             width = len(base)
53         else:
54             width = self.array._nfields
55 
56         s = slice(start, end, step).indices(width)
57         self.start, self.end, self.step = s
58         self.base = base
59 
60     def __getitem__(self, key):
61         if isinstance(key, str):
62             indx = _get_index(self.array.names, key)
63 
64             if indx < self.start or indx > self.end - 1:
65                 raise KeyError(f"Key '{key}' does not exist.")
66         elif isinstance(key, slice):
67             return type(self)(self.array, self.row, key.start, key.stop,
68                               key.step, self)
69         else:
70             indx = self._get_index(key)
71 
72             if indx > self.array._nfields - 1:
73                 raise IndexError('Index out of bounds')
74 
75         return self.array.field(indx)[self.row]
76 
77     def __setitem__(self, key, value):
78         if isinstance(key, str):
79             indx = _get_index(self.array.names, key)
80 
81             if indx < self.start or indx > self.end - 1:
82                 raise KeyError(f"Key '{key}' does not exist.")
83         elif isinstance(key, slice):
84             for indx in range(slice.start, slice.stop, slice.step):
85                 indx = self._get_indx(indx)
86                 self.array.field(indx)[self.row] = value
87         else:
88             indx = self._get_index(key)
89             if indx > self.array._nfields - 1:
90                 raise IndexError('Index out of bounds')
91 
92         self.array.field(indx)[self.row] = value
93 
94     def __len__(self):
95         return len(range(self.start, self.end, self.step))
96 
97     def __repr__(self):
98         """
99         Display a single row.
100         """
101 
102         outlist = []
103         for idx in range(len(self)):
104             outlist.append(repr(self[idx]))
105         return f"({', '.join(outlist)})"
106 
107     def field(self, field):
108         """
109         Get the field data of the record.
110         """
111 
112         return self.__getitem__(field)
113 
114     def setfield(self, field, value):
115         """
116         Set the field data of the record.
117         """
118 
119         self.__setitem__(field, value)
120 
121     @lazyproperty
122     def _bases(self):
123         bases = [weakref.proxy(self)]
124         base = self.base
125         while base:
126             bases.append(base)
127             base = base.base
128         return bases
129 
130     def _get_index(self, indx):
131         indices = np.ogrid[:self.array._nfields]
132         for base in reversed(self._bases):
133             if base.step < 1:
134                 s = slice(base.start, None, base.step)
135             else:
136                 s = slice(base.start, base.end, base.step)
137             indices = indices[s]
138         return indices[indx]
139 
140 
141 class FITS_rec(np.recarray):
142     """
143     FITS record array class.
144 
145     `FITS_rec` is the data part of a table HDU's data part.  This is a layer
146     over the `~numpy.recarray`, so we can deal with scaled columns.
147 
148     It inherits all of the standard methods from `numpy.ndarray`.
149     """
150 
151     _record_type = FITS_record
152     _character_as_bytes = False
153 
154     def __new__(subtype, input):
155         """
156         Construct a FITS record array from a recarray.
157         """
158 
159         # input should be a record array
160         if input.dtype.subdtype is None:
161             self = np.recarray.__new__(subtype, input.shape, input.dtype,
162                                        buf=input.data)
163         else:
164             self = np.recarray.__new__(subtype, input.shape, input.dtype,
165                                        buf=input.data, strides=input.strides)
166 
167         self._init()
168         if self.dtype.fields:
169             self._nfields = len(self.dtype.fields)
170 
171         return self
172 
173     def __setstate__(self, state):
174         meta = state[-1]
175         column_state = state[-2]
176         state = state[:-2]
177 
178         super().__setstate__(state)
179 
180         self._col_weakrefs = weakref.WeakSet()
181 
182         for attr, value in zip(meta, column_state):
183             setattr(self, attr, value)
184 
185     def __reduce__(self):
186         """
187         Return a 3-tuple for pickling a FITS_rec. Use the super-class
188         functionality but then add in a tuple of FITS_rec-specific
189         values that get used in __setstate__.
190         """
191 
192         reconst_func, reconst_func_args, state = super().__reduce__()
193 
194         # Define FITS_rec-specific attrs that get added to state
195         column_state = []
196         meta = []
197 
198         for attrs in ['_converted', '_heapoffset', '_heapsize', '_nfields',
199                       '_gap', '_uint', 'parnames', '_coldefs']:
200 
201             with suppress(AttributeError):
202                 # _coldefs can be Delayed, and file objects cannot be
203                 # picked, it needs to be deepcopied first
204                 if attrs == '_coldefs':
205                     column_state.append(self._coldefs.__deepcopy__(None))
206                 else:
207                     column_state.append(getattr(self, attrs))
208                 meta.append(attrs)
209 
210         state = state + (column_state, meta)
211 
212         return reconst_func, reconst_func_args, state
213 
214     def __array_finalize__(self, obj):
215         if obj is None:
216             return
217 
218         if isinstance(obj, FITS_rec):
219             self._character_as_bytes = obj._character_as_bytes
220 
221         if isinstance(obj, FITS_rec) and obj.dtype == self.dtype:
222             self._converted = obj._converted
223             self._heapoffset = obj._heapoffset
224             self._heapsize = obj._heapsize
225             self._col_weakrefs = obj._col_weakrefs
226             self._coldefs = obj._coldefs
227             self._nfields = obj._nfields
228             self._gap = obj._gap
229             self._uint = obj._uint
230         elif self.dtype.fields is not None:
231             # This will allow regular ndarrays with fields, rather than
232             # just other FITS_rec objects
233             self._nfields = len(self.dtype.fields)
234             self._converted = {}
235 
236             self._heapoffset = getattr(obj, '_heapoffset', 0)
237             self._heapsize = getattr(obj, '_heapsize', 0)
238 
239             self._gap = getattr(obj, '_gap', 0)
240             self._uint = getattr(obj, '_uint', False)
241             self._col_weakrefs = weakref.WeakSet()
242             self._coldefs = ColDefs(self)
243 
244             # Work around chicken-egg problem.  Column.array relies on the
245             # _coldefs attribute to set up ref back to parent FITS_rec; however
246             # in the above line the self._coldefs has not been assigned yet so
247             # this fails.  This patches that up...
248             for col in self._coldefs:
249                 del col.array
250                 col._parent_fits_rec = weakref.ref(self)
251         else:
252             self._init()
253 
254     def _init(self):
255         """Initializes internal attributes specific to FITS-isms."""
256 
257         self._nfields = 0
258         self._converted = {}
259         self._heapoffset = 0
260         self._heapsize = 0
261         self._col_weakrefs = weakref.WeakSet()
262         self._coldefs = None
263         self._gap = 0
264         self._uint = False
265 
266     @classmethod
267     def from_columns(cls, columns, nrows=0, fill=False, character_as_bytes=False):
268         """
269         Given a `ColDefs` object of unknown origin, initialize a new `FITS_rec`
270         object.
271 
272         .. note::
273 
274             This was originally part of the ``new_table`` function in the table
275             module but was moved into a class method since most of its
276             functionality always had more to do with initializing a `FITS_rec`
277             object than anything else, and much of it also overlapped with
278             ``FITS_rec._scale_back``.
279 
280         Parameters
281         ----------
282         columns : sequence of `Column` or a `ColDefs`
283             The columns from which to create the table data.  If these
284             columns have data arrays attached that data may be used in
285             initializing the new table.  Otherwise the input columns
286             will be used as a template for a new table with the requested
287             number of rows.
288 
289         nrows : int
290             Number of rows in the new table.  If the input columns have data
291             associated with them, the size of the largest input column is used.
292             Otherwise the default is 0.
293 
294         fill : bool
295             If `True`, will fill all cells with zeros or blanks.  If
296             `False`, copy the data from input, undefined cells will still
297             be filled with zeros/blanks.
298         """
299 
300         if not isinstance(columns, ColDefs):
301             columns = ColDefs(columns)
302 
303         # read the delayed data
304         for column in columns:
305             arr = column.array
306             if isinstance(arr, Delayed):
307                 if arr.hdu.data is None:
308                     column.array = None
309                 else:
310                     column.array = _get_recarray_field(arr.hdu.data,
311                                                        arr.field)
312         # Reset columns._arrays (which we may want to just do away with
313         # altogether
314         del columns._arrays
315 
316         # use the largest column shape as the shape of the record
317         if nrows == 0:
318             for arr in columns._arrays:
319                 if arr is not None:
320                     dim = arr.shape[0]
321                 else:
322                     dim = 0
323                 if dim > nrows:
324                     nrows = dim
325 
326         raw_data = np.empty(columns.dtype.itemsize * nrows, dtype=np.uint8)
327         raw_data.fill(ord(columns._padding_byte))
328         data = np.recarray(nrows, dtype=columns.dtype, buf=raw_data).view(cls)
329         data._character_as_bytes = character_as_bytes
330 
331         # Previously this assignment was made from hdu.columns, but that's a
332         # bug since if a _TableBaseHDU has a FITS_rec in its .data attribute
333         # the _TableBaseHDU.columns property is actually returned from
334         # .data._coldefs, so this assignment was circular!  Don't make that
335         # mistake again.
336         # All of this is an artifact of the fragility of the FITS_rec class,
337         # and that it can't just be initialized by columns...
338         data._coldefs = columns
339 
340         # If fill is True we don't copy anything from the column arrays.  We're
341         # just using them as a template, and returning a table filled with
342         # zeros/blanks
343         if fill:
344             return data
345 
346         # Otherwise we have to fill the recarray with data from the input
347         # columns
348         for idx, column in enumerate(columns):
349             # For each column in the ColDef object, determine the number of
350             # rows in that column.  This will be either the number of rows in
351             # the ndarray associated with the column, or the number of rows
352             # given in the call to this function, which ever is smaller.  If
353             # the input FILL argument is true, the number of rows is set to
354             # zero so that no data is copied from the original input data.
355             arr = column.array
356 
357             if arr is None:
358                 array_size = 0
359             else:
360                 array_size = len(arr)
361 
362             n = min(array_size, nrows)
363 
364             # TODO: At least *some* of this logic is mostly redundant with the
365             # _convert_foo methods in this class; see if we can eliminate some
366             # of that duplication.
367 
368             if not n:
369                 # The input column had an empty array, so just use the fill
370                 # value
371                 continue
372 
373             field = _get_recarray_field(data, idx)
374             name = column.name
375             fitsformat = column.format
376             recformat = fitsformat.recformat
377 
378             outarr = field[:n]
379             inarr = arr[:n]
380 
381             if isinstance(recformat, _FormatX):
382                 # Data is a bit array
383                 if inarr.shape[-1] == recformat.repeat:
384                     _wrapx(inarr, outarr, recformat.repeat)
385                     continue
386             elif isinstance(recformat, _FormatP):
387                 data._cache_field(name, _makep(inarr, field, recformat,
388                                                nrows=nrows))
389                 continue
390             # TODO: Find a better way of determining that the column is meant
391             # to be FITS L formatted
392             elif recformat[-2:] == FITS2NUMPY['L'] and inarr.dtype == bool:
393                 # column is boolean
394                 # The raw data field should be filled with either 'T' or 'F'
395                 # (not 0).  Use 'F' as a default
396                 field[:] = ord('F')
397                 # Also save the original boolean array in data._converted so
398                 # that it doesn't have to be re-converted
399                 converted = np.zeros(field.shape, dtype=bool)
400                 converted[:n] = inarr
401                 data._cache_field(name, converted)
402                 # TODO: Maybe this step isn't necessary at all if _scale_back
403                 # will handle it?
404                 inarr = np.where(inarr == np.False_, ord('F'), ord('T'))
405             elif (columns[idx]._physical_values and
406                     columns[idx]._pseudo_unsigned_ints):
407                 # Temporary hack...
408                 bzero = column.bzero
409                 converted = np.zeros(field.shape, dtype=inarr.dtype)
410                 converted[:n] = inarr
411                 data._cache_field(name, converted)
412                 if n < nrows:
413                     # Pre-scale rows below the input data
414                     field[n:] = -bzero
415 
416                 inarr = inarr - bzero
417             elif isinstance(columns, _AsciiColDefs):
418                 # Regardless whether the format is character or numeric, if the
419                 # input array contains characters then it's already in the raw
420                 # format for ASCII tables
421                 if fitsformat._pseudo_logical:
422                     # Hack to support converting from 8-bit T/F characters
423                     # Normally the column array is a chararray of 1 character
424                     # strings, but we need to view it as a normal ndarray of
425                     # 8-bit ints to fill it with ASCII codes for 'T' and 'F'
426                     outarr = field.view(np.uint8, np.ndarray)[:n]
427                 elif arr.dtype.kind not in ('S', 'U'):
428                     # Set up views of numeric columns with the appropriate
429                     # numeric dtype
430                     # Fill with the appropriate blanks for the column format
431                     data._cache_field(name, np.zeros(nrows, dtype=arr.dtype))
432                     outarr = data._converted[name][:n]
433 
434                 outarr[:] = inarr
435                 continue
436 
437             if inarr.shape != outarr.shape:
438                 if (inarr.dtype.kind == outarr.dtype.kind and
439                         inarr.dtype.kind in ('U', 'S') and
440                         inarr.dtype != outarr.dtype):
441 
442                     inarr_rowsize = inarr[0].size
443                     inarr = inarr.flatten().view(outarr.dtype)
444 
445                 # This is a special case to handle input arrays with
446                 # non-trivial TDIMn.
447                 # By design each row of the outarray is 1-D, while each row of
448                 # the input array may be n-D
449                 if outarr.ndim > 1:
450                     # The normal case where the first dimension is the rows
451                     inarr_rowsize = inarr[0].size
452                     inarr = inarr.reshape(n, inarr_rowsize)
453                     outarr[:, :inarr_rowsize] = inarr
454                 else:
455                     # Special case for strings where the out array only has one
456                     # dimension (the second dimension is rolled up into the
457                     # strings
458                     outarr[:n] = inarr.ravel()
459             else:
460                 outarr[:] = inarr
461 
462         # Now replace the original column array references with the new
463         # fields
464         # This is required to prevent the issue reported in
465         # https://github.com/spacetelescope/PyFITS/issues/99
466         for idx in range(len(columns)):
467             columns._arrays[idx] = data.field(idx)
468 
469         return data
470 
471     def __repr__(self):
472         # Force use of the normal ndarray repr (rather than the new
473         # one added for recarray in Numpy 1.10) for backwards compat
474         return np.ndarray.__repr__(self)
475 
476     def __getattribute__(self, attr):
477         # First, see if ndarray has this attr, and return it if so. Note that
478         # this means a field with the same name as an ndarray attr cannot be
479         # accessed by attribute, this is Numpy's default behavior.
480         # We avoid using np.recarray.__getattribute__ here because after doing
481         # this check it would access the columns without doing the conversions
482         # that we need (with .field, see below).
483         try:
484             return object.__getattribute__(self, attr)
485         except AttributeError:
486             pass
487 
488         # attr might still be a fieldname.  If we have column definitions,
489         # we should access this via .field, as the data may have to be scaled.
490         if self._coldefs is not None and attr in self.columns.names:
491             return self.field(attr)
492 
493         # If not, just let the usual np.recarray override deal with it.
494         return super().__getattribute__(attr)
495 
496     def __getitem__(self, key):
497         if self._coldefs is None:
498             return super().__getitem__(key)
499 
500         if isinstance(key, str):
501             return self.field(key)
502 
503         # Have to view as a recarray then back as a FITS_rec, otherwise the
504         # circular reference fix/hack in FITS_rec.field() won't preserve
505         # the slice.
506         out = self.view(np.recarray)[key]
507         if type(out) is not np.recarray:
508             # Oops, we got a single element rather than a view. In that case,
509             # return a Record, which has no __getstate__ and is more efficient.
510             return self._record_type(self, key)
511 
512         # We got a view; change it back to our class, and add stuff
513         out = out.view(type(self))
514         out._uint = self._uint
515         out._coldefs = ColDefs(self._coldefs)
516         arrays = []
517         out._converted = {}
518         for idx, name in enumerate(self._coldefs.names):
519             #
520             # Store the new arrays for the _coldefs object
521             #
522             arrays.append(self._coldefs._arrays[idx][key])
523 
524             # Ensure that the sliced FITS_rec will view the same scaled
525             # columns as the original; this is one of the few cases where
526             # it is not necessary to use _cache_field()
527             if name in self._converted:
528                 dummy = self._converted[name]
529                 field = np.ndarray.__getitem__(dummy, key)
530                 out._converted[name] = field
531 
532         out._coldefs._arrays = arrays
533         return out
534 
535     def __setitem__(self, key, value):
536         if self._coldefs is None:
537             return super().__setitem__(key, value)
538 
539         if isinstance(key, str):
540             self[key][:] = value
541             return
542 
543         if isinstance(key, slice):
544             end = min(len(self), key.stop or len(self))
545             end = max(0, end)
546             start = max(0, key.start or 0)
547             end = min(end, start + len(value))
548 
549             for idx in range(start, end):
550                 self.__setitem__(idx, value[idx - start])
551             return
552 
553         if isinstance(value, FITS_record):
554             for idx in range(self._nfields):
555                 self.field(self.names[idx])[key] = value.field(self.names[idx])
556         elif isinstance(value, (tuple, list, np.void)):
557             if self._nfields == len(value):
558                 for idx in range(self._nfields):
559                     self.field(idx)[key] = value[idx]
560             else:
561                 raise ValueError('Input tuple or list required to have {} '
562                                  'elements.'.format(self._nfields))
563         else:
564             raise TypeError('Assignment requires a FITS_record, tuple, or '
565                             'list as input.')
566 
567     def _ipython_key_completions_(self):
568         return self.names
569 
570     def copy(self, order='C'):
571         """
572         The Numpy documentation lies; `numpy.ndarray.copy` is not equivalent to
573         `numpy.copy`.  Differences include that it re-views the copied array as
574         self's ndarray subclass, as though it were taking a slice; this means
575         ``__array_finalize__`` is called and the copy shares all the array
576         attributes (including ``._converted``!).  So we need to make a deep
577         copy of all those attributes so that the two arrays truly do not share
578         any data.
579         """
580 
581         new = super().copy(order=order)
582 
583         new.__dict__ = copy.deepcopy(self.__dict__)
584         return new
585 
586     @property
587     def columns(self):
588         """A user-visible accessor for the coldefs."""
589 
590         return self._coldefs
591 
592     @property
593     def _coldefs(self):
594         # This used to be a normal internal attribute, but it was changed to a
595         # property as a quick and transparent way to work around the reference
596         # leak bug fixed in https://github.com/astropy/astropy/pull/4539
597         #
598         # See the long comment in the Column.array property for more details
599         # on this.  But in short, FITS_rec now has a ._col_weakrefs attribute
600         # which is a WeakSet of weakrefs to each Column in _coldefs.
601         #
602         # So whenever ._coldefs is set we also add each Column in the ColDefs
603         # to the weakrefs set.  This is an easy way to find out if a Column has
604         # any references to it external to the FITS_rec (i.e. a user assigned a
605         # column to a variable).  If the column is still in _col_weakrefs then
606         # there are other references to it external to this FITS_rec.  We use
607         # that information in __del__ to save off copies of the array data
608         # for those columns to their Column.array property before our memory
609         # is freed.
610         return self.__dict__.get('_coldefs')
611 
612     @_coldefs.setter
613     def _coldefs(self, cols):
614         self.__dict__['_coldefs'] = cols
615         if isinstance(cols, ColDefs):
616             for col in cols.columns:
617                 self._col_weakrefs.add(col)
618 
619     @_coldefs.deleter
620     def _coldefs(self):
621         try:
622             del self.__dict__['_coldefs']
623         except KeyError as exc:
624             raise AttributeError(exc.args[0])
625 
626     def __del__(self):
627         try:
628             del self._coldefs
629             if self.dtype.fields is not None:
630                 for col in self._col_weakrefs:
631 
632                     if col.array is not None:
633                         col.array = col.array.copy()
634 
635         # See issues #4690 and #4912
636         except (AttributeError, TypeError):  # pragma: no cover
637             pass
638 
639     @property
640     def names(self):
641         """List of column names."""
642 
643         if self.dtype.fields:
644             return list(self.dtype.names)
645         elif getattr(self, '_coldefs', None) is not None:
646             return self._coldefs.names
647         else:
648             return None
649 
650     @property
651     def formats(self):
652         """List of column FITS formats."""
653 
654         if getattr(self, '_coldefs', None) is not None:
655             return self._coldefs.formats
656 
657         return None
658 
659     @property
660     def _raw_itemsize(self):
661         """
662         Returns the size of row items that would be written to the raw FITS
663         file, taking into account the possibility of unicode columns being
664         compactified.
665 
666         Currently for internal use only.
667         """
668 
669         if _has_unicode_fields(self):
670             total_itemsize = 0
671             for field in self.dtype.fields.values():
672                 itemsize = field[0].itemsize
673                 if field[0].kind == 'U':
674                     itemsize = itemsize // 4
675                 total_itemsize += itemsize
676             return total_itemsize
677         else:
678             # Just return the normal itemsize
679             return self.itemsize
680 
681     def field(self, key):
682         """
683         A view of a `Column`'s data as an array.
684         """
685 
686         # NOTE: The *column* index may not be the same as the field index in
687         # the recarray, if the column is a phantom column
688         column = self.columns[key]
689         name = column.name
690         format = column.format
691 
692         if format.dtype.itemsize == 0:
693             warnings.warn(
694                 'Field {!r} has a repeat count of 0 in its format code, '
695                 'indicating an empty field.'.format(key))
696             return np.array([], dtype=format.dtype)
697 
698         # If field's base is a FITS_rec, we can run into trouble because it
699         # contains a reference to the ._coldefs object of the original data;
700         # this can lead to a circular reference; see ticket #49
701         base = self
702         while (isinstance(base, FITS_rec) and
703                 isinstance(base.base, np.recarray)):
704             base = base.base
705         # base could still be a FITS_rec in some cases, so take care to
706         # use rec.recarray.field to avoid a potential infinite
707         # recursion
708         field = _get_recarray_field(base, name)
709 
710         if name not in self._converted:
711             recformat = format.recformat
712             # TODO: If we're now passing the column to these subroutines, do we
713             # really need to pass them the recformat?
714             if isinstance(recformat, _FormatP):
715                 # for P format
716                 converted = self._convert_p(column, field, recformat)
717             else:
718                 # Handle all other column data types which are fixed-width
719                 # fields
720                 converted = self._convert_other(column, field, recformat)
721 
722             # Note: Never assign values directly into the self._converted dict;
723             # always go through self._cache_field; this way self._converted is
724             # only used to store arrays that are not already direct views of
725             # our own data.
726             self._cache_field(name, converted)
727             return converted
728 
729         return self._converted[name]
730 
731     def _cache_field(self, name, field):
732         """
733         Do not store fields in _converted if one of its bases is self,
734         or if it has a common base with self.
735 
736         This results in a reference cycle that cannot be broken since
737         ndarrays do not participate in cyclic garbage collection.
738         """
739 
740         base = field
741         while True:
742             self_base = self
743             while True:
744                 if self_base is base:
745                     return
746 
747                 if getattr(self_base, 'base', None) is not None:
748                     self_base = self_base.base
749                 else:
750                     break
751 
752             if getattr(base, 'base', None) is not None:
753                 base = base.base
754             else:
755                 break
756 
757         self._converted[name] = field
758 
759     def _update_column_attribute_changed(self, column, idx, attr, old_value,
760                                          new_value):
761         """
762         Update how the data is formatted depending on changes to column
763         attributes initiated by the user through the `Column` interface.
764 
765         Dispatches column attribute change notifications to individual methods
766         for each attribute ``_update_column_<attr>``
767         """
768 
769         method_name = f'_update_column_{attr}'
770         if hasattr(self, method_name):
771             # Right now this is so we can be lazy and not implement updaters
772             # for every attribute yet--some we may not need at all, TBD
773             getattr(self, method_name)(column, idx, old_value, new_value)
774 
775     def _update_column_name(self, column, idx, old_name, name):
776         """Update the dtype field names when a column name is changed."""
777 
778         dtype = self.dtype
779         # Updating the names on the dtype should suffice
780         dtype.names = dtype.names[:idx] + (name,) + dtype.names[idx + 1:]
781 
782     def _convert_x(self, field, recformat):
783         """Convert a raw table column to a bit array as specified by the
784         FITS X format.
785         """
786 
787         dummy = np.zeros(self.shape + (recformat.repeat,), dtype=np.bool_)
788         _unwrapx(field, dummy, recformat.repeat)
789         return dummy
790 
791     def _convert_p(self, column, field, recformat):
792         """Convert a raw table column of FITS P or Q format descriptors
793         to a VLA column with the array data returned from the heap.
794         """
795 
796         dummy = _VLF([None] * len(self), dtype=recformat.dtype)
797         raw_data = self._get_raw_data()
798 
799         if raw_data is None:
800             raise OSError(
801                 "Could not find heap data for the {!r} variable-length "
802                 "array column.".format(column.name))
803 
804         for idx in range(len(self)):
805             offset = field[idx, 1] + self._heapoffset
806             count = field[idx, 0]
807 
808             if recformat.dtype == 'a':
809                 dt = np.dtype(recformat.dtype + str(1))
810                 arr_len = count * dt.itemsize
811                 da = raw_data[offset:offset + arr_len].view(dt)
812                 da = np.char.array(da.view(dtype=dt), itemsize=count)
813                 dummy[idx] = decode_ascii(da)
814             else:
815                 dt = np.dtype(recformat.dtype)
816                 arr_len = count * dt.itemsize
817                 dummy[idx] = raw_data[offset:offset + arr_len].view(dt)
818                 dummy[idx].dtype = dummy[idx].dtype.newbyteorder('>')
819                 # Each array in the field may now require additional
820                 # scaling depending on the other scaling parameters
821                 # TODO: The same scaling parameters apply to every
822                 # array in the column so this is currently very slow; we
823                 # really only need to check once whether any scaling will
824                 # be necessary and skip this step if not
825                 # TODO: Test that this works for X format; I don't think
826                 # that it does--the recformat variable only applies to the P
827                 # format not the X format
828                 dummy[idx] = self._convert_other(column, dummy[idx],
829                                                  recformat)
830 
831         return dummy
832 
833     def _convert_ascii(self, column, field):
834         """
835         Special handling for ASCII table columns to convert columns containing
836         numeric types to actual numeric arrays from the string representation.
837         """
838 
839         format = column.format
840         recformat = getattr(format, 'recformat', ASCII2NUMPY[format[0]])
841         # if the string = TNULL, return ASCIITNULL
842         nullval = str(column.null).strip().encode('ascii')
843         if len(nullval) > format.width:
844             nullval = nullval[:format.width]
845 
846         # Before using .replace make sure that any trailing bytes in each
847         # column are filled with spaces, and *not*, say, nulls; this causes
848         # functions like replace to potentially leave gibberish bytes in the
849         # array buffer.
850         dummy = np.char.ljust(field, format.width)
851         dummy = np.char.replace(dummy, encode_ascii('D'), encode_ascii('E'))
852         null_fill = encode_ascii(str(ASCIITNULL).rjust(format.width))
853 
854         # Convert all fields equal to the TNULL value (nullval) to empty fields.
855         # TODO: These fields really should be converted to NaN or something else undefined.
856         # Currently they are converted to empty fields, which are then set to zero.
857         dummy = np.where(np.char.strip(dummy) == nullval, null_fill, dummy)
858 
859         # always replace empty fields, see https://github.com/astropy/astropy/pull/5394
860         if nullval != b'':
861             dummy = np.where(np.char.strip(dummy) == b'', null_fill, dummy)
862 
863         try:
864             dummy = np.array(dummy, dtype=recformat)
865         except ValueError as exc:
866             indx = self.names.index(column.name)
867             raise ValueError(
868                 '{}; the header may be missing the necessary TNULL{} '
869                 'keyword or the table contains invalid data'.format(
870                     exc, indx + 1))
871 
872         return dummy
873 
874     def _convert_other(self, column, field, recformat):
875         """Perform conversions on any other fixed-width column data types.
876 
877         This may not perform any conversion at all if it's not necessary, in
878         which case the original column array is returned.
879         """
880 
881         if isinstance(recformat, _FormatX):
882             # special handling for the X format
883             return self._convert_x(field, recformat)
884 
885         (_str, _bool, _number, _scale, _zero, bscale, bzero, dim) = \
886             self._get_scale_factors(column)
887 
888         indx = self.names.index(column.name)
889 
890         # ASCII table, convert strings to numbers
891         # TODO:
892         # For now, check that these are ASCII columns by checking the coldefs
893         # type; in the future all columns (for binary tables, ASCII tables, or
894         # otherwise) should "know" what type they are already and how to handle
895         # converting their data from FITS format to native format and vice
896         # versa...
897         if not _str and isinstance(self._coldefs, _AsciiColDefs):
898             field = self._convert_ascii(column, field)
899 
900         # Test that the dimensions given in dim are sensible; otherwise
901         # display a warning and ignore them
902         if dim:
903             # See if the dimensions already match, if not, make sure the
904             # number items will fit in the specified dimensions
905             if field.ndim > 1:
906                 actual_shape = field.shape[1:]
907                 if _str:
908                     actual_shape = actual_shape + (field.itemsize,)
909             else:
910                 actual_shape = field.shape[0]
911 
912             if dim == actual_shape:
913                 # The array already has the correct dimensions, so we
914                 # ignore dim and don't convert
915                 dim = None
916             else:
917                 nitems = reduce(operator.mul, dim)
918                 if _str:
919                     actual_nitems = field.itemsize
920                 elif len(field.shape) == 1:  # No repeat count in TFORMn, equivalent to 1
921                     actual_nitems = 1
922                 else:
923                     actual_nitems = field.shape[1]
924                 if nitems > actual_nitems:
925                     warnings.warn(
926                         'TDIM{} value {:d} does not fit with the size of '
927                         'the array items ({:d}).  TDIM{:d} will be ignored.'
928                         .format(indx + 1, self._coldefs[indx].dims,
929                                 actual_nitems, indx + 1))
930                     dim = None
931 
932         # further conversion for both ASCII and binary tables
933         # For now we've made columns responsible for *knowing* whether their
934         # data has been scaled, but we make the FITS_rec class responsible for
935         # actually doing the scaling
936         # TODO: This also needs to be fixed in the effort to make Columns
937         # responsible for scaling their arrays to/from FITS native values
938         if not column.ascii and column.format.p_format:
939             format_code = column.format.p_format
940         else:
941             # TODO: Rather than having this if/else it might be nice if the
942             # ColumnFormat class had an attribute guaranteed to give the format
943             # of actual values in a column regardless of whether the true
944             # format is something like P or Q
945             format_code = column.format.format
946 
947         if (_number and (_scale or _zero) and not column._physical_values):
948             # This is to handle pseudo unsigned ints in table columns
949             # TODO: For now this only really works correctly for binary tables
950             # Should it work for ASCII tables as well?
951             if self._uint:
952                 if bzero == 2**15 and format_code == 'I':
953                     field = np.array(field, dtype=np.uint16)
954                 elif bzero == 2**31 and format_code == 'J':
955                     field = np.array(field, dtype=np.uint32)
956                 elif bzero == 2**63 and format_code == 'K':
957                     field = np.array(field, dtype=np.uint64)
958                     bzero64 = np.uint64(2 ** 63)
959                 else:
960                     field = np.array(field, dtype=np.float64)
961             else:
962                 field = np.array(field, dtype=np.float64)
963 
964             if _scale:
965                 np.multiply(field, bscale, field)
966             if _zero:
967                 if self._uint and format_code == 'K':
968                     # There is a chance of overflow, so be careful
969                     test_overflow = field.copy()
970                     try:
971                         test_overflow += bzero64
972                     except OverflowError:
973                         warnings.warn(
974                             "Overflow detected while applying TZERO{:d}. "
975                             "Returning unscaled data.".format(indx + 1))
976                     else:
977                         field = test_overflow
978                 else:
979                     field += bzero
980 
981             # mark the column as scaled
982             column._physical_values = True
983 
984         elif _bool and field.dtype != bool:
985             field = np.equal(field, ord('T'))
986         elif _str:
987             if not self._character_as_bytes:
988                 with suppress(UnicodeDecodeError):
989                     field = decode_ascii(field)
990 
991         if dim:
992             # Apply the new field item dimensions
993             nitems = reduce(operator.mul, dim)
994             if field.ndim > 1:
995                 field = field[:, :nitems]
996             if _str:
997                 fmt = field.dtype.char
998                 dtype = (f'|{fmt}{dim[-1]}', dim[:-1])
999                 field.dtype = dtype
1000             else:
1001                 field.shape = (field.shape[0],) + dim
1002 
1003         return field
1004 
1005     def _get_heap_data(self):
1006         """
1007         Returns a pointer into the table's raw data to its heap (if present).
1008 
1009         This is returned as a numpy byte array.
1010         """
1011 
1012         if self._heapsize:
1013             raw_data = self._get_raw_data().view(np.ubyte)
1014             heap_end = self._heapoffset + self._heapsize
1015             return raw_data[self._heapoffset:heap_end]
1016         else:
1017             return np.array([], dtype=np.ubyte)
1018 
1019     def _get_raw_data(self):
1020         """
1021         Returns the base array of self that "raw data array" that is the
1022         array in the format that it was first read from a file before it was
1023         sliced or viewed as a different type in any way.
1024 
1025         This is determined by walking through the bases until finding one that
1026         has at least the same number of bytes as self, plus the heapsize.  This
1027         may be the immediate .base but is not always.  This is used primarily
1028         for variable-length array support which needs to be able to find the
1029         heap (the raw data *may* be larger than nbytes + heapsize if it
1030         contains a gap or padding).
1031 
1032         May return ``None`` if no array resembling the "raw data" according to
1033         the stated criteria can be found.
1034         """
1035 
1036         raw_data_bytes = self.nbytes + self._heapsize
1037         base = self
1038         while hasattr(base, 'base') and base.base is not None:
1039             base = base.base
1040             if hasattr(base, 'nbytes') and base.nbytes >= raw_data_bytes:
1041                 return base
1042 
1043     def _get_scale_factors(self, column):
1044         """Get all the scaling flags and factors for one column."""
1045 
1046         # TODO: Maybe this should be a method/property on Column?  Or maybe
1047         # it's not really needed at all...
1048         _str = column.format.format == 'A'
1049         _bool = column.format.format == 'L'
1050 
1051         _number = not (_bool or _str)
1052         bscale = column.bscale
1053         bzero = column.bzero
1054 
1055         _scale = bscale not in ('', None, 1)
1056         _zero = bzero not in ('', None, 0)
1057 
1058         # ensure bscale/bzero are numbers
1059         if not _scale:
1060             bscale = 1
1061         if not _zero:
1062             bzero = 0
1063 
1064         # column._dims gives a tuple, rather than column.dim which returns the
1065         # original string format code from the FITS header...
1066         dim = column._dims
1067 
1068         return (_str, _bool, _number, _scale, _zero, bscale, bzero, dim)
1069 
1070     def _scale_back(self, update_heap_pointers=True):
1071         """
1072         Update the parent array, using the (latest) scaled array.
1073 
1074         If ``update_heap_pointers`` is `False`, this will leave all the heap
1075         pointers in P/Q columns as they are verbatim--it only makes sense to do
1076         this if there is already data on the heap and it can be guaranteed that
1077         that data has not been modified, and there is not new data to add to
1078         the heap.  Currently this is only used as an optimization for
1079         CompImageHDU that does its own handling of the heap.
1080         """
1081 
1082         # Running total for the new heap size
1083         heapsize = 0
1084 
1085         for indx, name in enumerate(self.dtype.names):
1086             column = self._coldefs[indx]
1087             recformat = column.format.recformat
1088             raw_field = _get_recarray_field(self, indx)
1089 
1090             # add the location offset of the heap area for each
1091             # variable length column
1092             if isinstance(recformat, _FormatP):
1093                 # Irritatingly, this can return a different dtype than just
1094                 # doing np.dtype(recformat.dtype); but this returns the results
1095                 # that we want.  For example if recformat.dtype is 'a' we want
1096                 # an array of characters.
1097                 dtype = np.array([], dtype=recformat.dtype).dtype
1098 
1099                 if update_heap_pointers and name in self._converted:
1100                     # The VLA has potentially been updated, so we need to
1101                     # update the array descriptors
1102                     raw_field[:] = 0  # reset
1103                     npts = [len(arr) for arr in self._converted[name]]
1104 
1105                     raw_field[:len(npts), 0] = npts
1106                     raw_field[1:, 1] = (np.add.accumulate(raw_field[:-1, 0]) *
1107                                         dtype.itemsize)
1108                     raw_field[:, 1][:] += heapsize
1109 
1110                 heapsize += raw_field[:, 0].sum() * dtype.itemsize
1111                 # Even if this VLA has not been read or updated, we need to
1112                 # include the size of its constituent arrays in the heap size
1113                 # total
1114 
1115             if isinstance(recformat, _FormatX) and name in self._converted:
1116                 _wrapx(self._converted[name], raw_field, recformat.repeat)
1117                 continue
1118 
1119             _str, _bool, _number, _scale, _zero, bscale, bzero, _ = \
1120                 self._get_scale_factors(column)
1121 
1122             field = self._converted.get(name, raw_field)
1123 
1124             # conversion for both ASCII and binary tables
1125             if _number or _str:
1126                 if _number and (_scale or _zero) and column._physical_values:
1127                     dummy = field.copy()
1128                     if _zero:
1129                         dummy -= bzero
1130                     if _scale:
1131                         dummy /= bscale
1132                     # This will set the raw values in the recarray back to
1133                     # their non-physical storage values, so the column should
1134                     # be mark is not scaled
1135                     column._physical_values = False
1136                 elif _str or isinstance(self._coldefs, _AsciiColDefs):
1137                     dummy = field
1138                 else:
1139                     continue
1140 
1141                 # ASCII table, convert numbers to strings
1142                 if isinstance(self._coldefs, _AsciiColDefs):
1143                     self._scale_back_ascii(indx, dummy, raw_field)
1144                 # binary table string column
1145                 elif isinstance(raw_field, chararray.chararray):
1146                     self._scale_back_strings(indx, dummy, raw_field)
1147                 # all other binary table columns
1148                 else:
1149                     if len(raw_field) and isinstance(raw_field[0],
1150                                                      np.integer):
1151                         dummy = np.around(dummy)
1152 
1153                     if raw_field.shape == dummy.shape:
1154                         raw_field[:] = dummy
1155                     else:
1156                         # Reshaping the data is necessary in cases where the
1157                         # TDIMn keyword was used to shape a column's entries
1158                         # into arrays
1159                         raw_field[:] = dummy.ravel().view(raw_field.dtype)
1160 
1161                 del dummy
1162 
1163             # ASCII table does not have Boolean type
1164             elif _bool and name in self._converted:
1165                 choices = (np.array([ord('F')], dtype=np.int8)[0],
1166                            np.array([ord('T')], dtype=np.int8)[0])
1167                 raw_field[:] = np.choose(field, choices)
1168 
1169         # Store the updated heapsize
1170         self._heapsize = heapsize
1171 
1172     def _scale_back_strings(self, col_idx, input_field, output_field):
1173         # There are a few possibilities this has to be able to handle properly
1174         # The input_field, which comes from the _converted column is of dtype
1175         # 'Un' so that elements read out of the array are normal str
1176         # objects (i.e. unicode strings)
1177         #
1178         # At the other end the *output_field* may also be of type 'S' or of
1179         # type 'U'.  It will *usually* be of type 'S' because when reading
1180         # an existing FITS table the raw data is just ASCII strings, and
1181         # represented in Numpy as an S array.  However, when a user creates
1182         # a new table from scratch, they *might* pass in a column containing
1183         # unicode strings (dtype 'U').  Therefore the output_field of the
1184         # raw array is actually a unicode array.  But we still want to make
1185         # sure the data is encodable as ASCII.  Later when we write out the
1186         # array we use, in the dtype 'U' case, a different write routine
1187         # that writes row by row and encodes any 'U' columns to ASCII.
1188 
1189         # If the output_field is non-ASCII we will worry about ASCII encoding
1190         # later when writing; otherwise we can do it right here
1191         if input_field.dtype.kind == 'U' and output_field.dtype.kind == 'S':
1192             try:
1193                 _ascii_encode(input_field, out=output_field)
1194             except _UnicodeArrayEncodeError as exc:
1195                 raise ValueError(
1196                     "Could not save column '{}': Contains characters that "
1197                     "cannot be encoded as ASCII as required by FITS, starting "
1198                     "at the index {!r} of the column, and the index {} of "
1199                     "the string at that location.".format(
1200                         self._coldefs[col_idx].name,
1201                         exc.index[0] if len(exc.index) == 1 else exc.index,
1202                         exc.start))
1203         else:
1204             # Otherwise go ahead and do a direct copy into--if both are type
1205             # 'U' we'll handle encoding later
1206             input_field = input_field.flatten().view(output_field.dtype)
1207             output_field.flat[:] = input_field
1208 
1209         # Ensure that blanks at the end of each string are
1210         # converted to nulls instead of spaces, see Trac #15
1211         # and #111
1212         _rstrip_inplace(output_field)
1213 
1214     def _scale_back_ascii(self, col_idx, input_field, output_field):
1215         """
1216         Convert internal array values back to ASCII table representation.
1217 
1218         The ``input_field`` is the internal representation of the values, and
1219         the ``output_field`` is the character array representing the ASCII
1220         output that will be written.
1221         """
1222 
1223         starts = self._coldefs.starts[:]
1224         spans = self._coldefs.spans
1225         format = self._coldefs[col_idx].format
1226 
1227         # The the index of the "end" column of the record, beyond
1228         # which we can't write
1229         end = super().field(-1).itemsize
1230         starts.append(end + starts[-1])
1231 
1232         if col_idx > 0:
1233             lead = starts[col_idx] - starts[col_idx - 1] - spans[col_idx - 1]
1234         else:
1235             lead = 0
1236 
1237         if lead < 0:
1238             warnings.warn('Column {!r} starting point overlaps the previous '
1239                           'column.'.format(col_idx + 1))
1240 
1241         trail = starts[col_idx + 1] - starts[col_idx] - spans[col_idx]
1242 
1243         if trail < 0:
1244             warnings.warn('Column {!r} ending point overlaps the next '
1245                           'column.'.format(col_idx + 1))
1246 
1247         # TODO: It would be nice if these string column formatting
1248         # details were left to a specialized class, as is the case
1249         # with FormatX and FormatP
1250         if 'A' in format:
1251             _pc = '{:'
1252         else:
1253             _pc = '{:>'
1254 
1255         fmt = ''.join([_pc, format[1:], ASCII2STR[format[0]], '}',
1256                        (' ' * trail)])
1257 
1258         # Even if the format precision is 0, we should output a decimal point
1259         # as long as there is space to do so--not including a decimal point in
1260         # a float value is discouraged by the FITS Standard
1261         trailing_decimal = (format.precision == 0 and
1262                             format.format in ('F', 'E', 'D'))
1263 
1264         # not using numarray.strings's num2char because the
1265         # result is not allowed to expand (as C/Python does).
1266         for jdx, value in enumerate(input_field):
1267             value = fmt.format(value)
1268             if len(value) > starts[col_idx + 1] - starts[col_idx]:
1269                 raise ValueError(
1270                     "Value {!r} does not fit into the output's itemsize of "
1271                     "{}.".format(value, spans[col_idx]))
1272 
1273             if trailing_decimal and value[0] == ' ':
1274                 # We have some extra space in the field for the trailing
1275                 # decimal point
1276                 value = value[1:] + '.'
1277 
1278             output_field[jdx] = value
1279 
1280         # Replace exponent separator in floating point numbers
1281         if 'D' in format:
1282             output_field[:] = output_field.replace(b'E', b'D')
1283 
1284     def tolist(self):
1285         # Override .tolist to take care of special case of VLF
1286 
1287         column_lists = [self[name].tolist() for name in self.columns.names]
1288 
1289         return [list(row) for row in zip(*column_lists)]
1290 
1291 
1292 def _get_recarray_field(array, key):
1293     """
1294     Compatibility function for using the recarray base class's field method.
1295     This incorporates the legacy functionality of returning string arrays as
1296     Numeric-style chararray objects.
1297     """
1298 
1299     # Numpy >= 1.10.dev recarray no longer returns chararrays for strings
1300     # This is currently needed for backwards-compatibility and for
1301     # automatic truncation of trailing whitespace
1302     field = np.recarray.field(array, key)
1303     if (field.dtype.char in ('S', 'U') and
1304             not isinstance(field, chararray.chararray)):
1305         field = field.view(chararray.chararray)
1306     return field
1307 
1308 
1309 class _UnicodeArrayEncodeError(UnicodeEncodeError):
1310     def __init__(self, encoding, object_, start, end, reason, index):
1311         super().__init__(encoding, object_, start, end, reason)
1312         self.index = index
1313 
1314 
1315 def _ascii_encode(inarray, out=None):
1316     """
1317     Takes a unicode array and fills the output string array with the ASCII
1318     encodings (if possible) of the elements of the input array.  The two arrays
1319     must be the same size (though not necessarily the same shape).
1320 
1321     This is like an inplace version of `np.char.encode` though simpler since
1322     it's only limited to ASCII, and hence the size of each character is
1323     guaranteed to be 1 byte.
1324 
1325     If any strings are non-ASCII an UnicodeArrayEncodeError is raised--this is
1326     just a `UnicodeEncodeError` with an additional attribute for the index of
1327     the item that couldn't be encoded.
1328     """
1329 
1330     out_dtype = np.dtype((f'S{inarray.dtype.itemsize // 4}',
1331                          inarray.dtype.shape))
1332     if out is not None:
1333         out = out.view(out_dtype)
1334 
1335     op_dtypes = [inarray.dtype, out_dtype]
1336     op_flags = [['readonly'], ['writeonly', 'allocate']]
1337     it = np.nditer([inarray, out], op_dtypes=op_dtypes,
1338                    op_flags=op_flags, flags=['zerosize_ok'])
1339 
1340     try:
1341         for initem, outitem in it:
1342             outitem[...] = initem.item().encode('ascii')
1343     except UnicodeEncodeError as exc:
1344         index = np.unravel_index(it.iterindex, inarray.shape)
1345         raise _UnicodeArrayEncodeError(*(exc.args + (index,)))
1346 
1347     return it.operands[1]
1348 
1349 
1350 def _has_unicode_fields(array):
1351     """
1352     Returns True if any fields in a structured array have Unicode dtype.
1353     """
1354 
1355     dtypes = (d[0] for d in array.dtype.fields.values())
1356     return any(d.kind == 'U' for d in dtypes)
1357 
[end of astropy/io/fits/fitsrec.py]
[start of astropy/nddata/ccddata.py]
1 # Licensed under a 3-clause BSD style license - see LICENSE.rst
2 """This module implements the base CCDData class."""
3 
4 import itertools
5 
6 import numpy as np
7 
8 from .compat import NDDataArray
9 from .nduncertainty import (
10     StdDevUncertainty, NDUncertainty, VarianceUncertainty, InverseVariance)
11 from astropy.io import fits, registry
12 from astropy import units as u
13 from astropy import log
14 from astropy.wcs import WCS
15 from astropy.utils.decorators import sharedmethod
16 
17 
18 __all__ = ['CCDData', 'fits_ccddata_reader', 'fits_ccddata_writer']
19 
20 _known_uncertainties = (StdDevUncertainty, VarianceUncertainty, InverseVariance)
21 _unc_name_to_cls = {cls.__name__: cls for cls in _known_uncertainties}
22 _unc_cls_to_name = {cls: cls.__name__ for cls in _known_uncertainties}
23 
24 # Global value which can turn on/off the unit requirements when creating a
25 # CCDData. Should be used with care because several functions actually break
26 # if the unit is None!
27 _config_ccd_requires_unit = True
28 
29 
30 def _arithmetic(op):
31     """Decorator factory which temporarily disables the need for a unit when
32     creating a new CCDData instance. The final result must have a unit.
33 
34     Parameters
35     ----------
36     op : function
37         The function to apply. Supported are:
38 
39         - ``np.add``
40         - ``np.subtract``
41         - ``np.multiply``
42         - ``np.true_divide``
43 
44     Notes
45     -----
46     Should only be used on CCDData ``add``, ``subtract``, ``divide`` or
47     ``multiply`` because only these methods from NDArithmeticMixin are
48     overwritten.
49     """
50     def decorator(func):
51         def inner(self, operand, operand2=None, **kwargs):
52             global _config_ccd_requires_unit
53             _config_ccd_requires_unit = False
54             result = self._prepare_then_do_arithmetic(op, operand,
55                                                       operand2, **kwargs)
56             # Wrap it again as CCDData so it checks the final unit.
57             _config_ccd_requires_unit = True
58             return result.__class__(result)
59         inner.__doc__ = f"See `astropy.nddata.NDArithmeticMixin.{func.__name__}`."
60         return sharedmethod(inner)
61     return decorator
62 
63 
64 def _uncertainty_unit_equivalent_to_parent(uncertainty_type, unit, parent_unit):
65     if uncertainty_type is StdDevUncertainty:
66         return unit == parent_unit
67     elif uncertainty_type is VarianceUncertainty:
68         return unit == (parent_unit ** 2)
69     elif uncertainty_type is InverseVariance:
70         return unit == (1 / (parent_unit ** 2))
71     raise ValueError(f"unsupported uncertainty type: {uncertainty_type}")
72 
73 
74 class CCDData(NDDataArray):
75     """A class describing basic CCD data.
76 
77     The CCDData class is based on the NDData object and includes a data array,
78     uncertainty frame, mask frame, flag frame, meta data, units, and WCS
79     information for a single CCD image.
80 
81     Parameters
82     ----------
83     data : `~astropy.nddata.CCDData`-like or array-like
84         The actual data contained in this `~astropy.nddata.CCDData` object.
85         Note that the data will always be saved by *reference*, so you should
86         make a copy of the ``data`` before passing it in if that's the desired
87         behavior.
88 
89     uncertainty : `~astropy.nddata.StdDevUncertainty`, \
90             `~astropy.nddata.VarianceUncertainty`, \
91             `~astropy.nddata.InverseVariance`, `numpy.ndarray` or \
92             None, optional
93         Uncertainties on the data. If the uncertainty is a `numpy.ndarray`, it
94         it assumed to be, and stored as, a `~astropy.nddata.StdDevUncertainty`.
95         Default is ``None``.
96 
97     mask : `numpy.ndarray` or None, optional
98         Mask for the data, given as a boolean Numpy array with a shape
99         matching that of the data. The values must be `False` where
100         the data is *valid* and `True` when it is not (like Numpy
101         masked arrays). If ``data`` is a numpy masked array, providing
102         ``mask`` here will causes the mask from the masked array to be
103         ignored.
104         Default is ``None``.
105 
106     flags : `numpy.ndarray` or `~astropy.nddata.FlagCollection` or None, \
107             optional
108         Flags giving information about each pixel. These can be specified
109         either as a Numpy array of any type with a shape matching that of the
110         data, or as a `~astropy.nddata.FlagCollection` instance which has a
111         shape matching that of the data.
112         Default is ``None``.
113 
114     wcs : `~astropy.wcs.WCS` or None, optional
115         WCS-object containing the world coordinate system for the data.
116         Default is ``None``.
117 
118     meta : dict-like object or None, optional
119         Metadata for this object. "Metadata" here means all information that
120         is included with this object but not part of any other attribute
121         of this particular object, e.g. creation date, unique identifier,
122         simulation parameters, exposure time, telescope name, etc.
123 
124     unit : `~astropy.units.Unit` or str, optional
125         The units of the data.
126         Default is ``None``.
127 
128         .. warning::
129 
130             If the unit is ``None`` or not otherwise specified it will raise a
131             ``ValueError``
132 
133     Raises
134     ------
135     ValueError
136         If the ``uncertainty`` or ``mask`` inputs cannot be broadcast (e.g.,
137         match shape) onto ``data``.
138 
139     Methods
140     -------
141     read(\\*args, \\**kwargs)
142         ``Classmethod`` to create an CCDData instance based on a ``FITS`` file.
143         This method uses :func:`fits_ccddata_reader` with the provided
144         parameters.
145     write(\\*args, \\**kwargs)
146         Writes the contents of the CCDData instance into a new ``FITS`` file.
147         This method uses :func:`fits_ccddata_writer` with the provided
148         parameters.
149 
150     Attributes
151     ----------
152     known_invalid_fits_unit_strings
153         A dictionary that maps commonly-used fits unit name strings that are
154         technically invalid to the correct valid unit type (or unit string).
155         This is primarily for variant names like "ELECTRONS/S" which are not
156         formally valid, but are unambiguous and frequently enough encountered
157         that it is convenient to map them to the correct unit.
158 
159     Notes
160     -----
161     `~astropy.nddata.CCDData` objects can be easily converted to a regular
162      Numpy array using `numpy.asarray`.
163 
164     For example::
165 
166         >>> from astropy.nddata import CCDData
167         >>> import numpy as np
168         >>> x = CCDData([1,2,3], unit='adu')
169         >>> np.asarray(x)
170         array([1, 2, 3])
171 
172     This is useful, for example, when plotting a 2D image using
173     matplotlib.
174 
175         >>> from astropy.nddata import CCDData
176         >>> from matplotlib import pyplot as plt   # doctest: +SKIP
177         >>> x = CCDData([[1,2,3], [4,5,6]], unit='adu')
178         >>> plt.imshow(x)   # doctest: +SKIP
179 
180     """
181 
182     def __init__(self, *args, **kwd):
183         if 'meta' not in kwd:
184             kwd['meta'] = kwd.pop('header', None)
185         if 'header' in kwd:
186             raise ValueError("can't have both header and meta.")
187 
188         super().__init__(*args, **kwd)
189         if self._wcs is not None:
190             llwcs = self._wcs.low_level_wcs
191             if not isinstance(llwcs, WCS):
192                 raise TypeError("the wcs must be a WCS instance.")
193             self._wcs = llwcs
194 
195         # Check if a unit is set. This can be temporarily disabled by the
196         # _CCDDataUnit contextmanager.
197         if _config_ccd_requires_unit and self.unit is None:
198             raise ValueError("a unit for CCDData must be specified.")
199 
200     def _slice_wcs(self, item):
201         """
202         Override the WCS slicing behaviour so that the wcs attribute continues
203         to be an `astropy.wcs.WCS`.
204         """
205         if self.wcs is None:
206             return None
207 
208         try:
209             return self.wcs[item]
210         except Exception as err:
211             self._handle_wcs_slicing_error(err, item)
212 
213     @property
214     def data(self):
215         return self._data
216 
217     @data.setter
218     def data(self, value):
219         self._data = value
220 
221     @property
222     def wcs(self):
223         return self._wcs
224 
225     @wcs.setter
226     def wcs(self, value):
227         if value is not None and not isinstance(value, WCS):
228             raise TypeError("the wcs must be a WCS instance.")
229         self._wcs = value
230 
231     @property
232     def unit(self):
233         return self._unit
234 
235     @unit.setter
236     def unit(self, value):
237         self._unit = u.Unit(value)
238 
239     @property
240     def header(self):
241         return self._meta
242 
243     @header.setter
244     def header(self, value):
245         self.meta = value
246 
247     @property
248     def uncertainty(self):
249         return self._uncertainty
250 
251     @uncertainty.setter
252     def uncertainty(self, value):
253         if value is not None:
254             if isinstance(value, NDUncertainty):
255                 if getattr(value, '_parent_nddata', None) is not None:
256                     value = value.__class__(value, copy=False)
257                 self._uncertainty = value
258             elif isinstance(value, np.ndarray):
259                 if value.shape != self.shape:
260                     raise ValueError("uncertainty must have same shape as "
261                                      "data.")
262                 self._uncertainty = StdDevUncertainty(value)
263                 log.info("array provided for uncertainty; assuming it is a "
264                          "StdDevUncertainty.")
265             else:
266                 raise TypeError("uncertainty must be an instance of a "
267                                 "NDUncertainty object or a numpy array.")
268             self._uncertainty.parent_nddata = self
269         else:
270             self._uncertainty = value
271 
272     def to_hdu(self, hdu_mask='MASK', hdu_uncertainty='UNCERT',
273                hdu_flags=None, wcs_relax=True,
274                key_uncertainty_type='UTYPE', as_image_hdu=False):
275         """Creates an HDUList object from a CCDData object.
276 
277         Parameters
278         ----------
279         hdu_mask, hdu_uncertainty, hdu_flags : str or None, optional
280             If it is a string append this attribute to the HDUList as
281             `~astropy.io.fits.ImageHDU` with the string as extension name.
282             Flags are not supported at this time. If ``None`` this attribute
283             is not appended.
284             Default is ``'MASK'`` for mask, ``'UNCERT'`` for uncertainty and
285             ``None`` for flags.
286 
287         wcs_relax : bool
288             Value of the ``relax`` parameter to use in converting the WCS to a
289             FITS header using `~astropy.wcs.WCS.to_header`. The common
290             ``CTYPE`` ``RA---TAN-SIP`` and ``DEC--TAN-SIP`` requires
291             ``relax=True`` for the ``-SIP`` part of the ``CTYPE`` to be
292             preserved.
293 
294         key_uncertainty_type : str, optional
295             The header key name for the class name of the uncertainty (if any)
296             that is used to store the uncertainty type in the uncertainty hdu.
297             Default is ``UTYPE``.
298 
299             .. versionadded:: 3.1
300 
301         as_image_hdu : bool
302             If this option is `True`, the first item of the returned
303             `~astropy.io.fits.HDUList` is a `~astropy.io.fits.ImageHDU`, instead
304             of the default `~astropy.io.fits.PrimaryHDU`.
305 
306         Raises
307         ------
308         ValueError
309             - If ``self.mask`` is set but not a `numpy.ndarray`.
310             - If ``self.uncertainty`` is set but not a astropy uncertainty type.
311             - If ``self.uncertainty`` is set but has another unit then
312               ``self.data``.
313 
314         NotImplementedError
315             Saving flags is not supported.
316 
317         Returns
318         -------
319         hdulist : `~astropy.io.fits.HDUList`
320         """
321         if isinstance(self.header, fits.Header):
322             # Copy here so that we can modify the HDU header by adding WCS
323             # information without changing the header of the CCDData object.
324             header = self.header.copy()
325         else:
326             # Because _insert_in_metadata_fits_safe is written as a method
327             # we need to create a dummy CCDData instance to hold the FITS
328             # header we are constructing. This probably indicates that
329             # _insert_in_metadata_fits_safe should be rewritten in a more
330             # sensible way...
331             dummy_ccd = CCDData([1], meta=fits.Header(), unit="adu")
332             for k, v in self.header.items():
333                 dummy_ccd._insert_in_metadata_fits_safe(k, v)
334             header = dummy_ccd.header
335         if self.unit is not u.dimensionless_unscaled:
336             header['bunit'] = self.unit.to_string()
337         if self.wcs:
338             # Simply extending the FITS header with the WCS can lead to
339             # duplicates of the WCS keywords; iterating over the WCS
340             # header should be safer.
341             #
342             # Turns out if I had read the io.fits.Header.extend docs more
343             # carefully, I would have realized that the keywords exist to
344             # avoid duplicates and preserve, as much as possible, the
345             # structure of the commentary cards.
346             #
347             # Note that until astropy/astropy#3967 is closed, the extend
348             # will fail if there are comment cards in the WCS header but
349             # not header.
350             wcs_header = self.wcs.to_header(relax=wcs_relax)
351             header.extend(wcs_header, useblanks=False, update=True)
352 
353         if as_image_hdu:
354             hdus = [fits.ImageHDU(self.data, header)]
355         else:
356             hdus = [fits.PrimaryHDU(self.data, header)]
357 
358         if hdu_mask and self.mask is not None:
359             # Always assuming that the mask is a np.ndarray (check that it has
360             # a 'shape').
361             if not hasattr(self.mask, 'shape'):
362                 raise ValueError('only a numpy.ndarray mask can be saved.')
363 
364             # Convert boolean mask to uint since io.fits cannot handle bool.
365             hduMask = fits.ImageHDU(self.mask.astype(np.uint8), name=hdu_mask)
366             hdus.append(hduMask)
367 
368         if hdu_uncertainty and self.uncertainty is not None:
369             # We need to save some kind of information which uncertainty was
370             # used so that loading the HDUList can infer the uncertainty type.
371             # No idea how this can be done so only allow StdDevUncertainty.
372             uncertainty_cls = self.uncertainty.__class__
373             if uncertainty_cls not in _known_uncertainties:
374                 raise ValueError('only uncertainties of type {} can be saved.'
375                                  .format(_known_uncertainties))
376             uncertainty_name = _unc_cls_to_name[uncertainty_cls]
377 
378             hdr_uncertainty = fits.Header()
379             hdr_uncertainty[key_uncertainty_type] = uncertainty_name
380 
381             # Assuming uncertainty is an StdDevUncertainty save just the array
382             # this might be problematic if the Uncertainty has a unit differing
383             # from the data so abort for different units. This is important for
384             # astropy > 1.2
385             if (hasattr(self.uncertainty, 'unit') and
386                     self.uncertainty.unit is not None):
387                 if not _uncertainty_unit_equivalent_to_parent(
388                         uncertainty_cls, self.uncertainty.unit, self.unit):
389                     raise ValueError(
390                         'saving uncertainties with a unit that is not '
391                         'equivalent to the unit from the data unit is not '
392                         'supported.')
393 
394             hduUncert = fits.ImageHDU(self.uncertainty.array, hdr_uncertainty,
395                                       name=hdu_uncertainty)
396             hdus.append(hduUncert)
397 
398         if hdu_flags and self.flags:
399             raise NotImplementedError('adding the flags to a HDU is not '
400                                       'supported at this time.')
401 
402         hdulist = fits.HDUList(hdus)
403 
404         return hdulist
405 
406     def copy(self):
407         """
408         Return a copy of the CCDData object.
409         """
410         return self.__class__(self, copy=True)
411 
412     add = _arithmetic(np.add)(NDDataArray.add)
413     subtract = _arithmetic(np.subtract)(NDDataArray.subtract)
414     multiply = _arithmetic(np.multiply)(NDDataArray.multiply)
415     divide = _arithmetic(np.true_divide)(NDDataArray.divide)
416 
417     def _insert_in_metadata_fits_safe(self, key, value):
418         """
419         Insert key/value pair into metadata in a way that FITS can serialize.
420 
421         Parameters
422         ----------
423         key : str
424             Key to be inserted in dictionary.
425 
426         value : str or None
427             Value to be inserted.
428 
429         Notes
430         -----
431         This addresses a shortcoming of the FITS standard. There are length
432         restrictions on both the ``key`` (8 characters) and ``value`` (72
433         characters) in the FITS standard. There is a convention for handling
434         long keywords and a convention for handling long values, but the
435         two conventions cannot be used at the same time.
436 
437         This addresses that case by checking the length of the ``key`` and
438         ``value`` and, if necessary, shortening the key.
439         """
440 
441         if len(key) > 8 and len(value) > 72:
442             short_name = key[:8]
443             self.meta[f'HIERARCH {key.upper()}'] = (
444                 short_name, f"Shortened name for {key}")
445             self.meta[short_name] = value
446         else:
447             self.meta[key] = value
448 
449     # A dictionary mapping "known" invalid fits unit
450     known_invalid_fits_unit_strings = {'ELECTRONS/S': u.electron/u.s,
451                                        'ELECTRONS': u.electron,
452                                        'electrons': u.electron}
453 
454 
455 # These need to be importable by the tests...
456 _KEEP_THESE_KEYWORDS_IN_HEADER = [
457     'JD-OBS',
458     'MJD-OBS',
459     'DATE-OBS'
460 ]
461 _PCs = set(['PC1_1', 'PC1_2', 'PC2_1', 'PC2_2'])
462 _CDs = set(['CD1_1', 'CD1_2', 'CD2_1', 'CD2_2'])
463 
464 
465 def _generate_wcs_and_update_header(hdr):
466     """
467     Generate a WCS object from a header and remove the WCS-specific
468     keywords from the header.
469 
470     Parameters
471     ----------
472 
473     hdr : astropy.io.fits.header or other dict-like
474 
475     Returns
476     -------
477 
478     new_header, wcs
479     """
480 
481     # Try constructing a WCS object.
482     try:
483         wcs = WCS(hdr)
484     except Exception as exc:
485         # Normally WCS only raises Warnings and doesn't fail but in rare
486         # cases (malformed header) it could fail...
487         log.info('An exception happened while extracting WCS information from '
488                  'the Header.\n{}: {}'.format(type(exc).__name__, str(exc)))
489         return hdr, None
490     # Test for success by checking to see if the wcs ctype has a non-empty
491     # value, return None for wcs if ctype is empty.
492     if not wcs.wcs.ctype[0]:
493         return (hdr, None)
494 
495     new_hdr = hdr.copy()
496     # If the keywords below are in the header they are also added to WCS.
497     # It seems like they should *not* be removed from the header, though.
498 
499     wcs_header = wcs.to_header(relax=True)
500     for k in wcs_header:
501         if k not in _KEEP_THESE_KEYWORDS_IN_HEADER:
502             new_hdr.remove(k, ignore_missing=True)
503 
504     # Check that this does not result in an inconsistent header WCS if the WCS
505     # is converted back to a header.
506 
507     if (_PCs & set(wcs_header)) and (_CDs & set(new_hdr)):
508         # The PCi_j representation is used by the astropy.wcs object,
509         # so CDi_j keywords were not removed from new_hdr. Remove them now.
510         for cd in _CDs:
511             new_hdr.remove(cd, ignore_missing=True)
512 
513     # The other case -- CD in the header produced by astropy.wcs -- should
514     # never happen based on [1], which computes the matrix in PC form.
515     # [1]: https://github.com/astropy/astropy/blob/1cf277926d3598dd672dd528504767c37531e8c9/cextern/wcslib/C/wcshdr.c#L596
516     #
517     # The test test_ccddata.test_wcs_keyword_removal_for_wcs_test_files() does
518     # check for the possibility that both PC and CD are present in the result
519     # so if the implementation of to_header changes in wcslib in the future
520     # then the tests should catch it, and then this code will need to be
521     # updated.
522 
523     # We need to check for any SIP coefficients that got left behind if the
524     # header has SIP.
525     if wcs.sip is not None:
526         keyword = '{}_{}_{}'
527         polynomials = ['A', 'B', 'AP', 'BP']
528         for poly in polynomials:
529             order = wcs.sip.__getattribute__(f'{poly.lower()}_order')
530             for i, j in itertools.product(range(order), repeat=2):
531                 new_hdr.remove(keyword.format(poly, i, j),
532                                ignore_missing=True)
533 
534     return (new_hdr, wcs)
535 
536 
537 def fits_ccddata_reader(filename, hdu=0, unit=None, hdu_uncertainty='UNCERT',
538                         hdu_mask='MASK', hdu_flags=None,
539                         key_uncertainty_type='UTYPE', **kwd):
540     """
541     Generate a CCDData object from a FITS file.
542 
543     Parameters
544     ----------
545     filename : str
546         Name of fits file.
547 
548     hdu : int, str, tuple of (str, int), optional
549         Index or other identifier of the Header Data Unit of the FITS
550         file from which CCDData should be initialized. If zero and
551         no data in the primary HDU, it will search for the first
552         extension HDU with data. The header will be added to the primary HDU.
553         Default is ``0``.
554 
555     unit : `~astropy.units.Unit`, optional
556         Units of the image data. If this argument is provided and there is a
557         unit for the image in the FITS header (the keyword ``BUNIT`` is used
558         as the unit, if present), this argument is used for the unit.
559         Default is ``None``.
560 
561     hdu_uncertainty : str or None, optional
562         FITS extension from which the uncertainty should be initialized. If the
563         extension does not exist the uncertainty of the CCDData is ``None``.
564         Default is ``'UNCERT'``.
565 
566     hdu_mask : str or None, optional
567         FITS extension from which the mask should be initialized. If the
568         extension does not exist the mask of the CCDData is ``None``.
569         Default is ``'MASK'``.
570 
571     hdu_flags : str or None, optional
572         Currently not implemented.
573         Default is ``None``.
574 
575     key_uncertainty_type : str, optional
576         The header key name where the class name of the uncertainty  is stored
577         in the hdu of the uncertainty (if any).
578         Default is ``UTYPE``.
579 
580         .. versionadded:: 3.1
581 
582     kwd :
583         Any additional keyword parameters are passed through to the FITS reader
584         in :mod:`astropy.io.fits`; see Notes for additional discussion.
585 
586     Notes
587     -----
588     FITS files that contained scaled data (e.g. unsigned integer images) will
589     be scaled and the keywords used to manage scaled data in
590     :mod:`astropy.io.fits` are disabled.
591     """
592     unsupport_open_keywords = {
593         'do_not_scale_image_data': 'Image data must be scaled.',
594         'scale_back': 'Scale information is not preserved.'
595     }
596     for key, msg in unsupport_open_keywords.items():
597         if key in kwd:
598             prefix = f'unsupported keyword: {key}.'
599             raise TypeError(' '.join([prefix, msg]))
600     with fits.open(filename, **kwd) as hdus:
601         hdr = hdus[hdu].header
602 
603         if hdu_uncertainty is not None and hdu_uncertainty in hdus:
604             unc_hdu = hdus[hdu_uncertainty]
605             stored_unc_name = unc_hdu.header.get(key_uncertainty_type, 'None')
606             # For compatibility reasons the default is standard deviation
607             # uncertainty because files could have been created before the
608             # uncertainty type was stored in the header.
609             unc_type = _unc_name_to_cls.get(stored_unc_name, StdDevUncertainty)
610             uncertainty = unc_type(unc_hdu.data)
611         else:
612             uncertainty = None
613 
614         if hdu_mask is not None and hdu_mask in hdus:
615             # Mask is saved as uint but we want it to be boolean.
616             mask = hdus[hdu_mask].data.astype(np.bool_)
617         else:
618             mask = None
619 
620         if hdu_flags is not None and hdu_flags in hdus:
621             raise NotImplementedError('loading flags is currently not '
622                                       'supported.')
623 
624         # search for the first instance with data if
625         # the primary header is empty.
626         if hdu == 0 and hdus[hdu].data is None:
627             for i in range(len(hdus)):
628                 if (hdus.info(hdu)[i][3] == 'ImageHDU' and
629                         hdus.fileinfo(i)['datSpan'] > 0):
630                     hdu = i
631                     comb_hdr = hdus[hdu].header.copy()
632                     # Add header values from the primary header that aren't
633                     # present in the extension header.
634                     comb_hdr.extend(hdr, unique=True)
635                     hdr = comb_hdr
636                     log.info(f"first HDU with data is extension {hdu}.")
637                     break
638 
639         if 'bunit' in hdr:
640             fits_unit_string = hdr['bunit']
641             # patch to handle FITS files using ADU for the unit instead of the
642             # standard version of 'adu'
643             if fits_unit_string.strip().lower() == 'adu':
644                 fits_unit_string = fits_unit_string.lower()
645         else:
646             fits_unit_string = None
647 
648         if fits_unit_string:
649             if unit is None:
650                 # Convert the BUNIT header keyword to a unit and if that's not
651                 # possible raise a meaningful error message.
652                 try:
653                     kifus = CCDData.known_invalid_fits_unit_strings
654                     if fits_unit_string in kifus:
655                         fits_unit_string = kifus[fits_unit_string]
656                     fits_unit_string = u.Unit(fits_unit_string)
657                 except ValueError:
658                     raise ValueError(
659                         'The Header value for the key BUNIT ({}) cannot be '
660                         'interpreted as valid unit. To successfully read the '
661                         'file as CCDData you can pass in a valid `unit` '
662                         'argument explicitly or change the header of the FITS '
663                         'file before reading it.'
664                         .format(fits_unit_string))
665             else:
666                 log.info("using the unit {} passed to the FITS reader instead "
667                          "of the unit {} in the FITS file."
668                          .format(unit, fits_unit_string))
669 
670         use_unit = unit or fits_unit_string
671         hdr, wcs = _generate_wcs_and_update_header(hdr)
672         ccd_data = CCDData(hdus[hdu].data, meta=hdr, unit=use_unit,
673                            mask=mask, uncertainty=uncertainty, wcs=wcs)
674 
675     return ccd_data
676 
677 
678 def fits_ccddata_writer(
679         ccd_data, filename, hdu_mask='MASK', hdu_uncertainty='UNCERT',
680         hdu_flags=None, key_uncertainty_type='UTYPE', as_image_hdu=False,
681         **kwd):
682     """
683     Write CCDData object to FITS file.
684 
685     Parameters
686     ----------
687     filename : str
688         Name of file.
689 
690     hdu_mask, hdu_uncertainty, hdu_flags : str or None, optional
691         If it is a string append this attribute to the HDUList as
692         `~astropy.io.fits.ImageHDU` with the string as extension name.
693         Flags are not supported at this time. If ``None`` this attribute
694         is not appended.
695         Default is ``'MASK'`` for mask, ``'UNCERT'`` for uncertainty and
696         ``None`` for flags.
697 
698     key_uncertainty_type : str, optional
699         The header key name for the class name of the uncertainty (if any)
700         that is used to store the uncertainty type in the uncertainty hdu.
701         Default is ``UTYPE``.
702 
703         .. versionadded:: 3.1
704 
705     as_image_hdu : bool
706         If this option is `True`, the first item of the returned
707         `~astropy.io.fits.HDUList` is a `~astropy.io.fits.ImageHDU`, instead of
708         the default `~astropy.io.fits.PrimaryHDU`.
709 
710     kwd :
711         All additional keywords are passed to :py:mod:`astropy.io.fits`
712 
713     Raises
714     ------
715     ValueError
716         - If ``self.mask`` is set but not a `numpy.ndarray`.
717         - If ``self.uncertainty`` is set but not a
718           `~astropy.nddata.StdDevUncertainty`.
719         - If ``self.uncertainty`` is set but has another unit then
720           ``self.data``.
721 
722     NotImplementedError
723         Saving flags is not supported.
724     """
725     hdu = ccd_data.to_hdu(
726         hdu_mask=hdu_mask, hdu_uncertainty=hdu_uncertainty,
727         key_uncertainty_type=key_uncertainty_type, hdu_flags=hdu_flags,
728         as_image_hdu=as_image_hdu)
729     if as_image_hdu:
730         hdu.insert(0, fits.PrimaryHDU())
731     hdu.writeto(filename, **kwd)
732 
733 
734 with registry.delay_doc_updates(CCDData):
735     registry.register_reader('fits', CCDData, fits_ccddata_reader)
736     registry.register_writer('fits', CCDData, fits_ccddata_writer)
737     registry.register_identifier('fits', CCDData, fits.connect.is_fits)
738 
[end of astropy/nddata/ccddata.py]
[start of astropy/nddata/nduncertainty.py]
1 # Licensed under a 3-clause BSD style license - see LICENSE.rst
2 
3 import numpy as np
4 from abc import ABCMeta, abstractmethod
5 from copy import deepcopy
6 import weakref
7 
8 
9 # from astropy.utils.compat import ignored
10 from astropy import log
11 from astropy.units import Unit, Quantity, UnitConversionError
12 
13 __all__ = ['MissingDataAssociationException',
14            'IncompatibleUncertaintiesException', 'NDUncertainty',
15            'StdDevUncertainty', 'UnknownUncertainty',
16            'VarianceUncertainty', 'InverseVariance']
17 
18 
19 class IncompatibleUncertaintiesException(Exception):
20     """This exception should be used to indicate cases in which uncertainties
21     with two different classes can not be propagated.
22     """
23 
24 
25 class MissingDataAssociationException(Exception):
26     """This exception should be used to indicate that an uncertainty instance
27     has not been associated with a parent `~astropy.nddata.NDData` object.
28     """
29 
30 
31 class NDUncertainty(metaclass=ABCMeta):
32     """This is the metaclass for uncertainty classes used with `NDData`.
33 
34     Parameters
35     ----------
36     array : any type, optional
37         The array or value (the parameter name is due to historical reasons) of
38         the uncertainty. `numpy.ndarray`, `~astropy.units.Quantity` or
39         `NDUncertainty` subclasses are recommended.
40         If the `array` is `list`-like or `numpy.ndarray`-like it will be cast
41         to a plain `numpy.ndarray`.
42         Default is ``None``.
43 
44     unit : unit-like, optional
45         Unit for the uncertainty ``array``. Strings that can be converted to a
46         `~astropy.units.Unit` are allowed.
47         Default is ``None``.
48 
49     copy : `bool`, optional
50         Indicates whether to save the `array` as a copy. ``True`` copies it
51         before saving, while ``False`` tries to save every parameter as
52         reference. Note however that it is not always possible to save the
53         input as reference.
54         Default is ``True``.
55 
56     Raises
57     ------
58     IncompatibleUncertaintiesException
59         If given another `NDUncertainty`-like class as ``array`` if their
60         ``uncertainty_type`` is different.
61     """
62 
63     def __init__(self, array=None, copy=True, unit=None):
64         if isinstance(array, NDUncertainty):
65             # Given an NDUncertainty class or subclass check that the type
66             # is the same.
67             if array.uncertainty_type != self.uncertainty_type:
68                 raise IncompatibleUncertaintiesException
69             # Check if two units are given and take the explicit one then.
70             if (unit is not None and unit != array._unit):
71                 # TODO : Clarify it (see NDData.init for same problem)?
72                 log.info("overwriting Uncertainty's current "
73                          "unit with specified unit.")
74             elif array._unit is not None:
75                 unit = array.unit
76             array = array.array
77 
78         elif isinstance(array, Quantity):
79             # Check if two units are given and take the explicit one then.
80             if (unit is not None and array.unit is not None and
81                     unit != array.unit):
82                 log.info("overwriting Quantity's current "
83                          "unit with specified unit.")
84             elif array.unit is not None:
85                 unit = array.unit
86             array = array.value
87 
88         if unit is None:
89             self._unit = None
90         else:
91             self._unit = Unit(unit)
92 
93         if copy:
94             array = deepcopy(array)
95             unit = deepcopy(unit)
96 
97         self.array = array
98         self.parent_nddata = None  # no associated NDData - until it is set!
99 
100     @property
101     @abstractmethod
102     def uncertainty_type(self):
103         """`str` : Short description of the type of uncertainty.
104 
105         Defined as abstract property so subclasses *have* to override this.
106         """
107         return None
108 
109     @property
110     def supports_correlated(self):
111         """`bool` : Supports uncertainty propagation with correlated \
112                  uncertainties?
113 
114         .. versionadded:: 1.2
115         """
116         return False
117 
118     @property
119     def array(self):
120         """`numpy.ndarray` : the uncertainty's value.
121         """
122         return self._array
123 
124     @array.setter
125     def array(self, value):
126         if isinstance(value, (list, np.ndarray)):
127             value = np.array(value, subok=False, copy=False)
128         self._array = value
129 
130     @property
131     def unit(self):
132         """`~astropy.units.Unit` : The unit of the uncertainty, if any.
133         """
134         return self._unit
135 
136     @unit.setter
137     def unit(self, value):
138         """
139         The unit should be set to a value consistent with the parent NDData
140         unit and the uncertainty type.
141         """
142         if value is not None:
143             # Check the hidden attribute below, not the property. The property
144             # raises an exception if there is no parent_nddata.
145             if self._parent_nddata is not None:
146                 parent_unit = self.parent_nddata.unit
147                 try:
148                     # Check for consistency with the unit of the parent_nddata
149                     self._data_unit_to_uncertainty_unit(parent_unit).to(value)
150                 except UnitConversionError:
151                     raise UnitConversionError("Unit {} is incompatible "
152                                               "with unit {} of parent "
153                                               "nddata".format(value,
154                                                               parent_unit))
155 
156             self._unit = Unit(value)
157         else:
158             self._unit = value
159 
160     @property
161     def quantity(self):
162         """
163         This uncertainty as an `~astropy.units.Quantity` object.
164         """
165         return Quantity(self.array, self.unit, copy=False, dtype=self.array.dtype)
166 
167     @property
168     def parent_nddata(self):
169         """`NDData` : reference to `NDData` instance with this uncertainty.
170 
171         In case the reference is not set uncertainty propagation will not be
172         possible since propagation might need the uncertain data besides the
173         uncertainty.
174         """
175         no_parent_message = "uncertainty is not associated with an NDData object"
176         parent_lost_message = (
177             "the associated NDData object was deleted and cannot be accessed "
178             "anymore. You can prevent the NDData object from being deleted by "
179             "assigning it to a variable. If this happened after unpickling "
180             "make sure you pickle the parent not the uncertainty directly."
181         )
182         try:
183             parent = self._parent_nddata
184         except AttributeError:
185             raise MissingDataAssociationException(no_parent_message)
186         else:
187             if parent is None:
188                 raise MissingDataAssociationException(no_parent_message)
189             else:
190                 # The NDData is saved as weak reference so we must call it
191                 # to get the object the reference points to. However because
192                 # we have a weak reference here it's possible that the parent
193                 # was deleted because its reference count dropped to zero.
194                 if isinstance(self._parent_nddata, weakref.ref):
195                     resolved_parent = self._parent_nddata()
196                     if resolved_parent is None:
197                         log.info(parent_lost_message)
198                     return resolved_parent
199                 else:
200                     log.info("parent_nddata should be a weakref to an NDData "
201                              "object.")
202                     return self._parent_nddata
203 
204     @parent_nddata.setter
205     def parent_nddata(self, value):
206         if value is not None and not isinstance(value, weakref.ref):
207             # Save a weak reference on the uncertainty that points to this
208             # instance of NDData. Direct references should NOT be used:
209             # https://github.com/astropy/astropy/pull/4799#discussion_r61236832
210             value = weakref.ref(value)
211         # Set _parent_nddata here and access below with the property because value
212         # is a weakref
213         self._parent_nddata = value
214         # set uncertainty unit to that of the parent if it was not already set, unless initializing
215         # with empty parent (Value=None)
216         if value is not None:
217             parent_unit = self.parent_nddata.unit
218             if self.unit is None:
219                 if parent_unit is None:
220                     self.unit = None
221                 else:
222                     # Set the uncertainty's unit to the appropriate value
223                     self.unit = self._data_unit_to_uncertainty_unit(parent_unit)
224             else:
225                 # Check that units of uncertainty are compatible with those of
226                 # the parent. If they are, no need to change units of the
227                 # uncertainty or the data. If they are not, let the user know.
228                 unit_from_data = self._data_unit_to_uncertainty_unit(parent_unit)
229                 try:
230                     unit_from_data.to(self.unit)
231                 except UnitConversionError:
232                     raise UnitConversionError("Unit {} of uncertainty "
233                                               "incompatible with unit {} of "
234                                               "data".format(self.unit,
235                                                             parent_unit))
236 
237     @abstractmethod
238     def _data_unit_to_uncertainty_unit(self, value):
239         """
240         Subclasses must override this property. It should take in a data unit
241         and return the correct unit for the uncertainty given the uncertainty
242         type.
243         """
244         return None
245 
246     def __repr__(self):
247         prefix = self.__class__.__name__ + '('
248         try:
249             body = np.array2string(self.array, separator=', ', prefix=prefix)
250         except AttributeError:
251             # In case it wasn't possible to use array2string
252             body = str(self.array)
253         return ''.join([prefix, body, ')'])
254 
255     def __getstate__(self):
256         # Because of the weak reference the class wouldn't be picklable.
257         try:
258             return self._array, self._unit, self.parent_nddata
259         except MissingDataAssociationException:
260             # In case there's no parent
261             return self._array, self._unit, None
262 
263     def __setstate__(self, state):
264         if len(state) != 3:
265             raise TypeError('The state should contain 3 items.')
266         self._array = state[0]
267         self._unit = state[1]
268 
269         parent = state[2]
270         if parent is not None:
271             parent = weakref.ref(parent)
272         self._parent_nddata = parent
273 
274     def __getitem__(self, item):
275         """Normal slicing on the array, keep the unit and return a reference.
276         """
277         return self.__class__(self.array[item], unit=self.unit, copy=False)
278 
279     def propagate(self, operation, other_nddata, result_data, correlation):
280         """Calculate the resulting uncertainty given an operation on the data.
281 
282         .. versionadded:: 1.2
283 
284         Parameters
285         ----------
286         operation : callable
287             The operation that is performed on the `NDData`. Supported are
288             `numpy.add`, `numpy.subtract`, `numpy.multiply` and
289             `numpy.true_divide` (or `numpy.divide`).
290 
291         other_nddata : `NDData` instance
292             The second operand in the arithmetic operation.
293 
294         result_data : `~astropy.units.Quantity` or ndarray
295             The result of the arithmetic operations on the data.
296 
297         correlation : `numpy.ndarray` or number
298             The correlation (rho) is defined between the uncertainties in
299             sigma_AB = sigma_A * sigma_B * rho. A value of ``0`` means
300             uncorrelated operands.
301 
302         Returns
303         -------
304         resulting_uncertainty : `NDUncertainty` instance
305             Another instance of the same `NDUncertainty` subclass containing
306             the uncertainty of the result.
307 
308         Raises
309         ------
310         ValueError
311             If the ``operation`` is not supported or if correlation is not zero
312             but the subclass does not support correlated uncertainties.
313 
314         Notes
315         -----
316         First this method checks if a correlation is given and the subclass
317         implements propagation with correlated uncertainties.
318         Then the second uncertainty is converted (or an Exception is raised)
319         to the same class in order to do the propagation.
320         Then the appropriate propagation method is invoked and the result is
321         returned.
322         """
323         # Check if the subclass supports correlation
324         if not self.supports_correlated:
325             if isinstance(correlation, np.ndarray) or correlation != 0:
326                 raise ValueError("{} does not support uncertainty propagation"
327                                  " with correlation."
328                                  "".format(self.__class__.__name__))
329 
330         # Get the other uncertainty (and convert it to a matching one)
331         other_uncert = self._convert_uncertainty(other_nddata.uncertainty)
332 
333         if operation.__name__ == 'add':
334             result = self._propagate_add(other_uncert, result_data,
335                                          correlation)
336         elif operation.__name__ == 'subtract':
337             result = self._propagate_subtract(other_uncert, result_data,
338                                               correlation)
339         elif operation.__name__ == 'multiply':
340             result = self._propagate_multiply(other_uncert, result_data,
341                                               correlation)
342         elif operation.__name__ in ['true_divide', 'divide']:
343             result = self._propagate_divide(other_uncert, result_data,
344                                             correlation)
345         else:
346             raise ValueError('unsupported operation')
347 
348         return self.__class__(result, copy=False)
349 
350     def _convert_uncertainty(self, other_uncert):
351         """Checks if the uncertainties are compatible for propagation.
352 
353         Checks if the other uncertainty is `NDUncertainty`-like and if so
354         verify that the uncertainty_type is equal. If the latter is not the
355         case try returning ``self.__class__(other_uncert)``.
356 
357         Parameters
358         ----------
359         other_uncert : `NDUncertainty` subclass
360             The other uncertainty.
361 
362         Returns
363         -------
364         other_uncert : `NDUncertainty` subclass
365             but converted to a compatible `NDUncertainty` subclass if
366             possible and necessary.
367 
368         Raises
369         ------
370         IncompatibleUncertaintiesException:
371             If the other uncertainty cannot be converted to a compatible
372             `NDUncertainty` subclass.
373         """
374         if isinstance(other_uncert, NDUncertainty):
375             if self.uncertainty_type == other_uncert.uncertainty_type:
376                 return other_uncert
377             else:
378                 return self.__class__(other_uncert)
379         else:
380             raise IncompatibleUncertaintiesException
381 
382     @abstractmethod
383     def _propagate_add(self, other_uncert, result_data, correlation):
384         return None
385 
386     @abstractmethod
387     def _propagate_subtract(self, other_uncert, result_data, correlation):
388         return None
389 
390     @abstractmethod
391     def _propagate_multiply(self, other_uncert, result_data, correlation):
392         return None
393 
394     @abstractmethod
395     def _propagate_divide(self, other_uncert, result_data, correlation):
396         return None
397 
398 
399 class UnknownUncertainty(NDUncertainty):
400     """This class implements any unknown uncertainty type.
401 
402     The main purpose of having an unknown uncertainty class is to prevent
403     uncertainty propagation.
404 
405     Parameters
406     ----------
407     args, kwargs :
408         see `NDUncertainty`
409     """
410 
411     @property
412     def supports_correlated(self):
413         """`False` : Uncertainty propagation is *not* possible for this class.
414         """
415         return False
416 
417     @property
418     def uncertainty_type(self):
419         """``"unknown"`` : `UnknownUncertainty` implements any unknown \
420                            uncertainty type.
421         """
422         return 'unknown'
423 
424     def _data_unit_to_uncertainty_unit(self, value):
425         """
426         No way to convert if uncertainty is unknown.
427         """
428         return None
429 
430     def _convert_uncertainty(self, other_uncert):
431         """Raise an Exception because unknown uncertainty types cannot
432         implement propagation.
433         """
434         msg = "Uncertainties of unknown type cannot be propagated."
435         raise IncompatibleUncertaintiesException(msg)
436 
437     def _propagate_add(self, other_uncert, result_data, correlation):
438         """Not possible for unknown uncertainty types.
439         """
440         return None
441 
442     def _propagate_subtract(self, other_uncert, result_data, correlation):
443         return None
444 
445     def _propagate_multiply(self, other_uncert, result_data, correlation):
446         return None
447 
448     def _propagate_divide(self, other_uncert, result_data, correlation):
449         return None
450 
451 
452 class _VariancePropagationMixin:
453     """
454     Propagation of uncertainties for variances, also used to perform error
455     propagation for variance-like uncertainties (standard deviation and inverse
456     variance).
457     """
458 
459     def _propagate_add_sub(self, other_uncert, result_data, correlation,
460                            subtract=False,
461                            to_variance=lambda x: x, from_variance=lambda x: x):
462         """
463         Error propagation for addition or subtraction of variance or
464         variance-like uncertainties. Uncertainties are calculated using the
465         formulae for variance but can be used for uncertainty convertible to
466         a variance.
467 
468         Parameters
469         ----------
470 
471         other_uncert : `~astropy.nddata.NDUncertainty` instance
472             The uncertainty, if any, of the other operand.
473 
474         result_data : `~astropy.nddata.NDData` instance
475             The results of the operation on the data.
476 
477         correlation : float or array-like
478             Correlation of the uncertainties.
479 
480         subtract : bool, optional
481             If ``True``, propagate for subtraction, otherwise propagate for
482             addition.
483 
484         to_variance : function, optional
485             Function that will transform the input uncertainties to variance.
486             The default assumes the uncertainty is the variance.
487 
488         from_variance : function, optional
489             Function that will convert from variance to the input uncertainty.
490             The default assumes the uncertainty is the variance.
491         """
492         if subtract:
493             correlation_sign = -1
494         else:
495             correlation_sign = 1
496 
497         try:
498             result_unit_sq = result_data.unit ** 2
499         except AttributeError:
500             result_unit_sq = None
501 
502         if other_uncert.array is not None:
503             # Formula: sigma**2 = dB
504             if (other_uncert.unit is not None and
505                     result_unit_sq != to_variance(other_uncert.unit)):
506                 # If the other uncertainty has a unit and this unit differs
507                 # from the unit of the result convert it to the results unit
508                 other = to_variance(other_uncert.array <<
509                                     other_uncert.unit).to(result_unit_sq).value
510             else:
511                 other = to_variance(other_uncert.array)
512         else:
513             other = 0
514 
515         if self.array is not None:
516             # Formula: sigma**2 = dA
517 
518             if self.unit is not None and to_variance(self.unit) != self.parent_nddata.unit**2:
519                 # If the uncertainty has a different unit than the result we
520                 # need to convert it to the results unit.
521                 this = to_variance(self.array << self.unit).to(result_unit_sq).value
522             else:
523                 this = to_variance(self.array)
524         else:
525             this = 0
526 
527         # Formula: sigma**2 = dA + dB +/- 2*cor*sqrt(dA*dB)
528         # Formula: sigma**2 = sigma_other + sigma_self +/- 2*cor*sqrt(dA*dB)
529         #     (sign depends on whether addition or subtraction)
530 
531         # Determine the result depending on the correlation
532         if isinstance(correlation, np.ndarray) or correlation != 0:
533             corr = 2 * correlation * np.sqrt(this * other)
534             result = this + other + correlation_sign * corr
535         else:
536             result = this + other
537 
538         return from_variance(result)
539 
540     def _propagate_multiply_divide(self, other_uncert, result_data,
541                                    correlation,
542                                    divide=False,
543                                    to_variance=lambda x: x,
544                                    from_variance=lambda x: x):
545         """
546         Error propagation for multiplication or division of variance or
547         variance-like uncertainties. Uncertainties are calculated using the
548         formulae for variance but can be used for uncertainty convertible to
549         a variance.
550 
551         Parameters
552         ----------
553 
554         other_uncert : `~astropy.nddata.NDUncertainty` instance
555             The uncertainty, if any, of the other operand.
556 
557         result_data : `~astropy.nddata.NDData` instance
558             The results of the operation on the data.
559 
560         correlation : float or array-like
561             Correlation of the uncertainties.
562 
563         divide : bool, optional
564             If ``True``, propagate for division, otherwise propagate for
565             multiplication.
566 
567         to_variance : function, optional
568             Function that will transform the input uncertainties to variance.
569             The default assumes the uncertainty is the variance.
570 
571         from_variance : function, optional
572             Function that will convert from variance to the input uncertainty.
573             The default assumes the uncertainty is the variance.
574         """
575         # For multiplication we don't need the result as quantity
576         if isinstance(result_data, Quantity):
577             result_data = result_data.value
578 
579         if divide:
580             correlation_sign = -1
581         else:
582             correlation_sign = 1
583 
584         if other_uncert.array is not None:
585             # We want the result to have a unit consistent with the parent, so
586             # we only need to convert the unit of the other uncertainty if it
587             # is different from its data's unit.
588             if (other_uncert.unit and
589                 to_variance(1 * other_uncert.unit) !=
590                     ((1 * other_uncert.parent_nddata.unit)**2).unit):
591                 d_b = to_variance(other_uncert.array << other_uncert.unit).to(
592                     (1 * other_uncert.parent_nddata.unit)**2).value
593             else:
594                 d_b = to_variance(other_uncert.array)
595             # Formula: sigma**2 = |A|**2 * d_b
596             right = np.abs(self.parent_nddata.data**2 * d_b)
597         else:
598             right = 0
599 
600         if self.array is not None:
601             # Just the reversed case
602             if (self.unit and
603                 to_variance(1 * self.unit) !=
604                     ((1 * self.parent_nddata.unit)**2).unit):
605                 d_a = to_variance(self.array << self.unit).to(
606                     (1 * self.parent_nddata.unit)**2).value
607             else:
608                 d_a = to_variance(self.array)
609             # Formula: sigma**2 = |B|**2 * d_a
610             left = np.abs(other_uncert.parent_nddata.data**2 * d_a)
611         else:
612             left = 0
613 
614         # Multiplication
615         #
616         # The fundamental formula is:
617         #   sigma**2 = |AB|**2*(d_a/A**2+d_b/B**2+2*sqrt(d_a)/A*sqrt(d_b)/B*cor)
618         #
619         # This formula is not very handy since it generates NaNs for every
620         # zero in A and B. So we rewrite it:
621         #
622         # Multiplication Formula:
623         #   sigma**2 = (d_a*B**2 + d_b*A**2 + (2 * cor * ABsqrt(dAdB)))
624         #   sigma**2 = (left + right + (2 * cor * ABsqrt(dAdB)))
625         #
626         # Division
627         #
628         # The fundamental formula for division is:
629         #   sigma**2 = |A/B|**2*(d_a/A**2+d_b/B**2-2*sqrt(d_a)/A*sqrt(d_b)/B*cor)
630         #
631         # As with multiplication, it is convenient to rewrite this to avoid
632         # nans where A is zero.
633         #
634         # Division formula (rewritten):
635         #   sigma**2 = d_a/B**2 + (A/B)**2 * d_b/B**2
636         #                   - 2 * cor * A *sqrt(dAdB) / B**3
637         #   sigma**2 = d_a/B**2 + (A/B)**2 * d_b/B**2
638         #                   - 2*cor * sqrt(d_a)/B**2  * sqrt(d_b) * A / B
639         #   sigma**2 = multiplication formula/B**4 (and sign change in
640         #               the correlation)
641 
642         if isinstance(correlation, np.ndarray) or correlation != 0:
643             corr = (2 * correlation * np.sqrt(d_a * d_b) *
644                     self.parent_nddata.data *
645                     other_uncert.parent_nddata.data)
646         else:
647             corr = 0
648 
649         if divide:
650             return from_variance((left + right + correlation_sign * corr) /
651                                  other_uncert.parent_nddata.data**4)
652         else:
653             return from_variance(left + right + correlation_sign * corr)
654 
655 
656 class StdDevUncertainty(_VariancePropagationMixin, NDUncertainty):
657     """Standard deviation uncertainty assuming first order gaussian error
658     propagation.
659 
660     This class implements uncertainty propagation for ``addition``,
661     ``subtraction``, ``multiplication`` and ``division`` with other instances
662     of `StdDevUncertainty`. The class can handle if the uncertainty has a
663     unit that differs from (but is convertible to) the parents `NDData` unit.
664     The unit of the resulting uncertainty will have the same unit as the
665     resulting data. Also support for correlation is possible but requires the
666     correlation as input. It cannot handle correlation determination itself.
667 
668     Parameters
669     ----------
670     args, kwargs :
671         see `NDUncertainty`
672 
673     Examples
674     --------
675     `StdDevUncertainty` should always be associated with an `NDData`-like
676     instance, either by creating it during initialization::
677 
678         >>> from astropy.nddata import NDData, StdDevUncertainty
679         >>> ndd = NDData([1,2,3], unit='m',
680         ...              uncertainty=StdDevUncertainty([0.1, 0.1, 0.1]))
681         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
682         StdDevUncertainty([0.1, 0.1, 0.1])
683 
684     or by setting it manually on the `NDData` instance::
685 
686         >>> ndd.uncertainty = StdDevUncertainty([0.2], unit='m', copy=True)
687         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
688         StdDevUncertainty([0.2])
689 
690     the uncertainty ``array`` can also be set directly::
691 
692         >>> ndd.uncertainty.array = 2
693         >>> ndd.uncertainty
694         StdDevUncertainty(2)
695 
696     .. note::
697         The unit will not be displayed.
698     """
699 
700     @property
701     def supports_correlated(self):
702         """`True` : `StdDevUncertainty` allows to propagate correlated \
703                     uncertainties.
704 
705         ``correlation`` must be given, this class does not implement computing
706         it by itself.
707         """
708         return True
709 
710     @property
711     def uncertainty_type(self):
712         """``"std"`` : `StdDevUncertainty` implements standard deviation.
713         """
714         return 'std'
715 
716     def _convert_uncertainty(self, other_uncert):
717         if isinstance(other_uncert, StdDevUncertainty):
718             return other_uncert
719         else:
720             raise IncompatibleUncertaintiesException
721 
722     def _propagate_add(self, other_uncert, result_data, correlation):
723         return super()._propagate_add_sub(other_uncert, result_data,
724                                           correlation, subtract=False,
725                                           to_variance=np.square,
726                                           from_variance=np.sqrt)
727 
728     def _propagate_subtract(self, other_uncert, result_data, correlation):
729         return super()._propagate_add_sub(other_uncert, result_data,
730                                           correlation, subtract=True,
731                                           to_variance=np.square,
732                                           from_variance=np.sqrt)
733 
734     def _propagate_multiply(self, other_uncert, result_data, correlation):
735         return super()._propagate_multiply_divide(other_uncert,
736                                                   result_data, correlation,
737                                                   divide=False,
738                                                   to_variance=np.square,
739                                                   from_variance=np.sqrt)
740 
741     def _propagate_divide(self, other_uncert, result_data, correlation):
742         return super()._propagate_multiply_divide(other_uncert,
743                                                   result_data, correlation,
744                                                   divide=True,
745                                                   to_variance=np.square,
746                                                   from_variance=np.sqrt)
747 
748     def _data_unit_to_uncertainty_unit(self, value):
749         return value
750 
751 
752 class VarianceUncertainty(_VariancePropagationMixin, NDUncertainty):
753     """
754     Variance uncertainty assuming first order Gaussian error
755     propagation.
756 
757     This class implements uncertainty propagation for ``addition``,
758     ``subtraction``, ``multiplication`` and ``division`` with other instances
759     of `VarianceUncertainty`. The class can handle if the uncertainty has a
760     unit that differs from (but is convertible to) the parents `NDData` unit.
761     The unit of the resulting uncertainty will be the square of the unit of the
762     resulting data. Also support for correlation is possible but requires the
763     correlation as input. It cannot handle correlation determination itself.
764 
765     Parameters
766     ----------
767     args, kwargs :
768         see `NDUncertainty`
769 
770     Examples
771     --------
772     Compare this example to that in `StdDevUncertainty`; the uncertainties
773     in the examples below are equivalent to the uncertainties in
774     `StdDevUncertainty`.
775 
776     `VarianceUncertainty` should always be associated with an `NDData`-like
777     instance, either by creating it during initialization::
778 
779         >>> from astropy.nddata import NDData, VarianceUncertainty
780         >>> ndd = NDData([1,2,3], unit='m',
781         ...              uncertainty=VarianceUncertainty([0.01, 0.01, 0.01]))
782         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
783         VarianceUncertainty([0.01, 0.01, 0.01])
784 
785     or by setting it manually on the `NDData` instance::
786 
787         >>> ndd.uncertainty = VarianceUncertainty([0.04], unit='m^2', copy=True)
788         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
789         VarianceUncertainty([0.04])
790 
791     the uncertainty ``array`` can also be set directly::
792 
793         >>> ndd.uncertainty.array = 4
794         >>> ndd.uncertainty
795         VarianceUncertainty(4)
796 
797     .. note::
798         The unit will not be displayed.
799     """
800     @property
801     def uncertainty_type(self):
802         """``"var"`` : `VarianceUncertainty` implements variance.
803         """
804         return 'var'
805 
806     @property
807     def supports_correlated(self):
808         """`True` : `VarianceUncertainty` allows to propagate correlated \
809                     uncertainties.
810 
811         ``correlation`` must be given, this class does not implement computing
812         it by itself.
813         """
814         return True
815 
816     def _propagate_add(self, other_uncert, result_data, correlation):
817         return super()._propagate_add_sub(other_uncert, result_data,
818                                           correlation, subtract=False)
819 
820     def _propagate_subtract(self, other_uncert, result_data, correlation):
821         return super()._propagate_add_sub(other_uncert, result_data,
822                                           correlation, subtract=True)
823 
824     def _propagate_multiply(self, other_uncert, result_data, correlation):
825         return super()._propagate_multiply_divide(other_uncert,
826                                                   result_data, correlation,
827                                                   divide=False)
828 
829     def _propagate_divide(self, other_uncert, result_data, correlation):
830         return super()._propagate_multiply_divide(other_uncert,
831                                                   result_data, correlation,
832                                                   divide=True)
833 
834     def _data_unit_to_uncertainty_unit(self, value):
835         return value ** 2
836 
837 
838 def _inverse(x):
839     """Just a simple inverse for use in the InverseVariance"""
840     return 1 / x
841 
842 
843 class InverseVariance(_VariancePropagationMixin, NDUncertainty):
844     """
845     Inverse variance uncertainty assuming first order Gaussian error
846     propagation.
847 
848     This class implements uncertainty propagation for ``addition``,
849     ``subtraction``, ``multiplication`` and ``division`` with other instances
850     of `InverseVariance`. The class can handle if the uncertainty has a unit
851     that differs from (but is convertible to) the parents `NDData` unit. The
852     unit of the resulting uncertainty will the inverse square of the unit of
853     the resulting data. Also support for correlation is possible but requires
854     the correlation as input. It cannot handle correlation determination
855     itself.
856 
857     Parameters
858     ----------
859     args, kwargs :
860         see `NDUncertainty`
861 
862     Examples
863     --------
864     Compare this example to that in `StdDevUncertainty`; the uncertainties
865     in the examples below are equivalent to the uncertainties in
866     `StdDevUncertainty`.
867 
868     `InverseVariance` should always be associated with an `NDData`-like
869     instance, either by creating it during initialization::
870 
871         >>> from astropy.nddata import NDData, InverseVariance
872         >>> ndd = NDData([1,2,3], unit='m',
873         ...              uncertainty=InverseVariance([100, 100, 100]))
874         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
875         InverseVariance([100, 100, 100])
876 
877     or by setting it manually on the `NDData` instance::
878 
879         >>> ndd.uncertainty = InverseVariance([25], unit='1/m^2', copy=True)
880         >>> ndd.uncertainty  # doctest: +FLOAT_CMP
881         InverseVariance([25])
882 
883     the uncertainty ``array`` can also be set directly::
884 
885         >>> ndd.uncertainty.array = 0.25
886         >>> ndd.uncertainty
887         InverseVariance(0.25)
888 
889     .. note::
890         The unit will not be displayed.
891     """
892     @property
893     def uncertainty_type(self):
894         """``"ivar"`` : `InverseVariance` implements inverse variance.
895         """
896         return 'ivar'
897 
898     @property
899     def supports_correlated(self):
900         """`True` : `InverseVariance` allows to propagate correlated \
901                     uncertainties.
902 
903         ``correlation`` must be given, this class does not implement computing
904         it by itself.
905         """
906         return True
907 
908     def _propagate_add(self, other_uncert, result_data, correlation):
909         return super()._propagate_add_sub(other_uncert, result_data,
910                                           correlation, subtract=False,
911                                           to_variance=_inverse,
912                                           from_variance=_inverse)
913 
914     def _propagate_subtract(self, other_uncert, result_data, correlation):
915         return super()._propagate_add_sub(other_uncert, result_data,
916                                           correlation, subtract=True,
917                                           to_variance=_inverse,
918                                           from_variance=_inverse)
919 
920     def _propagate_multiply(self, other_uncert, result_data, correlation):
921         return super()._propagate_multiply_divide(other_uncert,
922                                                   result_data, correlation,
923                                                   divide=False,
924                                                   to_variance=_inverse,
925                                                   from_variance=_inverse)
926 
927     def _propagate_divide(self, other_uncert, result_data, correlation):
928         return super()._propagate_multiply_divide(other_uncert,
929                                                   result_data, correlation,
930                                                   divide=True,
931                                                   to_variance=_inverse,
932                                                   from_variance=_inverse)
933 
934     def _data_unit_to_uncertainty_unit(self, value):
935         return 1 / value ** 2
936 
[end of astropy/nddata/nduncertainty.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>
