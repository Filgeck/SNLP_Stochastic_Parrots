{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import polars as pl\n",
    "import lance\n",
    "\n",
    "from datasets import Dataset, load_dataset, IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_SEP = \"Q:\\n\\n\"\n",
    "A_SEP = \"\\n\\nA:\\n\\n\"\n",
    "\n",
    "def process_qna(series: pl.Series):\n",
    "    return series.str.strip_prefix(Q_SEP).str.splitn(A_SEP, 3).struct.rename_fields([\"question\", \"answer\", \"_\"]).struct.unnest().drop(\"_\")\n",
    "\n",
    "\n",
    "\n",
    "STREAM_BATCH_BYTES = 2 ** 27 # ~128MB\n",
    "\n",
    "def process_batch(path: str, batch_iter, i: int):\n",
    "    print(f\"Processing Batch: {i}\")\n",
    "    batch = next(batch_iter)\n",
    "    pl_series = pl.Series(\"text\", batch[\"text\"])\n",
    "    pl_final = process_qna(pl_series).with_columns(pl.Series(\"meta\", batch[\"meta\"]))\n",
    "    print(f\"Writing Batch: {i}\")\n",
    "    lance.write_dataset(pl_final.to_arrow(), \"\".join([path, f\"_batch_{i}\"]) if i > 0 else path)\n",
    "\n",
    "def merge_batches(path: str, batch_count: int):\n",
    "    base_dataset = lance.dataset(path)\n",
    "    for i in range(1, batch_count):\n",
    "        print(f\"Appending Batch: {i}\")\n",
    "        aux_path = \"\".join([path, f\"_batch_{i}\"])\n",
    "        aux_dataset = lance.dataset(aux_path)\n",
    "        base_dataset.insert(aux_dataset)\n",
    "        shutil.rmtree(aux_path)\n",
    "    \n",
    "\n",
    "def get_lance_dataset(path: str, batch_limit: int | None=None, force_download=False):\n",
    "    \"\"\"\n",
    "    **Parameters**\\n\n",
    "    path: str - make sure to make it an absolute path\\n\n",
    "    batch_limit: int - (optional) if included, only up to x batches, else downloads whole dataset\\n\n",
    "    force_download: bool - whether to redownload the dataset if one exists\n",
    "\n",
    "    **Returns**\\n\n",
    "    -> LanceDataset - Lance dataset (folder)\n",
    "    \"\"\"\n",
    "    if os.path.exists(path) and not force_download:\n",
    "        base_dataset = lance.dataset(path)\n",
    "\n",
    "    else:\n",
    "        ds = load_dataset(\"bigscience-data/roots_code_stackexchange\", streaming=True)[\"train\"]\n",
    "        train_info = ds.info.splits[\"train\"]\n",
    "        stream_batch_size = int(STREAM_BATCH_BYTES * (train_info.num_examples / train_info.num_bytes))\n",
    "        batch_iter = iter(ds.batch(stream_batch_size))\n",
    "\n",
    "        if batch_limit is not None:\n",
    "            c = None\n",
    "            for i in range(batch_limit):\n",
    "                try:\n",
    "                    process_batch(path, batch_iter, i)\n",
    "                except Exception as e:\n",
    "                    c = i\n",
    "                    if e is not StopIteration:\n",
    "                        warnings.warn(f\"Caught exception in iterator: {e}\")\n",
    "                    break\n",
    "            \n",
    "            c = c or batch_limit\n",
    "\n",
    "        else:\n",
    "            c = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    process_batch(path, batch_iter, i)\n",
    "                    c += 1\n",
    "                except Exception as e:\n",
    "                    if e is not StopIteration:\n",
    "                        warnings.warn(f\"Caught exception in iterator: {e}\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Processed Batches: {c}\")\n",
    "        assert c > 0, f\"Empty or error-ridden dataset: {path}\"\n",
    "\n",
    "        print(f\"\\nSaving Dataset...\")\n",
    "        base_dataset = lance.dataset(path)\n",
    "        if c > 1:\n",
    "            merge_batches(path, c)\n",
    "\n",
    "        print(\"Saved Dataset\")\n",
    "\n",
    "    return base_dataset\n",
    "\n",
    "db_name = \"stackexchange_base_db_lance\"\n",
    "lance_dataset = get_lance_dataset(os.path.sep.join([os.getcwd(), \"cache\", db_name]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_BATCH_SIZE = 2 ** 16\n",
    "\n",
    "lance_batches = lance_dataset.to_batches(batch_size=LOAD_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "question: large_string\n",
       "answer: large_string\n",
       "meta: large_string\n",
       "----\n",
       "question: [\"Does image on the page improves SEO rankings?\n",
       "\n",
       "My friend once told that I should include big image on the main page of my website. And it will be good for SEO.\n",
       "So does image on the page improves SEO rankings?\",\"What happens with WHO IS privacy protection when the domain is not active?\n",
       "\n",
       "I'm running small Wordpress site (about 15k visitors per month) and I want to buy a custom domain and monetize it. During the registration I'm asked for personal info, I know I have to provide it. I also want to buy WHO IS privacy protection, but I have a question - it's for one year, the domain is too. What happens if I won't pay for my domain after one year - will all my personal information be visible, or will it dissapear cause the domain is no longer active?\n",
       "Thanks for your answers. \",\"Using established domain names and phrases in your sentences as a keywords to boost your rank?\n",
       "\n",
       "Let's imagine a situation. I run a company that sells trucks and my nearest webrank competitors are websites called \"reliableblacktrucks.com\" and \"trucksreallycheap.net\".\n",
       "Could I potentially tap into their search egine space by using a sentences like \"We do not sell only reliable black trucks, but also we can offer you those trucks really cheap\" and so on? Just a food for though I had.\",\"PostmarkApp inbound domain creation for MX record redirection\n",
       "\n",
       "I'm trying to use Postmark App service (https://postmarkapp.com) to forward email to my subdomain to my PostmarkApp server.\n",
       "I'm trying to use inbound domain forwarding by configuring my MX record to point to my Postmark email server.\n",
       "The Postmark docs mentions to give the MX record a value of 10 (which i did) and to post to an inbound domain you will create at Postmark.\n",
       "The problem i have is creating a valid domain at PostmarkApp...\n",
       "Specifically, the docs mention:\n",
       "\"In your DNS configuration, create an MX record that points to inbound.postmarkapp.com and give it a value of 10\"\n",
       "Where inbound is this example is the name of the server they have created inside PostmarkApp.\n",
       "I can configure the inbound domain in the server settings page. Inbound domains are unique across Postmark and are server-specific.\n",
       "So i went on my PostmarkApp's server's setting page i went ahead and inside the Inbound Domain i gave a random name, to ensure there are no conflicts with other inbound domains from other users/servers inside PostmarkApp.\n",
       "I got an error with this accompanying message:\n",
       "\"We could not find an MX record pointing to inbound.postmarkapp.com for this domain.\"\n",
       "I don't even understand the nature of this message. I thought i would set up my inbound domain for my server on Postmark. Then i would go to my dns records on my domain and give an MX record of 10:myserversname.postmarkapp.com\n",
       "So what do i put inside the Inbound Domain settings in order to create the effect i want?\",\"does google webmaster track clicks on deeplinks or best way to capture deeplink clicks on seo results\n",
       "\n",
       "On google search results page on mobile browser. If native application is installed google shows the option of opening the link in app / on broswer.\n",
       "In that scenario, does that click gets captured in google webmaster click report ?\n",
       "What is the best way to capture these deep links through webmaster or any other tool ?\n",
       "Below image clearly suggests that google has given user option to open in app / or in browser.\",\"301 redirect or canonical\n",
       "\n",
       "I'm launching version 2 of my site. It has a new url structure, but largely the same contents. I have around 500+ content pages that gets affected.\n",
       "This is a custom build and I have no technical limitations.\n",
       "I have two choices:\n",
       "a) 301 redirect all the pages to the new url format\n",
       "b) Render the page from the legacy url and create a rel=canonical tag pointing to the same contents, but in the the url format.\n",
       "Matt Cutts says that there is no limit to the number of 301 redirects:\n",
       "https://www.searchenginejournal.com/matt-cutts-discusses-301-permanent-redirects-limits-on-websites/46611/\n",
       "Still I fear that Google will penalize the ranking of the 301 redirected pages. I'm afraid that Google will see it as \"black hat\" methods.\n",
       "In that light is the second method not the better option?\",\"Why does loading of an external \"async defer\" javascript code happen before page load?\n",
       "\n",
       "Recaptcha recommends loading of its script like this:\n",
       "<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\n",
       "\n",
       "I added this to the head section of the html document and checked with Chrome when it is loaded. Chrome says it is loaded before page load (the network tab of chrome devtools indicates the end of the page load with a vertical red line and the above script is loaded before that.\n",
       "Why is that? Shouldn't adding async defer as recommended by google defer loading of the script after page load?\",\"I have similar pages with little text, should I add canonical to all but one?\n",
       "\n",
       "I have a website with online puzzle games. Sometimes, I create a puzzle variation based on an existing one. A change in initial conditions or distribution of elements can change the game, the solution, etc. But description of puzzle remains basically the same.\n",
       "So I end up with two similar pages, like these:\n",
       "http://www.puzzlopia.com/puzzles/neutralizator/play\n",
       "http://www.puzzlopia.com/puzzles/neutralizator-ii/play\n",
       "What would be a good strategy to tell google this is similar content, but not duplicated? I want to avoid SEO penalties, even if almost all text is the same (What changes is the javascript code, some names and some images).\n",
       "I'm not sure if meta canonical tag would work or would confuse google.\n",
       "Thanks in advance!\",\"How to update domain name for search engine results?\n",
       "\n",
       "I've updated my site to use a different domain name. Now, in Google searches, old domain shows up and site became unreachable. Is there a way to tell Google that \"I've changed my domain name, update search results accordingly\"?\n",
       "Current situation: \n",
       "\n",
       "I can control both domains. \n",
       "Both domains are set up to use SSL\n",
       "Old domain has no new SSL certificate\n",
       "\n",
       "Note: \n",
       "Why 301 redirection is not working: Because link uses SSL, so when a user first clicks, it throws SSL error. If you say \"keep going\", then 301 redirection occurs. \n",
       "Edit\n",
       "This will take about 2 days Google to update the links.\",\"Putting CSS/JS in header for faster load times\n",
       "\n",
       "I've been reading many articles lately about SEO on load times. Trying to get the perfect load time can be quite time-consuming but also a good feeling when you see your site loading twice as quick.\n",
       "Now a lot of these tutorials and blogs are saying to put chunks of CSS/JS in you head tag to help above-the-fold content to load faster. \n",
       "\"external resources take longer to load\", each request back to the server for a file takes more time. So would it not make sense to actually just embed all CSS/JS in your head tag from the beginning.\n",
       "Sure this could look messy at first, but the only reason why it takes long for external scripts to download in the browser is because the output has already been sent to the browser.\n",
       "What if we simply do:\n",
       "<head>\n",
       "    ... Usual things here ...\n",
       "    <?php require('css/bootstrap'); ?>\n",
       "    <?php require('css/mystylesheet'); ?>\n",
       "</head>\n",
       "<body>\n",
       "    ... Usual things here ...\n",
       "    <?php require('js/bootstrap.js'); ?>\n",
       "    <?php require('js/myscript.js'); ?>\n",
       "</body>\n",
       "\n",
       "Now this will just be served as a simple HTML file with embedded CSS and JavaScript as soon as it hits the users browser. And it is just as clean looking in our text editor as before because we require the file in PHP.\n",
       "Would this improve overall speed performance or should i stay clear of this method?\",...,\"Can I mark unread references?\n",
       "\n",
       "When taking notes in my research notebook, I often end up citing many papers because the paper I am writing a summary of has cited them: I want to record adequate justification of each claim, and I want to be able to track down the sources if I need to in the future.\n",
       "However, it is not practical for me to stop and read every reference the current paper is citing. Therefore, some of my the references in my notebook I have read, and some I have not read. This bothers me, because they are not explicitly marked as such in the actual document.\n",
       "How can I cite my Bibtex sources (I am actually using JabRef with biber under Miktex/Texstudio) in such a way that I can distinguish those I have read from those I haven't?\n",
       "What logical levels I want\n",
       "Ideally, I would like three classes:\n",
       "\n",
       "References I have read\n",
       "References I haven't read but intend to read\n",
       "References I haven't read and don't plan to\n",
       "\n",
       "But if only binary discrimination is possible, the last two can be collapsed into a single group.\n",
       "What I want it to look like\n",
       "Ideally, I would like each class of reference be marked when it is referenced. For instance:\n",
       "\n",
       "It has recently been discovered that water is wet[1] and the sky is blue[+2], but it is still controversial whether a pound of iron is heavier than a pound of cotton[-3].\n",
       "\n",
       "Here, [1] is a reference I have read, [+2] is a reference I haven't read but plan to, and [-3] is a reference I don't want to read. The actual format doesn't matter, so long as it's compatible with major citation styles (numerical and author-year) - it can be stars, daggers, color, kind of bracket, font, a letter, etc. (though simple is better)\n",
       "Then the bibliography would look like so:\n",
       "\n",
       "1. Brooks, R. Surprising fluidic characteristics of dihydrogen monoxide. Journal of Fluidic Chemistry 30 (July 2013), 40-52.\n",
       "+2. Brown, N., Blum, M., Maruyama, E., and Martinez, P. A novel, powerful spectrometric method for evaluating chromaticity. Proceedings of the Royal Astrological Society (May 1999).\n",
       "-3. Martinez, M., and Morrison, R. T. Which one is heavier: Separating facts from fiction. 17th International Conference of Boring Arguments (Oct. 2012).\n",
       "\n",
       "Again, the exact symbols don't matter. Alternatively to this scheme, it would also be fine if it could output three separate bibliographies, with each class of reference in its own bibliography.\n",
       "I would prefer to be able to tell at a glance which references I've read and which ones I haven't both in the text and the bibliography(ies), but if that is hard, then it would suffice if I could by looking at the bibliography only.\",\"How to add the name of the supervisor in a @thesis field?\n",
       "\n",
       "In the biblatex doc, it says ( Version 1.7, p.11):\n",
       "\n",
       "thesis\n",
       "A thesis written for an educational institution to satisfy the\n",
       "  requirements for a degree. Use the type field to specify the type of\n",
       "  thesis.\n",
       "Required fields: author, title, type, institution, year/date\n",
       "Optional fields: subtitle, titleaddon, language, note, location,\n",
       "  month, isbn, chapter, pages, pagetotal, addendum, pubstate, doi,\n",
       "  eprint, eprintclass, eprinttype, url, urldate\n",
       "\n",
       "I would like to know if there is any \"canonical\" way to add the name of the supervisor(s) in a field of the .bib file (no .bbl manipulation please, I want my bibliography to be usable \"as it\" in several documents).\n",
       "The expect result could be something like  :\n",
       "\n",
       "Meand Myself. “A brilliant result, explained”. PhD thesis**, under the\n",
       "  joint supervision of Pr. Toto and Pr. Tata**. Université Paris 24,\n",
       "  Nov. 2013.\n",
       "\n",
       "The \"under the (joint) supervision\" could be a string that would be declined in four ways:\n",
       "\n",
       "under the supervision of \n",
       "under the joint supervision of\n",
       "and the abridged forms\",\"Shading faces of solid (described using using \\parametricplotThreeD)\n",
       "\n",
       "Does anyone know how to shade only one of the faces of a solid whose faces are described using parametricplot? For example, shade the faces of the solid below? In general, how to color each of the sides with different colors/patterns? It might be necessary to change the whole code (?)\n",
       "Thank you!\n",
       "\\documentclass[pstricks,border=2mm]{standalone}\n",
       "\\usepackage{pstricks}\n",
       "\\begin{document}\n",
       "\n",
       "    \\begin{pspicture}(-1.5,-1)(2.5,2)\n",
       "    \\psset{unit=2cm,Alpha=70,Beta=15,fillstyle=solid}\n",
       "    %\\psgrid\n",
       "    \\pstThreeDCoor[linecolor=gray,xMin=0,xMax=2,yMin=0,yMax=2,zMin=0,zMax=1.5]\n",
       "\n",
       "    \\pstThreeDLine[linecolor=lightgray]{-}(1,-0.1,0)(1,0.1,0)\n",
       "    \\pstThreeDPut(1,-0.2,0.1){$1$}\n",
       "\n",
       "    \\pstThreeDLine[linecolor=lightgray]{-}(-0.1,1,0)(0.1,1,0)\n",
       "    \\pstThreeDPut(-0.3,1,0.1){$1$}\n",
       "\n",
       "    \\pstThreeDLine[linecolor=lightgray]{-}(-0.1,0,1)(0.1,0,1)\n",
       "    \\pstThreeDPut(-0.3,-0.3,1){$1$}\n",
       "\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,90){%\n",
       "     t cos %\n",
       "     0 %\n",
       "     t sin}%\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,90){%\n",
       "     t cos %\n",
       "     t sin %\n",
       "     0}%\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,90){%\n",
       "     t cos %\n",
       "     t sin %\n",
       "     t sin }%\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,1){%\n",
       "     0 %\n",
       "     1 %\n",
       "     t}%\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,1){%\n",
       "     0 %\n",
       "     t %\n",
       "     1}%\n",
       "     \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,1){%\n",
       "     0 %\n",
       "     0 %\n",
       "     t}%\n",
       "\n",
       "    \\psline[linewidth=0.5pt]{->}(1.2,-0.6)(0.5,-0.3)\n",
       "    \\put(1.3,-0.7){$x^2+y^2=1$}\n",
       "\n",
       "    \\psline[linewidth=0.5pt]{->}(-1.1,0.5)(-0.4,0.35)\n",
       "    \\put(-2.2,0.5){$x^2+z^2=1$}\n",
       "\n",
       "    \\end{pspicture}\n",
       "\\end{document}\",\"\\subsection{AAA \\Rey } error in beamer\n",
       "\n",
       "My MWE is\n",
       "\\documentclass[presentation]{beamer}\n",
       "\\let\\Tiny\\tiny\n",
       "\\usepackage{hyperref}\n",
       "\\usetheme{Berkeley}\n",
       "\n",
       "\\newcommand\\Rey{\\mbox{\\textit{Re}}}  \n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "\\section{A section}\n",
       "\\subsection{AAA \\Rey }\n",
       "\\begin{frame}\n",
       "  \\frametitle{Frame title1}\n",
       "  \\framesubtitle{frame subtitle1}\n",
       "  Some text s\n",
       "\\end{frame}\n",
       "\\end{document}\n",
       "\n",
       "But it get errors: \n",
       "<inserted text> \n",
       "                \\par \n",
       "l.14 \\subsection{AAA \\Rey }\n",
       "\n",
       "Want to get help, Thanks. :-D\",\"Resetting text width in a nested tikzpicture\n",
       "\n",
       "I'm nesting a tikzpicture inside a tikzpicture, and having problems with the 'text width' parameter leaking through from the outer tikzpicture to the inner. (The nesting is pretty much unavoidable -- in my actual use case the tikzpicture s are defined in different macros.) \n",
       "MWE:\n",
       "\\documentclass[a4paper,11pt,landscape]{article}\n",
       "\\usepackage{tikz,nopageno}\n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "\\newcommand{\\cost}{%\n",
       "\\begin{tikzpicture}\n",
       "    \\node[fill=red!85!black,shape=circle,inner sep=0.3mm,draw=black,text=white] {\\raisebox{0pt}[\\height][0pt]{\\bf 1}};\n",
       "\\end{tikzpicture}}\n",
       "\n",
       "\\cost\n",
       "\n",
       "\\begin{tikzpicture}\n",
       "    \\node[text width=10mm] {\\cost};\n",
       "\\end{tikzpicture}\n",
       "\n",
       "\\end{document}\n",
       "\n",
       "How should I modify 'cost' so that it behaves as if text width was not set?\",\"Prevent biblatex date output for online citations\n",
       "\n",
       "I'd like to not print the year (nor a possible author), but only for Online types of citations.\n",
       "Here's an illustration of what I would like to remove (in red):\n",
       "\n",
       "Here's my MWE:\n",
       "% arara: xelatex\n",
       "% arara: biber\n",
       "% arara: xelatex\n",
       "\n",
       "\\documentclass{scrartcl}\n",
       "\\usepackage[backend=biber,style=authoryear,sorting=nty]{biblatex}\n",
       "\\addbibresource{mwe.bib}\n",
       "\n",
       "\\begin{document}\n",
       "Example\\cite{tab:test}\n",
       "\n",
       "\\printbibliography\n",
       "\\end{document}\n",
       "\n",
       "mwe.bib:\n",
       "@Online{tab:test,\n",
       "  Url                      = {http://www.example.com/images/image.jpg},\n",
       "  Urldate                  = {2014-02-25},\n",
       "  Timestamp                = {2014.06.13},\n",
       "  Note                     = {Table \\ref{tab:test}}\n",
       "}\n",
       "\n",
       "This question is motivated with difficulties in resolving Custom biblatex style for citing images and tables, and represents a narrower requirement\",\"tikz: Better raised text effect\n",
       "\n",
       "If you look carefully at the raised text in this image, you can see that it has two drop shadows:  as well as the black one to the southeast of the text, there is a white one to the northwest. I know how to insert one drop shadow in tikz using e.g.\n",
       "drop shadow={shadow xshift=-0.5mm,shadow yshift=-0.5mm,black}\n",
       "\n",
       "Is there any easy way to add two shadows of the kind specified?  (NB. Being able to specify the 'light direction' via shadow xshift,shadow yshift is very useful.)\",\"Custom biblatex style for citing images and tables\n",
       "\n",
       "I need to cite tables and images with a very specific style to satisfy a requirement from the institution. The resulting output of the \\printbibliography should look like this for those citations:\n",
       "\n",
       "Figure 3. http://www.example.com/img/name.jpg, visited on 6.3.2014\n",
       "...\n",
       "Table 6. http://www.example.com/tables/tables.pdf, visited on 9.3.2014.\n",
       "\n",
       "Manually, this would be something like:\n",
       "\\newcommand{\\tablebib}[3]{%\n",
       "    Table \\ref{#1} \\url{#2}, visited on \\displaydate{date}%\n",
       "}\n",
       "\n",
       "I'm using biblatex for other citations and would like to use it for those references as well. How can biblatex output such a format?\n",
       "The final bibliography result should a mix between a more-or-less standard authoryear style (I managed to do this), and the above style for citing image and table sources from the Internet (I don't know how to do this):\n",
       "\n",
       "Tomplinson, D., Baeyer, C.L., Stinson, J.N., Sung, L. (2010): A\n",
       "  Systematic Review of Faces Scales for the Self-report of Pain\n",
       "  Intensity in Children, Pediatrics, Vol.126, 5\n",
       "...\n",
       "Figure 3. http://www.example.com/img/name.jpg, visited on 6.3.2014.\n",
       "\n",
       "Another manual try would be:\n",
       "% arara: xelatex\n",
       "% arara: biber\n",
       "% arara: xelatex\n",
       "\\documentclass{scrartcl}\n",
       "\\usepackage[backend=biber,style=authoryear,sorting=nty]{biblatex}\n",
       "\\addbibresource{mwe.bib}\n",
       "\n",
       "\\begin{document}\n",
       "\\begin{table}[htb]\n",
       "\\begin{tabular}{l}\n",
       "test\n",
       "\\end{tabular}\n",
       "\\caption{Description\\cite{tab:test}}\n",
       "\\end{table}\n",
       "\n",
       "\\printbibliography\n",
       "\\end{document}\n",
       "\n",
       "with the bibliography:\n",
       "@Online{tab:test,\n",
       "  Url                      = {http://www.example.com/images/image.jpg},\n",
       "  Urldate                  = {2014-02-25},\n",
       "  Timestamp                = {2014.06.13},\n",
       "  Note                     = {Table \\ref{tab:test}}\n",
       "}\n",
       "\n",
       "This outputs:\n",
       "\n",
       "(2014) Table ??. URL: http://www.example.com/images/image.jpg (visited on 02/25/2014)\n",
       "\n",
       "Year should not be visible. The reference should be resolved. URL: should not be displayed, but the url itself should be visible.\",\"Column break and beginning two new columns on same page\n",
       "\n",
       "I am new to LaTeX, so please bear with me if my question does not meet the requirements, or if the solution is obvious.\n",
       "I am creating a document with two colums, and after my text, I want some kind of break, so my references begin in the left column. So to speak, I want a new page, just beginning at the same page as the text. I hope it makes sense. I would prefer if I could do it without any additional packages, but please let me know if I have to install one to make it work. This is my code:\n",
       "\\documentclass[11pt,twoside,twocolumn,a4paper]{article} \n",
       "\\usepackage{natbib} \n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "Bla bla \n",
       "\n",
       "\\newpage\n",
       "\n",
       "Bla Bla\n",
       "\n",
       "\\bibliographystyle{plain}\n",
       "\\bibliography{/Users/Nikolaj/Documents/Samf/library}\n",
       "\n",
       "\\end{document}\",\"How to vertically center an image in table, using p{} and without using m for tex4ht?\n",
       "\n",
       "Ok, I gave up on this. This is the problem. I'd like to center an image in table columns. \n",
       "There are solutions for this, for example here but this does not work when I use p for column specifications. Another solution I know that works is using m for all the columns, as shown here but this uses m and I need to use p because I need to use tex4ht to also generate these tables. I just tried makecell package. I can get it to center the image horizontally, (which also works with tex4ht) but do not know how to use this package to center vertically also. Package documentation not clear to me. \n",
       "The problem is tex4ht does not support m and it ignores the column width. So have to use p but I do not know how center the image using p \n",
       "So I am looking for solution to center images in columns, but without using m and must be able to use p. This is what I tried (MWE)\n",
       "\\documentclass{article}\n",
       "\n",
       "\\usepackage{mwe}\n",
       "\\usepackage{longtable}\n",
       "\\usepackage{hyperref}\n",
       "\\usepackage{array}\n",
       "\\usepackage{makecell} %for makecell\n",
       "%\\usepackage{blindtext}\n",
       "\\newcommand{\\mytext}{bla bla bla bla bla bla bla bla bla bla bla bla\n",
       "  bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla\n",
       "  bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla}\n",
       "\\begin{document}\n",
       "\n",
       "\\begin{longtable}{|p{1in}|p{1in}|p{2in}|}\\hline\n",
       "\\mytext&\\mytext&\n",
       "\\raisebox{-.5\\height}{\\includegraphics[scale=0.25]{example-image}} \n",
       "\\\\\\hline\n",
       "\\end{longtable}\n",
       "\n",
       "\\begin{longtable}{|p{1in}|p{1in}|p{2in}|}\\hline\n",
       "\\mytext&\\mytext&\n",
       "\\makecell[cc]{\\includegraphics[scale=0.25]{example-image}} \n",
       "           %centered ok, but only horizonatlly, how to make it center vertically?\n",
       "%\\makecell[{{m{2in}}}]{\\includegraphics[scale=0.25]{example-image}} %not working\n",
       "\\\\\\hline\n",
       "\\end{longtable}\n",
       "\n",
       "\\begin{longtable}{|m{1in}|m{1in}|m{2in}|}\\hline\n",
       "\\mytext&\\mytext&\n",
       "\\includegraphics[scale=0.25]{example-image}\n",
       "\\\\\\hline\n",
       "\\end{longtable}\n",
       "\\end{document}\n",
       "\n",
       "pdflatex output\n",
       "\n",
       "So the m solution works in pdflatex (but not with tex4ht). The \\raisebox{-.5\\height} works if one does not use p but I need to use p else table will be too wide. \n",
       "Here is the output from tex4ht (must use p-width option, else tex4ht will ignore p)\n",
       " htlatex foo.tex \"htm,p-width\"\n",
       "\n",
       "One can see the p worked, but the image is not aligned in center. WIth makecell it is aligned ok, but only horizontally, and With m the image is aligned, but lost the width specification. (isn't Latex fun?)\"]\n",
       "answer: [\"Yes and no.\n",
       "It is content that ranks and image tags offer some to the total performance of the page, however, it is limited.\n",
       "Where your friend is right is engagement. Google looks for elements that offer engagement. An image is the most basic engagement there is. At one point, Google was not shy about saying that an image should exist at the top of the content. SEO's agreed for several reasons and so for almost every page you will see on the web, there is an image at the top of the content.\n",
       "So besides the value of the alt text and any potential that the path and file name give, Google credits the image as engagement. However, this too is limited.\n",
       "From a user experience (UX) perspective, it does cause a pause in the users visit and may entice the user to read your content. It is just one element, however, it is the UX effect that is the greatest. It can extend the time spent on page and reduce bounce rates somewhat which are other metrics Google can use to rank a page.\",\"The short answer is that it depends on how you chose to protect your privacy. Terms vary by Registrar and by extension. For example, .je domains never reveal anything more than the Registrant's name regardless of the seller's policy - it is a decision made by the Registry managing the extension.\n",
       "The TL;DR is \"Choose the TLD and  Registrar wisely\".\n",
       "If you purchased a third-party WHOIS protection service (note it's spelled as one word, all caps, 'WHOIS' as per RFC 3912) then the provider's terms govern that separate purchase, which is a different transaction from the domain name purchase. There are multiple WHOIS proxy/anonymity providers and you need to check the specific Terms for each to learn the answer.\n",
       "One thing to watch out for with this arrangement is that you are typically registering the domain by proxy. You are not the domain owner, the proxy service owns the domain. As an example, here is GoDaddy's Terms & Conditions page which makes this clear. They own the domain, you pay the middle-man proxy to purchase and administer it. There are legal implications to this that you should talk to a lawyer about if you are serious about monetizing the domain.\n",
       "Another approach which gives you more rights and control is to choose a Registrar that provides anonymity service as standard and by default. It's all one transaction, no third-party involvement or extra purchase to make. One I use and recommend is Gandi.net (no affiliation, just a customer) and they protect your data and never sell it by default. You are the Registrant and you directly own the domain, the Gandi privacy scheme is not a proxy service.\",\"There are a few things you should know.\n",
       "1] Most websites are poorly optimized.\n",
       "2] Most sites with SEO help are still poorly optimized.\n",
       "3] Terms found in a URL domain name are significant.\n",
       "4] Terms found within content can easily overcome the lack of the same terms within the URL domain name.\n",
       "5] Keywords, in of themselves, is not how search works.\n",
       "6] Most keywords people chase are not actually used in search.\n",
       "Most of the time, it does not pay to try and compete directly against another web site because most are not optimized well. Playing catch-up and competing directly against a poorly optimized website often leads you in the wrong direction.\n",
       "What matters is how people actually search.\n",
       "It is difficult to figure out what people search for. Many SEOs will swear by certain keywords. In my comments, I lament the use of the term best by SEOs when in fact, most people do not use the term to search. When was the last time you entered the term best in a search query? You may very well never have. And that is the point. Do a search for best chocolate chip cookie and the results will be staggering. The number of sites that try and compete for a useless search term makes the useless search term effectiveness completely moot.\n",
       "Search is about whole language. RankBrain and Google's index are semantic based and not term based. In otherwords, semantics is used to understand the query and any site/page topic and make an appropriate match. It is not a search term match, it is a semantics topic match along with syntactic analysis. The analysis gets very deep indeed, however, I will not be going into that here. I have written about semantics in enough other answers you can read them if you want.\n",
       "One of the things people do not realize is that certain terms are less effective in search for some sites. I used the example of adjectives in my comments. Adjectives are important, however, they are often less important in search with some exception. For example, a red car makes it clear that adjectives are important. However, when was the last time you searched for a red anything? This example dove-tails into your examples which I realize are just off the top of your head. Granted. In your example, black and cheap are likely fairly useless. Anyone searching for a car will likely not use these terms. For this reason, these are likely useless terms for a URL domain name.\n",
       "If you were to search for a car, what would you search for?\n",
       "This is the toughest nut to crack. Ask a few friends to do searches on the net and keep track of what search queries they use and what SERP links and snippets they preferred. What you will likely find is that the searches are simple, short, and devoid of adjectives except for a few. You can give them specific tasks such as finding a low-cost black pick-up truck. Be careful not to implant into their minds search term suggestions that may skew the results.\n",
       "You can use the SERP links and snippets to better understand why these sites stand out. Do not do this yourself. It is okay to be around and ask questions as your friends search. You may learn more that way.\n",
       "What you may find is that regardless of the use of the search term black, the results may not yield results for a black pick-up truck.\n",
       "For example, yesterday, I was searching for Triumph motorcycles. I was looking for dealers specifically. I was looking for specific years and models and other specific information. Most all of my searches did not result in what I was looking for. For example, I did not find one dealer but primarily websites that people use to post their ad to sell their motorcycle. I found one dealer that had not sold Triumphs in years and did not contain the term triumph on their site at all. As well, searching for models worked fairly well. Year did not. Many of my results often showed Harley Davidson motorcycles and not Triumphs. As well, since I was looking for an older Triumph, mostly new Triumphs were shown regardless of what I did. Even then, not one of them was cheap. All results were for premium priced Triumphs well over the market value. Very frustrating for someone who simply wants a reliable older Triumph from a reputable dealer at a reasonable price. The terms reliable, older, reasonable, vintage, used, pre-owned, etc., all were relatively useless.\n",
       "For the record, E-commerce sites are a bit different. You will find adjectives within the product page/description and not within a domain name, path, file name, etc. This is an important point because this is not where you would expect to find them. If you are trying to boost performance for an e-commerce site, you will be working a bit differently than for a blog.\n",
       "Using your example, adjectives can better perform within the h1 or lesser header tag, the product description, and within any store-front form element. As well, I discovered that using a few h3 and h4 header tags can weigh fairly heavily if done well. For example, for a list of products, the lower header tags can contain details such as color etc. H2 tags are often used for this. Adding an appropriate h3 header along with a paragraph tag below each h2 may help.\n",
       "What you do will depend upon your site. I am not suggesting using header tags to boost search results for every situation. I was just using that as an example.\n",
       "What is important is to use whole language. If you are selling a black pick-up truck, then say so. For example, Chevy 1500 pick-up truck, black exterior, black interior with 5.0 liter v8 engine and automatic transmission. Best in class fuel efficiency and towing. within a header tag should help. These content elements should appear elsewhere such as within a product description paragraph below the header tag to support the semantic weight.\n",
       "However, this is not the end of the story. Site trust metrics and quality organic links as well as many other metrics such as CTR (click-through rate) and bounce rate also influence search.\n",
       "This is how sites compete in search.\n",
       "Without a realistic example, I cannot give good solid advice on how better to compete. Hypothetical scenarios do not always lend themselves to better answers. Sorry. Hopefully, I gave you some ideas anyhow.\",\"You need to add the MX record to your domain first - go to your domain's DNS records (wherever these are managed), and add an MX record for your subdomain to point its mail to inbound.postmarkapp.com with a priority of 10.\n",
       "For example:\n",
       "sub.john-john.com MX inbound.postmarkapp.com 10\n",
       "In Postmark's documentation, inbound.postmarkapp.com isn't just an example - it's the address of their inbound mail server and is where you need to point your MX to - and is also where they ask all of their inbound customers to point their MX records to.\n",
       "How will Postmark then know where to deliver your mail to? They'll deliver it based on where it is addressed. Firstly, the sending server will do a DNS lookup of your domain, find inbound.postmarkapp.com, and then contact Postmark's server to say \"I've got an e-mail for john@sub.john-john.com, can you accept that?\" Because your subdomain is indeed set up with Postmark, Postmark will then accept the message and deliver it accordingly.\n",
       "As for what to call your subdomain, if you own your own domain, it's completely up to you. I've just used sub and john-john.com in this example, but replace that with whatever your domain is and whatever you would like your subdomain to be. :)\",\"Yes you can track these click from Google webmaster (Search console)\n",
       "First let me breakdown all the process and explain what is deeplinking and app indexing.\n",
       "App indexing appears on Google when someone search for the brand name or anything related to it, your app will appear from google play or itunes with \"Download\" link.\n",
       "Deep linking appears on any page of your website and you can open the app through the app if your app/website support deeplinking.\n",
       "To track deeplinking click and app installs you have to add your application on Google webmaster as a new property then verify your website from Google play, here you will have a connection from the website to the app and follow all the instructions to support deep linking.\n",
       "after one week Google will start give you the numbers in search console (installs and pages) and you can track everything you want.\n",
       "This resource will help you from google\n",
       "https://developer.android.com/training/app-indexing/deep-linking.html\",\"Do a 301 redirect.\n",
       "In your case Google won't see it as Black Hat. \n",
       "The only way to get a \"Black Hat\" mark with a 301 is when you're doing \"chain redirection\" (i.e. you're redirecting from a first to a second to a third domain etc.).\n",
       "In the situation you've described, there won't be any penalty for this redirection.\",\"The page is only fully loaded when all linked resources (including <script async defer) have loaded. This is when the onload event fires.\n",
       "However, the DOMContentLoaded event is likely to fire before this - which will probably be before the async defer scripts have loaded. (I say \"probably\" - if the browser is able to determine that it can load the script in another thread at the same time without slowing things down then I guess it probably will; but with no guarantee.)\",\"Your question totally depends on how you want to solve it :\n",
       "Use single page for puzzle + its variants\n",
       "If you have multiple variation of same puzzle, then you can showcase them on single page**\n",
       "http://www.puzzlopia.com/puzzles/neutralizator/play\n",
       "\n",
       "show all variants of above puzzle on above page only\n",
       "\n",
       "Multiple Pages For Variants :\n",
       "1) Primary / Main Page rank only\n",
       "If different pages are required, then do you want them to be indexed/searchable separately in google index or you want to get the main page ranked and how variants links on the main page\n",
       "Example :\n",
       "Puzzle 1 page is ranked and all variants links are shown on this page.\n",
       "Then setting canonical on variants to main puzzle page will not confuse google as setting canonical indicates that the content is attributed to main page or is similar to the main page\n",
       "In this case only the main puzzle page will be indexed by google and it will get seo extract from all variants as well.\n",
       "2) Variants to be ranked separately \n",
       "If you want all variants also to rank separately.\n",
       "Yes this can also be done by setting proper namespacing in the urls i.e. don't make urls like: puzzle/1/1 puzzle/1/2\n",
       "Making url names meaningful help you index your content better:\n",
       "So make them like : puzzle/rubik/three-cross-three , puzzle/rubik/four-cross-four ......\n",
       "Also the major content on page title, description, h1 etc should differ for all pages.\n",
       "Some content should be different and some of the content i.e. the base content can be same.\n",
       "Following above + other seo practices you can rank individual variants as well.\",\"You will need to renew your old SSL certificate (or grab a free trial SSL, such as this one from Comodo).\n",
       "Although I can't find a reference right now, Google is not going to look kindly on a 301 beneath an expired certificate, if it even gets to that point at all. As far as Google's concerned, your site could be compromised and the 301 can't be trusted.\n",
       "Aside from the SSL issue, a 301 redirect is exactly the way to go in this case.\n",
       "\n",
       "EDIT - Adding further information from discussion in the comments:\n",
       "Further, although not entirely neccessary because the 301 redirects will pick it up, you can use the Change of address tool in the Google Search Console (Webmaster Tools). It's worth noting that 301 redirects are still required in order for this to work.\n",
       "Google also has a decent overview of the URL changing process in their help centre. It goes into detail on four steps:\n",
       "\n",
       "Prepare the new site and test it thoroughly.\n",
       "Prepare a URL mapping from the current URLs to their corresponding new format.\n",
       "Start the site move by configuring the server to redirect from the old URLs to the new ones.\n",
       "Monitor the traffic on both the old and new URLs.\n",
       "\n",
       "Another useful tool to test that everything's working from Google's perspective is the Fetch as Google tool (more info on that here).\",\"This question might belong in Stackoverflow and not here but you are including the complete stylesheets and scripts in each of your pages. That increases the download and computation time for each page significantly while the recommendation you speak of is only talking about less significant amounts of styling and code. Doing what you show can increase your page download size perhaps 10x or more.\n",
       "Imagine trying to load all your markup, styling and code while on a phone with a bad connection. Lots of sites can barely function serving up HTML and basic CSS, much less all that.\n",
       "What the suggestion is trying to do is get you to provide a minimal amount of markup to make the initial user experience usable as quickly as possible so you can add all the other fluff later.\n",
       "Now, is this time consuming and difficult? Yep. Depending on the site.\",...,\"The recent-developed Bibulous project makes this easy to implement. Taking the OP's example, we have a database file (readtag.bib) in which a readtag field is inserted into each entry:\n",
       "@article{Brooks2013,\n",
       "  author = {R. Brooks},\n",
       "  title = {Surprising fluidic characteristics of dihydrogen monoxide},\n",
       "  journal = {Journal of Fluidic Chemistry},\n",
       "  volume = 30,\n",
       "  month = 7,\n",
       "  year = 2013,\n",
       "  pages = {40-52},\n",
       "  readtag = {{}}\n",
       "}\n",
       "\n",
       "@article{Brown1999,\n",
       "  author = {N. Brown and M. Blum and E. Maruyama and P. Martinez},\n",
       "  title = {A novel, powerful spectrometric method for evaluating chromaticity},\n",
       "  journal = {Proceedings of the Royal Astrological Society},\n",
       "  month = 5,\n",
       "  year = 1999,\n",
       "  readtag = {+}\n",
       "}\n",
       "\n",
       "@inproceedings{Martinez2012,\n",
       "  author = {M. Martinez and R. T. Morrison},\n",
       "  title = {Which one is heavier: Separating facts from fiction},\n",
       "  booktitle = {17th International Conference of Boring Arguments},\n",
       "  month = 10,\n",
       "  year = 2012,\n",
       "  readtag = {-}\n",
       "}\n",
       "\n",
       "Next we can develop a style file (readtag.bst) that makes use of the readtag field:\n",
       "TEMPLATES:\n",
       "article = \\textbf{[<readtag>]<citenum>}. <au>, <title>. \\textit{<journal>}[ <volume>] ([<month.monthabbrev()> ]<year>)[, <startpage>--<endpage>|, <startpage>|, <eid>].\n",
       "inproceedings = \\textbf{[<readtag>]<citenum>}. <au>, <title>. \\textit{<booktitle>}[ <volume>] ([<month.monthabbrev()> ]<year>)[, <startpage>--<endpage>|, <startpage>|, <eid>].\n",
       "\n",
       "SPECIAL-TEMPLATES:\n",
       "citelabel = (<citenum>)\n",
       "readtag = -                     ## default tag is \"have not read\"\n",
       "\n",
       "OPTIONS:\n",
       "namelist_format = last_name_first\n",
       "\n",
       "Finally, we can use the readtag.tex file\n",
       "\\documentclass{article}\n",
       "\\makeatletter\n",
       "   \\renewcommand{\\@biblabel}[1]{}\n",
       "   \\renewcommand{\\@cite}[2]{{#1\\if@tempswa , #2\\fi}}\n",
       "\\makeatother\n",
       "\n",
       "\\begin{document}\n",
       "It has recently been discovered that water is wet\\cite{Brooks2013} and the sky is blue\\cite{Brown1999}, but it is still controversial whether a pound of iron is heavier than a pound of cotton\\cite{Martinez2012}.\n",
       "\n",
       "\\bibliographystyle{readtag}\n",
       "\\bibliography{readtag}\n",
       "\\end{document}\n",
       "\n",
       "The \\renewcommand{\\@cite}... line here is used here to allow the citation labels in the text to be different than those in the reference list. Compiling readtag.tex and replacing BibTeX with Bibulous as the bibliography engine gives the resulting formatted reference list\",\"By default, biblatex does not seem to support a supervisor, so some changes have to be made, but we can keep these changes to a minimum. \n",
       "Firstly, we define the new name list supervisor via a new datamodel (thesis.dbx)\n",
       "\\ProvidesFile{thesis.dbx}[2014/06/14 supervisor for theses]\n",
       "\\RequireBiber[3]\n",
       "\\DeclareDatamodelFields[type=list,datatype=name]{supervisor}\n",
       "\\DeclareDatamodelEntryfields[thesis]{supervisor}\n",
       "\n",
       "Save the lines above in a file called thesis.dbx and put it somewhere LaTeX can find it.\n",
       "In the MWE below, this is done automatically via filecontents.\n",
       "The datamodel needs to be loaded via the datamodel option (datamodel=thesis, e.g. \\usepackage[style=authoryear,backend=biber,datamodel=thesis]{biblatex}).\n",
       "Secondly, we need to declare the new strings you asked for in an .lbx file (the file should be called english-thesis.lbx)\n",
       "  \\ProvidesFile{english-thesis.lbx}[2014/06/14 english for thesis]\n",
       "  \\InheritBibliographyExtras{english}\n",
       "  \\NewBibliographyString{supervision,jointsupervision}\n",
       "  \\DeclareBibliographyStrings{%\n",
       "    inherit           = {english},\n",
       "    supervision       = {{under the supervision of}{under sup\\adddotspace of}},\n",
       "    jointsupervision  = {{under the joint supervision of}{under joint sup\\adddotspace of}},\n",
       "  }\n",
       "\n",
       "Make sure to save the file somewhere LaTeX can find it (as above: in the MWE below, the file is created with filecontents).\n",
       "We then employ this language variant via \\DeclareLanguageMapping{english}{english-thesis}.\n",
       "Finally, we define a new bibmacro\n",
       "\\newbibmacro*{thesissupervisor}{%\n",
       "  \\ifnameundef{supervisor}{}{%\n",
       "    \\ifnumgreater{\\value{supervisor}}{1}\n",
       "      {\\bibstring{jointsupervision}}\n",
       "      {\\bibstring{supervision}}\n",
       "    \\printnames{supervisor}}}\n",
       "\n",
       "that prints the supervisor and the introducing string depending on the number of supervisors.\n",
       "We then patch the @thesis driver to use our new macro (that is done with the awesome xpatch package).\n",
       "\\xpatchbibdriver{thesis}\n",
       "  {\\printfield{type}}\n",
       "  {\\printfield{type}\n",
       "   \\newunit\n",
       "   \\usebibmacro{thesissupervisor}}\n",
       "  {\\typeout{yep}}\n",
       "  {\\typeout{no}}\n",
       "\n",
       "The supervisor is now simply added to the supervisor field like so\n",
       "@thesis{geer,\n",
       "  author       = {de Geer, Ingrid},\n",
       "  title        = {Earl, Saint, Bishop, Skald~-- and Music},\n",
       "  type         = {phdthesis},\n",
       "  institution  = {Uppsala Universitet},\n",
       "  date         = 1985,\n",
       "  subtitle     = {The Orkney Earldom of the Twelfth Century. A Musicological\n",
       "                  Study},\n",
       "  location     = {Uppsala},\n",
       "  supervisor   = {James Oint and Stan Upervisor},\n",
       "}\n",
       "\n",
       "MWE\n",
       "\\documentclass[british,12pt,a4paper]{article}\n",
       "\\usepackage{filecontents}\n",
       "\\begin{filecontents*}{\\jobname.bib}\n",
       "@thesis{geer,\n",
       "  author       = {de Geer, Ingrid},\n",
       "  title        = {Earl, Saint, Bishop, Skald~-- and Music},\n",
       "  type         = {phdthesis},\n",
       "  institution  = {Uppsala Universitet},\n",
       "  date         = 1985,\n",
       "  subtitle     = {The Orkney Earldom of the Twelfth Century. A Musicological\n",
       "                  Study},\n",
       "  location     = {Uppsala},\n",
       "  supervisor   = {James Oint and Stan Upervisor},\n",
       "}\n",
       "\n",
       "@thesis{loh,\n",
       "  author       = {Loh, Nin C.},\n",
       "  title        = {High-Resolution Micromachined Interferometric Accelerometer},\n",
       "  type         = {mathesis},\n",
       "  institution  = {Massachusetts Institute of Technology},\n",
       "  date         = 1992,\n",
       "  location     = {Cambridge, Mass.},\n",
       "  supervisor   = {Stan Upervisor},\n",
       "}\n",
       "\\end{filecontents*}\n",
       "\\usepackage[utf8]{inputenc}\n",
       "\\usepackage{babel}\n",
       "\\usepackage{csquotes}\n",
       "\\usepackage{xpatch}\n",
       "\\usepackage[style=authoryear,backend=biber,datamodel=thesis]{biblatex}\n",
       "\\usepackage{hyperref}\n",
       "\n",
       "\\begin{filecontents*}{thesis.dbx}\n",
       "  \\ProvidesFile{thesis.dbx}[2014/06/14 supervisor for theses]\n",
       "  \\RequireBiber[3]\n",
       "  \\DeclareDatamodelFields[type=list,datatype=name]{supervisor}\n",
       "  \\DeclareDatamodelEntryfields[thesis]{supervisor}\n",
       "\\end{filecontents*}\n",
       "\n",
       "\\begin{filecontents*}{english-thesis.lbx}\n",
       "  \\ProvidesFile{english-thesis.lbx}[2014/06/14 english for thesis]\n",
       "  \\InheritBibliographyExtras{english}\n",
       "  \\NewBibliographyString{supervision,jointsupervision}\n",
       "  \\DeclareBibliographyStrings{%\n",
       "    inherit           = {english},\n",
       "    supervision       = {{under the supervision of}{under sup\\adddotspace of}},\n",
       "    jointsupervision  = {{under the joint supervision of}{under joint sup\\adddotspace of}},\n",
       "  }\n",
       "\\end{filecontents*}\n",
       "\n",
       "\\DeclareLanguageMapping{english}{english-thesis}\n",
       "\n",
       "\\newbibmacro*{thesissupervisor}{%\n",
       "  \\ifnameundef{supervisor}{}{%\n",
       "    \\ifnumgreater{\\value{supervisor}}{1}\n",
       "      {\\bibstring{jointsupervision}}\n",
       "      {\\bibstring{supervision}}\n",
       "    \\printnames{supervisor}}}\n",
       "\n",
       "\\xpatchbibdriver{thesis}\n",
       "  {\\printfield{type}}\n",
       "  {\\printfield{type}\n",
       "   \\newunit\n",
       "   \\usebibmacro{thesissupervisor}}\n",
       "  {\\typeout{yep}}\n",
       "  {\\typeout{no}}\n",
       "\n",
       "\\addbibresource{\\jobname.bib}\n",
       "\\nocite{*}\n",
       "\\begin{document}\n",
       "  \\printbibliography\n",
       "\\end{document}\n",
       "\n",
       "If you cannot create the new name list supervisor, you could try and abuse the editor field, then there is no need for the .dbx file.\n",
       "This solution should work with BibTeX as well.\n",
       "The macros thesissupervisor becomes\n",
       "\\newbibmacro*{thesissupervisor}{%\n",
       "  \\ifnameundef{editor}{}{%\n",
       "    \\ifnumgreater{\\value{editor}}{1}\n",
       "      {\\bibstring{jointsupervision}}\n",
       "      {\\bibstring{supervision}}\n",
       "    \\printnames{editor}}}\n",
       "\n",
       "You then give the supervisor in the editor field in the bib entry like so\n",
       "@thesis{geer,\n",
       "  author       = {de Geer, Ingrid},\n",
       "  title        = {Earl, Saint, Bishop, Skald~-- and Music},\n",
       "  type         = {phdthesis},\n",
       "  institution  = {Uppsala Universitet},\n",
       "  date         = 1985,\n",
       "  subtitle     = {The Orkney Earldom of the Twelfth Century. A Musicological\n",
       "                  Study},\n",
       "  location     = {Uppsala},\n",
       "  editor       = {James Oint and Stan Upervisor},\n",
       "}\n",
       "\n",
       "MWE\n",
       "\\documentclass[british,12pt,a4paper]{article}\n",
       "\\usepackage{filecontents}\n",
       "\\begin{filecontents*}{\\jobname.bib}\n",
       "@thesis{geer,\n",
       "  author       = {de Geer, Ingrid},\n",
       "  title        = {Earl, Saint, Bishop, Skald~-- and Music},\n",
       "  type         = {phdthesis},\n",
       "  institution  = {Uppsala Universitet},\n",
       "  date         = 1985,\n",
       "  subtitle     = {The Orkney Earldom of the Twelfth Century. A Musicological\n",
       "                  Study},\n",
       "  location     = {Uppsala},\n",
       "  editor       = {James Oint and Stan Upervisor},\n",
       "}\n",
       "\n",
       "@thesis{loh,\n",
       "  author       = {Loh, Nin C.},\n",
       "  title        = {High-Resolution Micromachined Interferometric Accelerometer},\n",
       "  type         = {mathesis},\n",
       "  institution  = {Massachusetts Institute of Technology},\n",
       "  date         = 1992,\n",
       "  location     = {Cambridge, Mass.},\n",
       "  editor       = {Stan Upervisor},\n",
       "}\n",
       "\\end{filecontents*}\n",
       "\\usepackage[utf8]{inputenc}\n",
       "\\usepackage{babel}\n",
       "\\usepackage{csquotes}\n",
       "\\usepackage{xpatch}\n",
       "\\usepackage[style=authoryear,backend=bibtex]{biblatex}\n",
       "\\usepackage{hyperref}\n",
       "\n",
       "\\begin{filecontents*}{english-thesis.lbx}\n",
       "  \\ProvidesFile{english-thesis.lbx}[2014/06/14 english for thesis]\n",
       "  \\InheritBibliographyExtras{english}\n",
       "  \\NewBibliographyString{supervision,jointsupervision}\n",
       "  \\DeclareBibliographyStrings{%\n",
       "    inherit           = {english},\n",
       "    supervision       = {{under the supervision of}{under sup\\adddotspace of}},\n",
       "    jointsupervision  = {{under the joint supervision of}{under joint sup\\adddotspace of}},\n",
       "  }\n",
       "\\end{filecontents*}\n",
       "\n",
       "\\DeclareLanguageMapping{english}{english-thesis}\n",
       "\n",
       "\\newbibmacro*{thesissupervisor}{%\n",
       "  \\ifnameundef{editor}{}{%\n",
       "    \\ifnumgreater{\\value{editor}}{1}\n",
       "      {\\bibstring{jointsupervision}}\n",
       "      {\\bibstring{supervision}}\n",
       "    \\printnames{editor}}}\n",
       "\n",
       "\\xpatchbibdriver{thesis}\n",
       "  {\\printfield{type}}\n",
       "  {\\printfield{type}\n",
       "   \\newunit\n",
       "   \\usebibmacro{thesissupervisor}}\n",
       "  {\\typeout{yep}}\n",
       "  {\\typeout{no}}\n",
       "\n",
       "\\addbibresource{\\jobname.bib}\n",
       "\\nocite{*}\n",
       "\\begin{document}\n",
       "  \\printbibliography\n",
       "\\end{document}\",\"\\documentclass[pstricks,border=2mm]{standalone}\n",
       "\\usepackage{pst-3dplot}\n",
       "\\begin{document}\n",
       "\n",
       "\\begin{pspicture}(-4.5,-1.5)(5,3.5)\n",
       " \\psset{unit=2cm,Alpha=70,Beta=15,fillstyle=solid}\n",
       "    %\\psgrid\n",
       "  \\pstThreeDCoor[linecolor=gray,xMin=0,xMax=2,yMin=0,yMax=2,zMin=0,zMax=1.5]\n",
       "  \\pstThreeDLine[linecolor=lightgray]{-}(1,-0.1,0)(1,0.1,0)\n",
       "  \\pstThreeDPut(1,-0.2,0.1){$1$}\n",
       "  \\pstThreeDLine[linecolor=lightgray]{-}(-0.1,1,0)(0.1,1,0)\n",
       "  \\pstThreeDPut(-0.3,1,0.1){$1$}\n",
       "  \\pstThreeDLine[linecolor=lightgray]{-}(-0.1,0,1)(0.1,0,1)\n",
       "  \\pstThreeDPut(-0.3,-0.3,1){$1$}\n",
       "  \\parametricplotThreeD[linecolor=black,linewidth=.5pt,xPlotpoints=200,plotstyle=curve,arrows=-](0,90)\n",
       "      { t cos 0 t sin }\n",
       "  \\pscustom[fillstyle=solid,fillcolor=red!50,opacity=0.5]{%Build a closed apth:\n",
       "    \\parametricplotThreeD[linecolor=black,%\n",
       "     linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,90){%\n",
       "     t cos t sin 0}\n",
       "    \\parametricplotThreeD[linecolor=black,linewidth=.5pt,xPlotpoints=200,%\n",
       "    plotstyle=curve,arrows=-](0,1){%\n",
       "     0 1 t}\n",
       "    \\parametricplotThreeD[linecolor=black,linewidth=.5pt,xPlotpoints=200,plotstyle=curve,arrows=-](90,0)\n",
       "      { t cos t sin t sin }\n",
       "}\n",
       "\n",
       "    \\parametricplotThreeD[linecolor=black,linewidth=.5pt,xPlotpoints=200,plotstyle=curve,arrows=-](0,1)\n",
       "      { 0 t 1}\n",
       "    \\parametricplotThreeD[linecolor=black,linewidth=.5pt,xPlotpoints=200,plotstyle=curve,arrows=-](0,1)\n",
       "      { 0 0 t}\n",
       "    \\psline[linewidth=0.5pt]{->}(1.2,-0.6)(0.5,-0.3)\n",
       "    \\put(1.3,-0.7){$x^2+y^2=1$}\n",
       "    \\psline[linewidth=0.5pt]{->}(-1.1,0.5)(-0.4,0.35)\n",
       "    \\put(-2.2,0.5){$x^2+z^2=1$}\n",
       "\\end{pspicture}\n",
       "\\end{document}\",\"If \\Rey is for the Reynolds number, then you should use a different definition:\n",
       "\\documentclass[presentation]{beamer}\n",
       "\\let\\Tiny\\tiny\n",
       "\\usepackage{hyperref}\n",
       "\\usetheme{Berkeley}\n",
       "\n",
       "\\newcommand\\Rey{\\mathit{Re}}\n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "\\section{A section}\n",
       "\\subsection{AAA $\\Rey$}\n",
       "\\begin{frame}\n",
       "  \\frametitle{Frame title1}\n",
       "  \\framesubtitle{frame subtitle1}\n",
       "  Some text s\n",
       "\\end{frame}\n",
       "\\end{document}\n",
       "\n",
       "If you want to avoid the warning about \n",
       "Token not allowed in a PDF string (PDFDocEncoding):\n",
       "removing `math shift'\n",
       "\n",
       "then say\n",
       "\\subsection{AAA \\texorpdfstring{$\\Rey$}{Re}}\n",
       "\n",
       "I'm not sure if the symbol should be typeset in italics, but it will be easy to change the definition to\n",
       "\\newcommand\\Rey{\\mathrm{Re}}\n",
       "\n",
       "in case you decide to print it upright.\",\"Package tikz maps the value of key text width to the macro content of \\tikz@text@width. The default is empty, thus you can reset the key text width by option setting text width={}:\n",
       "\\documentclass[a4paper,11pt,landscape]{article}\n",
       "\\usepackage{tikz,nopageno}\n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "\\newcommand{\\cost}{%\n",
       "  \\begin{tikzpicture}\n",
       "    \\node[\n",
       "      text width={},\n",
       "      text height={}, % dito\n",
       "      fill=red!85!black,\n",
       "      shape=circle,\n",
       "      inner sep=0.3mm,\n",
       "      draw=black,\n",
       "      text=white,\n",
       "    ] {\\raisebox{0pt}[\\height][0pt]{\\bf 1}};\n",
       "  \\end{tikzpicture}%\n",
       "}\n",
       "\n",
       "\\cost\n",
       "\n",
       "\\begin{tikzpicture}\n",
       "  \\node[text width=10mm] {\\cost};\n",
       "\\end{tikzpicture}\n",
       "\n",
       "\\end{document}\",\"This will remove the date. But note that this code will print only the 'Note', 'URL' and 'URLdate' fields in the bibliography for bib items of type 'online', even if there are other fields\n",
       "\\documentclass{scrartcl}\n",
       "\\usepackage[colorlinks, urlcolor=black]{hyperref} %if you want the link to be clickable\n",
       "\n",
       "\\usepackage{filecontents} \n",
       "\\begin{filecontents} {mwe.bib}\n",
       "@online{tab:test,\n",
       "  author                   = {Doe, John},\n",
       "  Url                      = {http://www.example.com/images/image.jpg},\n",
       "  Urldate                  = {2014-02-25},\n",
       "  Timestamp                = {2014.06.13},\n",
       "  Note                     = {Table \\ref{tab:test}}, %not sure I understand why you are cross-referencing here\n",
       "  year                     = {2014}, %this is necessary for the in-text citation to work\n",
       "}\n",
       "\\end{filecontents}\n",
       "\n",
       "\\usepackage[backend=biber, backref=true, style=authoryear, sorting=nty]{biblatex}\n",
       "\\addbibresource{mwe.bib}\n",
       "\n",
       "\\DeclareBibliographyDriver{online}{%this defines what will be printed in the bibliography for entries of type 'online'\n",
       "  \\usebibmacro{note+pages}\n",
       "  \\setunit{\\addperiod\\addspace}%Period+space after 'Note' field\n",
       "  \\usebibmacro{url+urldate}\n",
       "  \\usebibmacro{finentry}}\n",
       "\n",
       "\\begin{document}\n",
       "Example \\citeyear{tab:test} %This lets you cite only the year even if there is an author listed in your bib file, as opposed to \\cite\n",
       "\n",
       "\\printbibliography\n",
       "\\end{document}\",\"Duplicate shadows\n",
       "With key preaction another shadow can be inserted, e.g.:\n",
       "\\documentclass{article}\n",
       "\\usepackage{xcolor}\n",
       "\\usepackage{tikz}\n",
       "\\usetikzlibrary{shadows}\n",
       "\n",
       "\\begin{document}\n",
       "\\begin{tikzpicture}\n",
       "  \\fill[lightgray] (0,0) rectangle (2,2);\n",
       "  \\node[\n",
       "    fill=darkgray,\n",
       "    preaction={\n",
       "      drop shadow={\n",
       "        fill=white,\n",
       "        opacity=1,\n",
       "        shadow xshift=-.5mm,\n",
       "        shadow yshift=.5mm,\n",
       "      },\n",
       "    },  \n",
       "    drop shadow={\n",
       "      fill=black,\n",
       "      opacity=1,\n",
       "      shadow xshift=.5mm,\n",
       "      shadow yshift=-.5mm\n",
       "    },\n",
       "  ] at (1,1) {\\rule{8mm}{0mm}\\rule{0mm}{8mm}};\n",
       "\\end{tikzpicture}\n",
       "\\end{document}\n",
       "\n",
       "This can be further simplified. drop shadow uses general shadow, which is already implemented via preaction. Thus the preaction part can be dropped:\n",
       "  \\node[\n",
       "    fill=darkgray,\n",
       "    drop shadow={ \n",
       "      fill=white, \n",
       "      opacity=1,  \n",
       "      shadow xshift=-.5mm,\n",
       "      shadow yshift=.5mm, \n",
       "    },\n",
       "    drop shadow={\n",
       "      fill=black,\n",
       "      opacity=1, \n",
       "      shadow xshift=.5mm,\n",
       "      shadow yshift=-.5mm\n",
       "    },\n",
       "  ] at (1,1) {\\rule{8mm}{0mm}\\rule{0mm}{8mm}};\n",
       "\n",
       "Also a style can be defined to simplify the use case, e.g.:\n",
       "\\documentclass{article}\n",
       "\\usepackage{xcolor}\n",
       "\\usepackage{tikz}\n",
       "\\usetikzlibrary{shadows}\n",
       "\n",
       "\\pgfkeysdefnargs{/my/get value}{2}{%\n",
       "  \\pgfkeysgetvalue{#1}{#2}%\n",
       "}\n",
       "\\tikzset{\n",
       "  /tikz/drop raised shadow/.style={\n",
       "    drop shadow={%\n",
       "      fill=white,\n",
       "      opacity=1,\n",
       "      #1,%\n",
       "      /my/get value={/tikz/shadow xshift}{\\xshift},\n",
       "      /my/get value={/tikz/shadow yshift}{\\yshift},\n",
       "      shadow xshift=-\\xshift,\n",
       "      shadow yshift=-\\yshift,\n",
       "    },\n",
       "    drop shadow={%\n",
       "      fill=black,\n",
       "      opacity=1,\n",
       "      #1%\n",
       "    }\n",
       "  }  \n",
       "}\n",
       "\n",
       "\\begin{document}\n",
       "\\begin{tikzpicture}\n",
       "  \\fill[lightgray] (0,0) rectangle (2,2);\n",
       "  \\node[\n",
       "    fill=darkgray,\n",
       "    drop shadow={\n",
       "      fill=white,\n",
       "      opacity=1,\n",
       "      shadow xshift=-.5mm,\n",
       "      shadow yshift=.5mm,\n",
       "    },\n",
       "    drop shadow={\n",
       "      fill=black,\n",
       "      opacity=1,\n",
       "      shadow xshift=.5mm,\n",
       "      shadow yshift=-.5mm\n",
       "    },\n",
       "  ] at (1,1) {\\rule{8mm}{0mm}\\rule{0mm}{8mm}};\n",
       "\\end{tikzpicture}\n",
       "\\quad \n",
       "\\begin{tikzpicture}\n",
       "  \\fill[lightgray] (0,0) rectangle (2,2);\n",
       "  \\node[\n",
       "    fill=darkgray,\n",
       "    drop raised shadow={\n",
       "      shadow xshift=.5mm,\n",
       "      shadow yshift=-.5mm,\n",
       "    },\n",
       "  ] at (1,1) {\\rule{8mm}{0mm}\\rule{0mm}{8mm}};\n",
       "\\end{tikzpicture}\n",
       "\n",
       "\\end{document}\n",
       "\n",
       "Text shadows\n",
       "The following version without package tikz sets the text three times, first in color white, moved to the left and raised, then in color black to the right and below, finally the the text:\n",
       "\\documentclass{article}\n",
       "\\usepackage{xcolor}\n",
       "\n",
       "\\newcommand*{\\RaisedText}[1]{%\n",
       "  \\begingroup\n",
       "    \\leavevmode\n",
       "    \\rlap{\\kern-.1pt\\raise.1pt\\hbox{\\color{white}#1}}%\n",
       "    \\rlap{\\kern.1pt\\raise-.1pt\\hbox{\\color{black}#1}}%\n",
       "    \\hbox{#1}%\n",
       "  \\endgroup\n",
       "}\n",
       "\n",
       "\\begin{document}\n",
       "\\colorbox{lightgray}{%\n",
       "  \\color{darkgray}\\RaisedText{Raised Text}%\n",
       "}\n",
       "\\end{document}\n",
       "\n",
       "A variant with package pdfrender:\n",
       "\\documentclass{article}\n",
       "\\usepackage{xcolor}\n",
       "\\usepackage{pdfrender}\n",
       "\n",
       "\\newcommand*{\\RaisedText}[1]{%\n",
       "  \\begingroup\n",
       "    \\leavevmode\n",
       "    \\rlap{\\kern-.1pt\\raise.1pt\\hbox{%\n",
       "      \\pdfrender{\n",
       "        TextRenderingMode=Stroke,\n",
       "        LineWidth=.2pt,\n",
       "        StrokeColor=white,\n",
       "      }#1%\n",
       "    }}%\n",
       "    \\rlap{\\kern.1pt\\raise-.1pt\\hbox{%\n",
       "      \\pdfrender{\n",
       "        TextRenderingMode=Stroke,\n",
       "        LineWidth=.2pt,\n",
       "        StrokeColor=black,\n",
       "      }#1%\n",
       "    }}%\n",
       "    \\rlap{%\n",
       "      \\pdfrender{\n",
       "        TextRenderingMode=Stroke,\n",
       "        LineWidth=.2pt,\n",
       "      }#1%\n",
       "    }%\n",
       "    \\hbox{#1}%\n",
       "  \\endgroup\n",
       "}\n",
       "\n",
       "\\begin{document}\n",
       "\\colorbox{lightgray}{%\n",
       "  \\color{darkgray}\\RaisedText{Raised Text}%\n",
       "}\n",
       "\\end{document}\",\"Your \\ref should point to the label of the table caption, not to the bibitem itself. This should resolve the reference:\n",
       "  \\documentclass{scrartcl}\n",
       "\\usepackage[colorlinks, urlcolor=black, linkcolor=black]{hyperref} %if you want the link to be clickable\n",
       "\n",
       "\\usepackage{filecontents} \n",
       "\\begin{filecontents} {mwe.bib}\n",
       "@online{tab:test,\n",
       "  author                   = {Doe, John},\n",
       "  Url                      = {http://www.example.com/images/image.jpg},\n",
       "  Urldate                  = {2014-02-25},\n",
       "  Timestamp                = {2014.06.13},\n",
       "  Note                     = {Table \\ref{tab1}}, %<== this should point to your table caption label\n",
       "  year                     = {2014}, %this is necessary for the in-text citation to work\n",
       "}\n",
       "\\end{filecontents}\n",
       "\n",
       "\\usepackage[backend=biber, backref=true, style=authoryear, sorting=nty]{biblatex}\n",
       "\\addbibresource{mwe.bib}\n",
       "\n",
       "\\DeclareBibliographyDriver{online}{%this defines what will be printed in the bibliography for entries of type 'online'\n",
       "  \\usebibmacro{note+pages}\n",
       "  \\setunit{\\addperiod\\addspace}%Period+space after 'Note' field\n",
       "  \\usebibmacro{url+urldate}\n",
       "  \\usebibmacro{finentry}}\n",
       "\n",
       "\\begin{document}\n",
       "\\begin{table}\n",
       "\\caption{Figure legend here (\\citeyear{tab:test})}{\\label{tab1}} %<== this is what your \\ref should be pointing to\n",
       "\\end{table}\n",
       "Example \\citeyear{tab:test} %This lets you cite only the year even if there is an author listed in your bib file, as opposed to \\cite\n",
       "\n",
       "\\printbibliography\n",
       "\\end{document}\n",
       "\n",
       "EDIT:\n",
       "Adding this will remove URL:\n",
       "\\DeclareFieldFormat{url}{%\n",
       "    {\\url{#1}}}\",\"The only way I can think of a solution is to close the current two column mode, and the start a new one. You need to use the multicol package for that purpose. \n",
       "I failed to solve the problem when you use a global two column mode.\n",
       "\n",
       "In this example, lipsum and xampl are for dummy texts and bibliographies only.\n",
       "\\documentclass[11pt,a4paper]{article} \n",
       "\n",
       "\\usepackage{lipsum}\n",
       "\n",
       "\\usepackage{multicol}\n",
       "\n",
       "\\begin{document}\n",
       "\n",
       "\\begin{multicols}{2}\n",
       "  \\lipsum[1]\n",
       "  \\nocite{*}\n",
       "\\end{multicols}\n",
       "\n",
       "\\begin{multicols}{2}\n",
       "  \\bibliographystyle{plain}\n",
       "  \\bibliography{xampl}\n",
       "\\end{multicols}\n",
       "\n",
       "\\end{document}\n",
       "\n",
       "Here is the output.\",\"I found a solution that works for both tex4ht and pdflatex to vertically align an image in a table cell.\n",
       "For pdf, I'll use m since it works with little extra effort. For tex4ht I had to resort to using \\Css configuration after much trial and error. \n",
       "Then in the latex file, I check if I am running tex4ht or pdflatex and set things accordingly. Here is the output from the same latex file for pdf and html\n",
       "\n",
       "Here is the code\n",
       "\\documentclass[11pt]{article}        \n",
       "\\usepackage{mwe}\n",
       "\\usepackage{longtable}\n",
       "\\usepackage{array}\n",
       "\\newcommand{\\mytext}{bla bla bla bla bla bla bla bla bla bla bla bla\n",
       "  bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla\n",
       "  bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla}\n",
       "\\usepackage{hyperref}\n",
       "\n",
       "\\begin{document}    \n",
       "\\ifdefined\\HCode\n",
       "\\Css{td img {display:block; margin: 0 auto;}}  \n",
       "\\Css{td {vertical-align: middle;}}\n",
       "\\begin{longtable}{|p{1in}|p{1in}|p{2in}|}\\hline\n",
       "\\else\n",
       "\\begin{longtable}{|m{1in}|m{1in}|m{2in}|}\\hline\n",
       "\\fi    \n",
       "\\mytext &\\mytext & \n",
       "\\includegraphics[scale=0.25]{example-image}%\n",
       "\\\\\\hline\n",
       "\\end{longtable}\n",
       "\n",
       "\\end{document} \n",
       "\n",
       "To run with tex4ht, the command is\n",
       "  pdflatex foo.tex\n",
       "\n",
       "texlive 2013, Linux mint\"]\n",
       "meta: [\"{'file': 'webmasters.stackexchange_0000093760.txt'}\",\"{'file': 'webmasters.stackexchange_0000093144.txt'}\",\"{'file': 'webmasters.stackexchange_0000093801.txt'}\",\"{'file': 'webmasters.stackexchange_0000093791.txt'}\",\"{'file': 'webmasters.stackexchange_0000093822.txt'}\",\"{'file': 'webmasters.stackexchange_0000093827.txt'}\",\"{'file': 'webmasters.stackexchange_0000093837.txt'}\",\"{'file': 'webmasters.stackexchange_0000093843.txt'}\",\"{'file': 'webmasters.stackexchange_0000093852.txt'}\",\"{'file': 'webmasters.stackexchange_0000093826.txt'}\",...,\"{'file': 'tex.stackexchange_0000184634.txt'}\",\"{'file': 'tex.stackexchange_0000184848.txt'}\",\"{'file': 'tex.stackexchange_0000184864.txt'}\",\"{'file': 'tex.stackexchange_0000179000.txt'}\",\"{'file': 'tex.stackexchange_0000184890.txt'}\",\"{'file': 'tex.stackexchange_0000184881.txt'}\",\"{'file': 'tex.stackexchange_0000184897.txt'}\",\"{'file': 'tex.stackexchange_0000184815.txt'}\",\"{'file': 'tex.stackexchange_0000184898.txt'}\",\"{'file': 'tex.stackexchange_0000184886.txt'}\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(lance_batches))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>question</th><th>answer</th><th>meta</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Does image on the page improve…</td><td>&quot;Yes and no.\n",
       "It is content that…</td><td>&quot;{&#x27;file&#x27;: &#x27;webmasters.stackexch…</td></tr><tr><td>&quot;What happens with WHO IS priva…</td><td>&quot;The short answer is that it de…</td><td>&quot;{&#x27;file&#x27;: &#x27;webmasters.stackexch…</td></tr><tr><td>&quot;Using established domain names…</td><td>&quot;There are a few things you sho…</td><td>&quot;{&#x27;file&#x27;: &#x27;webmasters.stackexch…</td></tr><tr><td>&quot;PostmarkApp inbound domain cre…</td><td>&quot;You need to add the MX record …</td><td>&quot;{&#x27;file&#x27;: &#x27;webmasters.stackexch…</td></tr><tr><td>&quot;does google webmaster track cl…</td><td>&quot;Yes you can track these click …</td><td>&quot;{&#x27;file&#x27;: &#x27;webmasters.stackexch…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────────────────────┬──────────────────────────────────┬─────────────────────────────────┐\n",
       "│ question                    ┆ answer                           ┆ meta                            │\n",
       "│ ---                         ┆ ---                              ┆ ---                             │\n",
       "│ str                         ┆ str                              ┆ str                             │\n",
       "╞═════════════════════════════╪══════════════════════════════════╪═════════════════════════════════╡\n",
       "│ Does image on the page      ┆ Yes and no.                      ┆ {'file': 'webmasters.stackexch… │\n",
       "│ improve…                    ┆ It is content that…              ┆                                 │\n",
       "│ What happens with WHO IS    ┆ The short answer is that it de…  ┆ {'file': 'webmasters.stackexch… │\n",
       "│ priva…                      ┆                                  ┆                                 │\n",
       "│ Using established domain    ┆ There are a few things you sho…  ┆ {'file': 'webmasters.stackexch… │\n",
       "│ names…                      ┆                                  ┆                                 │\n",
       "│ PostmarkApp inbound domain  ┆ You need to add the MX record …  ┆ {'file': 'webmasters.stackexch… │\n",
       "│ cre…                        ┆                                  ┆                                 │\n",
       "│ does google webmaster track ┆ Yes you can track these click …  ┆ {'file': 'webmasters.stackexch… │\n",
       "│ cl…                         ┆                                  ┆                                 │\n",
       "└─────────────────────────────┴──────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_batch = pl.from_arrow(batch)\n",
    "pl_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Yes and no.\\nIt is content that ranks and image tags offer some to the total performance of the page, however, it is limited.\\nWhere your friend is right is engagement. Google looks for elements that offer engagement. An image is the most basic engagement there is. At one point, Google was not shy about saying that an image should exist at the top of the content. SEO's agreed for several reasons and so for almost every page you will see on the web, there is an image at the top of the content.\\nSo besides the value of the alt text and any potential that the path and file name give, Google credits the image as engagement. However, this too is limited.\\nFrom a user experience (UX) perspective, it does cause a pause in the users visit and may entice the user to read your content. It is just one element, however, it is the UX effect that is the greatest. It can extend the time spent on page and reduce bounce rates somewhat which are other metrics Google can use to rank a page.\",\n",
       " 'The short answer is that it depends on how you chose to protect your privacy. Terms vary by Registrar and by extension. For example, .je domains never reveal anything more than the Registrant\\'s name regardless of the seller\\'s policy - it is a decision made by the Registry managing the extension.\\nThe TL;DR is \"Choose the TLD and  Registrar wisely\".\\nIf you purchased a third-party WHOIS protection service (note it\\'s spelled as one word, all caps, \\'WHOIS\\' as per RFC 3912) then the provider\\'s terms govern that separate purchase, which is a different transaction from the domain name purchase. There are multiple WHOIS proxy/anonymity providers and you need to check the specific Terms for each to learn the answer.\\nOne thing to watch out for with this arrangement is that you are typically registering the domain by proxy. You are not the domain owner, the proxy service owns the domain. As an example, here is GoDaddy\\'s Terms & Conditions page which makes this clear. They own the domain, you pay the middle-man proxy to purchase and administer it. There are legal implications to this that you should talk to a lawyer about if you are serious about monetizing the domain.\\nAnother approach which gives you more rights and control is to choose a Registrar that provides anonymity service as standard and by default. It\\'s all one transaction, no third-party involvement or extra purchase to make. One I use and recommend is Gandi.net (no affiliation, just a customer) and they protect your data and never sell it by default. You are the Registrant and you directly own the domain, the Gandi privacy scheme is not a proxy service.',\n",
       " \"There are a few things you should know.\\n1] Most websites are poorly optimized.\\n2] Most sites with SEO help are still poorly optimized.\\n3] Terms found in a URL domain name are significant.\\n4] Terms found within content can easily overcome the lack of the same terms within the URL domain name.\\n5] Keywords, in of themselves, is not how search works.\\n6] Most keywords people chase are not actually used in search.\\nMost of the time, it does not pay to try and compete directly against another web site because most are not optimized well. Playing catch-up and competing directly against a poorly optimized website often leads you in the wrong direction.\\nWhat matters is how people actually search.\\nIt is difficult to figure out what people search for. Many SEOs will swear by certain keywords. In my comments, I lament the use of the term best by SEOs when in fact, most people do not use the term to search. When was the last time you entered the term best in a search query? You may very well never have. And that is the point. Do a search for best chocolate chip cookie and the results will be staggering. The number of sites that try and compete for a useless search term makes the useless search term effectiveness completely moot.\\nSearch is about whole language. RankBrain and Google's index are semantic based and not term based. In otherwords, semantics is used to understand the query and any site/page topic and make an appropriate match. It is not a search term match, it is a semantics topic match along with syntactic analysis. The analysis gets very deep indeed, however, I will not be going into that here. I have written about semantics in enough other answers you can read them if you want.\\nOne of the things people do not realize is that certain terms are less effective in search for some sites. I used the example of adjectives in my comments. Adjectives are important, however, they are often less important in search with some exception. For example, a red car makes it clear that adjectives are important. However, when was the last time you searched for a red anything? This example dove-tails into your examples which I realize are just off the top of your head. Granted. In your example, black and cheap are likely fairly useless. Anyone searching for a car will likely not use these terms. For this reason, these are likely useless terms for a URL domain name.\\nIf you were to search for a car, what would you search for?\\nThis is the toughest nut to crack. Ask a few friends to do searches on the net and keep track of what search queries they use and what SERP links and snippets they preferred. What you will likely find is that the searches are simple, short, and devoid of adjectives except for a few. You can give them specific tasks such as finding a low-cost black pick-up truck. Be careful not to implant into their minds search term suggestions that may skew the results.\\nYou can use the SERP links and snippets to better understand why these sites stand out. Do not do this yourself. It is okay to be around and ask questions as your friends search. You may learn more that way.\\nWhat you may find is that regardless of the use of the search term black, the results may not yield results for a black pick-up truck.\\nFor example, yesterday, I was searching for Triumph motorcycles. I was looking for dealers specifically. I was looking for specific years and models and other specific information. Most all of my searches did not result in what I was looking for. For example, I did not find one dealer but primarily websites that people use to post their ad to sell their motorcycle. I found one dealer that had not sold Triumphs in years and did not contain the term triumph on their site at all. As well, searching for models worked fairly well. Year did not. Many of my results often showed Harley Davidson motorcycles and not Triumphs. As well, since I was looking for an older Triumph, mostly new Triumphs were shown regardless of what I did. Even then, not one of them was cheap. All results were for premium priced Triumphs well over the market value. Very frustrating for someone who simply wants a reliable older Triumph from a reputable dealer at a reasonable price. The terms reliable, older, reasonable, vintage, used, pre-owned, etc., all were relatively useless.\\nFor the record, E-commerce sites are a bit different. You will find adjectives within the product page/description and not within a domain name, path, file name, etc. This is an important point because this is not where you would expect to find them. If you are trying to boost performance for an e-commerce site, you will be working a bit differently than for a blog.\\nUsing your example, adjectives can better perform within the h1 or lesser header tag, the product description, and within any store-front form element. As well, I discovered that using a few h3 and h4 header tags can weigh fairly heavily if done well. For example, for a list of products, the lower header tags can contain details such as color etc. H2 tags are often used for this. Adding an appropriate h3 header along with a paragraph tag below each h2 may help.\\nWhat you do will depend upon your site. I am not suggesting using header tags to boost search results for every situation. I was just using that as an example.\\nWhat is important is to use whole language. If you are selling a black pick-up truck, then say so. For example, Chevy 1500 pick-up truck, black exterior, black interior with 5.0 liter v8 engine and automatic transmission. Best in class fuel efficiency and towing. within a header tag should help. These content elements should appear elsewhere such as within a product description paragraph below the header tag to support the semantic weight.\\nHowever, this is not the end of the story. Site trust metrics and quality organic links as well as many other metrics such as CTR (click-through rate) and bounce rate also influence search.\\nThis is how sites compete in search.\\nWithout a realistic example, I cannot give good solid advice on how better to compete. Hypothetical scenarios do not always lend themselves to better answers. Sorry. Hopefully, I gave you some ideas anyhow.\",\n",
       " 'You need to add the MX record to your domain first - go to your domain\\'s DNS records (wherever these are managed), and add an MX record for your subdomain to point its mail to inbound.postmarkapp.com with a priority of 10.\\nFor example:\\nsub.john-john.com MX inbound.postmarkapp.com 10\\nIn Postmark\\'s documentation, inbound.postmarkapp.com isn\\'t just an example - it\\'s the address of their inbound mail server and is where you need to point your MX to - and is also where they ask all of their inbound customers to point their MX records to.\\nHow will Postmark then know where to deliver your mail to? They\\'ll deliver it based on where it is addressed. Firstly, the sending server will do a DNS lookup of your domain, find inbound.postmarkapp.com, and then contact Postmark\\'s server to say \"I\\'ve got an e-mail for john@sub.john-john.com, can you accept that?\" Because your subdomain is indeed set up with Postmark, Postmark will then accept the message and deliver it accordingly.\\nAs for what to call your subdomain, if you own your own domain, it\\'s completely up to you. I\\'ve just used sub and john-john.com in this example, but replace that with whatever your domain is and whatever you would like your subdomain to be. :)',\n",
       " 'Yes you can track these click from Google webmaster (Search console)\\nFirst let me breakdown all the process and explain what is deeplinking and app indexing.\\nApp indexing appears on Google when someone search for the brand name or anything related to it, your app will appear from google play or itunes with \"Download\" link.\\nDeep linking appears on any page of your website and you can open the app through the app if your app/website support deeplinking.\\nTo track deeplinking click and app installs you have to add your application on Google webmaster as a new property then verify your website from Google play, here you will have a connection from the website to the app and follow all the instructions to support deep linking.\\nafter one week Google will start give you the numbers in search console (installs and pages) and you can track everything you want.\\nThis resource will help you from google\\nhttps://developer.android.com/training/app-indexing/deep-linking.html',\n",
       " 'Do a 301 redirect.\\nIn your case Google won\\'t see it as Black Hat. \\nThe only way to get a \"Black Hat\" mark with a 301 is when you\\'re doing \"chain redirection\" (i.e. you\\'re redirecting from a first to a second to a third domain etc.).\\nIn the situation you\\'ve described, there won\\'t be any penalty for this redirection.',\n",
       " 'The page is only fully loaded when all linked resources (including <script async defer) have loaded. This is when the onload event fires.\\nHowever, the DOMContentLoaded event is likely to fire before this - which will probably be before the async defer scripts have loaded. (I say \"probably\" - if the browser is able to determine that it can load the script in another thread at the same time without slowing things down then I guess it probably will; but with no guarantee.)',\n",
       " \"Your question totally depends on how you want to solve it :\\nUse single page for puzzle + its variants\\nIf you have multiple variation of same puzzle, then you can showcase them on single page**\\nhttp://www.puzzlopia.com/puzzles/neutralizator/play\\n\\nshow all variants of above puzzle on above page only\\n\\nMultiple Pages For Variants :\\n1) Primary / Main Page rank only\\nIf different pages are required, then do you want them to be indexed/searchable separately in google index or you want to get the main page ranked and how variants links on the main page\\nExample :\\nPuzzle 1 page is ranked and all variants links are shown on this page.\\nThen setting canonical on variants to main puzzle page will not confuse google as setting canonical indicates that the content is attributed to main page or is similar to the main page\\nIn this case only the main puzzle page will be indexed by google and it will get seo extract from all variants as well.\\n2) Variants to be ranked separately \\nIf you want all variants also to rank separately.\\nYes this can also be done by setting proper namespacing in the urls i.e. don't make urls like: puzzle/1/1 puzzle/1/2\\nMaking url names meaningful help you index your content better:\\nSo make them like : puzzle/rubik/three-cross-three , puzzle/rubik/four-cross-four ......\\nAlso the major content on page title, description, h1 etc should differ for all pages.\\nSome content should be different and some of the content i.e. the base content can be same.\\nFollowing above + other seo practices you can rank individual variants as well.\",\n",
       " \"You will need to renew your old SSL certificate (or grab a free trial SSL, such as this one from Comodo).\\nAlthough I can't find a reference right now, Google is not going to look kindly on a 301 beneath an expired certificate, if it even gets to that point at all. As far as Google's concerned, your site could be compromised and the 301 can't be trusted.\\nAside from the SSL issue, a 301 redirect is exactly the way to go in this case.\\n\\nEDIT - Adding further information from discussion in the comments:\\nFurther, although not entirely neccessary because the 301 redirects will pick it up, you can use the Change of address tool in the Google Search Console (Webmaster Tools). It's worth noting that 301 redirects are still required in order for this to work.\\nGoogle also has a decent overview of the URL changing process in their help centre. It goes into detail on four steps:\\n\\nPrepare the new site and test it thoroughly.\\nPrepare a URL mapping from the current URLs to their corresponding new format.\\nStart the site move by configuring the server to redirect from the old URLs to the new ones.\\nMonitor the traffic on both the old and new URLs.\\n\\nAnother useful tool to test that everything's working from Google's perspective is the Fetch as Google tool (more info on that here).\",\n",
       " 'This question might belong in Stackoverflow and not here but you are including the complete stylesheets and scripts in each of your pages. That increases the download and computation time for each page significantly while the recommendation you speak of is only talking about less significant amounts of styling and code. Doing what you show can increase your page download size perhaps 10x or more.\\nImagine trying to load all your markup, styling and code while on a phone with a bad connection. Lots of sites can barely function serving up HTML and basic CSS, much less all that.\\nWhat the suggestion is trying to do is get you to provide a minimal amount of markup to make the initial user experience usable as quickly as possible so you can add all the other fluff later.\\nNow, is this time consuming and difficult? Yep. Depending on the site.',\n",
       " \"What would you recommend, to keep products docs on the same site as\\n  products information itself or to create separate domain for docs?\\n\\nI'm doing all on the same domain because of the easier organisation and for better user experience.\\nFor example if you have 3 products in your company, it's better to have documentation linked on that products site (eg. www.site.com/product1/doc.pdf than www.anothersite.com/product1.pdf)\\nIf you need bigger organization of documents, than put it like\\nwww.site.com/docs/doc1.pdf etc.\\nBut... there's a way with no follow and iframes. \\nFurther reading: https://moz.com/blog/the-hidden-power-of-nofollow-links\",\n",
       " \"Country code top-level domains (ccTLD's) such as .tk (Tokelau, New Zealand) and .ga (Gabon) have extra weight in their own country's search engine (like google.tk and google.ga for instance) but that is as far as it goes. \\nThat being said however, some ccTLD are considered gTLD (generic) by Google, .tk being one of them as can be seen under the 'More about domain determination' section here (thanks to Stephen in the comments). ccTLD's are an extra signal for Google to perceive the site as likely being more relevant to users in those regions/countries.\\nGoogle's John Mueller said this on the matter just in July 2015.\\n\\nQ: What about real ccTLDs (country code top-level domains) : will\\n  Google favour ccTLDs (like .uk, .ae, etc.) as a local domain for people\\n  searching in those countries?\\nA: By default, most ccTLDs (with exceptions) result in Google using these to geotarget the website; it tells us that the website is probably more relevant in the appropriate country. Again, see our help centre for more information on\\n  multi-regional and multilingual sites.\",\n",
       " 'As stated on SO:\\n\\nThe browsers are OK with it. However,\\n  how the application library parses it\\n  may vary.\\nPrograms are supposed to group\\n  identically named items together.\\n  While the HTML specification doesn\\'t\\n  explicitly say this, it is implicitly\\n  stated in the documentation on\\n  checkboxes:\\n\"Several checkboxes in a form may share\\n  the same control name. Thus, for\\n  example, checkboxes allow users to\\n  select several values for the same\\n  property.\"\\n\\nOn a side note, in PHP you can have form fields with the same name if you use PHP\\'s arry syntax for the  field name:\\n<form>\\n<input name=\"email[]\" />\\n<input name=\"email[]\" />\\n<input name=\"email[]\" />\\n</form>\\n\\nThis will cause the browser to send over the form fields in a syntax that PHP will use to populate an array for these fields: $_REQUEST[\\'email\\']. If you use PHP this is the preferred way to handle it.',\n",
       " \"The key really to ensuring your mail server does not get blacklisted for spamming is to make sure that you only send emails to people who want them and have asked for them, and if you include a working unsubscribe link people can leave your mailing list without any issues, since the main reasons servers get blacklisted are for high numbers of unwanted email messages and unsubscribe links that don't work properly.\\nIn answer to your question the best time to stop using your own server and pay for a professional service is probably when one or more of the following are true:\\n\\nthe server is also used for your companies corporate email accounts and so if blacklisted would have a detrimental impact on your day-to-day business communications;\\nthe bandwidth usage required to send all the messages impacts on your business;\\nyour server seems to frequently get blacklisted and you wish another company to manage the process (including the time and cost) of getting unblacklisted and to at any one time always have a pool of good IP addresses from which they can send emails to maintain a working service for you;\\nyour server fails to respond to its other core purposes while sending out email campaigns, for example if it is also responsible for your website or webmail hosting;\\nthe features of an external service are worth the cost (such as WYSIWYG editors, templating, HTML and Plain Text emailing, reporting tools etc.);\\nthe cost of outsourcing works out to be cheaper than the costs involved in running your own server;\\nyou worry about the security of your server and do not have the expertise to ensure it is kept backed-up, software and anti-virus kept up-to-date, and you are not sure if you would even know if the email contact lists are sufficiently protected from hackers, and therefore the security and privacy features of an external service provide peace-of-mind;\\nthe frequency of mail-outs is such that your server has not finished sending out the first email campaign when it must already begin the next.\\n\\nObviously there could be many more reasons but perhaps this covers the more prominent. If keeping costs low is the most important priority then paying for a professional service may never help you toward this objective.\",\n",
       " 'While most web hosts have fairly robust systems, not all web hosts have robust DNS servers. By contrast, most all registrars do have robust DNS servers as a requirement to do business. It is not uncommon for someone to use the web host DNS servers over the registrars DNS servers and be disappointed.\\nAs well, it is often far simpler to set-up your DNS records on the registrars systems due to better control panels. In order to use the web host DNS, at least one other step is required. For anyone who is not that familiar with DNS that chooses to use the web host DNS, future confusion as to which DNS servers is the SOA (statement of authority), the web host or registrar, is very common. We get questions related to the confusion here a lot!\\nLastly, some web hosts do require that you use their DNS. That is something to check out. If the web host does not require you to use their DNS servers, then I would stick with the registrars DNS for these simple reasons; it is faster, cheaper, simpler, less confusing, performs better, and does not require two fees and accounts just to keep the domain name alive if you chose to go a different route.',\n",
       " \"This is actually a well known and very old problem with social network referrals and some fields being undefined. It's largely referred to as 'dark social media traffic'. It's a term that covers other lost forms of social referral but this is one of the core issues.\\nAn example of a contributing factor to 'Dark Social Traffic' is Facebook's referral protection - which uses a redirect to hide which page you've been referred from and therefore mask your/others Facebook accounts. This can cause Google to drop information depending on which broken loop is used to spit the client out. Facebook is a development mess. When you add an extra dimension to the Analytics query - you're asking the database for all records which have both values. If one is missing it can cause records to be dropped from the batch. Many Analytics sections have an 'unset' or 'undefined' aggregation for these. Ones that don't will cause the numbers to change and information to be dropped.\\nWithout Analytics Premium you're working with smaller random samples and having stats extrapolated, you can get some really odd numbers. It fakes consistency by recalling past estimates where possible but overall you are working with something closer to a divining rod than a measuring stick if you're looking for absolute numbers. Try to work in relative terms with Analytics.\",\n",
       " \"AFAIK you won't get any pricing data from the event.  But you can get the line item and potentially other useful info from the event.  You can query the DFP reporting (even without premium) to get the impressions/revenue for that line item.  \\nIf you have other key-value data you pass through, you can get more detailed info about revenue broken apart by the different KV.\",\n",
       " \"I doubt this will give you any problems (if anything, it should be a net benefit as it will reduce bounce). If there are any issues with inbounds not seeing the content they expect, it won't be a problem for long.  Assuming you have pagination links set up properly and/or use sitemaps, Google will adjust its index in due course and begin directing users to the correct page. \\nIf I were you, I would make sure the pagination navigation is easy to see and also consider allowing a single-page view as a navigation option.  That would give you the best of both worlds.\",\n",
       " \"The AdsBot crawler with user agent AdsBot-Google (+http://www.google.com/adsbot.html), is part of AdWords not AdSense. It is supposedly used for checking landing page content to determine quality score.\\nThis bot appears to also visit pages that are not ad landing pages, including pages with no ads on them, no ads to them, and no links to them. Based on the query string parameters it tries to use, I suspect that it may be getting its URL's from Analytics, although I have no idea why Google is using its AdBot to crawl pages that are logged in Analytics but aren't related to advertising in any way.\\nIf you aren't running any ads on the page or to the page, you might try protecting the page with a conditional 410 Gone response code based on user agent if the page is of a temporary nature and only works with a token. Once the token is used, Gone would be an appropriate response since the page no longer exists.\",\n",
       " 'Using your example, www.example.com/dashboard versus www.example.com/dashboard?tab=something_public versus www.example.com/dashboard?tab=something_else_public, regardless of the resulting HTML, Google will see these as separate pages. The reason for this is simple. The URL is one of two major keys within the index. As long as each URL is unique, it is a unique page regardless of what your website does.\\nTo correct the record, you need to use a canonical tag within the HTML head tag to indicate which page is the original.\\nStill using your example, www.example.com/dashboard?tab=something_public would have a canonical tag pointing to www.example.com/dashboard. In fact, if it makes it easier, www.example.com/dashboard can have a canonical tag pointing to itself.\\nHere is Google\\'s page in this:\\nhttps://support.google.com/webmasters/answer/139066?hl=en\\nLook for the header: Indicate the preferred URL with the rel=\"canonical\" link element\\nHere is the sample code on that page:\\n<link rel=\"canonical\" href=\"https://blog.example.com/dresses/green-dresses-are-awesome\" />\\n\\nThe advice is to use absolute URLs and not relative URLs for canonical tags.',\n",
       " 'You should be able to do this if the DNS Zone Admin for www.somecompany.example  adds ourapp.somecompany.example as a subdomain entry pointing to the IP of www.ourapp.example on their DNS server and add ourapp.somecompany.example as a valid domain on your server in IIS.\\nNot knowing about IIS specifics, https://stackoverflow.com/questions/11737065/server-alias-in-iis could be of help.',\n",
       " 'The way you/Magento are doing it is correct. As long as you have breadcrumb markup Google will display the category chain in SERP. In the case of multiple categories, the chain displayed depends on which category has most relevance for the keyword. When clicking the SERP you will be taken to the direct canonical page for the item, ie domain.tdl/product-name.html, even if there is no direct way to get there via link.\\nThis also brings up an interesting concept that many OP\\'s disregard - true permalinks. This could help prevent future SEO and human access hiccups in certain cases. I don\\'t mean the Wordpress definition of permalink either (they define it wrong). I mean a link based on item ID and not keyworded url/slug at all. The item ID never changes, whereas the keyworded slug could. Why is this useful? Less 301\\'s and urls that are accessible forever in the case of a future name change.\\nLet\\'s say you have an item with a url domain.tdl/red-round-widget.html and you want to link to it from like 100 other item descriptions. You paste in that url, the links work, all is good. Then you change the name to domain.tdl/red-circle-widget.html. Unless you don\\'t forget to update, or you are using some kinda dynamic url construct/update method (ie PHP), this doesn\\'t seem like a big deal -- just 301 the old url to the new, all those 100 links still work right. But then you change it again a few months later. And again later. You are now at 4 redirects via 301 to the newest url. This is not good since it\\'s approaching a limit of 301 chaining that Googlebot does not like. How do you prevent this \"does not like\"? Make those 100 items use a permalink url of item ID instead, completely bypassing the keyworded slug. When bots run through the 100 links, they are dropped into an \"ugly\" url, but your canonical underneath tells them to see/use the most recent keyworded slug instead, without using a bunch of 301\\'s.\\nI don\\'t know off the top of the head what those permalink style urls look like in Magento, but it may be something like domain.tdl/index.php?get=product&id=12345 to display product with an ID of 12345 (as an over-simplified example). You can use the same style permalink for things such as \"email this page to a friend\", social, bookmarking, or other widgets of that nature where long term url accessibility is paramount. This ensures that as long as the item has the same ID, the item will be accessible regardless of how much you change the url structure, the slug, or the parent category(s). It\\'s not a super big deal if you don\\'t ever change names or chain 301\\'s, but it\\'s helpful if you do. Hope that makes sense!',\n",
       " \"Your attempting to access the site using a IP address when your virtual host file is setup to use a domain name, not an IP address... so it'll throw up the default folder, in this case /var/www/html/.\\nFix 1. Local Virtual host file\\nThe most common and easy method to fix the issue would be to edit your host file within Windows or Mac, this will allow you to access all sites locally, using the domain name as you would externally...\\nThe host file would look something like this:\\n# Copyright (c) 1993-2009 Microsoft Corp.\\n#\\n# This is a sample HOSTS file used by Microsoft TCP/IP for Windows.\\n#\\n# This file contains the mappings of IP addresses to host names. Each\\n# entry should be kept on an individual line. The IP address should\\n# be placed in the first column followed by the corresponding host name.\\n# The IP address and the host name should be separated by at least one\\n# space.\\n#\\n# Additionally, comments (such as these) may be inserted on individual\\n# lines or following the machine name denoted by a '#' symbol.\\n#\\n# For example:\\n#\\n#      102.54.94.97     rhino.acme.com          # source server\\n#\\n192.168.1.1 exampleA.com\\n192.168.1.1 exampleB.com\\n192.168.1.1 exampleC.com\\n\\nFix 2. Virtual IP addresses\\nSince you have 3 sites and if you intend to be able to visit all of these via the internal lan IP then are faced with a greater issue of how can you tell Apache2, which site to return when you only have one IP address...\\nThe a fix would be to use virtual IP addresses, and assign a virtual IP address to the virtual host, rather than using *. \\nSomething like this:\\n# Site A\\n<VirtualHost 100.100.100.1>\\n    ServerName www.example-1.com\\n    DocumentRoot /var/www/exampleA\\n</VirtualHost>\\n\\n# Site A\\n<VirtualHost 100.100.100.2>\\n    ServerName www.example-2.com\\n    DocumentRoot /var/www/exampleB\\n</VirtualHost>\\n\\n# Site C\\n<VirtualHost 100.100.100.3>\\n    ServerName www.example-3.com\\n    DocumentRoot /var/www/exampleC\\n</VirtualHost>\\n\\nUsing the above will allow you to visit the sites using the virtual IP addresses.\\nFix 3. Map folders\\nYou can map folders as alias so that when someone visits http://100.100.100.1/exampleA they get site A, or going to /exampleB will get site B etc.\\nSomething like this: \\n<VirtualHost *:80>\\n    ServerAdmin webmaster@localhost\\n    DocumentRoot /var/www/html\\n\\n    Alias /ExampleA /var/www/exampleA\\n    Alias /exampleB /var/www/exampleB\\n\\n    <Directory /var/www/>\\n            Options Indexes FollowSymLinks MultiViews\\n            AllowOverride All\\n            Order allow,deny\\n            allow from all\\n     </Directory>\\n</VirtualHost>\",\n",
       " \"I don't think this is the place for these types of questions, you should use stackoverflow. \\nHowever one way to deal with this is to edit the bootsrap source file. On line 4321 of bootstrap.css you will find the following:\\n@media (min-width: 768px) {\\n  .navbar-toggle {\\n    display: none;\\n  }\\n}\\n\\nChange the above media query to display the navbar-toggle (which is the button) at whatever screen size you want. \\nFrom the bootstrap definition:\\nDevice Width        Container Width\\n1200px or higher    1170px (large screens - large desktops)\\n992px to 1199px     970px (medium screens - small desktops/laptops)\\n768px to 991px      750px (small screens - tablets)\\nLess than 768px     auto (extra small screens - phones)\\n\\nYou may have to make some other changes, but I don't have time to sift through what those are because I'm on my way out the door, but this should at least show you where to look. You could also just overwrite this in your style.css with the !important tag.\",\n",
       " 'I think there should be double quotes around your TXT entry, just like for that other TXT entry you have shown.\\nAs far as I can state, this double quotes normally happen automatically (my provider), but maybe check with your own provider.\\nSee how nslookup resolves the two entries differently',\n",
       " \"Based on a strict reading of Google's webmaster guidelines and definitions it is considered cloaking, it may not have been designed to present alternate content to Google over the users but as it has that effect Google needs to treat it as such.\",\n",
       " 'I finally found out that I was missing a trailing / on the ProxyPass commands.\\nPreviously I had written:\\nProxyPass / http://cloud.myDomain.com\\nProxyPassReverse / http://cloud.myDomain.com\\n\\nIt should be:\\nProxyPass / http://cloud.myDomain.com/\\nProxyPassReverse / http://cloud.myDomain.com/',\n",
       " 'the \"Too many redirects\" issue comes from the fact that redirects are tied together. For example, http://example.com redirects to https://example.com and https://example.com redirects back to http://example.com. \\nThis process can repeat itself over and over again until the redirect count established by the server is reached, then when that is exceeded, the server then stops trying to process redirects.\\nIn apache, this can be discovered by seeing the \"this page can be found here\" message on the screen (which is an actual redirect page) instead of the behind-the-scenes automatic redirect that the browser processes.\\nWhat I would suggest you do when testing your scripts is to make them non-cacheable to get current results every time. You should also check to ensure the user requests the HTTPS version of the site, and only perform the redirect if the request isn\\'t HTTPS. One way to check is to validate the server port number the user connects to and if the port number is 80 then make the redirect happen for the user since port 80 is HTTP.\\nAnother thing helpful is to turn off automatic redirects in your browser while testing your pages. \\nIf that is not possible, use redbot.org and put your URL that is causing problems in it then you\\'ll see what the new URL is under \"Location:\" in the HTTP headers. Keep following the URL until you no longer see the \"Location:\" value. If you can\\'t then you have an endless redirect.',\n",
       " \"I've always used the favicon generator at realfavicongenerator.net, which allows you to see how your chosen artwork will appear on different devices. There are quite a few options and you can try with their example icon to test. Start with a square SVG for the best results as the site will convert it to the necessary formats and give you a compressed folder with everything ready to use.\\nIt's also a free service with no registration or other nonsense required. Just use it and donate if you find it useful.\",\n",
       " \"Google has a long held position that 301 links are fine and very appropriate for many situations.  They pass page rank and most sites rely on them for a host of reasons like:\\n1) page url was update to new url\\n2) Company was acquired and all their urls now redirect to cooresponding urls on acquirers website\\n3) Total site redesign in which all urls changed.\\nGoogle has stated that the amount of page rank that flows over a 301 is nearly identical to the amount that flows over a straight link. \\nSee here https://www.youtube.com/watch?v=Filv4pP-1nw \\nSome webmasters feel that there may be slightly more loss due to a 301 than a straight link, but there are no hard numbers, and it's generally thought to be a very tiny loss, perhaps 1%.\\nSo the short answer, is if you can do a straight link do it, but if not, don't lose any sleep over using a 301, it's fine.\",\n",
       " 'SEO. The linked sites may not receive full PageRank from a redirect URL. Some SEOs believe there is evidence that sites which link out to other parts of the web are viewed more approvingly by Google than sites that do not link out to anyone, so there could be a slight negative value to keeping all of your links inside.\\nUsability. In addition to the slight time delay, a redirect URL makes it harder for the person to see where they are going by preveiwing the URL in the status bar, so they may be less likely to click.\\nSecurity. This is only a minor issue, but allowing a redirect from your own website makes it possible for malicious users to give someone a URL like \"www.yoursite.com/redirect.aspx=fhdfweelkjelkfj.com\" which could take a user to a malicious website while appearing to have your safe address on it, unless you are very selective about how your redirect works.\\nAlso, as mentioned by other users there are alternatives to tracking these metrics. As far as your Facebook page goes, it has its own tracking through View Insights where you can see your referrals. And of course there are ways to use javascript for more complete solutions.',\n",
       " 'Although I still do not know why it happened, I resolved the issue by moving remotebase.io to a new server. It took around 24 hours for the change to take affect.\\nI also removed vym.io from Google using webmaster tools. I do not know for sure if doing so helped at all.',\n",
       " 'It is generally unwise to allow www or apache to have write access, however, this can be done safely by limiting the access to a single directory and not allowing any executables in that directory.\\nFor example, the docroot of your php based site is /var/www/example.com\\nYou would want your webserver setup so that the only php executable access point is /var/www/example.com/index.php and the only directory that the webserver can write to would be /var/www/example.com/files. This is often how php web applications handle image manipulation.\\nYou would also want to have your webserver set so that no php files can be executed from inside the /files directory.\\nAlso, you could pass these files through another source. php executes a shell script that writes the files, but this can be very complicated to do safely and it can add another vector of attack. Always remember, the more complicated a process the more things that can go wrong.',\n",
       " 'Desktop — index:\\nAdd a rel=\"alternate\" tag pointing to the corresponding mobile index page:\\n    \\nMobile — index:\\nAdd a link rel=\"canonical\" tag that points to the corresponding desktop index page:\\n    \\nAlso on the mobile index page, link to the next mobile page by adding a rel=\"next\" tag:\\n    \\nMobile — second-page:\\n<link rel=\"prev\" href=\"http://example.com/\"/>\\n<link rel=\"next\" href=\"http://example.com/third-page\"/>\\n\\nMobile — third-page:\\n<link rel=\"prev\" href=\"http://example.com/second-page\"/>\\n\\nYou can read more on rel=\"next\" and rel=\"prev\" here.\\n\\nThis was the best answer I could find through my discussion with @titico. I tried getting them to update the answer but they haven\\'t yet so for now, this is the correct answer.\\nThis answer is only theoretically correct and there seems to be no documentation on what to do in these situations. If you have any additional insight/information, please feel free to change this answer as I\\'ve made it a community wiki.',\n",
       " \"So, my question is: Is there a performance reason to nail virtual\\n  hosts to fixed IP specifications?\\n\\nThe only performance related reasons for using name based vs ip based is if you've got a limited pool. Name based virtual hosting will not occupy precious IP addresses. The server will be parsing/processing the http headers whether or not you use name based hosting. It's also easier to manage named DNS records than it is config files full of IPs. These things considered, in certain situations IP based virtual hosting could be weaker.\\n\\nIs this needed for SSL setups (which applies to some but not all of my sites)?\\n\\nI can't think of a reason you'd only set up SSL on a fixed IP. The cert will be for a specific domain - name based virtual hosting would be easiest way to configure/manage it.\",\n",
       " 'Microsoft word mail-merge with the excel sheet as database is perfect for this use case.\\nMake a page in word with all the elements n design. Then from Mail Merge menu, add the variable data, like Date, Price etc.In the end, merge, and then print on paper or as pdf.',\n",
       " 'Take a look at mod_macro, sounds like it may be exactly what you need for your use case.',\n",
       " \"You can change the default page value in your school's computers to something like\\nhttp://www.example.com/?utm_source=computerlab&utm_medium=referral&utm_campaign=bounceratestudy\\nThat will report out as a referral source named 'computerlab'\\nOf course, you'll need to manually change every browser to add the analytics tags. :/\",\n",
       " 'Although matts answer gave me some direction for the \"page rule\" tip in cloudflare, a bunch of stuff still had to be made.\\nI\\'m assuming you already have an application running in openshift: let\\'s suppose it\\'s subdomain is php-example.rhcloud.com, and now you want to add custom domains example.com, example.com.cc and example.com.co to redirect to the application: remember that example.com.cc and example.com.co will redirect to example.com.\\n1 - Configure domain aliases for example.com, example.com.cc and example.com.co in the openshift web console;\\n2 - Create a Cloudflare account, adding example.com, example.com.cc and example.com.co as your domains;\\n3 - Go to your domain registrar (I\\'m using GoDaddy) and change the nameservers to the ones that were given to you when you were adding the domains in cloudflare (in my case it was alec.ns.cloudflare.com and june.ns.cloudflare.com);\\n4 - \"Refresh\" your domains in cloudflare, in the \"Overview\" section to check that cloudflare is now in charge of your DNS;\\n5 - Time to configure the DNS and Page Rules for your domains:\\nexample.com\\nDNS\\nType: CNAME \\nName: example.com\\nValue: php-example.rhcloud.com\\nType: CNAME \\nName: www\\nValue: example.com\\nPS: Remember to check the cloud icon, it needs to be orange.\\nPage Rules\\nwww.example.com/*\\nForwarding URL - 301\\nhttp://example.com/$1\\nexample.com.cc\\nDNS\\nType: CNAME \\nName: example.com.cc\\nValue: php-example.rhcloud.com\\nType: CNAME \\nName: www\\nValue: example.com.cc\\nPS: Remember to check the cloud icon, it needs to be orange.\\nPage Rules\\nwww.example.com.cc/*\\nForwarding URL - 301\\nhttp://example.com/$1\\nexample.com.cc/*\\nForwarding URL - 301\\nhttp://example.com/$1\\nexample.com.co\\nDNS\\nType: CNAME \\nName: example.com.co\\nValue: php-example.rhcloud.com\\nType: CNAME \\nName: www\\nValue: example.com.co\\nPS: Remember to check the cloud icon, it needs to be orange.\\nPage Rules\\nwww.example.com.co/*\\nForwarding URL - 301\\nhttp://example.com/$1\\nexample.com.co/*\\nForwarding URL - 301\\nhttp://example.com/$1\\nThat\\'s it: when you use https, an invalid certificate will issue because you\\'re using a custom domain (example.com) but the certificate is for php-example.rhcloud.com: for you to use custom ssl certificates on the server, you need Bronze or Silver plan from openshift or use Cloudflare certificate.',\n",
       " \"It would seem there was a Manual Action required as suggested by Dan.  It had also gone through a recent change but doubt that change was the cause.\\nThe reason for all the images to have gone is because Google doesn't like the way I've implemented the HotLink protection scheme.  The scheme similar to what Fansshare.com does to protect against Google from showing just pictures rather than showing the webpage.  There's a lot of comments about this issue on the internet, I'm not the only one.  For example, if you view any picture on google image search and click on 'View Image', you bypass the webpage and just see the picture.  Fansshare.com and my website had got round that way but Google doesn't like the way I do.\\nhttp://mistercopyright.org/google-image-search-and-misappropriation-of-copyrighted-images\\nI've gone back to showing just a hotlinking image rather anything fancy like Fansshare.com.  I made a resubmission and wait to hear back from them.\",\n",
       " \"It's just spam :) Scripts can send Pageview and Event data to random GA tracking codes without visiting your site. The best fix is to create a filter that only allows requests originating from your hostname(s). This is a great article on it:\\nhttp://help.analyticsedge.com/spam-filter/definitive-guide-to-removing-google-analytics-spam/\",\n",
       " 'You\\'re touching on a \"feature\" of DNS known as DNS Round Robin. It\\'s deliberate. You have no control over the order in which records which satisfy a particular query are provided. \\nhttp://en.wikipedia.org/wiki/Round-robin_DNS\\nIt\\'s crude; but surprisingly effective as a load-balancer.\\nIf, in fact, the \"backup\" machines should not be touched unless the primary fails, you need to be more sophisticated with your zone files. You could change the TTL (time-to-live period) for the A record to a small number (e.g. 60 seconds) and then set up a script to change that A record via RFC 2136 when you want folks to fail over. \\nIf you have 3 A records coughing up 3 different answers, you are supposed to get rough distribution equally amongst the 3. (one can also assume that M$ DNS will screw this up, but I don\\'t actually know...)',\n",
       " \"Firstly the use of canonical:\\nThe use of canonical is not exactly creating duplicate pages. Every application should avoid creating duplicate pages.\\n1)Canonical is meant to give seo wieghtage to the main or parent page from which it is derived. \\n2)This needs not to be exact duplicate, but this means this new page is also generating similar information with the canonical.\\n3)\\nThis is lot used in blogging/news sites where i want to publish content from other sources to my site and give credits to the actual owner\\n4) Also used in cases like running a site on both http and https and you take one of them as base for seo crawling. So you add that to another as canonical.\\nOther useful cases as well exists which make canonical a very important tag and should be used carefully\\nAdding Canonical via javascript\\nThough google crawlers are now becoming intelligent enough to understand javascript. But this should be done with best implementation practices and recommendations, otherwise you end up messing with your seo health. As if google does not read canonical properly, you end up penalizing for duplicate content.\\nhttps://developers.google.com/webmasters/ajax-crawling/docs/learn-more\\nUnnecessary content for user, saving space - Useless in described case\\nThe required seo tags don't add up that much which will hurt you front-end performance or help reducing network transfer time. \\nYes if you want to tune your page such that you pass all crawler related +  most important components of your site in first load, and then load content on demand via ajax then you should go ahead but following best practices if you want that content to be seo friendly.\\nBut for canonical addition via javascript just to save few bytes of transfer is not recommended.\",\n",
       " 'Standard .htaccess Block\\nI covered this a while back on my blog about stopping WP brute force attacks, you can IP block xmlrpc.php by using <Files>, or <FilesMatch, here\\'s what I use:\\n<Files \"xmlrpc.php\">\\n    Order Allow,Deny\\n    deny from all\\n</Files>\\n\\nThe above code will block any requests that attempt to fetch that file.\\nModsecurity\\nIf you need more of an advanced solution that uses that mod_security then I recommend that you use rate limiting solution, something like this:\\n\\nSOURCE\\nSecRuleEngine On\\n\\n<LocationMatch \"^/somepath\">\\n  SecAction initcol:ip=%{REMOTE_ADDR},pass,nolog\\n  SecAction \"phase:5,deprecatevar:ip.somepathcounter=1/1,pass,nolog\"\\n  SecRule IP:SOMEPATHCOUNTER \"@gt 60\" \"phase:2,pause:300,deny,status:509,setenv:RATELIMITED,skip:1,nolog\"\\n  SecAction \"phase:2,pass,setvar:ip.somepathcounter=+1,nolog\"\\n  Header always set Retry-After \"10\" env=RATELIMITED\\n</LocationMatch>\\n\\nErrorDocument 509 \"Rate Limit Exceeded\"\\n\\nHowever, unless you actually use XMLRPC for real purposes, there should be no reason why you should use a rate limiter for a file that either doesn\\'t exist, or isn\\'t needed.\\nFail2ban Rocks\\nIf you have root access to your host then I strongly recommend Fail2ban, since its more flexible, has way more features and is lot easier to use than mod_security.\\nWordPress Users\\nUsers that use both xmlrpc and WordPress should protect their wp-login.php since it is normally a bruteforce attack, therefore you should also make the extra effort to lock down your wp-login.php from also being attacked, I use this:\\n<FilesMatch \"wp-login.php\">\\n    deny from all\\n    # Broadband that changes IP ADDRESSES\\n    # Change the below to the domain that your broadband IP resolves too\\n    allow from .isp.com\\n    # Broadband that IP does not change\\n    allow from 1.1.1.1\\n</FilesMatch>',\n",
       " \"Indirectly, yes:\\n\\nCreate a page with links to all the URLs you'd like re-crawled (like a sitemap) and add that to your website.*\\nSubmit the URL to that page to Fetch as Google, selecting Desktop as the fetching strategy, as detailed here: Use Fetch as Google.\\nOnce Fetch as Google is complete, and within 4 hours time, from the Fetches Table next to the status of the fetch, select Submit to Index, then select Crawl this URL and its direct links, followed by Submit.\\n\\nAs indicated in Ask Google to re-crawl your URLs, the above will:\\n\\nSubmit the URL as well as all the other pages that URL links to for\\n  re-crawling. You can submit up to 10 of requests of this kind per\\n  month.\\n\\n*As commented by John Mueller, you can also submit a sitemap file or a text file containing a list of URLs.\",\n",
       " \"This sounds like a common misconception and understanding of linkage on the web and a typical view of many that are thinking about this all wrong...\\nThe average natural website's link profile will be made up of a higher percentage of nofollow links than followable links. This is common and to be expected. It isn't always the case though and some websites link profiles will largely be made up of followable links. This factor alone won't determine the organic performance success of a website though, there are many other factors involved.\\n\\nJust because you have many nofollow links, this shouldn't cause alarm or concern. This won't lead to a penalty nor will be frowned upon by Google. Nor will this categorically not help your website perform/rank. Whilst there is a lot of opinions about nofollow links, more than can be discussed in a single answer to your question, there are benefits that come of them and these count towards a natural and balanced link profile.\\nThe fact that your content is viewed a lot, shared/liked a lot in social media and is generating healthy 'engagement' related metrics will have an indirect influence on your webpage's performance. That coupled with only a few 'quality' links is not confirmation that the page(s) won't perform or rank well. In terms of generating more real and worthwhile links though, that depends upon the nature of what content you are producing. Most content that goes viral tends to be the type of shareable media across social channels and not the type that many other website's would link to - those pages still perform ridiculously well.\\n\\nPerhaps, for your next piece of viral-intended content, consider what your target audience will find useful which is not well-documented already on the web. Again though, what things to consider to earn natural links is a much bigger topic again that could cause much debate.\",\n",
       " \"If you 301 redirect an URL then the search engine will merge the records on it's end. Your /latest is filed under /3.0, which is what it will index. This not only prevents /latest being a static resource request but prevents it from even appearing in search.\\nAccording to best practices, duplicate content must be canonically linked. This means your latest version page has to canonically link to the /latest URI. You'll have to add a rel=canonical ->/latest to the newest version page. Then remove it and place it in the next version when you upgrade. This means /3.0 won't be indexed until it's superseded because the search engine has merged records into /latest. When /latest is drawing from /3.1, it will have to re-index /3.0.\",\n",
       " 'Schema.org is not only for search engines. There can be various consumers and tools making use of the data. For example, a browser add-on that allows users to download search results in a structured format.\\nGoogle is not the only search engine that consumes content marked up with Schema.org. Other search engines might have other guidelines about search result pages. And I think there can be good reasons for allowing bots to crawl and to index your search result pages, even for Google Search. As far as I know, Google does not provide a search result feature that makes use of SearchResultsPage.\\nCrawlers could use the SearchResultsPage type as signal that indexing these pages might not be worthwhile. If a service does not want to index search result pages, but the site doesn’t restrict the crawling (via robots.txt) or the indexing (via meta-robots) of these pages, this service can learn about the nature of the page by checking the structured data and finding the SearchResultsPage type (ah, it’s most likely a page containing search results, let’s ignore it).',\n",
       " 'It depends on how you\\'re actually importing the code and the contents of it.\\nIf the code contains actual text valuable to search engine crawlers, then chances are, it might be bad for SEO because some search engine robots might not understand the new import tags.\\nIf the target audience are people with older web browsers, then importing might not even work at all and those people might not see the page the way it is intended to be displayed.\\nBut the part that you need to take into account is the number of requests required to display the page. any code that causes the browser to seek a resource can be bad if too many of them exist. The most common example of this is code as follows:\\n<script src=\"http://example.com/code.js\"></script>\\n<script src=\"http://example.com/code2.js\"></script>\\n<script src=\"http://example.com/code3.js\"></script>\\n<script src=\"http://example.com/code4.js\"></script>\\n\\nAs you can see, four requests are made in addition to the main HTML page initially requested.\\nAnd this is even more terrible if the server all resources are on is the same server because the same server will run slower while trying to serve everyone their documents.\\nSo for increased browser compatibility and increased speed, I\\'d suggest having all HTML in one file and not making requests to other HTML within the same file. This improves overall loading speed which is a factor in SEO because google hates slow sites.\\nHere\\'s more info on why you should minimize server requests per page load:\\nhttp://webdesign.about.com/od/speed/qt/minimize-http-requests.htm',\n",
       " 'Internal server errors can have multiple causes:\\n\\nCode syntax errors that happen whenever you hit a particular page\\nProblems parsing user data that only happen for a specific user\\nIntermittent problems caused by something like the database going down\\n\\nIt is not surprising that different crawling bots find different numbers of 500 errors.   You may have some 500 errors that always keep certain pages from loading, but many are going to come and go, or only effect a specific crawler.\\nYour server logs are usually a better source for finding these errors than the crawl reports from Google and Moz.   Your server logs will also show the ones that are effecting actual users.  \\nI routinely monitor my site for 500 errors using the logs and fix them as they come up.',\n",
       " \"This isn't a direct answer to your question but to your problem:\\nTry a hostname filter on your Analytics account instead. Filter only for your domain. The only situation in which you'll get views without your domain being the hostname is if you're serving content on other domains - such as via an iFrame.\\nWhen it comes to crawlers, there's many techniques. I block 'mozilla compatible user agent' as a user agent as a lazy option. You can also match lists, for example:\\nhttp://help.analyticsedge.com/spam-filter/definitive-guide-to-removing-google-analytics-spam/\",\n",
       " 'Your error tells me that the sub-domain name did not resolve.\\nWhile there are several reasons why a sub-domain does not resolve, the most common reason is that it is not defined.\\n\\nYou have to add the sub-domain using a CNAME record in the DNS that is\\n  the SOA (statement of authority) for the domain.\\n\\nMost of the time, the SOA is the DNS server provided by your registrar. If this is not the case, then often it is the DNS server provided by your web host. For new users, this is not always clear. If this is the case, I suggest starting with the registrar and if you need to, simply contact tech support of the registrar or web host and ask. Often tech support can walk you through including the CNAME record for your sub-domain very easily. So if you need help, do not be afraid to ask.\\nAs well, sometimes the DNS setting on the server may be the issue. While walking through this would be a broad topic, most of the time, the local DNS server, if used, does not refer to the localhost properly. Using 127.0.0.1 is standard but may not always work in all cases. You may have to also add the servers IP address. However, default ACLs may not allow this and may also have to be investigated. Also consider that some installs of Bind are configured as forwarders, which is fine, or cache which may have to expire. I installed a newer copy of Bind on one server which gave me fits. I expected the DNS to be configured as a forwarder and it was configured as a cache which held onto a bad IP address. It drove me nuts for quite a while! It was just a matter of clearing the cache or configuring the DNS to be forward only. Much of this depends on what you are trying to do. Again, DNS is too broad to get into details here. Just know that sometimes, DNS installs are not what you expect.',\n",
       " \"I'd suggest updating the links in your website to the absolute URL and avoid redirections (unless of course you have valuable links pointing to the URL ending with the ID)\\nThis is an unnecessary redirection and should better be avoided.\",\n",
       " 'The best practice is to omit the previous link (on the first page) and the next link (on the last page), because\\n\\nuser agents without CSS support will display them, which might confuse users\\nhaving them is pointless\\n\\nThe link to the current page would ideally be omitted, too, because \\n\\nuser agents without CSS support will display it in the same way like the other links, so users might not be able to tell on which page they currently are\\nhaving it is pointless\\n\\nHaving or not having these links should have no effect on SEO, though. \\nI can’t imagine any reason why having these hidden links would be a good idea. My guess (without checking their code) is that they kept them because it was easier to implement it like that.',\n",
       " \"If the pages of the forum no longer exist, then you may do either of the following:-\\n\\nBlock your forum with robots.txt and then manually submit the URLs in Google Search Console for removal.\\nIgnore these warnings and they will eventually go away. Google has those pages in its index and so when they're coming back to re-crawl they're getting the 404 error and logging them. After a few repeated 404 errors, Google will likely drop those pages from their index anyway.\\n\\nThe warning message is intended to notify you of any mis-configuration that might have led to 404 being returned on otherwise valid (200) pages. If the pages really do not exist anymore, then follow what Google has mentioned in the last line\\n\\nIf these URLs don't exist, no action is necessary.\",\n",
       " 'I think these structures make the most sense (from least detailed to most detailed, and most likely also from most popular to least popular):\\n\\nArticle\\nWebPage mainEntity Article\\nWebSite hasPart WebPage mainEntity Article\\n\\nThe hasPart/isPartOf properties are rather broadly defined (a work that is \"in some sense\" part of another work), so one could even argue to use it in addition to mainEntity (i.e., WebPage hasPart/mainEntity Article), but in my opinion that’s not a perfect match and probably not necessary.\\nWhich to choose? This mostly depends on what data you want to provide. If you don’t have relevant data about your website, there is probably no point in providing a WebSite item. If you provide a WebSite item (e.g. for Google’s Sitelinks Searchbox), it might make sense to define this item only on the homepage. If you want to state that your WebPage items are part of this WebSite, you could reference its URI instead of providing a separate WebSite item on each of your pages, repeating the same information every time.',\n",
       " 'I described the different ways how mainEntityOfPage can be specified in an answer on Stack Overflow.\\nThe difference between your two examples is that the second one creates an item (with the type WebPage), while the first one just points to another page (which might or might not define a type).\\nFrom the perspective of Schema.org, both ways are fine. The mainEntityOfPage property expects a URL (= your first example) or a CreativeWork (= your second example) as value.\\nIn my opinion, the second example is not very elegant. The content attribute of the meta element will be ignored (because of the itemscope), but HTML5+Microdata requires it to be there. Pro: it adds (on the source page) the information that the URL represents a web page; in theory, consumers would not have to visit the URL to learn what it represents (= a WebPage). However, that is probably the default assumption in that context anyway.  \\nIt shouldn’t matter which way you choose. FWIW, Google’s Testing Tool seems to be fine with both.',\n",
       " \"If you discount that a person has some inside knowledge, the answer is no.  How can anyone know about something that they can't see or know about?\\nBut what is the point of having these folders if you aren't using some of the content?  Assuming you do use the content, unless you are doing some really fancy coding, then you have to link to the content and therefore the answer is also yes, as those links are going to be visible in source code.\",\n",
       " 'Having something appended to your URLs in search console is usually either a malformed link on your site (in this case your twitter link probably), or Googlebot being dumb and interpreting a JavaScript string like var=\"tag/tutorials/news@example.com\"; as something that the JS could use as a link, maybe.   \\nThe first would effect users, but not the second.   I tend to fix them all, just because I hate errors in the reports, but fixing an error that users don\\'t see shouldn\\'t be needed.',\n",
       " \"It appears you're using query strings on a PHP page to generate the file links. Try adding a wildcard to the end of your ALLOW, as this will form part of the URI.\\nAlso, have you checked the URL parameters section to see how Google is treating these variables? You can also explicitly set behaviour, make sure Google understands how to use the query strings when it's indexing.\",\n",
       " 'You should of course use properties to relate items wherever it is possible.\\nIf you have the two top-level items Person and Book, it’s not clear if or how they are related. The person could have read or written or bought or reviewed or … the book. But when you use a property (like author), you make the relation explicit.\\nIn case of the syntaxes Microdata and RDFa, it doesn’t matter if you nest the items via HTML. Microdata/RDFa parsers will parse the data according to the specifications, and these only relate items if you use properties accordingly.\\nFor example, the following two snippets generate the same Microdata output (= two top-level items), even though the item is nested in the first case:\\n<div itemscope itemtype=\"http://schema.org/Person\">\\n  <div itemscope itemtype=\"http://schema.org/Book\">\\n  </div>\\n</div>\\n\\n<div itemscope itemtype=\"http://schema.org/Person\">\\n</div>\\n\\n<div itemscope itemtype=\"http://schema.org/Book\">\\n</div>\\n\\nIt’s only appropriate not to use properties if\\n\\nthere is no suitable property defined, or\\nthe items are not related somehow.',\n",
       " 'Cloaking is when you present one page to a search engine that is different than what you present to users. What you are describing is something entirely different. I will explain.\\nI describe in several answers here that Google does not match search terms directly to content. While it may seem that it does, the reality is that this notion derives from highlighting search terms in the search engine result page (SERP) after a lot of semantic based analysis and algorithms are applied. It is simply the last step.\\nInstead, what Google does is analyze the search query for intent using as much semantic analysis as possible. It then matches it to the same semantic analysis performed on web pages. Semantics are topic based. Each term is considered in context and not directly matched. For example, the term left can mean to leave or the remainder of something or an indication of a side of something. Simply matching terms would yield in false results.\\nHaving said that, what exists within a title tag, description meta-tag, one and only h1 tag, other header tags, and content must match topically. However, the best strategy is to create a short title for your content that is about 50-55 characters and a longer version of the same title. Both titles should be as compelling as you can make them to entice someones curiosity. The short title would be your title tag and the longer title would be your h1 tag. Then take the longer title and expand upon it to no more than 170 characters and make it more compelling. This would be your description meta-tag.\\nAs far as terms are concerned, Google will chose not to use the description meta-tag and replace it with something else often from content but never a header tag. To better control how your site is found, figure out what the 2-3 most important search terms your page is to be found by. I do not advocate using keywords, however, you do have to use search terms that apply to the topic. Keep in mind that you need to know what people are actually searching for and not what you think they are searching for. Even then, researching for keywords is like driving looking into the rear-view mirror. Not very practical. Use these 2-3 search terms in your title naturally. Add one or two more and use these in your h1 tag. Add a few more to your description meta-tag. In order for a description meta-tag to appear as a snippet for a search term, the term must be supported in the title tag, h1 tag, as well as within content. This is key.\\nAll search terms used within a title tag, h1 tag, and description meta-tag should exist within the content. Why? Because semantics is based upon scoring and search algorithms looking for strong semantic signals. Part of this is scoring parts of your web page to see what topics and sub-topics are where on your page and how important they are. It is best that terms you use are important terms to your pages topic. In otherwords, do not wedge terms in you think are important onto your web page. It would not be supported semantically and score very low resulting in poor search performance.\\nBe that as it may, the best advice is to write your content naturally using the terms that apply to the topic as you would talking to a peer. Do not get cute and try and use terms unnaturally or synonyms to influence search engines. They do not need your help. Just write as you would if you were talking to someone. Semantics will figure out what is important about your work and return the results you should have. This means happier users. Just keep one eye on your tags as I have described so that their behavior benefits you and not hurts you.',\n",
       " 'There is no reason for Google to index your website just because of the GA code (it might index it through other methods though, i.e. - links to your site). Moreover, Google obeys robots.txt and the META ROBOTS protocol, so to prevent your website from being indexed by Google, you may upload a robots.txt file with the following instruction\\nuser-agent: googlebot\\nDisallow: /\\n\\nIf you want to disallow all SEs, then replace Googlebot with *. However, do note that despite adding the robots.txt file, your website will still show up in Google if your URL is searched with but it won\\'t display any content from your website on the search results.\\nIf you want to completely avoid getting into the google index, add the following META ROBOTS tag in the <header> of all your pages\\n<meta name=\"googlebot\" content=\"noindex\">\\n\\nIf you don\\'t want to be indexed by any search engines, replace googlebot with robots in the above meta tag',\n",
       " 'Here\\'s my answer considering your comments.\\nBack end pages shouldn\\'t be indexed by search engines because they are useless for visitors. It means that optimizing SEO of these pages is not necessary.\\nTo no-index these pages, you can use the no-index meta tag for the <head> section of your pages and add these pages to your robots.txt. If you can\\'t modify the <head> section, you can use the no-index feature with your .htaccess file if you use Apache as a web server (described here).\\nFor a non indexed web page, the rel=\"canonical\" is useless.',\n",
       " 'Okay. This gets a bit complicated. While no-one short of a search engineer can tell you what any search engine will do specifically, we know a few things based upon what Google tells us in a whole host of places. I will explain.\\nGoogle will parse the HTML DOM objects from top to bottom assigning each HTML element with an ID that will uniquely identify the HTML element, assign an order to the element, assign parent-child relationships between elements such as in ordered lists, and finally, when how one element relates to another such as paragraphs following a header. Using the IDs, the original HTML can be reconstructed as well as be referenced in semantic block elements and dependencies from one to another. Keep this in mind.\\nAlso, keep in mind that Google will compare pages to know what HTML DOM elements exist on several or all pages so that it can properly identify content from other elements such as header, navigation, side-bar, footer, etc.\\nSemantics is based upon this.\\nTaking what we know, both of your list elements will be taken as templated HTML DOM elements separate from content. Your first list will be given more importance simply because it appears first, however, since you are using list elements (at least in your example), the importance difference would be small. Your example does not use the nav tag. I invite people to comment if this tag is necessary or important. I use it.\\nIt may be to your advantage to use the tag to signal that both lists are navigation. The tag may help search engines see your navigation properly. Search engines, consider navigation as very important signals to your most important pages. Using the tag, in theory, should tie your two list elements semantically as important (navigation) DOM elements and weigh them higher than (templated) content elements.\\nWhile I am a believer that navigation should be as simplified as possible to avoid issues, I rather suspect that your two list elements should be seen okay. I prefer a single list element, however, I rather suspect having two lists should be fine. Just know that it is very possible that the second list element would be seen as less important, though not much. For this reason, be careful to keep this in mind and organize your links accordingly.',\n",
       " 'Much of this depends upon what country you are in. In the U.S., the law is quite clear.\\nYour scenario does not provide enough information. For example, do you have permission to use the RSS and images even if you do not cache them? The following should apply either way. Hopefully, the answer will be clear.\\nIf you are using the image for commercial use (profit or monetary gain), then you cannot use the image as it is without a license or grant (permission). Get this in writing that carefully spells out the details of use.\\nIf you are not using the image for commercial use, you can use the image without license or grant in a derivative work (not simply modifying the image) and in a transformative work without permission. A derivative work will not apply for your site, however, a transformative work could. It has been determined that a transformative work of an image would be something like a thumbnail image. As far as I know, this is the only transformative work of an image that has been tested legally and survived.\\nIf a thumbnail is not what you are looking for, then attribution would be required along with permission in the way of a license or grant. Attribution is simply giving credit for the work. Often this is both text and a link.\\nWhether you create a cache copy or simply hot-link the image, you would need to follow the guidelines above.\\nAs well, keep in mind three things: one, the site that hosts the image that you are using, may not hold the copyright and may be bound by license or not using them legally; two, they may not be in a position to grant a license or permission; and three, the host site may require images to be hot-linked for auditing and/or to maintain control over the image itself such as to update it.\\nIf you use any portion of any other site on your site, you will need to get permission and attribution should always be given unless the grant or license specifically states that attribution is not required.',\n",
       " 'WPHeader and WPFooter are some kind of CreativeWork.\\nYou can see this in their type hierarchies (displayed at the top of every Schema.org type’s page):\\n\\nThing > CreativeWork > WebPageElement > WPHeader\\n\\nThing > CreativeWork > WebPageElement > WPFooter \\n\\nSo having WPHeader/WPFooter as value for the hasPart property is perfectly fine (\"expected\"), and using hasPart to denote that the WPHeader/WPFooter is part of a WebPage seems to be appropriate, too (header/footer are \"in some sense\" part of a web page).\\n\\nThat said, in my opinion there is no point to use the WPHeader/WPFooter types on normal web pages (details: 1, 2, 3).',\n",
       " \"Someone is trying to manipulate your URLs and getting away with them because of a flaw in your original PHP script.\\nYour script is allowing anything entered between /directory/ and /index.php to be rewritten (through your mod_rewrite rules) and 'allowing' it to actually return a valid page instead of a 404. So if I am your competitor or just a malicious individual, there is nothing stopping me from creating random urls like /corvid-memory-is-a-fraud/index.phpcreating not only a bad reputation for your website but also invoking a massive issue of content duplication.\\nYou need to revise your mod_rewrite rules in a way that when a request for any URL is generated, it should pass the part preceding /index.php to the script and the script should validate it by matching it with the database entry for the page name. It no matches are found, it should return an HTTP 404 error.\\nTalk to your developers/ server admins about it and they should be able to come up with the right solution to address this situation.\",\n",
       " 'An active forum with good quality posts entice users to sign up for the forum. The initial phase of running a forum is tough as it technically lies empty. It creates a catch-22 situation where lack of users create a lack of content and vice versa.\\nYou need to hire friends, freelancers to start posting on your forums on a regular basis to get the ball rolling. Adding content not only improves the credibility, but also leads to increased traffic and users. Several of these posts will appear in search engines for a lot of longtail keywords thereby bringing you targeted traffic.\\nIt takes quite some time to simulate a buzz and entice user signups. Keep posting content on a regular basis and it will start attracting users eventually.\\nA word of caution though - do keep an eye out to make sure that no duplicate or plagiarized content is posted in your forums. It will eventually lead to other complications with Google (PANDA)',\n",
       " \"In my honest opinion, that would be a very aggressive way to collect user emails and won't even serve the purpose you're looking for in the long run (they would temporarily feed in the email to access the content but later unsubscribe)\\nSuch overlays are also bad for ultimate UX since it's nothing short of forcing people to part their emails to read your content. Google appreciate websites that provide a good user experience and anything that goes against it would quite likely go against Google as well.\\nRemove the overlay and keep a prominent non-enforcing email subscription box on your pages for the users to subscribe. For effective email marketing, you need users who voluntarily give away their emails, not being forced to do so.\",\n",
       " 'I think this should do the trick for you. On the old domain, put this code in the .htaccess file (this assumes a 301 redirect is used).\\nRewriteEngine On\\nRewriteRule (.*) https://newdomain.example [R=301,L]',\n",
       " 'I use the following in the .htaccess on my bluehost account (as recommended by a friend)\\n  <Files ~ \"^[^\\\\.]+$\">\\n    ForceType application/x-httpd-php\\n    SetOutputFilter DEFLATE\\n  </Files>\\n\\n  <FilesMatch \"\\\\.(js|css|html|htm|php|xml)$\">\\n    SetOutputFilter DEFLATE\\n  </FilesMatch>\\n\\n  ExpiresActive on\\n  ExpiresByType image/png \"access plus 1 month\"\\n  ExpiresByType image/gif \"access plus 1 month\"\\n  ExpiresByType image/jpeg \"access plus 1 month\"\\n  ExpiresByType text/css \"access plus 1 month\"\\n  AddType image/vnd.microsoft.icon .ico\\n  ExpiresByType image/vnd.microsoft.icon \"access plus 3 months\"',\n",
       " \"If you are going to buy a premium theme for WordPress (or any theme/template for any CMS), you are better off buying one from a developer who has a track record of updating the theme as WordPress evolves.  This doesn't just apply for premium themes...when assessing free themes, check the changelog for the theme and see if the developer releases regular updates.\\nThemes do not get hacked all that often and when they do, it's because they bundle in functions that would otherwise be added as plugins in an attempt to make the theme more attractive for sale. These functions need to be tended to as if they were actually plugins, so as WordPress security evolves, so must the theme in order to stay secure. The other big security problem with themes are the free ones you get from random web sites (e.g. not wordpress.org or known-good developers) that may contain all sorts of backdoors.  If you don't know the reputation of the theme developer and the themes are not on wordpress.org's repository, stay away.\\nAs far as who you buy your theme from, that's really an opinion question and fairly hard to answer. However, whenever you pay for something it should come with some form of support.  Themes from Genesis, Woo, ElegantThemes, and other reputable names in the space DO provide support and updates, so that's a major plus. Services like ThemeForest provide a platform for developers to sell their themes/plugins and do not provide a blanket level of service in return. Instead, each developer is supposed to support their product.  My experiences there are mixed...some do a good job, some don't.  Because of that, I tend to not buy from places like that and instead give my business to the companies that are known to stand behind their product.\",\n",
       " 'Utilization in EC2 for cost purposes means the number of hours in a month that the instance is up and running, compared to the number of hours in a month.   \\nThe workload is not a factor in price.\\nInstances in the t2 class also have a concept of \"CPU Credits,\" because t2 machines are burstable.  They earn credits at a fixed rate while running,  spend credits at a rate that varies with CPU utilization, and can only use 100% CPU when there are sufficient credits on hand (1 credit = 1 core used at 100% for 1 minute, or 1 core used at only 10% for 10 minutes, etc.), but this also is not a factor in cost.  The credits are included with the instance, and the charge is the same whether they are used or not.  There is no way to purchase more credits, and the performance of the instance does not vary unless the balance is depleted, and will recover in its own whenever the instance is idle.',\n",
       " \"It's always good to go for % because it guarantees fluid content. It's good both for Google and your users to have a good mobile version of your site.\\nI don't think there would be a difference in the performance or at least not a difference that you can spot. You can test it by putting both of the properties and testing your page speed with a tool for both of them but I am certain that there wouldn't be any difference.\\nIn my opinion, % would be better than cover since it's easier for the browsers to render it and make it fluid for the different resolutions.\",\n",
       " \"You have no control over the headers of the Google Analytics javascript file, as it is not served by your server. The .htaccess rules you mentioned can only apply to your own files.\\nYou'll just have to ignore that particular error, and hope Google make the file a bit more cache friendly in future.\",\n",
       " \"You can indeed renew a .com domain (and many others) through another registrar - it's basically just a normal transfer process.\\nProviding your domain name is not locked, you can transfer it to a new registrar now and pay for an additional year of registration at the same time (which you have to do when you transfer anyway).\\nBecause transfers can take a couple of days to process though, if I were you I'd do this ASAP so your domain doesn't end up expiring in the process.\\nIf your domain is locked at your current registrar, you'll need to unlock it by logging in to your registrar's control panel. You can also grab your authorization key (sometimes called an EPP key) at the same time, because you'll need that to initiate the transfer. While you're there also double check your registrant e-mail address is up-to-date, because you'll need to approve the transfer through there as well.\\nChecking each of these above items will ensure the transfer proceeds smoothly and in time. Once you've done this, initiate the transfer and renewal through your new registrar.\",\n",
       " 'Found the answer.\\nOn an EC2 instance, ensure the following:\\n\\nNginx is on, and whatever server_name you\\'re using will allow you to access it.\\nIf your server has UFW on, then make sure incoming TCP connections are allowed (on ports 80 and 443)\\nLastly, go to \"edit inbound rules\" in your instance\\'s security group, and allow incoming connections via 80 and 443.\\n\\nGood to go!',\n",
       " 'The line is drawn at one redirect. \\nThere should be at most one redirect. In your case, that means if house A is out and house B and C are out but house D is still active, rather than going from A to B, B to C, then C to D, you would simply redirect the \"link juice\" from A to D for these reasons:\\n\\nIt places less load on the server. Only one extra request is made to the server instead of three.\\nIt creates an environment which gives the user the impression that the site loads faster. In other words, a blank screen won\\'t last as long when the page is requested. This can be easily observed if you test your URLs in webpagetest.org\\nIt puts less strain on search engine robots since they don\\'t have to examine all three redirects, thereby increasing the odds that the destination page is indexed faster.I believe google has a set budget in the number of page crawls it makes per website, and adding extra unnecessary redirect pages counts as a page crawl.\\n\\nSee https://developers.google.com/speed/docs/insights/AvoidRedirects for more info on why a redirect to a redirect is bad.\\nAlso, http://www.lostsaloon.com/technology/seo-how-to-avoid-landing-page-redirects-in-your-website/ explains an example of how a redirect should be done.',\n",
       " 'Apart from unserious offerings, you can distinguish between cheaper domain-validated SSL certificates and the more expensive extended-validation SSL certificates (EV).\\nBoth certificates are technically the same (the connection is encrypted), but domain-validated certificates are cheaper, because the seller only have to check the domain. The EV-certificates also require information about the owner of the domain, and the seller should check, if this information is correct (more administrative effort).\\nNormally you can see the difference when you visit the site with a browser. Firefox for example will highlight the domain in blue for domain-validated SSL, and green for extended-validation SSL.\\nTwo examples:\\n\\nhttps://accounts.google.com/  (domain-validated)\\nhttps://www.postfinance.ch/  (extended-validated)\\n\\nIn most cases the domain-validated certificate is fine, the user will have no disadvantages and the EV-certificates are really (too) expensive.',\n",
       " '\"Cloaking\" in this situation would be fine.   When the user agent contains bot|crawl|slurp|spider you should not use session id parameters or check for cookies.  You are delivering the same content to users and bots.  Google won\\'t have a problem with this particular cloak.\\nI use a similar technique for deciding whether or not to use Data URI for the images on my site.   I treat all bots the same as IE 7 and earlier which cannot handle Data URIs.  Technically it is cloaking, but all bots get the same data and would render the same pixels on the screen as users.  They just get that data through different technical means.\\nIt might also be worth exploring different ways of handling the issue. If it were my site I might set cookies, and then use JavaScript like this on the links: onclick=\"if(!document.cookie.indexOf(\\'session\\')this.href+=\\'?session=abcdef1234\\';\" Googlebot doesn\\'t execute the onclick when following links, so it would still be able to crawl your site without parameters.\\nWhen you are using session parameters, you should log into Google Search Console and tell Google to ignore them.   Open the URL Parameters Tool or view the crawl parameters documenation.  You will want to add your session parameter and set it to \"Doesn\\'t effect page content (ex. tracks usage)\".',\n",
       " \"The span element doesn't mean anything on its own.\\n\\nhttps://developers.whatwg.org/text-level-semantics.html#the-span-element\\nThis element is used as a hook for styling or javascript access and doesn't change the document flow. So Google might even just ignore the tag for that very reason and only look at the content of it except if one were to use it to cloak content.\",\n",
       " \"All links have some value even minute amounts including nofollow links. However, if the links come from a site/page that is fully egregious, then you may want to consider disavowing them.\\nGoogle considers whois sites as a whole as low quality, however, not all whois sites are considered junk. This is also true for the various aggregator sites. They may be junk in your eyes, but not junky enough in Google's eyes. To know where a site stands is to see if it suffers a penalty. If so, then disavow the links from that site. If not, then consider if you think the site falls below your personal level of tolerance. For example, the site really really sucks while others do not suck as bad. (sorry if I broke a few rules here)\\nGenerally speaking, these links are fine though not great.\\nI found one well ranking/performing site, the domain has since been deleted, that only had links from two sites; 36,000 from one site and just a bit less from another. There was about 6 or so organic links. Both linking sites were pure junk. However, sites that link out and receiving links from these sites can have value, albeit small per link. Promiscuous linking, low quality links, and the like, can still build rank and pass rank. Just in much smaller doses. Still, it adds up.\\nOne company famously stated their philosophy as, I'd rather get pennies from many people than nickles from less. It is easier for someone to give up a penny than a nickle. (paraphrasing - not an exact quote)\\nThere seems to be an argument as to whether nofollow links pass value or not. Google as misinformed us in the pass. Some would say outright lied. Some Googlers have contradicted themselves within just a few minutes. The up-shot is that nofollow does still seem to provide some value regardless of what has been said by Google. I have seen evidence that this is a true statement. Nofollow or not, I would treat every link as if they were follow. It is far safer that way.\\nSo, are these links bad? Likely not. But some may be.\\nShould I disavow them? Likely not. But for sites that Google thinks are beyond the pale, consider it.\",\n",
       " \"Your third option is definitely the way to go - leave all the bloat behind and go static. I recommend Jekyll for managing the static site since it has the largest number of users, which makes finding tutorials and troubleshooting answers easier.\\nThere are two approaches you can take, either export your WP site to Jekyll-ready output with a plugin or set up Jekyll first and use it to import your site.\\nPosting a complete tutorial on how to export/import the site and set up Jelyll are way beyond the scope of this site but that should get you started. Once you have the site converted to completely static files, you can put everything on your choice of hosting services. AWS's S3 or CloudFront are popular choices and are inexpensive, but you can put the static files on almost any hosting service.\\nIf you are worried about setting up Jekyll, try doing it on a virtual machine using something like VirtualBox. It's free and you won't make a mess of your system if you run into problems. As always, make a backup of the site database and files before trying anything new.\\nIf you have SSH access to the machine currently hosting the site you can take a quick backup like this, substituting the UPPERCASE for whatever your variables happen to be (hopefully this is self-explanatory):\\nDatabase backup mysqldump --lock-tables -u DATABASE_USER -pPASSWORD DATABASE_NAME > /PATH/TO/BACKUP/SITE_NAME-database-backup-$(date +%Y%m%d).sql\\nSite files: tar czf /SAVE/LOCATION/SITE_NAME-files-backup$(date +%Y%m%d).tgz -C /var/www/SITE-FOLDER/ . (don't forget that trailing space-then-dot . it's not punctuation or a typo).\\nDon't output the site or DB backup to somewhere the web server can serve them obviously, or anyone can download them and discover your configuration files and passwords. I see a surprising number of WP sites doing this, try visiting the occasional WP site's IP address directly to see for yourself - it shouldn't take many tries.\",\n",
       " 'There is no boost in SERP placement as a result of using mark-up with one round-about exception. I will explain.\\nA page has mark-up and a search query makes a match against that page. Simply because the matched page has mark-up, one cannot assume there is a boost in SERP placement. That is not how it works and an overly simplistic view. There is no metric and/or algorithm that will boost a pages placement simply because a page has mark-up.\\nHowever, answers from the knowledge graph can influence trust for the match over others and early search query algorithms may place any match found from the knowledge graph over others. Mark-up is not necessarily a requirement for the knowledge graph since the knowledge graph predates mark-up, however, it is a tool for trusting the data from content. One requirement for inclusion into the knowledge graph is that the data is verifiable/corroborated in more than one trusted place. Please keep in mind that this is extremely early in the search query process. What has to be remembered is that are quite a few algorithms that influence SERP placement and can ultimately remove entries from the SERPs including filters that effect SERP level penalties. So again, mark-up in of itself does not influence SERP placement, however, matches from the knowledge graph that may be a result of mark-up may influence placement.\\nPure and simple, mark-up is not available to all websites. For example, there is (last I looked anyway) no mark-up for whois and other network information. There are other examples primarily in the data presentation market. Mark-up is targeted to sites that can benefit most such as e-commerce, business, organizations, etc.\\nBe that as it may, mark-up is still a good idea. Mark-up is intended to communicate data directly to search engines in a way that can easily be understood without having to parsing pages. Parsers are difficult to write and can fail from time to time due to unseen exceptions. While Google has gotten it right for a long time without mark-up, mistakes are still made. I saw a few just yesterday! Mark-up is your opportunity to be unambiguous. It is wise to take the opportunity even if it does not appear to be needed.',\n",
       " \"One, two or even a few months sometimes isn't enough time for Google to update its index with changes in a website and start displaying results in the way the website owner is desiring. And it's not only time what is needed, but thorough SEO work in all aspects.\\nNow, regarding Google showing main links pages for a certain website like in your screenshot, it is Google that will decide when to show this and on what kind of search. Website owners have less control of what links Google will decide to display there, but they can tell Google to remove certain links, if they don't want them to appear using Google's Webmasters Tools.\",\n",
       " 'It makes no difference to SEO. Just remember to put your keyword at the beginning of the title and the branding at the end. Search engines always place more importance at the beginning of a text block and you should always rank for your branded terms. Check out this resource of best practice for writing meta titles:\\nhttp://www.seomoz.org/learn-seo/title-tag',\n",
       " 'From experience I can tell you it will skew your clientele toward more adult sites and away from non-adult sites. This is a natural result. Adult site owners are looking for friendly site developers while companies will shy away from a developer that does adult sites. That is a given.\\nAs for a penalty, that depends upon the site, what happens to the site in the future, and how many adult sites that link to you that go bad. Beyond that, Google tolerates links within headers, sidebars, and footers to your own sites, however, considers site wide links such as \"developed by\" in footers links as link spam.\\nFrom: https://support.google.com/webmasters/answer/66356?hl=en\\n\\nAny links intended to manipulate PageRank or a site\\'s ranking in\\n  Google search results may be considered part of a link scheme and a\\n  violation of Google’s Webmaster Guidelines.\\n...\\nThe following are examples of link schemes which can negatively impact\\n  a site\\'s ranking in search results:\\n...\\nWidely distributed links in the footers or templates of various sites\\n\\nYou also run the risk of any site going bad. One famous example is whitehouse.com which ultimately changed from one of the most successful adult sites to one of the worst spam sites causing all kinds of problems. It was a specific target of Google\\'s anti-spam team and will remain as a prime example of a spam site throughout history.\\nThis is not something I would do. I also do not recommend it.',\n",
       " \"Google,\\xa0conceptually, uses an HTML DOM parser. What this does is break any web page HTML down into its basic structure and each HTML tag is given an ID. This ID represents the order of the HTML tags from beginning to end, any dependency between HTML elements such as a li tag is dependent upon a ul tag, any parent-child relationship between HTML elements such as nesting li tags, any content block relationships between HTML elements such as a p tag following a header tag such as h1. This structure is represented using a language such as XML which is traditional.\\nKeep in mind that HTML to XML parsers have existed a very long time.\\nOnce the elements are broken apart, any a tag can further be broken down into its elements. Any time a page is parsed, the first thing that is done is that all links are stored into the index within a link table. This link table is a relational table that has a relationship with a URL table. The URL table stores the URLs of pages while the link table simply make relations between records in the URL table with the link text. If you are not familiar with relational databases, this may not fully make sense. To that end, each table is like a spread sheet. One sheet has URLs. One sheet has link text and references to records within the URL sheet.\\nA link within the index has three basic elements; the source URL (reference), the target URL (reference), and the link text. If a link is stored into an index where only the page it was parsed from (source) has a URL within the index, meaning that the target URL has not been fetched yet, it is a dangling link. The URL the link is pointing to (target) is then placed within the fetch queue to have the page fetched, indexed, etc. If the target page cannot be fetched, it is a broken link and remains within the index as a broken link for reference.\\nThis is a recursive process, meaning that it begins and ends repeatedly; fetching pages, parsing pages, and indexing pages. For search engines, these processes are broken into individual independent processes. Some search engine processes are queue based, meaning they take a record from a queue (list or databse) and processes it, or trigger based, meaning that a trigger event starts the process, or batch based, meaning that it performs a process against the entire database.\\nPages are fetched from a queue of URLs. Once the page is fetched and stored, a trigger event is set to parse the page. Once the page is parsed, various other processes are triggered including one that processes links. Each trigger based process is considered real-time. Contrast this to the PageRank algorithm which is batched based and runs periodically.\\nThis process is called crawling. It is like a spider that crawls the web. As each page is fetched, parsed, and link target URLs added to the queue to be fetched, most pages are discovered very easily. For the remaining pages that do not have a link, the sitemap comes into play. While it is not generally necessary for a site to have a sitemap, it can help the search engine know that it is able to fetch all of the site's pages adequately. Sitemaps are primarily used to audit whether a site can properly be crawled. For any page listed within the sitemap that does not have a target link, the URL is submitted, as read from the sitemap, to the fetch queue to ensure that the search engine as as many pages as can be fetched from any site.\\nThat is it. It is a simple process that has existed for a very long time and works amazingly well.\\nPages are periodically refetched. This is based upon a network concept TTL meaning Time To Live. This is simply a number representing seconds. For example, 5 minutes is 300 seconds and 24 hours is 86400 seconds. While no-one knows what the starting TTL time for a web page is, this TTL is adjusted for each paged from either a longer time period or a shorter time period depending upon whether the pages changes or not. There is a process to determine if either the page content changes or templated content changes with an algorithm to determine what changes are of value or not. This means that links in a sidebar may not make a page's TTL time shorter while a change within the content will.\\nThis is important to know because this is how a search engine determines, in part, a page's freshness. Of course any new page is also fresh. If a page changes frequently, it is fetched more frequently using the TTL time as a trigger. The shorter the TTL time, the more often the page is refetched, parsed, indexed, etc. Each time a page is refetched, the TTL time is shortened to determine how often a page should be fetched. It is the shortening and lengthening of the TTL that allows the page to be fetched appropriately according to how often it changes. There is a maximum TTL. For example, any page that does not change will be checked using the maximum TTL. This allows a search engine to timely process any page.\\nThe freshness TTL time exists for each page and will effect how links are found on that page. Pages with shorter TTL times will have links found quicker than pages with longer TTL times.\\nThe reason why this is important to this answer is because of links. More often than not, the pages that are fresh have links to other pages that may also be fresh. Blogs are a prime example of this. Are you getting the picture? These links get submitted to the fetch queue just as before making link discovery that much faster.\",\n",
       " 'I create URLs like this automatically daily. I scrub non-alpha-numeric characters in the process. I would avoid characters that require encoding in the URL and link. It is not necessary. Search engines can figure out what you mean especially if you specify it correctly within content.\\nMy advice would be to fix your URLs and links so that these special characters do not exist. As for existing URLs that the search engines already have, I would just let them 404 or present a 410 if you want to do the work and just let the process work itself out. It will take a while, however, you will better off for it in the end.',\n",
       " \"...in case the webhoster disabled the use of .htaccess files?\\n\\nIf the webhost has disabled the use of .htaccess files then there is no direct alternative. (.htaccess = per-directory Apache config file)\\n.htaccess files are not necessary if you have access to the Apache server config. In fact, it is preferable to use the server config instead of .htaccess anyway, but by the sounds of it, you do not have that luxury.\\n\\nI can set some PHP variables and headers inside the PHP files\\n\\nWith PHP 5.3+ under CGI/FastCGI then you can use .user.ini files for per-directory PHP config settings. But that's just for PHP config settings, eg. error_reporting, include_path, etc. (Mind you, under CGI/FastCGI you need to use .user.ini, since you'll get a 500 internal server error if you try to use php_flag and php_value directives in .htaccess - these are for when PHP is installed as an Apache module.)\\nIf your (shared) host has disabled .htaccess and you are wanting to do more that serve simple files then... find a new host.\",\n",
       " \"You'll need to use mod_rewrite (as opposed to a mod_alias Redirect) and check the HTTP_HOST server variable (which tells you which site has been accessed). Something like the following at the top of your .htaccess file:\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} ^(www\\\\.)?website1\\\\.com$ [NC]\\nRewriteRule ^example/(.*)$ http://www.website2.com/example/$1 [R=302,L]\\n\\nChange the 302 (temporary) to a 301 (permanent) redirect if you need a permanent redirect, but only after you have tested to make sure it's working OK. (301 redirects are cached by the browser so can make testing problematic - unless you test with the browser cache disabled.)\\nThis redirects /example/<something> to http://www.website2.com/example/<something>, in much the same way as the original Redirect directive would do (which is prefix matching).\\n\\nUPDATE: To redirect just the homepage, ie. http://www.website1.com/ to http://www.website2.com/, you can use something like the following:\\nRewriteCond %{HTTP_HOST} ^(www\\\\.)?website1\\\\.com$ [NC]\\nRewriteRule ^$ http://www.website2.com/ [R=302,L]\\n\\nNote the RewriteRule pattern ^$ - this matches the empty URL (ie. the homepage only). (Note that the URL matched by the RewriteRule pattern is less the directory-prefix.)\",\n",
       " 'Nothing special to do here, the code is your content. You mark up your code with appropriate elements, and consumers (like search engines) then can do whatever they want to do with this information. A code search engine might be especially interested in it, other search engines might ignore it, most will probably don’t care and handle it in the same way like non-code content.\\nYou should use the code element, for blocks of code as well as inline code, just like I did in this very sentence (see example).\\nIf you have a block of code where the formatting (i.e., multiple lines, indentation) matters, you should use the pre element in addition to code (see example).',\n",
       " \"On a WordPress site\\n\\nWordPress uses mod_rewrite to handle the URL routing (pretty URLs) - which I assume you are using - so you should avoid using a mod_alias redirect (Redirect, RedirectMatch, etc.) in this instance. (Different modules run at different times, regardless of the order of the directives in the .htaccess file, so you can get unexpected results/conflicts. mod_rewrite usually runs before mod_alias.)\\nIn .htaccess it's easy enough to redirect example.com/subdomain/parent-page/<anything>, but if you only want to redirect existing WordPress pages and 404 when that page does not exist then that is more complex and you should probably seek a WP solution, rather than .htaccess.\\nTry something like the following at the top of your example.com/subdomain/.htaccess file. These directives must come before any existing WordPress routing directives.\\nRewriteEngine On\\nRewriteRule ^parent-page/. /subdomain/parent-page [R=302,L]\\n\\nChange the 302 (temporary) redirect to 301 (permanent) when you are sure it's working OK. (301's are cached by the browser so can make testing problematic.)\\nAny URL that starts /subdomain/parent-page/<something> will be redirected (regardless of whether that sub-page exists or not). This also helps to avoid a redirect loop.\\nRewriteEngine only needs to appear once in the .htaccess file - preferably (more logical) at the top.\\n\\nRedirectMatch 301 parent-page/?* example.com/subdomain/parent-page\\n\\nAside... There are a few things wrong here. The regex is not strictly valid. * matches the preceding char 0 or more times (the preceding char is ?, which is itself a meta character). If you are specifying the domain in the target URL then it must be absolute with protocol etc.\",\n",
       " \"Every folder can have an index.html file, this is the first file your webserver will tipically look if you do not specify another file in in URI.\\nI have the same folder structure in my personal website as you (with Spanish instead of French) and works pretty good.\\nSo the problem here has to do with how your server access your subdirectories or how your web framework handles routes. You should investigate in each logs errors as having an index.html in each folder isn't the problem, is a very  common practice.\",\n",
       " \"Once you start using Adsense, they detect that you have placed the ad code on the specific page you sent when signing up, they will \\n\\nautomatically review your entire site (not just the page that you\\n  submitted in your application)\\n\\nFrom my personal experience It doesn't matter if you change some of your website content while you are awaiting for the review process, the content of many websites changes everytime and Adsense is aware of that.\\nIf your new content doesn't comply with any of the Adsense policy they will send you an email telling you the problem they have found expecting you to fix it as soon as possible.\",\n",
       " \"Yes, Google has started supporting javascript in their bots, this is due to the increasing popularity of javascript in the web and frameworks like Angularjs. \\nTaking the bigger picture, the web isn't just about urls, it is about content and user experience, every day they improve their algorithm to make it better with these goals in mind, so one hack that could work today to trick bots would be detected in the future and your site is gone.\\nIf the content of your webpage differs from what the user sees, they will detect it, the user experience will be awful and your website will be very low in SERPs.\",\n",
       " 'According to Google, \"Impressions - How many links to your site a user saw on Google search results, even if the link was not scrolled into view. However, if a user views only page 1 and the link is on page 2, the impression is not counted.\" (source)',\n",
       " 'The current GoogleBot Smartphone agent, as tested with the \\'Fetch as Google\\' Tool is essentially a fake iPhone using a headless Webkit Engine, running on a Linux x86_64 desktop machine.\\nThe default non-responsive viewport width is that of an iPhone at 980px.\\nWith a viewport\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> applied, it has a Screen Size and CSS Pixel Display Resolution of an iPhone 5s at 320x568** pixels with a 2x Device Pixel Resolution emulation.\\nNB: The Engine does not support Touch nor window.orientation like an emulator would.\\nUPDATE: *As of Dec 2015, the bot has now been updated to a viewport size of 375x667 pixels (size of an iPhone 6) and the engine is now using a Pre-Blink QtWebKit engine with a v8 JavaScript engine.\\nUPDATE 2: **As of April 18 2016, The size has changed again for a Nexus 5X screen size of 411x731 pixels and a new user agent. Noting that 411x731 is the screen size info. But the actual viewport size (clientWidth and clientHeight) are reported as 410x730px,\\nwith an emulated DPR (devicePixelResolution) of 2.625. The platform\\'s given architecture is armv8l.\\nUPDATE 3: As of April 18 2017, it has switched to the blink engine. Same dimensions, resolution and platform. UA unchanged.',\n",
       " 'In general, showing different content to Google than is shown to users is regarded as \"cloaking\". It\\'s a violation of Google\\'s guidelines and so, yes, a potential problem for SEO.    \\nHowever for subscription sites, Google operates a number of systems that allow for subscription-only content to be crawled and indexed.\\nOne is \"first click free\", which allows non-subscribing visitors a free view of the content. The other is a \"subscription\" label on the search result.  \\nBoth offer a compromise between the publisher\\'s desire to promote the content and Google\\'s desire to maintain good user experience by ensuring users can either preview the content, or know before clicking that it\\'s inaccessible without a subscription.',\n",
       " 'I don\\'t think it makes a difference unless the domain itself contains special (multibyte) characters, such as the ones recently approved by ICANN.\\nYou can have a page with a .cn domain that is all in English, and it will turn up in English results. Additionally, I\\'ve seen lots of .us domains in Russian, Chinese, Spanish, etc.\\nWhat matters is the locale used when publishing, and vanity. If an e-commerce shop boasts \"Located right in the heart of the UK!\", then it would probably want a .co.uk domain. Then again, most US businesses would want .com domains, rather than .us.\\nThe other use is organizing your network. Servers in the US might be servername.foo.us, where servers in China might be servername.foo.cn, especially if redirection based on geo location  is in use.',\n",
       " 'I would canoncilise the original page without the parameters to itself, which explains to google this is the original and you can ignore the rest.\\n<link rel=\"canonical\" href=\"http://domain.org/site/UserLogin\" />\\n\\nWe had to do this to a client website recently, took a few days to kick in but got rid of the duplicates. ',\n",
       " \"Google is indexing only a portion of your pages because your site doesn't have enough reputation.   Splitting your reputation between three subdomains isn't going to help.   You will still only be able to get 1M pages indexed.\\nYou need to increase the reputation of your site.\",\n",
       " \"It really boils down to the queries you expect to make. \\nIf you use hit-scope, you will be able to create segments that differentiate actions that take place within a session. So a user could start as a guest, then log in to become a Member, the pay for the subscription to become a Subscriber, and finally log out to become a Guest again. With a hit-scoped dimension you can query all of these, but the queries can become quite complex.\\nIf you use session-scope, then the last value you sent into the custom dimension will apply for the entire session. Thus you'll lose the granularity. The good thing about session-scope is that you only need to send it once. It will automatically apply to all hits of the session.\\nUser-scope is like session-scope, but it will apply from the first time it is set for as long as the user has the same client ID.\\nHow I would approach this is to look at the three levels. Member and Subscription are clearly scoped to the user, since if you are in one, you are in that until you upgrade, right?\\nGuest on the other hand tells you nothing about the user level. They could be a Subscriber but they just haven't logged in.\\nSo here's how I would track it:\\n1) Do not track Guest at all. You can query for Guest by creating a segment for all sessions where the User State custom dimension does not contain Member|Subscriber.\\n2) Track Member and Subscriber as User-scoped dimensions.\\n3) Track Login and Logout as events, and send the current user state as an Event Label or a hit-scoped custom dimension.\\nWith these three you will be able to:\\n\\nTrack sessions where the user didn't log in (exclude sessions with Event Category = login), and you can also combine it with the user-scoped dimensions to see how often members and subscribers browse the site without logging in.\\nTrack member and subscriber traffic also on sessions where they didn't login (thanks to user-scoped dimension)\\nTrack sessions where the user logged in as member (include sessions where Event Category is Login and Event Label is member), but the state is subscriber. This means that these sessions were the users upgraded their subscription.\\n\\nAnd so on.\\nSimo\",\n",
       " \"The main domain record (foosite.com), unlike whatever.foosite.com cannot be answered as CNAME, only A or AAAA. If I remember it properly, it is by RFC.\\nSolution: e.g. amazon has tricky way. The key here is not 'the main record cannot have cname record' but 'the main record cannot be answered as cname record'. They made their own record type - alias.\\nThe difference between cname and alias is that when a dns client queries a site with cname - it receives a reference to another domain and have to resolve it too, whilst for alias record the dns server itself resolves the result and returns the IP - as it should be by RFC.\\nTL;DR\\nUse Route53 at amazon or something like that.\",\n",
       " \"There are two WordPress plugins I've used for managing 301 redirects: Redirection & Simple 301 Redirects.\\nEither plugin makes it easy to define redirects from Old -> New.\\n\\nThe WordPress Codex also has a guide to Creating an Error 404 Page. If that doesn't work, you may try contacting your web host to see if there's a configuration option you're missing.\",\n",
       " 'The tag attributerel=\"autor\" is not relevant for Google anymore, the preferred way to specify the authorship is through the proper schema.org snippet.\\n\\nAuthorship in web-search\\nAuthorship markup is no longer supported in web search.\\nTo learn about what markup you can use to improve search results,\\n  visit rich snippets.\\n\\nRegarding the publisher markup, it also has changed the way to specify it, now it is suggested to link the G+ page with the website, this can be done through the Search Console:\\n\\nVerify your website in Search Console. So you can connect the website with the brand page\\nGo to the G+ website profile and in the \"About\" tab, then link it to the website. \\n\\nThese is what it looks like:\\n\\nGoogle plus About section:\\n\\nGoogle Search Console with the linked brand (publisher) page',\n",
       " \"Don't bother getting that IP off those lists. Too much effort, outcome uncertain, and no guarantee for the future. \\nHow much mail do you send? Is that hundreds of mails per months, or tens of thousands? If it's not more than hundreds, use the mailserver of your ISP - your home cable account. Or rent another cheap mail account like mailbox.org for 1 euro per month. \\nYou can sent mail from your webserver via their mailserver using authenticated SMTP, with username and password. It's the same as you send mail from your laptop when you're in another location. From home, on the network of the ISP, you can probably do without authentication, but from other networks, you need to authenticate to send mail. \\nAnother option is to find a hosting service which gives you a personal IP address. Then nobody can abuse your server. The other option is probably a lot easier!\",\n",
       " \"1) No because it's the same content and only the protocol has changed. You may notice a temporary dip as Google discovers the change and updates its indexes but it will go back to what it was before.\\n2) Not with a redirect. Service resources will increase somewhat because you would be encrypting your data. Otherwise, no.\",\n",
       " 'You are missing some closing </div> tags. If I correctly interpret your intentions, it should look like this:\\n<div itemscope itemtype=\"http://schema.org/SocialEvent\">\\n\\n  <a itemprop=\"url\" href=\"www.convention-name.org\"><div itemprop=\"name\"><strong>Bob\\'s Convention</strong></div></a>\\n  <div itemprop=\"description\">blah blah blah blah blah</div>\\n  <div><meta itemprop=\"startDate\" content=\"2015-05-01:00.000\">Starts: 2015-05-01:00.000</div>\\n  <div><meta itemprop=\"endDate\" content=\"2015-05-02:00.000\">Ends: 2015-05-02:00.000</div>\\n\\n  <div itemprop=\"location\" itemscope itemtype=\"http://schema.org/PostalAddress\">\\n    <div itemprop=\"streetAddress\">55 street</div>\\n    <div>\\n      <span itemprop=\"addressLocality\">Nashville</span>,\\n      <span itemprop=\"addressRegion\">TN</span>\\n    </div>\\n    <div itemprop=\"postalCode\">37225</div>\\n    <div itemprop=\"addressCountry\">usa</div>\\n  </div> <!-- /PostalAddress -->\\n\\n</div> <!-- /SocialEvent -->\\n\\nNow checking this markup, Google’s Testing Tool will report different errors for PostalAddress:\\n\\naddress: missing and required\\nname: missing and required\\nField location may not be empty.\\n\\nAdding a name to the PostalAddress item fixes the last two errors, so it leaves you with this strange error: \"address: missing and required\".\\nStrange, because PostalAddress can’t have an address property, as I’ve explained in an answer to a similar question on Stack Overflow. My guess: Google’s Testing Tool is bugged.',\n",
       " 'To answer this, you have to know something of the mechanics behind how all of this works.\\nEvery time a link is discovered, it is entered into a link table within he index. The source URL, the page with the link, is at least already within the URL table within the index. If the target URL, the page being linked to, is not in the index, it is a dangling link. This means that Google has not investigated both ends of the link yet and therefore no metrics can exist. When a link is discovered and entered into the link table, the target URL is entered into the fetch queue.\\nWhen the page is fetched, assuming nothing goes wrong, it is indexed and the link value metrics can be calculated and applied. This is done in a batch process periodically. It runs quickly and the last I heard this was run monthly, however, likely can run more often. The process is recursive. Let me give you an example. If a page has a particular PR, as links to that page are added and dropped, the PR for that page changes and therefore can effect the pages it links to. This is not a linear calculation and must be repeated over and over until the effective change would be 0 or minimal enough to have no real effect. Also please understand that the PR model we see where a PR6 page with 2 outbound (external) links to other pages passes PR3 through each link is completely invalid. Authority caps and link value scores are applied to give a natural curve in the PR model. Otherwise, no site could ever compete against a super authority site and the whole PR model would be forever skewed.\\nBe that as it may, only links that are complete, meaning links where the source and target URLs exist can have calculations applied. Dangling links and broken links (links to non-existing pages), are missing the requirement of a target page. It is impossible and impractical to pass link value to nothing.\\nSo in your case, since the pages are restricted by the robots.txt file, there is no target URL and therefore no value passed.\\nWhat would be needed to further understand your scenario would be a better understanding of exactly what is being redirected and how. Anytime a series of redirects exist, the original page must be fetched to know this. There is a period of time as the search engines fetch these pages, measure the value of these pages, change the metrics, and compare targets against the robots.txt files, etc. All of this takes time and has the potential to be disruptive. It is possible that you are working through the process now.\\nMy advice to people is to make life as simple for the search engines as possible and not to confuse things too much. Search engines like 404 errors and 301 redirects, however, a 301 redirect can have funny results sometimes and not as webmasters always expect. If it is possible to think this through a bit and simplify things, this may be a good opportunity to do so.',\n",
       " \"There is no way to tell Googlebot to forget about something it has crawled.  \\nYour only recourse is to:\\n\\nFix the problem with your HTML.\\nRedirect any faulty URLs that were caused by the problem.\\nWait until Googlebot has recrawled all the pages with faulty HTML and all the bad links that those pages might have generated.\\n\\nNOARCHIVE prevents Google from showing a cache of the page to users.   It has no effect on whether they crawl it again, remember it internally, or use its links to crawl other pages.   Google will always crawl links in a page unless that page has the NOFOLLOW attribute.   However, NOFOLLOW cannot be applied retroactively.\\nI tend not to put much into my .htaccess files either, but I don't usually disable them entirely.   I find that redirects are often best implemented in the programming logic that powers the web application rather than in .htaccess.   You might consider moving your redirects into your software, however that isn't an option if your site is static.\",\n",
       " \"You need to let Googlebot crawl all your sites regardless of what country the crawler is based in.    Once you disable the IP redirects for Googlobot, then Google will be able to see all your hreflangs and show the correct content to users in the Google search engine.\\nGooglebot does most of its crawling from the US and right now, you are likely redirecting Googlebot to your US site.   Googlebot doesn't have separate crawlers from separate countries, so it tries to treat every URL as global unless it sees a hreflang on the page.   You need to let the US Googlebot browse you Canada site as well as your US site.\",\n",
       " 'Without knowing the domains linking to your website and indeed your own domain, it is impossible to be 100% certain of what nature and reason those domains have a number of links pointing to a page on your website but there is not necessarily any cause for alarm or concern.\\nFirstly metric-wise, don\\'t worry that a domain has a low Moz domain authority that is linking to you, DA is based on authority passed through linkage so if a website is relatively new or for whatever reason, does not have that many external links pointing to it, it does not mean that it is spammy or should not be trusted or can cause harm.\\nThere are many possible instances that could lead to a sitewide link pointing to one of your pages, or multiple links from a domain pointing to one of your pages. When you have multiple links coming from the same domain to the same page on your own website, the sitewide/multiple nature diminishes the links anyhow to something roughly equivalent to just one link from that domain to your page being counted/paid attention to.\\nSomething to note, there is nothing (or at least, very little) you can detect as \"unnatural\" in Google Search Console as it is quite a basic tool that only touches the surface of website behaviour.\\nIf you want to check for any malicious activity in relation to your website and external linkage, ensure that you don\\'t have an abnormally high count of unnatural and spammy looking exact match keyword anchors pointing building to your website. This will be the quickest way a competitor can win with negative SEO against you.\\nHope that helps - of course, we could provide more information knowing the domains in question censored in your screenshot.',\n",
       " \"The disadvantage would be if your server is slow or your connection is slow. If both are fast then, no, you don't need a separate server.\\nYour server can get bogged down, even with small files, if you get a lot of random requests for different files and it doesn't have enough memory to keep it all in cache. Even Gigabytes of ram won't help if your internet connection is slow. Even two servers won't help if either is slow.\\nSometimes it helps to have separate servers in a CDN fashion where your users are closer to a CDN server but, again, that's only if you have high traffic.\\nThose are the main considerations.\",\n",
       " 'The term duplicate content typically refers to multiple pages that have the same (or mostly the same) content, not to multiple pages that have a few chunks of the same content but different main content.\\n(In the case of Google Search, they call it \"substantive blocks of content\" which have to be the same or similar.)\\nIn your case, it seems that you don’t have duplicate content in that sense. Each of your products has a unique description (this would be the main content) which just happens to share a few parts with other products. \\nI think you do want to get this \"duplicated\" content indexed for your products, otherwise your products wouldn’t be a good match for search queries like \"hiking boot Gore-Text\" or \"hiking boot sticky rubber\", so loading it via Ajax might not be a good idea (not all search engines support JavaScript).\\nThat said, if a repeated feature description becomes lengthy, you might want to consider shortening it and link to a separate page which gives the full information about that feature. This would be similar to Google’s advice (from the page linked above):\\n\\nMinimize boilerplate repetition: For instance, instead of including lengthy copyright text on the bottom of every page, include a very brief summary and then link to a page with more details.\\n\\nWhat \"lengthy\" might mean depends on how much other content your page has. If the unique description only consists of two sentences, but you have five blocks of (repeated) feature descriptions, each a few sentences long, it might become problematic. If the feature descriptions are similar to your example, you should be fine.',\n",
       " 'Your understanding is correct. The sequential link types are for documents that are \"part of a sequence\". \\nI would only use it for blog posts that belong together (\"… Part 1\", \"… Part 2\"), for a blog post that is paginated, or for blogs where the order of the posts matters (e.g., in a diary, where each day has its own post). Or in other words, only in cases where it can be expected that users want to read the next post.\\nBut there is no objective definition what would/wouldn’t count as \"sequence\". Using it for blog posts that aren’t really related (i.e., they just happen to be posted in that chronological order) doesn’t seem too far off. As far as the semantics are concerned, I wouldn’t expect any negative effect resulting from this.\\nHowever, WHATWG’s HTML Living Standard changed the next link type recently (this change might become part of W3C’s HTML 5.2, too). If it gets used with a link element, user agents should use a Resource Hint. This means that user agents should act as if the author also specified one of these link types: dns-prefetch, preconnect, prefetch, prerender. So depending on which resource hint a user agent chooses, it can cost bandwith, battery life, affect the performance, etc. \\nSo this would be one more reason to use the sequential link types only in cases where users can be expected to want to visit the next page. Using them for posts that don’t really belong together could waste resources.',\n",
       " 'I disagree with the comment that SEO is extremely complex. Actually, it is common sense stuff. There is no magic, voodoo, special formula, incarnations, specific sequence of buttons and switches, etc. You do not need the voodoo priestess Bloody Mary to come to your house or office. How search engines work is very simple and only a handful of techniques that are well documented in research papers (having little or nothing to do with search) are applied to most algorithms. The largest SEO gyrations are where search engines decide that something is a good idea and in reality, it is not. For example, the notion of keyword exact match domains deserve special weighting that makes it stand out. That was foolish right from the start. Had the keywords had been weighted naturally as they are most everywhere else, we could have avoided that silliness.\\nHaving said that.\\nDo not complicate things. Think simple.\\nThe notion that a sub-domain performs better as in the comment cannot be counted upon. The only advantage is adding one or two keywords within the URL which is weighted very lightly these days due to the aforementioned weighting of exact domains. You have to contend with the fact that the sub-domain would have to be populated with content as a full site and optimized in of itself in order to have any effect at all. Far too much work for such little gain. As well, it is too much risk.\\nNow on to your question:\\nThe reason why, as you mentioned, Google will prefer the page which have search query up in the URL hierarchy, can be explained in how Google and most search engines makes query matches. Let us use your example.\\nTo begin:\\n\\nGoogle weighs keywords from left to right with some exception.\\nGoogle weighs known keyword phrases more heavily.\\nGoogle weighs URI keyword phrases/clusters separated by a slash [/] from\\nleft to right.\\nGoogle weighs keywords used more frequently overall less than\\nkeywords that are more specific.\\nGoogle weighs keyword modifiers more heavily.\\nGoogle weighs keywords based upon popularity trends.\\nGoogle will remove all special (non-alpha-numeric) characters when\\nweighing keywords.\\n\\nUsing your example, /the-matrix/trailers/\\nGoogle sees the URI as the matrix trailers. The use of the, a stop word, would normally be discarded as having little or no value, however, since it is part a known phrase The Matrix, the keywords are taken together as a phrase and will weigh higher than they would separately. The use of trailer would be seen in past search history and linguistics analysis as a modifier to the phrase The Matrix and would weigh higher than the phrase itself. This would also be true of the matrix reviews and other similar situations.\\nAlso, consider search history and SERP link CTRs. When someone searches for the matrix, they are not interested in The Matrix per se\\' but something about The Matrix. They are looking for additional information. Generally, a search for the matrix may result in CTR on SERP links that give additional clues. For example, the URI could be /the-matrix/reviews/, /the-matrix/ratings/, /the-matrix/trailer/, /the-matrix/cast/, etc. Each page will experience different SERP CTRs over a period of time. This adds weight to the specific page and keyword modifiers. As well, in overall search for the matrix, keywords ratings may perform better than cast, reviews may perform better than ratings, and trailer may perform better than reviews. But what if the user does not click on anything but does another more specific search? Google often looks for search clues through secondary searches and considers this as important to search intent. In this case, the search is more specific in an effort to create a more desirable SERP link list. The secondary search history may mirror the original SERP CTR for keyword modifiers or not. Still, these are weighed much the same way and may be weighted slightly higher than the modifiers for a search for just the matrix.\\nGoogle will take the URI and split it at the slash [/] and, in a sense, take the URI segments as an array. The first segment will weigh more than the second segment which will weigh more than the third segment. This is predicated on the notion that sub-directories are more narrow in topic scope than the parent therefore requiring a more narrow search intent and the further away from the home page another page is, the less important it is. So the URI /the-matrix/trailer/ will weigh the matrix more than trailer. Having a URI /trailer/the matrix/ will weigh trailer higher than the matrix. Keep this in mind.\\nConsider how people search. The search query is always ordered in importance from left to right by the searcher when entered. This is because for most of us, we learn to read left to right and hence begin to think in terms of left to right. There are exceptions for other languages of course that Google takes into account. So a search for the matrix trailer would weigh the phrase the matrix higher than trailer. But trailer we know is a modifier and has more weight. Google reorders the search query by weight (and therefore intent) to be trailer the matrix. Since the matrix is a known phrase, the effect would be trailer \"the matrix\". A search for the matrix trailer would result in a different SERP list than if you had quoted the entire search as \"the matrix trailer\". Because Google likes exact intent (pay attention to this word) matches, not exact keyword matches, any match for trailer \"the matrix\", would be placed higher in the SERPs.\\nOkay. It is only slightly more complicated than this, but it all fits within the same realm. So you get my point.\\nOn to some other points quickly.\\nPlease also understand that it is not always necessary to put keywords in the URL/URI, title tag, or h1 tag to have the same or better effect. For example, I found that keywords found in the description meta-tag and h2, h3, etc. tags can outperform keywords found in the URL/URI which are sometimes totally ignored because they are so common. In this case, I would not put these keywords in the URL/URI, title tag, or h1 tag, but in the h2 and possibly h3 tags. The reason for this is simple. Overuse of keywords by many sites can have a detrimental effect. In this case, Google will ignore some keywords used in these tags and prefer them within content hence the h2 tag. This is because Google will simply prefer content clues over keyword optimization especially very common keyword optimization that is not strong enough to warrant a penalty or even a second look.\\nBut what about creating your site for humans? This would mean that the URI /the-matrix/trailer/ would be more desirable? Yes it could be. You have to weigh whether anyone would manually type in your URL/URI or use it in a predictable way. If the answer is yes, then /the-matrix/trailer/ might be best depending. If the answer is no, then /trailer/the-matrix/ may yield more search users with clear intent. It is all about matching the users expectations more than anything. Because no amount of optimization can beat a high bounce rate.\\nSo to be more specific, is /the-matrix/trailer/ better or worse than /trailer/the-matrix/? That would depend upon search history. We can never really know this for sure. But it is likely that /trailer/the-matrix/ will match user intent more than /the-matrix/trailer/ based upon what we know about how Google handles searches. The only way to know for sure is to experiment.\\nLet\\'s make one more final consideration. How many trailers would I have for The Matrix? One. But how many trailers would I have on my site? More than one (I would assume). So it makes sense that trailers would be a TLD (top level directory) based upon that notion. Organizationally, this may make better sense.',\n",
       " 'I would say it wouldn\\'t harm much, nor will it add anything. I prefer to keep my header tags clean and wouldn\\'t add the i. The inline styling would be a bigger problem, which isn\\'t really an big issue.\\nInstead of doing this, you can add the gear icon to the H2 directly:\\nh2.Geared:before{\\n    display: inline-block;\\n    color: rgb(102, 149, 45); \\n    font-family: \"FontAwesome\";\\n    content: \"\\\\f013\"; /*The actual character FontAwesome uses */\\n}\\n\\n<h2 class=\"Geared\">This text will have the gear</h2>',\n",
       " 'Using a CreativeWork type does not seem to be appropriate. The result of your work is a CreativeWork, but not your work (i.e., the process to create it) itself. So you can use it for finished projects (e.g., in a portfolio), but not for the services you offer.\\nThe Service type seems to be appropriate. I don’t think that there is a relevant semantic difference between a delivery service and web development. In both cases you provide a service for others, offering your expertise and time.\\nAnother type that could be used is Offer. It has some overlap with Service (\"An offer to […] to provide a service\"), and you have to use Offer if you want to sell (i.e., with a price) your service, but you don’t have to provide a price.\\nIf using Service, and you decide that you want to provide prices at some point, you can use the offers property to provide an Offer for the Service.\\nBy the way, for a list of all your services, you could also use hasOfferCatalog → OfferCatalog (where each item can be a Service). This also allows you to convey the hierarchy of related services (by using nested OfferCatalog items). But this doesn’t work so well if you use several pages instead of one page.',\n",
       " 'Regarding the comment on your question, you only have one site which is served to your users with the two versions of the site (with www and without www).\\nTherefore, you need to 301 redirect one version to the others (as you wish). For this, you can use a .htaccess file if you use Apache as a web server (at the root of your site).\\nFor exemple, here the code to redirect all www URLs to no-www URLs:\\n<IfModule mod_rewrite.c>\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} !^example\\\\.com [NC]\\nRewriteRule (.*) http://example.com/$1 [QSA,R=301,L]\\n</IfModule>\\n\\nOr the opposite (no-www to www):\\n<IfModule mod_rewrite.c>\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} ^example.com$ [NC]\\nRewriteRule ^(.*)$ http://www.example.com/$1 [R=301,L]\\n</IfModule>',\n",
       " 'What I would suggest is to create a brand new set of URLs for search result pages and then for anyone requesting the old URLs, produce an error page with an HTTP 410 status code. Also, make your search pages only accessible via the POST request method. \\nGoogle won\\'t crawl pages that are requested via POST if it is requested as a result from filling out a proper form. An example is logging in to a specific section of a website. \\nFor example:\\nIf your current search result URLs are in the form of: \\nhttp://example.com/results.php?query=abc\\n\\nthen that needs to return a 410 status code with an error indicating the page is no longer available.\\nWhat you need to do is create a proper search form. In HTML, this will work:\\n<form action=\"searchfor.php\" method=\"post\">\\nQuery: <input type=\"text\" name=\"query\">\\n<input type=\"submit\" value=\"search\">\\n</form>\\n\\nWhen the user clicks search, the page requested will be searchfor.php and the data posted will be query=whatever (replace whatever with the text user entered) and the value can be extracted in the server script. In all search result pages, the URL in the address bar will always remain the same but the page will be different based on the query.\\nFor best results, make sure the form action value points to a different script name. The script however must exist on the server. You can insert the full URL to the script if that makes things easier for you.\\nJust make sure there is no other way for people to access the search results pages and then google will not try to access them. If you must allow users to access them another way via a hyperlink, you should include a rel=\"nofollow\" inside the anchor tag and in the particular result page, make it non-indexable. See other answers in this thread for instructions on how to make the page non-indexable.',\n",
       " 'You\\'ll get a penalty if you\\'re trying (regardless of your level of intention) to have two or more copies of the same page indexed.\\nIf you still want both copies live, then you\\'ll have to determine which ONE page from each set of same pages you want indexed. \\nAs for the remaining pages in each set, you can either:\\nDeclare it canonical to the original. In your HTML, insert the following after the <head> opening tag:\\n <link rel=\"canonical\" href=\"original.htm\">\\n\\nreplace original.htm with the URL to the chosen page of each set which will then become the real original version.\\nOR\\nMake the copies non-indexable. This method is simpler since you won\\'t need to know the URL to the original page in the set, and its also more compatible with more search engines since this was an older way to declare duplicate content. Use the following right after the <head> opening tag:\\n<meta name=\"ROBOTS\" content=\"NOINDEX\">\\n\\nSide note: the tags I mentioned to insert right after <head> can actually go anywhere between <head> and </head> as long as no other tag appears malformed. For example, this is not acceptable: <head><title><meta name=\"ROBOTS\" content=\"NOINDEX\">page title</title></head> because the title tag is malformed. In all cases, run your HTML through a validator after making changes.',\n",
       " \"If two pages can become one\\nThe best solution here is to only have 1 page. So if you can merge the category page and product list page that would be best. Put a 301-redirect from page A to page B, page B being the page that ranks best for that keyword. Verify that using Google Search Console. Doing this will lead to consolidating the authority both pages have.\\nAlternative #1\\nIf it's not possible to merge the pages and they should coexist, my next thought would be: create more distance between the two pages in terms of keyword focus. Perhaps combine one of the pages with another keyword so they stop being relevant for the same keyword?\\nAlternative #2\\nThird option: use a canonical instead of a noindex, since canonical URLs pass on some link authority (more info here). Nonindex will just mean the page will be removed from the index, and all link authority will be lost.\\nLet me know if you have any questions!\",\n",
       " \"Cloaking is when you show content to a search engine that is different than what you show to a user.\\nGoogle tests web pages from outside of it's own network and you will never know. If there is a difference between what you show googlebot and users, Google will spot-check more pages for differences. If enough pages appear to have significant enough differences, the penalty is applied.\\nIt is as simple as that.\\nTo answer your question, it can be cloaking if the content change is significant enough. Never show different content to search engines than to users. Just keep it all simple.\\n[Update]\\nThank you for the example of what you are specifically asking.\\nCloaking is what I have defined earlier, however, there is a bit of tolerance especially in light of desktop versus mobile. In the early days, cloaking could simply be determined by capturing the page twice, once via the crawler, and once external to the crawler often from another network, and comparing the checksum for each page. However, these days, it is not so simple with desktop versus mobile.\\nWe know that Google can fetch a series of pages and determine templated content versus page content rather easily. In light of the state of the web these days, I would have to assume that some level of analysis takes place to compare the content portion of the page and possibly the template portion of the page separately. How pages are analyzed for cloaking these days will likely remain a mystery. However, it is reasonable to assume that some minor differences in the non-content portion of the page is to be expected in some cases.\\nThe next question is, Is it wise to present JSON data to crawlers only?\\nNo one can say specifically if a search engine, Google in particular since Bing seems to be rather tolerant, will see the omission of JSON as being deceptive. It has to be recognized as a risk even if it appears to be small and a reasonable thing to do. As a recommendation, I would say to include the JSON data to both users and crawlers to avoid any issues. Why? Because cloaking is not a small violation at least in Google's eyes. If cloaking is detected, Google will spot check the site and then apply the penalty. This is an automated process. Once the penalty is applied, it can take quite a while to remove the penalty and likely is a knock on the sites trust metrics effecting search even after the penalty is lifted.\",\n",
       " \"These are sitelinks, you can read all about this over here :\\nhttps://support.google.com/webmasters/answer/47334?hl=en\\nDemote a sitelink URL:\\nOn the Search Console Home page, click the site you want.\\nUnder Search Appearance, click Sitelinks.\\nIn the For this search result box, complete the URL for which you don't want a specific sitelink URL to appear. (How to find the right URL.)\\nIn the Demote this sitelink URL box, complete the URL of the sitelink you want to demote.\\nAs you see, you can demote these links. In short, you cannot say which Google has to show but you can say which he may not show. \\nHope this helps!\",\n",
       " 'Few questions:-\\n\\nAny drop in traffic and ranks?\\nSince when have you started noticing the drop in the number of indexed pages with the site: operator?\\nAny message from Google in the search console?\\n\\nThe site: operator is supposed to show the approximate number of pages indexed in Google. However, do note that if your pages have thin content or are deep down in the hierarchy that they do not have enough link equity reaching them, Google will drop them from their index.\\nOf course, there could be other issues like Panda which is specifically targeted towards sites with low quality content.\\nIf you can PM me or post the link to yoir site, we can check further and advise accordingly.',\n",
       " 'It\\'s often a common mistake for companies, brands and blogs to use a H1 as the site name, and H2 as the slogan repeated on every page throughout the entire site, Google and Bing ain\\'t stupid and can work out the site name, and slogans without using header tags, the same way it works out the content in a footer.\\nA better approach would be to remove the site name and slogan from using the header tags completely, and reserving these for actual content. \\nThis would look like:\\n<a href=\"/\">\\n    <img alt=\"example\" src=\"example.png\">\\n    <span>WEBSITE NAME</span>\\n</a>\\n<span>WEBSITE SLOGAN</span>\\n\\nIf your concerned about not having a H1 or H2 on the front page then you could use the current setup, only on the front page and there-after using span, not header tags.\\nIn regards of the modal itself, it seems pretty pointless... you might as well just style the page the way you have it now, without overlapping the menu. Simply remove the menu from those pages completely, as its considered unnecessary bloat code. \\nThe modal pages you would be better of using something like this (without your hidden menu):\\n<div class=\"container\" itemscope itemprop=\"organization\" itemtype=\"http://schema.org/Organization\">\\n    <header role=\"banner\">\\n        <img src=\"company.png\" alt=\"company\" itemprop=\"logo\">\\n        <h1 itemprop=\"name\">COMPANY NAME</h1>\\n        <span itemprop=\"description\">COMPANY SLOGAN</span>\\n        <a href=\"https://example.com/\" target=\"_blank\" itemprop=\"url\">OUTBOUND</a>\\n        <nav>\\n            <ul>\\n                <li>Overview</li>\\n                <li>Example 1</li>\\n                <li>Example 2</li>\\n                <li>Example 3</li>\\n                <li>Example 4</li>\\n            </ul>\\n        </nav>\\n        <time class=\"updated\" datetime=\"2016-03-21T13:14:55+00:00\">March 21st, 2016</time>\\n    </header>\\n    <main role=\"content\">\\n        <!-- actual page content here -->\\n    </main>\\n    <footer role=\"contentinfo\">\\n        Site Name - 2016 All Rights Reserved\\n    </footer>\\n</div>',\n",
       " \"This is probably bot or spam data.\\nThe easiest way to tell is to look at the Hostname dimension, which you can do by adding it as a secondary dimension to a standard report. You'll probably see a strange hostname such as 'clickhere.xyz' or 'freesocialbuttons.xyz'. Don't try going to the site!\\nThis kind of spam is caused by people firing off huge batches of hits to random Google Analytics IDs in the hope that someone like you will notice, and try visiting the site where it came from. Don't feed the trolls.\",\n",
       " \"A lot of it depends on the resources you have available, email is a great option if you have a list, social sites such as stumbleupon if you have the right content. The search engines can drive a lot of traffic, if your site is not yet ranking, you can take advantage of any applicable universal search features, in addition to the shopping results you already mentioned there are video, images, news results, realtime search if you're using twitter and more. \\nIf you offer coupons there are quite a few coupon sites that let you list your deals for free. If you have unique products or are doing something news worthy press releases can be worthwhile. There are plenty of other options but hopefully this will give you enough to get started.\",\n",
       " 'Yes, you should remove the old ecommerce tracking code. They should not co-exist. Thanks.',\n",
       " \"Google changes this around every now and then, and whenever they do I have to 're-find' where each of the settings are.\\nSo you're not alone!\\nYou're right that a browser key is essentially 'public' and should be limited by domain to protect it from being stolen. It's worth noting the worst that can happen if its stolen is that someone could use up your free API request limit (assuming you're not on a billing plan of course) and cause requests from your site to be denied. You could then remove the key and create a new one without too much trouble. But of course, it's much better to avoid this altogether!\\nTo limit API requests to come from your domain(s) only, start at the Google API Console, then:\\n\\nClick the hamburger menu at the top left\\nClick API Manager\\nClick Credentials\\nSelect or create your project\\nClick Create Credentials, then choose API key and Browser Key\\nEnter the name for your key (this is just for you to identify it by)\\nUnder Accept requests from these HTTP referrers (websites), this is where you limit the key to your domain name - you can enter *.domain.com/* to cover anything on your domain.\\nClick Create\\n\\nIf you're editing a key that you've already created, instead of step 5 and 6 above just click the name of the key, and you'll see the same field there ready for you to enter your domain name into.\\nThe Domain Verification you found causes Google to check that you have already verified ownership of your domain in the Search Console. This allows Google to send hooks back to your domain which I imagine are required for some of the APIs (I'm not sure which ones as all I generally use is the JavaScript Maps API, which is fairly simple!)\",\n",
       " \"If you never intended to have such urls in your site and they truly mean nothing for you, then it's better to keep serving them as a 404 - a page that is not found - not existing in the site. \\nUse redirects if you used to have a page with important content, indexed and present in SERPs, that brings a lot of traffic to your site. \\nIn the 404 section of Google Webmasters you can read for the 404 urls: \\n\\nGooglebot couldn't crawl this URL because it points to a non-existent\\n  page. Generally, 404s don't harm your site's performance in search,\\n  but you can use them to help improve the user experience.\\n\\nAnd the 404 read more in Google Support: \\n\\nGenerally, 404 errors don’t impact your site’s ranking in Google, and\\n  you can safely ignore them. Typically, they are caused by typos,\\n  misconfigurations (for example, for links that are automatically\\n  generated by a content management system) or by Google’s increased\\n  efforts to recognize and crawl links in embedded content such as\\n  JavaScript....\\n  \\n  ... 404s are a perfectly normal (and in many ways desirable) part of the\\n  web. You will likely never be able to control every link to your site,\\n  or resolve every 404 error listed in Search Console. Instead, check\\n  the top-ranking issues, fix those if possible, and then move on.\\n\\nGenerally you can't handle/redirect all possible 404's, it's impossible. Anyone may arrive at a page and type any url. \\nAlso search engines crawlers always trying to discover new urls by following anything they find. Mistyped/misspelled links, javascript, external (from other websites) links, or try to discover content by combining url parameters, if a site is dynamic e.g. a CMS. \\nSo make sure you have under your control things like above, using robots.txt, noindex, your content maintained and cleaned up, your software up to date, and use Google Webmaster Tools to monitor and manipulate as possible it is the google bot.\\nUpdate:\\nAnd as @closetnoc has noted with his comments, consider reviewing your website's health against being hacked - as if you are discovering many such urls this can be a signal.\",\n",
       " \"The shorter the distance between the 2 end-points (server - client), the faster is the communication, that's a fact; although using CDNs can help achieve faster distribution of the data/content across the globe.\\nAs for SEO, server location would hardly make any difference if anything else is optimized and focused on your target location. **Although page load speeds can affect rankings- and page load speeds can be increased when you host far away from your users.\\nFrom G Webmasters FAQs:\\n\\nQ: Is the server location important for geotargeting?\\n  A: If you can\\n  use one of the other means to set geotargeting (ccTLD or Search\\n  Console’ geotargeting tool), you don’t need worry about the server’s\\n  location. We do, however, recommend making sure that your website is\\n  hosted in a way that will give your users fast access to it (which is\\n  often done by choosing hosting near your users).\\n\\nSelecting hosting provider:\\nApart of the server location, there are more important things to consider when choosing hosting provider. Some of the most important:\\n\\nExpertise\\nSecurity\\nSupport and communication channels\\nTechnology and Infrastructure - And compatibility with your needs.\\nGeneral Terms of Services and Limitations\\nPerformance, Reliability and Uptime\\nScalability\\nAdd-on Services and Essential Features e.g. backups\\n\\nPrice is important as well, but shouldn't be the most important factor when you plan ahead for an important business project/website.\\nGenerally, when something sounds as too good to be true, in most cases it isn't, so be aware and do extended research - ask for advices, recommendations, get in touch with the hosting providers for pre-sales questions etc.\",\n",
       " \"Any used framework under the hood has no effect on the SEO. What it matters SEO wise is the front-end (on Site SEO), and the off-site SEO.\\nGenerally any structural, mark-up, content changes have their own effect on SEO. Such changes can be done even with no underlying technology change.\\nFor sure changing framework can't affect off-Site SEO.\\nFor on-site, if after changing your framework you end-up with the exact same mark-up, content, meta and micro data, URLs, speed and anything else that constitute your current site from the SEO perspective, then there will be no effect. \\nIf you have decided to change the framework, this should be a decision based on your project requirements to meet new needs, that another framework can't offer. \\nYou could use this chance to improve your on-site SEO as well. So you may want to plan early and thoroughly, in order to end up with a more robust and solid website, that will bring in new features, functionality and be more optimized for search engines and users alike.\",\n",
       " \"You need to have two webmaster tools profiles, one for www.example.com and the other for example.com.\\nFrom now on, you should be looking on example.com webmaster tools profile.\\nAlso you need to specify in the webmaster tools that you prefer the non-www domain, go to site settings \\nSite Settings\\nPreferred domain\\n    Don't set a preferred domain\\n    Display URLs as www.example.com\\n    Display URLs as example.com\\n\\nAnd select the last one. \\n(GWT domain settings will be in a url similar to this one: https://www.google.com/webmasters/tools/settings?hl=en&siteUrl=http://example.com/ )\\nBoth properties should be in GWT, www and non-www:\\n\\nYou may need to verify ownership of both the www and non-www versions\\n  of your domain. Because setting a preferred domain impacts both\\n  crawling and indexing, we need to ensure that you own both versions.\\n  Typically, both versions point to the same physical location, but this\\n  is not always the case.\",\n",
       " \"At Gmail Support area there's the reference to the daily sending limit, which is 500 messages per day or a huge number of undeliverable messages.\\n\\nIn an effort to fight spam and prevent abuse, Google will temporarily\\n  disable your account if you send messages to more than 500 recipients\\n  or if you send a large number of undeliverable messages. If you use a\\n  POP or IMAP client (for example: Microsoft Outlook or Apple Mail), you\\n  may only send a message to 100 people at a time. Your account should\\n  be re-enabled within 24 hours.\\n\\nAs stated, your account should be back within 24 hours. \\nIf you want to keep using Gmail, you can use Google Apps, as the limits are higher (2000).\\nMy recommendation is that you use a professional Email Sending service like Amazon SES.\",\n",
       " 'How does google determine these pages are compromised if there are no malicious code can be found from both my browser or google\\'s \"fetch as google\"?\\n\\nGoogle may be using a separate IP address that is not the same as that used when the \"fetch as google\" operation is performed. For example, someone actually working at google might be randomly manually scanning your page and could find something different.\\n\\nHow does the hackers take advantage of these \"injected URLs\" if there are no actual malicious content or script on them? Or is the malicious content/script only visible to some certain people(not including me obviously)?\\n\\nThe latter. Your server is programmed to deliver different content based on IP address and/or group of IP addresses. Look for configuration files like .htaccess if you have apache and remove any lines that look like IP addresses. You want to serve the same guest content to all guests to your site.\\nAlso, check the opencart PHP code and look for anything in there that would cause different content to load based on remote IP address. It\\'s likely that your .htacccess or the PHP code itself is modified to the hackers needs.',\n",
       " 'Chrome and Safari send an X-Purpose: preview HTTP header when pre-fetching/rendering web content. [Source]\\nFirefox sends a similar header called X-moz: prefetch. [Source]\\nTo block pre-fetching, you could return a 404 response when such headers are detected, as suggested by Peter Freitag in this blog post. He recommends adding these lines to .htaccess to block Firefox prefetching:\\nRewriteEngine On\\nSetEnvIf X-moz prefetch HAS_X-moz \\nRewriteCond %{ENV:HAS_X-moz} prefetch \\nRewriteRule .* /prefetch-attempt [L]\\n\\nYou can extend this to block Firefox, Safari, and Chrome prefetching like this (untested, but should work):\\nRewriteEngine On\\nSetEnvIf X-moz prefetch HAS_preview \\nSetEnvIf X-Purpose preview HAS_preview\\nRewriteCond %{ENV:HAS_preview} .\\nRewriteRule .* /prefetch-attempt [L]',\n",
       " 'I’m assuming that the Article can be rated, but it got no ratings yet. (If the Article can’t be rated, don’t specify aggregateRating.)\\nSpecifying aggregateRating and giving ratingCount the value 0 seems to be appropriate (but that’s, of course, not required, you could simply omit it). That way, a consumer might learn that the Article could have a rating and that it’s not rated yet (which is \"more\" information than they get if aggregateRating is not specified).\\nIf you should specify ratingValue (with the value 0) depends on the way your site works. Do you actually have the rating \"0\" (i.e., you specify worstRating = 0)? In that case it seems to be appropriate to specify it. But if you only want to say that it has no \"real\" rating value, while for example normal ratings start at 1, you should omit ratingValue.',\n",
       " 'Something like this, though of course other properties are required for this to meet Google\\'s requirements for article features in search results.\\n<div itemscope itemtype=\"http://schema.org/Article\">\\n<!-- blah blah -->\\n  <div itemprop=\"publisher\" itemscope itemtype=\"https://schema.org/Organization\">\\n    <div itemprop=\"logo\" itemscope itemtype=\"https://schema.org/ImageObject\">\\n      <img src=\"http://www.mycorp.com/logo.jpg\"/>\\n      <meta itemprop=\"url\" content=\"http://www.mycorp.com/logo.jpg\">\\n      <meta itemprop=\"width\" content=\"400\">\\n      <meta itemprop=\"height\" content=\"60\">\\n    </div>\\n    <meta itemprop=\"name\" content=\"MyCorp\">\\n  </div>\\n</div>',\n",
       " 'You have to create a custom dimension with product as scope in your Google Analytics Account.\\n\\nThen you have to apply it to your ec:addProduct code:\\n<script type=\"text/javascript\">\\n    _ga(\\'ec:addProduct\\', {\\n        \\'id\\': \\'<product sku>\\',\\n        \\'name\\': \\'<productname>\\',\\n        \\'category\\': \\'<product category>\\',\\n        \\'brand\\': \\'<product brand>\\',\\n        \\'variant\\': \\'<product variant>\\',\\n        \\'price\\': \\'<product price>\\',\\n        \\'quantity\\': \\'<quantity>\\',\\n        \\'dimension2\\': \\'<product ean>\\',\\n    });\\n</script>\\n\\nDo not forget to send a hit or event, otherwise GA will ignore your ec:addProduct.\\nThis documents helped me.',\n",
       " \"Absolutely a domain can be owned by a organisational entity as opposed to an individual person, this in fact the most common type of domain name registration and it isn't a specific service that is needed for it.\\nWhen you register for the domain name most registrars will ask you if it is for an individual or for a business and will provide a field for you to enter the organisation name. There are rules around having a first name and last name on record as the domain owner but that can be anyone at all and many companies (including my own) choose to use a psuedo-name such as First Name: Officer In Charge, Last Name: Company Name, Company Name: Company Name. In this way the domain shows up as belonging to the officer in charge of the organisation whomsooever that may be.\\nEvery domain will ask you for a technical contact and an administrative contact but the name can be a psuedo-name as well such as\\nAdmin Contact\\nName: DNS Admin\\nEmail: dns-admin@domain.com\\nTechnical Contact\\nName: DNS Technical\\nEmail: dns-technical@domain.com\\nOr anything similar.\\nAs for the second part of your question regarding requiring the approval of two authorized users for a change to be made this would have to be enforced through internal policies and procedures as most registrars do not have such a facility available and even larger companies where something like this would be a very important feature don't use it, instead implementing internal policies, procedures, and controls, and ensuring access to the tools to make those changes are restricted to very few authorised users.\",\n",
       " 'To answer the first part of your question, no Google Analytics does not add additional tracking codes to your site if it has a parent tracking code. EG: If you have a university wide tracking code as well as your own tracking code and then your tracking code is removed and the university code stays it will report only on the university tracking code and not your removed one. The only reason why I can think that you would still see traffic from it with the tracking code removed is if there where users who where accessing a cached version of your site on a business network or through an ISP who cached the website as the cached copy would still have your own tracking code within the HTML. If this occurred you would see less traffic that what actually hit your site due to the fact that not all users would have accessed a cached copy of it.',\n",
       " 'There are two options here, one would be to upgrade to a more powerful server if that is an option for you.\\nThe second, which is what you specifically reference is known as load balancing. This is where you have two servers running side by side and where the traffic to your application is split between the two, in effect each server takes half of the traffic to your site. This can be done in one of two ways...\\nLoad Balancer\\nThis is a hardware or software based solution depending on the provider whereby all traffic goes through one device and is then \"balanced\" between the end servers. This is the most intelligent method of doing it in the sense of machine intelligence as this can be configured to direct the user to the server with the least load at the time.\\nMultiple DNS A Records\\nThis is the second way to achieve load balancing but does not assure even distribution of load across your services. With this method either server may be sent the traffic and once it has been connected to all connections from that network should go to the same server until the DNS cache expires and it hits your DNS server again.\\nIn all honesty unless your website depends on high availability or another use case to justify the costs of a load balancer you may be better off to either upgrade your server or use DNS round-robin for load balancing.',\n",
       " \"It sounds like a security feature on the server is redirecting the connection when html escaped data is added to the URL. Take a look through your Apache configuration files and the vhost files and check any redirect rules you find, you will probably fine one has a regex pattern for identifying html escaped data and blocking it as a security precaution. If you don't want to go through all the configuration files you could try base64_encoding the escaped values which are causing the problems, I have done this before where a server was configured to block html escaped data in the URL and it worked as the base64_encoded string passed the regex pattern.\",\n",
       " '(Answer added for @Festus Martingale to mark question as answered).\\nReplacing the ? with a # solved the problem as it prevents the query string from being transmitted to cloudflare as it appears as a hash fragment.',\n",
       " \"SRV records are meant for services which are designed to check for the existence of service records such as connecting to an XMPP server or a SIP server. DNS should not be used to provide a list of services for end users on your network to browse as DNS is not designed for users to directly read.\\nThe best option is to add the information you need for users to identify the services on your network to an intranet page on your local network or other such publications your users can see them with.\\nAs an example if you where to try to create an SRV record of...\\n_ftp._tcp.example.com it wouldn't work as the FTP protocol isn't designed to work with SRV records whereas if you where to create one for _sip._tcp.example.com this would work as the SIP protocol is designed to work with SRV records so that when you try to connect to a SIP address at example.com it will automatically look for the _sip._tcp.example.com SRV record to identify the DNS record for the specific SIP server which may be some-sip-machine.example.com.\",\n",
       " 'The only memory limitations currently supported are the ones that you have asked about. There is no way currently in cPanel to apply a memory limit on an account basis, only on a per script or per process basis. The issue you would encounter is how to apply a memory limit on a per account basis as each account does not spawn its own Apache process and if it did would be a waste of resources as the account would not always need that process to be active. If you are seeing performance issues try applying a per-scriipt php.ini memory limitation.',\n",
       " \"A week isn't all that long (though I know it sure feels like it is). Steps you can take include:\\n\\nDouble/triple check your 301 redirects are pointing to the correct new URLs.\\nEnsure your sitemap.xml includes all of the new URLs and none of the old. Resubmit the sitemap in Google Search Console.\\nCheck your canonical tags include the new URLs.\\nLook at any external links you can find and have them updated to use the new URLs.\\n\\nOther than that, have a lot of patience.\",\n",
       " 'The two are part of two different standards. They shall never meet. For the HTML Description Meta-Tag, that would describe the entire page. For the Itemprop Description, that would apply to only the content segment you are marking up.\\nThere may be cases where it is appropriate that they are the same such as a product page within an e-commerce site. It is all up to you and what you think is right for your users.\\nAlways create content for your users and not machines.\\nThere is no effect if these two elements are identical or not. While the HTML Description Meta-Tag is used for weighting a page within a search engine and can potentially be used as a search engine results page (SERP), that is the extent of the HTML element. The Itemprop Description will not be taken into account the same way. The intent of mark-up is to mark-up segments of your content as data so that it can be properly read and ultimately parsed into a database. Google, for example, will trust what it is seeing more when the content is marked-up. This has limitations of course. For now, it does help with the semantic understanding of content and what to do with it in regard to better search results.\\nFor these reasons, both should be the best you can devise for whatever purpose they serve.',\n",
       " \"What you are searching for is known as a CNAME record. With Windows Azure you will be given a public URL for the service usually under a domain associated with Azure. From there you change the current A record to a CNAME record with the same sub domain name and simply add the public domain for the Azure service. What will happen then is when a user tries to connect to the sub domain in question they will get back a response saying that it is a CNAME of the Azure domain and will automatically do a DNS lookup on the Azure domain to get the right IP address. The comment made above talking about getting the machines IP address is a bad idea unless you have had a dedicated IP assigned to it as many Azure endpoints have empherial IP's where the IP address can change over time with little or no notice which would break your site each time it happens, whereas by using a CNAME record the end users browser will always get the correct IP address as of that point in time.\",\n",
       " \"This is going to be incredibly difficult for you to achieve. Firstly Google awards valuable original content and penalizes sites which simply contain duplicate content from other sites. Secondly Google has started cracking down on search results pages being listed in the Google index as these results pages don't present any useful content to the end user and only require the end user to click on something else to go to the page they where looking for (http://searchengineland.com/google-warning-against-letting-your-search-results-get-indexed-10709).\\nYou should ensure that you try and comply with the Webmaster Guidelines that Google publishes which specifically state key to your question to avoid\\n\\nCreating pages with little or no original content\\nScraped content\",\n",
       " 'Adwords\\n-> you search the keywords you want. And use them as advertisement. Google puts your advert to matching content or search requests.paid keywords\\nGoogle.de Search keywords \\n-> google ranks your page. According to a lot of stuff, including your content and external links. Google evaluates this content and assings each of your pages several keywords regarding their content and external link texts. If another person searches in google, he will find your page under these keywords which google assesed. If he clicks this link at google, google analytics will show this keyword as organic keyword',\n",
       " 'The following picks up on a fresh working LAMP installation under Red Hat 7 or clone (CentOS 7, Scientific Linux 7, Orcale 7, etc). Set Selinux to permissive for the installation.\\nsetenforce 0\\n\\nFirst get the Mediawiki version you want from https://releases.wikimedia.org/mediawiki/ , at time of writing latest is https://releases.wikimedia.org/mediawiki/1.27/mediawiki-1.27.0.tar.gz and unpack it in /var/www/html/w.\\nNavigate to https://www.example.com/w and follow on-screen instructions to generate content used for LocalSettings.php. Create LocalSettings.php with\\nvi /var/www/html/w/LocalSettings.php\\n\\nand paste content into file (i -> enter insert mode, CTRL+SHIFT+v to paste content, ESC -> to exit insert mode, ZZ (twice letter Z) to save and exit vi). Now secure LocalSettings.php with\\nchown root:apache /var/www/html/w/LocalSettings.php\\nchmod 640 /var/www/html/w/LocalSettings.php\\n\\nDelete mw-config if it exists, since it is only used for first time setup of mediawiki.\\nrm -rf /var/www/html/w/mw-config\\n\\nEnable use of .htaccess files by creating custom configuration file for Apache httpd.\\ncat >> /etc/httpd/conf.d/custom.conf << EOF\\n<Directory \"/var/www/html/w\">\\n AllowOverride All\\n</Directory>\\nEOF\\n\\nNow one should customize LocalSettings.php to one\\'s taste. Here an overview of variables that can be customized: https://www.mediawiki.org/wiki/Manual:Configuration_settings\\nSince we want to access our Mediawiki installation under https://www.example.com/wiki we need to set $wgArticlePath in LocalSettings.php. Just add the following line a the bottom of LocalSettings.php \\n$wgArticlePath = \"/wiki/$1\";\\n\\nand update /etc/httpd/conf.d/ssl.conf by adding one line.\\n<VirtualHost _default_:443>\\nAlias /wiki /var/www/html/w/index.php  # <-- only add this line\\n\\nCaptchas are annoying but if configured properly, effective to reduce SPAM. See https://www.mediawiki.org/wiki/Extension:ConfirmEdit for configuration options. \\nNow we want to have a blacklist of words that cannot be used in any article.\\n$wgSpamRegex = \"/\".\\n                \"s-e-x|zoofilia|sexyongpin|grusskarte|geburtstagskarten|animalsex|\".\\n                \"sex-with|dogsex|adultchat|adultlive|camsex|sexcam|livesex|sexchat|\".\\n                \"chatsex|onlinesex|adultporn|adultvideo|adultweb.|hardcoresex|hardcoreporn|\".\\n                \"teenporn|xxxporn|lesbiansex|livegirl|livenude|livesex|livevideo|camgirl|\".\\n                \"spycam|voyeursex|casino-online|online-casino|kontaktlinsen|cheapest-phone|\".\\n                \"laser-eye|eye-laser|fuelcellmarket|lasikclinic|cragrats|parishilton|\".\\n                \"paris-hilton|paris-tape|2large|fuel-dispenser|fueling-dispenser|huojia|\".\\n                \"jinxinghj|telematicsone|telematiksone|a-mortgage|diamondabrasives|\".\\n                \"reuterbrook|sex-plugin|sex-zone|lazy-stars|eblja|liuhecai|\".\\n                \"buy-viagra|-cialis|-levitra|boy-and-girl-kissing|\".\\n                \"dirare\\\\.com|adipex|phentermine|adult-website\\\\.com|\".\\n                \"overflow\\\\s*:\\\\s*auto|\".\\n                \"height\\\\s*:\\\\s*[0-4]px|\".\\n                \"==<center>\\\\[|\".\\n                \"\\\\<\\\\s*a\\\\s*href|\".\\n                \"display\\\\s*:\\\\s*none\". \\n                \"/i\";\\n$wgSummarySpamRegex = \"/\".\\n                \"s-e-x|zoofilia|sexyongpin|grusskarte|geburtstagskarten|animalsex|\".\\n                \"sex-with|dogsex|adultchat|adultlive|camsex|sexcam|livesex|sexchat|\".\\n                \"chatsex|onlinesex|adultporn|adultvideo|adultweb.|hardcoresex|hardcoreporn|\".\\n                \"teenporn|xxxporn|lesbiansex|livegirl|livenude|livesex|livevideo|camgirl|\".\\n                \"spycam|voyeursex|casino-online|online-casino|kontaktlinsen|cheapest-phone|\".\\n                \"laser-eye|eye-laser|fuelcellmarket|lasikclinic|cragrats|parishilton|\".\\n                \"paris-hilton|paris-tape|2large|fuel-dispenser|fueling-dispenser|huojia|\".\\n                \"jinxinghj|telematicsone|telematiksone|a-mortgage|diamondabrasives|\".\\n                \"reuterbrook|sex-plugin|sex-zone|lazy-stars|eblja|liuhecai|\".\\n                \"buy-viagra|-cialis|-levitra|boy-and-girl-kissing|\".\\n                \"dirare\\\\.com|adipex|phentermine|adult-website\\\\.com|\".\\n                \"overflow\\\\s*:\\\\s*auto|\".\\n                \"height\\\\s*:\\\\s*[0-4]px|\".\\n                \"==<center>\\\\[|\".\\n                \"\\\\<\\\\s*a\\\\s*href|\".\\n                \"display\\\\s*:\\\\s*none\".\\n                \"/i\";\\n\\nFor small to medium sites the extension \"Moderation\", https://www.mediawiki.org/wiki/Extension:Moderation is quite effective to fight SPAM. Add the following lines at the end of LocalSettings.php\\n$wgGroupPermissions[\\'user\\'][\\'move\\']                = false;\\n$wgGroupPermissions[\\'user\\'][\\'upload\\']              = false;\\n$wgGroupPermissions[\\'user\\'][\\'purge\\']               = false;\\n$wgGroupPermissions[\\'user\\'][\\'SpecialPages\\']        = false;\\n$wgGroupPermissions[\\'bureaucrat\\'][\\'move\\']          = true;\\n$wgGroupPermissions[\\'bureaucrat\\'][\\'upload\\']        = true;\\n$wgGroupPermissions[\\'bureaucrat\\'][\\'purge\\']         = true;\\n$wgGroupPermissions[\\'bureaucrat\\'][\\'SpecialPages\\']  = true;\\n\\n#Moderation (This section to be placed at the bottom of LocalSettings.php)\\nrequire_once \"$IP/extensions/Moderation/Moderation.php\";\\n$wgGroupPermissions[\\'sysop\\'][\\'moderation\\'] = true; # Allow sysops to use Special:Moderation\\n$wgGroupPermissions[\\'sysop\\'][\\'skip-moderation\\'] = true; # Allow sysops to skip moderation\\n$wgGroupPermissions[\\'bot\\'][\\'skip-moderation\\'] = true; # Allow bots to skip moderation\\n$wgGroupPermissions[\\'bureaucrat\\'][\\'skip-moderation\\'] = true; # Allow bureaucrat to skip moderation\\n#$wgGroupPermissions[\\'checkuser\\'][\\'moderation-checkuser\\'] = false; # Don\\'t let checkusers see IPs on Special:Moderation\\n$wgAddGroups[\\'sysop\\'][] = \\'automoderated\\'; # Allow sysops to assign \"automoderated\" flag\\n$wgRemoveGroups[\\'sysop\\'][] = \\'automoderated\\'; # Allow sysops to remove \"automoderated\" flag\\n$wgLogRestrictions[\"newusers\"] = \\'moderation\\';\\n\\nGet Moderation extension with\\ngit clone https://github.com/edwardspec/mediawiki-moderation.git\\n\\nand copy the files to \"/var/www/html/w/extensions/Moderation\". Now run the mediawiki update script with\\ncd /var/www/html/w\\nphp maintenance/update.php\\n\\nNow finish securing the Mediawiki installation. This may be needed for Selinux, e.g. database on different server, etc.\\nsetsebool -P httpd_can_network_connect 1\\nsetsebool -P httpd_can_network_connect_db 1\\n\\nThis is needed for Selinux to run Mediawiki\\nsetsebool -P httpd_builtin_scripting 1\\nsetsebool -P httpd_execmem 1\\n\\nSet userrights and special Selinux rights, so Apache httpd has read access, but other users beside root don\\'t.\\nchown -R root:apache /var/www/html/\\nfind /var/www/html/w -type d -exec chmod 750 {} \\\\;\\nfind /var/www/html/w -type f -exec chmod 640 {} \\\\;\\n\\nMediawiki writes to images and cache, so they need special write premissions.\\nchown -R apache:apache /var/www/html/w/images\\nchown -R apache:apache /var/www/html/w/cache\\nsemanage fcontext -a -t httpd_sys_rw_content_t \"/var/www/html/w/cache(/.*)?\"\\nsemanage fcontext -a -t httpd_sys_rw_content_t \"/var/www/html/w/images(/.*)?\"\\nrestorecon -R /var/www/html/w\\n\\nFound Selinux complaining about hugetlbfs, so add an exception.\\ncd /var/log/audit\\ngrep hugetlbfs audit.log | audit2allow -M hugetlbfs\\nsemodule -i hugetlbfs.pp\\n\\nNow restart Apache httpd and set Selinux back to enforcing.\\nsetenforce 1\\nsystemctl restart httpd\\n\\nUnderstandably this covers only the basics and Mediawiki offers thousands of ways to customize it further to one\\'s taste and security needs.\\nDon\\'t forget to make regular backups.\\nFurther suggestions can be found here https://www.pozzo-balbi.com/help/Mediawiki .',\n",
       " 'You don\\'t have to create a subdomain in OVH. You onle have to create the \"A record\" in the 1and1 subdomain, pointing to the VPS IP. That should be enough',\n",
       " \"Looks like someone is spoofing your ourmailer@gmail.com address.\\n\\nThis line shows a spam relay server:\\nDiagnostic-Code: smtp;554 resimta-ch2-11v.sys.comcast.net comcast 23.227.123.207 found on one or more DNSBLs, see http://postmaster.comcast.net/smtp-error-codes.php#BL000010\\n\\nThis line shows that the original e-mail lacked an rfc822 message ID:\\nMessage-Id: <5576b8b2.c86cb60a.7629.4259SMTPIN_ADDED_MISSING@mx.google.com>\\n\\nGoogle's servers (very helpfully?) added 5576b8b2.c86cb60a.7629.4259SMTPIN_ADDED_MISSING as the message ID.\",\n",
       " 'The biggest mistake webmasters make is trying to focus too much on SERP rankings to the deficit of the user experience. It is unlikely that a user will specifically type in a decimal coordinate to your site to access the fictitious details of ants living in that location but it is also unlikely that they will want to describe the area based on highly subjective landmark references. There are a range of situations where having the decimal coordinates of a location in the URL is very appropriate and the best option and whether you use subjective landmark based URL\\'s or decimal based location coordinates will not affect your SERP rankings. The important thing to remember is that your URL\\'s should make sense to someone who sees them. A website which deals with drilled down locations will make sense to use coordinates, especially if the only effective way to differentiate between locations, say two separate locations within a wide open field, is to use decimal coordinates in lieu of descriptors. As a side note this is the way that Google maps builds its URL\\'s. The coordinates of the center point of the map are used in the URL to tell the Google Maps App where to center the map. You can imagine how unusable Google Maps could become if to center the map you needed to describe an area using words such as \"100m south of the big oak tree and 200m west of the lake\".',\n",
       " \"The best and easiest way to do this is from within your application instead of from within Google Analytics. Easiest way is to add a switch to your code to detect if you are connecting to your own site (which can be detected anyway you wish really) and if you are detected then don't add the Google Analytics code to the page at all, otherwise add it no problems. We use this ourselves. Connections from within our own network don't have the Google Analytics code added to the websites but connections from the public gateway have Google Analytics added.\",\n",
       " \"There is no definitive way to check this as the user agent strings don't show the difference at this time and the only way to change that would be for Microsoft to make a change to the user agent string for IE10 Metro.\\nThere are a range of checks you an do such as checking ActiveX support (Metro doesn't support ActiveX but IE10 Desktop can have ActiveX switched off as well), check for full screen (Metro will always be full screen but IE10 can also be set to full screen), or check if it is 64 bit (Metro will always be 64 bit but IE10 Desktop can also be 64 bit).\\nThe point being that no matter what check you make it will still be an educated guess. You will need to make a decision yourself as to if you need to prioritise it or not and if you do then simply install IE10 Metro and use that to check the website on as part of your QA as well as the other browsers you check it on.\",\n",
       " \"What I finally did was to always block the price to bots.\\nUsed a code similar to this:\\nfunction _bot_detected() {\\n\\n  if (isset($_SERVER['HTTP_USER_AGENT']) && preg_match('/bot|crawl|slurp|spider/i', $_SERVER['HTTP_USER_AGENT'])) {\\n    return TRUE;\\n  }\\n  else {\\n    return FALSE;\\n  }\\n\\n}\\n\\nMore info at: https://stackoverflow.com/questions/677419/how-to-detect-search-engine-bots-with-php\",\n",
       " 'The rich snippets that you are talking about are pulled from the Google knowledge graph which allows certain rich content to be appended to the top search results where the Google algorithms deem it most appropriate. Google uses machine learning techniques to distinguish data from the structured layout of your page. As a webmaster you can use microdata to make this easier for Google and other search engines to access the information however there is no real way to force this information to be shown and only if the Google algorithms detect that the search someone is trying to perform would be improved by appending the rich snippets to the search results will they do it. As more websites adopt microdata and the standards become more accepted by the industry at large you will start to see it happening more.\\nReferences\\nhttps://searchenginewatch.com/sew/news/2372043/googles-new-structured-snippets-pull-facts-into-a-pages-search-results\\nhttps://en.wikipedia.org/wiki/Knowledge_Graph\\nhttp://schema.org/docs/gs.html',\n",
       " \"Regarding the first aspect of your question regarding censorship thanks to the first amendment there are no laws forcing censorship on user generated content to the extent that the service you are providing should not be designed for the breach of the law. A prime example of this is the torrent site  The Pirate Bay. While the argument could be made all content was user generated content the courts ruled that the webmasters designed the site and marketed it specifically for the purpose of breaching US and international laws. Focusing specifically on chatroom's this would more boil down to the exceptions to the first amendment such as (https://en.wikipedia.org/wiki/United_States_free_speech_exceptions)...\\n\\nadvocacy of the use of force\\nFalse statements of fact\\nObscenity\\nChild pornography\\nFighting words and offensive speech\\nSpeech owned by others\\nCommercial speech\\nRestrictions based on special capacity of Government\\n\\nWebmasters can and have been held liable for the user generated content (including chatroom posts) on their sites and the expectations in court are that a webmaster who publishes a service like that adopt a reasonable moderation toolkit and procedure to deal with illegal content in a timely manner (such as reporting users or posts, having them removed from chatroom history, banning repeat offenders, etc). As for the extent of your liability that would depend on the circumstances and would really need to be discussed with a legal professional.\\nSome specific laws which would be applicable to a chatroom site are...\\n\\nChildren's Online Privacy Protection Act\\nDigital Millennium Copyright Act\\nFirst Amendment to the United States Constitution\\nCommunications Decency Act\\n\\nThis is not a definitive list but simply the top 4 which may apply to you.\\nAs for the second part of your question regarding SERP rankings or the site being classified as SPAM the SERP ranking could most certainly be affected depending on the type of posts being made by the users. As for the SPAM aspect not as likely, especially if the rest of your site has genuine useful content and this makes up a simple feature of your site but still possible, becomes more likely if you are intending your whole site to be nothing more that a chatroom, especially if it becomes a room whereby the users involved in the chatroom are not having what would be classified as reasonable chats and instead are simply being obscene (cyber sexual chats) or having flame wars.\\nFor all the above reasons while there is no reason not to setup a chatroom site I would strongly recommend ensuring you have the appropriate tools in place on the site to identify illegal activities and remove them from the chatroom and potentially also ban the offending users to cover your own exhaust vent.\",\n",
       " \"To answer your specific question the only difference between a standard web host and a HDS approved web host is that the HDS web host has gone through HDS approval through the French government. In effect it is to do with health data security. HDS approved hosts are authorised under the law to host health data related systems where a patients data is stored on the hosts network. The requirements are complicated but boiled down it is to do with physical access to the servers, electronic access to the data, data encryption, firewalls, virus protection, etc.\\nThe key difference I can think off just of the top of my head between HDS and non HDS hosts would be around encryption of data with most web hosts not encrypting data. Most web hosts already adopt strict access controls for physical access to data floors, electronic access to machines, etc. In addition where the machines in question are physical or virtual servers as opposed to shared hosting servers generally the hosting provider doesn't have electronic access to the machine as the passwords are held by the customer and not the host.\\nBasically unless your boss has a business case for shifting hosting to a HDS approved host I would recommend talking him out of it as the hosting costs are generally higher given the costs of getting accredited in the first place and the relatively small number of approved HDS hosts compared to the overall number of hosts available.\",\n",
       " \"This is actually quite a common practice, be careful though, while it is perfectly acceptable to do this for common mis-speellings of your own domain name to keep your competitors from grabing them as well as your own domain name under multiple TLD's.\\nWhere you will encounter problems is with Cyber Squatting. This is where you are registering, trafficking in, or using an Internet domain name with bad faith intent to profit from the goodwill of a trademark belonging to someone else. The cybersquatter then offers to sell the domain to the person or company who owns a trademark contained within the name at an inflated price. This is an offense under the Anticybersquatting Consumer Protection Act which is a US law.\",\n",
       " 'These in-app browsers are basically a cut down version of the native browser and so have the same user agent string. Unfortunately there is no way strictly speaking to detect them and break out of them as the apps (such as the facebook app) will always open the links using the in-app browser instead of closing the app and opening up the complete browser app. The only thing I can suggest is using javascript to check and see if the feature you are trying to use is available in the browser before presenting it to the end user. You can take a look at http://diveinto.html5doctor.com/detect.html which shows how to use Modernizr to check if certain HTML5 features are supported in the users browser which can be used to switch off unsupported features on your site.',\n",
       " \"The canonical link is 2-days old now, does it take a certain amount of time to get updated?\\n\\nYes, and this process can take a LOT longer than 2 days.\\n\\nCan the fact that the post went so viral and have multiple links on other websites and social media play a role in the canonical link getting ignored?\\n\\nI wouldn't say it's getting ignored, it's more that Google doesn't recalculate instantly.\\n\\n6-hours after setting up the canonical on the old domain I went to google webmaster console and fetched the page with the newly added canonical link, and submitted it to index, was this a stupid thing to do?\\n\\nNo, it neither helps nor hurts you. As long as you have the sitemaps fed into GWT and Yoast is working properly, Google will re-index and see the canonical tag in its own time.\\ntl;dr Patience and, as has been said by others, 301 Redirect Permanent is your friend over time. \\nAlso, you know when rel=canonical is not working because a site gets penalized for duplicate content.  As long as one post or the other is ranking well you're okay.\",\n",
       " 'Okay. Without knowing the sites in question, I will try and explain a bit of what is going on and I will provide just a few links.\\nFrom:\\nhttp://www.cradlecloud.com/ban-block-blackhatworth-com-spam-referrals/\\nI get the following domain names associated with the new method of referrer spam that people are seeing of late.\\n\\nBlackHatWorth.com\\nIskalko.ru\\nLomb.co\\nLombia.co\\nEconom.co\\nDarodar.com\\nILoveVitaly.Com\\nPriceg.com\\nHulfingtonpost.com (New- added Jan 16 2015)\\nBestwebsitesawards.com (New- added Feb 3 2015)\\nRanksonic.info (New- added Feb 3 2015)\\nCenoval.ru (New- added Feb 6 2015)\\no-o-6-o-o.com (New- added Feb 25 2015)\\nHumanorightswatch.org (New- added Mar 4 2015)\\nS.click.aliexpress.com (New- added Mar 17 2015 - Suspected)\\nwww1.social-buttons.com (New- added Mar 23 2015 - Suspected)\\n4webmasters.org (New- added Mar 26 2015 - Suspected)\\nGooglsucks.com (New- added Apr 07 2015)\\nAddons.mozilla.org (New- added Apr 07 2015 - Suspected)\\nSmallseotools.com (New- added Apr 13 2015 - Suspected)\\nTheguardlan.com (New- added Apr 14 2015)\\nBuy-cheap-online.info (New- added Apr 16 2015 - Suspected)\\nSite1.free-share-buttons.com (New- added Apr 29 2015 - Suspected)\\nSanjosestartups.com (New- added May 25 2015)\\nTrafficmonetize.org (New- added June 03 2015 - Suspected)\\nHowtostopreferralspam.eu (New- added June 09 2015 - Suspected)\\nWww10.free-social-buttons.com (New- added June 16 2015 - Suspected)\\nGetitfree.us (New - added June 18 2015 Ownership cannot be determined. Thank You - Trey Copeland)\\nWww6.free-social-buttons.com (New- added June 18 2015 - Suspected)\\nErot.co (New- added June 26 2015 - Suspected)\\n3g2upl4pq6kufc4m.onion (New- added July 04 2015 - Suspected)\\nTraffic2money.com (New- added July 28 2015 - Suspected)\\n\\nNote: Suspected items- do appear to follow the same pattern of ownership, and may not be tied to the same offender.\\nA rather exhaustive list of spam referrers maintained by Piwik can be found here: https://github.com/piwik/referrer-spam-blacklist/blob/master/spammers.txt (Thank You - user2428118) \\nTo Quote:\\n\\nBlackHatWorth.com is a relatively new domain created only on January\\n  7th, 2015 which is now being used for referrer spam. As a matter of\\n  fact, this referral spam website is being hidden behind the name of\\n  shopping search engine and beautiful scenery images.\\n...the IP address of BlackHatWorth.com which is 78.110.60.230 is the\\n  same one associated with other referral spam websites...\\nIn fact, the domain BlackHatWorth.com is owned by the same Russian who\\n  owns the other referral spam domains such as ILoveVitaly.com,\\n  Econom.co, and Darodar.com. The domain owner’s name is supposedly\\n  Vitaly A Popov of Samara (city), Samaraskaya Oblast (state), Russia.\\n\\nYou cannot block this!\\nFrom:\\nhttp://www.blackmoreops.com/2014/12/19/darodar-com-referrer-spam/\\nTo Quote:\\n\\nHere’s a quick primer on how Google Analytics works.\\nSo, you get setup on GA and get a code from them. The code looks like\\n  UA-number-1 or some such thing. That number is your “account number”\\n  on GA. Now, this code and a bit of javascript go onto your webpage.\\n  Now, somebody visits your page, and their browser runs that javascript\\n  code.\\nThat javascript code is what “records” their visit. It makes their\\n  browser talk to Google Analytics. Specifically, it makes certain types\\n  of HTTP requests that Google records information about, and then GA\\n  displays summaries of that information to you.\\nPretty basic, right? Still with me? Okay, now, if all it is is this\\n  Javascript sending the “visit” to them, then anybody can fake that.\\n  Anybody at all. All I have to do to make your GA show false\\n  information is to send my fake information directly to GA.\\nI don’t need to visit your site at all. I don’t need to run javascript\\n  at all. I just need to reproduce those HTTP requests, which are public\\n  and so anybody can see them and how they work. They’re even fairly\\n  well documented, publicly, by Google themselves.\\nSo, now, let’s say I’m a spammer jerk. I want to get people to see my\\n  spammy site. So, what do I do? I write a small bit of code to send\\n  thousands upon thousands of these fake requests to GA, and I simply\\n  cycle through all the UA numbers, in order, at random, whatever. I\\n  send a fake visit, with a fake referrer, and my spammy domain name.\\n  And guess what? It shows up in your Google Analytics screens.\\nYou see this spam like any other normal visit. Because as far as GA is\\n  concerned, it was a normal visit. All they’re recording are those HTTP\\n  requests, which normally come from the GA javascript code. But a\\n  request is a request, and making a fake one is very, very easy.\\nThat is what is going on. All I need is your UA number and with only a\\n  minor bit of effort I can fake a visit to your site without ever\\n  actually connecting to your site at all. That fake visit can have any\\n  domain name and any referrer in it that I choose.\\nThis is an attack on Google Analytics, to promote whatever site is\\n  showing up. You cannot block it on your server, because your server is\\n  not involved at all.\\n\\nYou can do two things: one, set-up a filter as John Conde suggests; and two, see if there is a way to inform Google. For that I do not have an answer, but I have an idea.\\n[Update]\\nThis is beginning to reach outrageous proportions from hundreds of spam hits a day to full out advertising such as this one:',\n",
       " 'Here is my take on this:\\nFirstly, ask the \"legal team\" to explain to you what a cookie is, and how that law protects users. Remind the \"legal team\" that the out-of-touch \"politicians\" who made that \"law\" didn\\'t actually understand what a cookie is either, what a cookie shares around, and therefore didn\\'t understand how futile their \"law\" would be at protecting anything/anyone\\'s privacy whatsoever.\\nSecondly, show the \"legal team\" this: http://arstechnica.com/tech-policy/2016/07/tech-industry-gangs-up-on-european-commission-calls-for-cookie-law-to-be-scrapped/ and imply to them that this \"law\" is not going to stick for much longer, mostly because of the first reason mentioned above [incompetatant polititions].\\nFinally, you are fine to ignore it if you are outside the EU. They are not going to do anything anyways. But to comply is very easy -- as in you don\\'t have to make a fancy popup (that ironically uses cookies) or actually get a users consent. All you have to do is add \"By using this site you agree to our cookie policy\" as a link to your privacy policy/terms with a basic cookie audit in those docs. Put this in the footer of your site(s) beside the copyright. Just like any other types of terms, by using the service they agree to the service. If they do not agree, they are violating your terms and must discontinue use of the service.\\nA link takes like 10 seconds to implement and it\\'s buried way below the fold. No one cares anyways, and the only reason they click \"I agree\" on popups/bars is to get it out of their face [viewport]. Better to convert (+$$) then to annoy visitors (-$$). Legal teams, spending time/cash, and annoying visitors means this: (---$$) which is surely what your company doesn\\'t want to do.',\n",
       " 'The rules are simple regarding to this:\\nYou are not allowed to wrap an inline element around a block element.\\nThe exception: If you use HTML5, you may wrap in anchor around a block element.\\nFrom a SEO perspective it doesn\\'t have influence, it\\'s valid html so they wont discredit for that.  \\nIt doesn\\'t help either. You\\'re ramming a whole paragraph into an anchor, making the value of each word in it very very small, and you also stuff it with words your don\\'t care about (\"This is an example\" » only keyword is \\'example\\', yet the whole sentence is the anchor).\\nFrom User Interface point of view, it can (depends on the implementation) be very odd, the whole paragraph is a link? \"Huh, I just clicked plain text and now I\\'ve changed page?!\".\\n\\nDavid Walsh has a small article with almost the exact same example\\nWhich is more correct: <h1><a>..</a></h1> OR <a><h1>..</h1></a> from SEO point of view?\\nDifference between a block and an inline element',\n",
       " 'Lets explain a few things first:\\nNameserver records are typically maintained by the domain registrar.  (i.e. who the domain-name was purchased from.)  If you don\\'t own the domain-name directly... you might have a battle on your hands to get ownership back.  It is not unheard of for shady companies to hold your domain-name hostage.  \\nMost registrars will typically allow you to do one of two options:\\n\\nPoint the SoA or NS records to another dns hosting company (or your own private DNS servers)... \\nOffer/include their own DNS hosting services for your domain.  It might be a simple process to talk to them and point it back to their own DNS servers.\\n\\nYou can typically look at your DNS records to figure out who the registrar is with online tools like nwtools.com.  But if you don\\'t actually own the domain... you might need to fight in court or pay them off, or even just abandon hope and buy a new domain.\\nWebsites need more than nameservers to work.  You\\'ll also need \"A\" records to point to the IP of your webservers.  An \"A\" record is simply a record that takes the hostname and translates that to an IP.  If this company you are leaving is also the webhost... you\\'ll need to add your own A records that point to your new hosting company\\'s webservers.  (most webhosting companies will work with you on getting the right records pointed to the right IPs)\\nAnd of course... if you don\\'t have a webserver, (or the content from your old webhosting company) you\\'ll need to rebuild your website.',\n",
       " 'Blame Your Expectations:\\nThis one hasn\\'t really been a big issue on here in the past but still deserves an honourable mention. If your only reason for thinking that your pageviews are wrong are that they aren\\'t at the level you expect them to be, especially when first starting out, don\\'t panic, this doesn\\'t mean there is anything wrong to be fixed. Definitely go through the list below in case there is a problem to be fixed but be prepared to live with a lower page view that you may expect, want, or thought was reality.\\nUsing Old Code:\\nThere is a large amount of old code for Google Analytics out there, especially in pre-canned plugins where you simply add your tracking code and leave it at that. Google Analytics has gone through several version changes over recent years and older versions are simply not as accurate as some of the newer code. If you are using older Google Analytics code (such as the old ga.js code) then you will want to upgrade to Google Tag Manager or Universal Analytics.\\nBad Code Location:\\nAlong with old code there are old code placement methods. Some developers will tell you that Google Analytics code should be placed at the bottom of the page and insist on doing this (arguably for page load time and non-blocking reasons), but those rules are a number of years old. Universal Analytics and especially Google Tag Manager are designed out of the box to be placed right after the opening body tag, this is in fact the recommended placement for the code snippets by Google themselves (and you would assume they would know best).\\nMissing Tracking Code:\\nIt actually helps to have the correct tracking code on all the pages you want to track. I have spoken to some developers in the past who have thought that by adding it to the main homepage Google automatically knows to track all pages under that domain, unfortunately that is not the case. The only way Google has to know that a page needs to be tracked is to add the tracking code to each and every page that needs to be tracked. Google Analytics does have a fancy admin feature on this though, which is powered by the standard Googlebot crawling. Using Googlebot Google checks all the known pages under the domain and identifies pages which don\\'t have the tracking code added, now while Google can\\'t track the pages yet, they do show an alert on your Google Analytics console to inform you that there are a number of pages on your site that are missing the tracking code and not being tracked, but don\\'t depend on this, make sure to add it to each and every page, and if using a content management system, add it to the core template so that it is automatically included on each and every page under the CMS.\\nIncorrect Tracking Code:\\nSometimes the tracking code on the page is just plain wrong. Somehow an error has been inserted into the code, and it just breaks. Sometimes the wrong Google Analytics property is listed and data is being sent to the wrong Google Analytics account. If the code is wrong for whatever reason you are not going to collect data from it so open your Google Analytics and take a look at the Google generated property code and then compare it to the code already on your pages, is there a difference, if so fix it.\\nDouble Tracking:\\nIf your code is great, and in the right place, and is reporting without a problem, but you have it on the page more than once that is what is known as double tracking. The tracking code will fire two pageview events. This is often caused by adding the updated Google Analytics code to the page without removing the old code. This can be some of the worst news to a marketing executive, needing to tell them \"Hey, you know how you where wrong about our pageviews, well you where right to be worried, they are actually half what the reports say\".\\nCustom Code:\\nCustom code isn\\'t all that bad but you can easily do bad things to it. When you send a hit you can pretty much change anything about it including the URL that Google Analytics sees. There are numerous problems that can crop up because of custom code being used instead of the actual Google Analytics or Google Tag Manager code. I have seen this done before for a $300\\'000 government website redevelopment and it took months to identify the issue.\\nLocally Hosted analytics.js:\\nRegardless of whether you are using ga.js, analytics.js, ua.js, etc, don\\'t save it to your own web server and host it from there, let Google serve the latest correct version for you. If Google makes a change to the javascript file (which they do quite often) and don\\'t tell you what they changed (which the often don\\'t) then you will be sending data to Google using an old file when Google is expecting data in another format or to another endpoint being sent through the correct code, and in this instance everything just breaks.\\nAJAX Frameworks:\\nSites that use AJAX frameworks such as Single Page Apps or custom variations on the same can do a variety of things where page content changes but a pageview event is not fired, usually through the page rewriting itself instead of reloading. If your site uses AJAX in this way then you need to plan for this and fire virtual pageviews when the page changes, rather than relying on a normal page load that\\'s not going to happen for Google Analytics to detect. Page views are fired when the page loads, if the content is swapped or substantially altered without a page reload then no additional pageviews are recorded.\\nVirtual Pageviews Instead of Events:\\nSometimes a developer or webmaster may use virtual pageviews where an Analytics Event is more appropriate. If you\\'re firing pageviews when someone scrolls down the page or clicks on a banner, your pageview figures are going to artificially skyrocket when the vast majority of them are not actually pageviews. Remember a pageview should be fired when the substance of the page changes, such as new content being on the page, anything else is an event.\\nFrames, Frames, and IFrames:\\nIf you put your code on every page but then use frames heavily on your site you could be loading one page but wind up with multiple pageviews all firing thanks to the frames. Google Analytics doesn\\'t automatically reject analytics from frames as you may actually be wanting to track them.\\nMETA Refresh:\\nI encountered a strange problem which had me laughing after a few hours of searching. A company dashboard which was still in staging and not deployed to the public yet was recording a new pageview every 5 seconds on the nose and yet no one was meant to be using this dashboard yet at all. Of course this was initially flagged by security as a hacking attempt but after I reviewed server logs and checked the site HTML I found that one of the developers had added a META refresh tag to the top of the dashboard page to save them from needing to click refresh each time they updated something minor in the UI (live code changes to staging at this time).\\nBad Filters:\\nBad filters in Google Analytics can destroy otherwise amazing reports. Incorrectly using an exclusion or inclusion filter can cause you to loose significant traffic from your reports, even all of it, even though the traffic is actually going to the site with no issues.\\nQuery Parameters:\\nQuery parameters in your URL will cause the data to be split into different buckets. Where appropriate use the exclude URL query parameters feature to remove any query parameters that aren\\'t meaningful to your analysis. As a hint pageview reports generally don\\'t need to be broken up by page number query parameters.\\nBad Triggers:\\nIf you are using Google Tag Manager, your analytics tag may not be firing on the right pages because you set up a bad trigger. If your triggers are bad then your tags won\\'t fire and your pageviews won\\'t get tracked. There are lots of ways you can have triggers setup so the best option is to go back to the Google Tag Manager console and follow their step by step guides to suit your own specific implementation.\\nContainer Version Not Published:\\nSo you have made sure your triggers are all good, but what use is it if your tag container version is not published and the published version contains something else. Make sure you are working with the correct version of the published container.\\nRobots, SPAM, and Spiders:\\nThere is a ridiculously huge amount of traffic on the internet these days originating with robots, spam, and spiders. Using a robots.txt file you can control access to your site from robots and spiders which follow the rules and are legitimate (most search engines), additionally Google Analytics is generally intelligent enough to differentiate between a search engine robot and a user if the robot is following the rules. Spiders on the other hand indiscriminately crawl the internet scanning entire websites (often to download the entire site to the local disk such as HTCopy), unfortunately there is no practical way to prevent this as these spiders often don\\'t respect a robots.txt file anyway. SPAM is more of a concern for web forms but can still cause pageview inconsistencies due to it not being a genuine visit. While you generally can filter this out based on hostnames there is no real way to prevent it in advance so you need to be prepared for it.\\nUser Disabled Javascript and/or Cookies:\\nNot much that can be done about this one, when users disable javascript and cookies (usually for security reasons) there is no way to track the users due to the fact that analytics depends on javascript and cookies to properly work.\\nSampling of Data:\\nI have answered that many questions for clients, questions on Stack Exchange, and on other sites regarding sampling you would be amazed. As a quick explanation sampling occurs where you have so much data that whenever you look at anything other than a standard report, by adding another dimension for instance, you will get sampled data. Why is this, it is because while the standard reports are already compiled by Google compiling an analytical report of the raw data takes a fair bit of computational power and hence time, when you have a huge amount of data the way Google speeds this up is to sample the data only instead of compiling all of the data. The important thing to remember is that when you are looking at sampled data the numbers will not be accurate, and how inaccurate they are can vary. I am not personally a fan of sampled data, especially in traffic analysis, but as long as you realise that the data is sampled you can start to work with the data treating it as such.\\nReal Time Data:\\nPageviews take time to process, anywhere up to a few hours depending on how well Google is doing at the time and how much data from other clients Google is processing at the same time. If you are wanting to get a better understanding of how many hits your site got at a specific time of day the best thing to do is wait until the following day to give all the data a chance to be processed. You can look at the real time reports to see if hits are coming in, but don\\'t read the standard reports as accurate until the following day (and that doesn\\'t mean midnight, that means start of business the next morning when the sun is up).\\nThe important thing to note here is that while this is a fairly comprehensive list these are just the more common issues. For every single reason on this list there are dozens more which have not been covered. In most cases the problem will be with how analytics has been implemented on your site or how you have configured the interface to read the reports.',\n",
       " 'RewriteCond %{REQUEST_URI} !^/Joomla3\\nRewriteRule ^(.*)$ Joomla3/$1 [L]\\n\\nWhat these directives do is... if the requested URL does not start \"/Joomla3\" then internally rewrite the request to the \"Joomla3\" folder.\\nWhat you need to do is add another condition to state that... if the requested URL does not start \"/Joomla3\" and it does not start \"/owncloud\" then internally rewrite the request to the \"Joomla3\" folder.\\nFor example, this could be achieved by adding a second RewriteCond (condition) directive:\\nRewriteCond %{REQUEST_URI} !^/Joomla3\\nRewriteCond %{REQUEST_URI} !^/owncloud\\nRewriteRule ^(.*)$ Joomla3/$1 [L]\\n\\nRewriteCond directives are implicitly AND\\'d.',\n",
       " 'Each custom report is based on the same data - the tabs are meant to merely display the data in a different way.\\nTo use different data, you need to create a new report. You will not be able to do it in a new tab.\\nEdit:\\nThis link lists how different dimensions and metrics work together and exclude each other, which is helpful in making your reports work.',\n",
       " 'Your example of the hierarchical layout is very close to the suggested usage of the mainEntity property suggested by the official protocol documentation:\\nAs mainEntity definition \"indicates the primary entity described in some page\", this is a perfect fit for your case, so my suggestion is that you choose this well structured code, Google should understand it perfectly.\\nThis is the relevant code for this example used in schema.org main property:\\n<body itemscope itemtype=\"http://schema.org/WebPage\">\\n...\\n<div itemprop=\"breadcrumb\">\\n  <a href=\"category/books.html\">Books</a> >\\n  <a href=\"category/books-literature.html\">Literature & Fiction</a> >\\n  <a href=\"category/books-classics\">Classics</a>\\n</div>\\n<div itemprop=\"mainEntity\" itemscope itemtype=\"http://schema.org/Book\">\\n<img itemprop=\"image\" src=\"catcher-in-the-rye-book-cover.jpg\"\\n     alt=\"cover art: red horse, city in background\"/>\\n<span itemprop=\"name\">The Catcher in the Rye</span> -\\n <link itemprop=\"bookFormat\" href=\"http://schema.org/Paperback\">Mass Market Paperback\\nby <a itemprop=\"author\" href=\"/author/jd_salinger.html\">J.D. Salinger</a>\\n<div itemprop=\"aggregateRating\" itemscope itemtype=\"http://schema.org/AggregateRating\">\\n  <span itemprop=\"ratingValue\">4</span> stars -\\n  <span itemprop=\"reviewCount\">3077</span> reviews\\n</div>\\n<div itemprop=\"offers\" itemscope itemtype=\"http://schema.org/Offer\">\\n  Price: $<span itemprop=\"price\">6.99</span>\\n  <meta itemprop=\"priceCurrency\" content=\"USD\" />\\n  <link itemprop=\"availability\" href=\"http://schema.org/InStock\">In Stock\\n</div>\\nProduct details\\n<span itemprop=\"numberOfPages\">224</span> pages\\nPublisher: <span itemprop=\"publisher\">Little, Brown, and Company</span> -\\n <meta itemprop=\"datePublished\" content=\"1991-05-01\">May 1, 1991\\nLanguage: <span itemprop=\"inLanguage\">English</span>\\nISBN-10: <span itemprop=\"isbn\">0316769487</span>\\n</div>\\n...\\n</body>\\n\\nThe above example and yours, both are being fully processed by https://search.google.com/structured-data/testing-tool with very similar results.',\n",
       " 'According to this we are still waiting to see what Bing will do, so I would assume (I know) not. \\nHere is an excerpt regarding Bing:\\n\\nWhat Can We Expect?\\nGoogle’s disregard for JavaScript in the past has also been used by\\n  many websites to their advantage. It will be important to understand\\n  the repercussions of Google being able to index all of the content and\\n  assets on your pages, as it will be able to see your pages as a user\\n  can now.\\nThis announcement creates more questions than it answers, especially\\n  considering the versatility and expanding capabilities of JavaScript.\\n  We don’t truly know what Google’s goals are for this update, but RKG\\n  is formulating testing to better understand the ramifications.\\nA few scenarios to think about:\\nIs your site is hiding duplicate content behind JavaScript, perhaps by\\n  a review aggregator, will Googlebot now crawl all of that duplicate\\n  content?\\nWill all JavaScript links in  tags or JavaScript jump menus from\\n  forms suddenly count as links?\\nHow will faceted navigation within JavaScript be interpreted?\\nWill the execution of JavaScript by Googlebot be in small, incremental\\n  waves or  will this new capability be a fire-hose that is simply\\n  turned on?\\nHow will Bing respond?\\nDo the crawlers that you use internally for your own site analysis\\n  have the ability to execute JavaScript?  Will they no longer be able\\n  to mimic Googlebot?',\n",
       " \"Here's what I found about having the GTM container in the <body>:\\n\\nFirst, if you add the <noscript/> tag as well (as you should), this should always be in the body of the document. The tag (which is shown for browsers without JavaScript enabled) contains an iFrame which loads the GTM library. If you add the <noscript/> tag into the head of your document, it can perform pretty wildly with some browsers. You could experiment with leaving the <noscript/> in the body, and placing the JavaScript in the <head/>, but I haven’t tested it and certainly don’t recommend it.\\nThe second reason is simple: to maximize data collection, the snippet should be the first thing loaded when the body of the page is rendered. Because the library is loaded asynchronously, there’s no reason to delay it at all. So have it load as the first thing when the body is rendered, so that you don’t risk losing any data due to sluggish DOM elements delaying the page load.\\n\\nand potential problems with having it in the <head>:\\n\\nYou can place the container snippet in the HEAD of the document. Usually this should work fine. The only problem I can think of is that GTM loads before the browser reaches BODY, and thus might fail because many GTM features require the document node, which is created when BODY is rendered.\\n\\n(cf. http://www.simoahava.com/analytics/container-snippet-gtm-secrets-revealed/#gref in the comments)\",\n",
       " \"Looks like you don't have the DNS server configured at the domain name registrar. Right now the World doesn't know about your domain, it even doesn't know where to ask about it:\\nroot@dev2 [~]# dig fash.lk ns +short\\nroot@dev2 [~]#\\n\\ninstead you should see something like that:\\nroot@dev2 [~]# dig gov.lk ns +short\\nns1.gov.lk.\\nd.nic.lk.\\nns2.gov.lk.\\nc.nic.lk.\\nm.nic.lk.\\nroot@dev2 [~]#\\n\\nSolution: go to the domain name registrar where you bought it, and configure the name servers. Usually you must create at least two NS (or have two NS records).\\nAlternatively you can use the NSes of the registrar, please contact support there.\\nAnd yes, I assume you did registered(=bought) the domain name, not just configured it :)\",\n",
       " 'Data compressed with gzip contain all the information needed to decompress the data. This means an attacker can simply decompress the data, same as the browser. So this is no more secure than plain text. And the compression level in gzip is actually irrelevant for decompression since it only says how much efforts will be done in finding common pattern in the input to get the best compression ratio.\\n\\nraw compressed data (which to humans is garbage)\\n\\nIt does not matter if it looks like garbage for most humans. A technical person looking at the traffic will easily see that these are compressed data because the HTTP response actually says this (\"Content-Encoding: gzip\", without this header the browser will not decompress it) and the data contain the typical gzip header.',\n",
       " \"Advantages are very few, and they are only for hobby-blogger:\\n\\nNo technical workload (nothing to setup).\\n(Mostly) No troubles with site performance.\\nLow entry point (one can start blogging without any background knowledge).\\nSecurity (noone hacks blog platforms).\\nSocial networking by design (blog platform users are the first and easy-to-reach readers)\\nSEO out of the box (free platform templates are mostly enough optimized to provide findability).\\n\\nDisadvantages are different, but they appear only if one wants to blog like a pro (make money with):\\n\\nNo or limited extensions.\\nNo or limited customization of design, functionality and SEO (no  or limited access to the template source, no access to the core source).\\nLimited e-commerce and monetization (one isn't absolutely free in how one monetizes the blog).\\nLimited webspace.\\nNo custom user journey scenarios.\\nVisible blog platform footprints (less trust).\",\n",
       " \"This very much does sound like a caching issue. Some internet service providers and many businesses ass caching proxy servers to their networks to speed up internet speeds for users on their networks as well as to minimize upstream bandwidth as a cost saving measure. A Google PageSpeed Insight's is designed to be able to test your site as you make minor changes to improve the site speed Google is very careful about not caching the content and always testing against the origin server. The way most developers beat this problem is by adding cache break strings to the end of their script files during the development stage to force any caches in between their servers and browsers to ignore the cached content and fetch the most recent version from the server. The most common way of achieving this cache breaking string is by appending a random number as a query string to the end of the static resource filename, eg:\\n/styles.css?nocache=2934729349234 or /scripts.js?2398402802234.\\nAs the random number is being added as a query string the web server still serves the correct file and as the static resource is not designed to handle the query string the query string is in effect ignored by everything except the caching proxy servers, but the caches treat each request for the same static resource as a request for a new resource as the query string is different each time making it a different and new URL which hasn't been cached before.\",\n",
       " \"In this instance you have taken the option that is the best practice and is what is done by most websites that have to run tasks at regular intervals. What it boils down to is you are using the servers native task scheduling tool (cron) to run a regularly scheduled task, this is absolutely what you need to do. Running it every single time the page loads not only will cause a huge number of queries to be made to the API every single second, but will also slow down the page load as the user will have to wait while the server side code checks the API's for data and commits the data to the database. What you have done achieves everything you need, uses tools for what they where designed to do, and does not cause issues for your end users' experiences. Well done.\",\n",
       " \"WordPress on shared hosting assumes that the end user understands how to maintain a WordPress install.  The big things being updating WordPress, and even more importantly updating their WordPress plugins.\\nWordPress plugins are notorious for having security vulnerabilities and may result in the site being modified by a malicious party.  A hosted solution should be able to respond to these issues more quickly, and remove the malicious changes, and update and/or protect the plugins.\\nAs well a hosted solution will likely have backup systems in place as well as being hosted on high availability infrastructure and caching layers for busy sites to mitigate load to keep the amount of CPU resources at a minimum, allowing busy sites to host a lot of traffic and slow traffic sites to have the CPU needed to render their site in a reasonable (milliseconds) amount of time.  These benefits come from the host knowing they are hosting WordPress and only WordPress, and thus can deploy specialised solutions.\\nOn shared hosting one usually gets an Apache, with some random version of PHP that is decided to be good for most people, and access to a database.  If your application gets slammed you chew the CPU on the host and the host will likely throttle access to CPU resources rendering the site effectively offline.  This means that a hosted solution will usually be mostly impervious to a DDoS and a shared solution can be taken offline with modest resources.  As well, the end provider doesn't have a way to test everyone's website with updates they are going to apply, so sometimes the host may break some sites to perform updates to the installed PHP or other supporting application software.\",\n",
       " \"While this isn't an ideal practice Google is able to now detect the CSS used in a page to detect when elements are being hidden and when elements are visible as long as the robots.txt file isn't blocking access to the CSS and javascript files which control which version of the menus is visible. You shouldn't see any issues with SERP ranking.\\nHaving said that however from a manageability standpoint this is far from ideal and if possible should be addressed so you have a single menu instantiation and you use javascript to define the styles and location based on your needs (IE: is the menu needed for above the fold display, scrolling display, or mobile display).\",\n",
       " \"A domain with the same A record can not be shared across multiple servers due to the limitations of DNS. However there are technologies and software that will allow you to overcome these issues, some include mounting the remote resources or using a reverse proxy.\\nSome methods to mount remote folders include:\\n\\nMounting the remote folder over SSH and SFTP using sshfs.\\nMounting the remote folder over FTP using Mount curlftpfs.\\nMounting the remote folder over Samba using cifs-utils.\\n\\nSome guides on setting up a reverse proxy:\\nIf you don't like the idea of setting up remote mounts then you could use a reverse proxy that will basically use two HTTPD to serve the content. I've included the guides because you will need a tutorial as setting up a reverse proxy isn't as simple as telling you do this, or that.\\n\\nHow To Use Apache HTTP Server As Reverse-Proxy Using mod_proxy Extension\\nTutorial: Apache 2.4 as reverse proxy\\nSetup Apache2 reverse proxies\\n\\nThe other options:\\n\\nUse a sub domain.\\nHost the one page on the same server.\",\n",
       " \"It's not quite a case of Google choosing to replace the keyword with Not Provided. The issue here is that Google runs all of Google search using HTTPS. A safety restriction built into all browsers is not to provide the referrer header when the referrer is a HTTPS site, this is an intentional security feature. The way most libraries have worked in the past to get the keywords used for the Google search that brought the user to the site in question was to get the data out of the referrer header which had the Google SERP URL that the user came from and which contained the search string and keywords. As this is no longer happening due to the HTTPS restriction there is no data provided hence the Not Provided result if you try to get the referrer keyword.\",\n",
       " \"71.4% of w3schools users use chrome, not the world as whole. w3schools' audience are programmers who tend to use Chrome because they are more technical than the average user and know that is is better than IE and (arguably) better than Firefox. They also like it's very powerful developer tools which obviously makes their web development easier.\\nWhen you get the statistics from other providers who get their data from multiple sites that are not technical in nature the market share of Chrome is about 48% according to NetMarketshare which gets their data from over 40,000 different websites which helps to eliminate the bias you get from using only one website for your data.\",\n",
       " \"Are you displaying ads on your site from Amobee (formerly known as Kontera)?\\nThe PTR record for this IP is nat.aws.kontera.com, suggesting that it is a crawler looking for your page's content in order to determine what ads may be relevant to that URL.\\nIf you've loaded the Kontera JavaScript on login-protected pages, then you will find that they are crawled any time a logged-in user visits those pages. Try removing the JavaScript call from protected pages, and see if the crawling of those pages stops.\\nIt's also possible that code has been added to the web page by a man in the middle attack before it reached the user who actually visited your site. Such an attack might have been launched by a network operator or malicious party in order to gain revenue from your content, or for other reasons.\\nThis is one of many reasons that every web site should run on HTTPS.\",\n",
       " \"Well, this is little bit too broad because there could be a numerous reasons for this, but I'll try to narrow down.\\nYou are banned.\\nFrom this article here you can see many reasons for account being removed / banned etc.\\nIf your stumbles are not showing up in your SU profile, your account might be banned. Here are some possible reasons for banning in SU.\\n\\nToo many different SU usernames voting from the same IP address.\\nReciprocal voting activity, based on tracked patterns or published       confirmation (i.e., a blog post or social media campaign suggesting\\n  potential reciprocal voting activities)\\nToo many users voting on the same story and coming from the same       referring URL – e.g., from a forum listing.\\nMisuse of the ‘send’ button. The SU browser tool bar has a Send       button that lets you message your SU friends on some content you’d\\n  like them to look at. If you you’re only sending them your stories,\\n  votes for your site could be discounted.\\nComplaints. This is a pretty broad area, and there can be any sort of    complaint from other users which might cause you problems on SU.\\n\\nStumbleUpon is wonderful for traffic. Try to contact them directly to see what's up with your account.\\nEDIT: \\nBut, from my experiences, I know that in first week or two I get many views. Eventually I'm getting lower and lower and lower views. You need to put fresh and new articles over and over and over again, and that's it. This is not unusuall. I thought that you were banned. But, this is (from my experience) normal behaviour. It's stumble upon dude. I don't use it so often as I used to do. I don't know what else to say about this matter. It site for generating traffic.\",\n",
       " 'No, you wouldn\\'t include tracking parameters in the canonical link element. \\nIt\\'s certainly not true to say that the \"general way to do this [set canonical URLs] should include any URL parameters\". Most often, the canonical link element is being used, at least in part, to exclude parameters. \\nWhat is the correct approach to canonicalisation will differ by site, of course: parameters may very well be part of a valid canonical URL, but not necessarily.\\nAlso, no need to speculate on whether Google disregard utm parameters. They don\\'t. See screenshot below, showing a duplication due to utm parameters.',\n",
       " \"To do what you are asking all you need to do is open the domain record record for registration.abc.com and change it from point to the shared server to pointing to the VPS by changing the A record from the shared servers IP address to the VPS's IP address.\\nAs for no downtime by keeping the sub domain in place on the shared hosting server as well for the short term this will mean that while the old IP address is still cached users will still have access to it using the old shared server and as cached copies of the DNS entry expire the users will be directed to the new server using the new IP address. This should be completed at most 48-72 hours after the DNS change depending on your time to live settings and any upstream caching that is being done.\",\n",
       " \"You shouldn't try to cut the ties with the old overview page. You need to redirect to a page that is most like the old page to still get the valuable link juice. That will likely be your home page. From there you should ensure you have good structure with internal linking to the different product pages. Gradually the indexing should become more aligned with the actual website structure that you have as you continue work on your landing pages and inbound linking.\",\n",
       " \"Hiding content using display: none; is only an issue if you are trying to artificially influence your SERP ranking.\\nGoogle is now crawling CSS and javascript in order to identify situations where display: none; is being used to game the system and when it is being used for responsive and interactive features of a website. As long as Google is able to access your CSS and javascript files (IE: you are not blocking those directories using your robots.txt file) then you won't have an issue. In addition to that there are a large number of legitimate reasons to use display: none; in modern websites.\\n\\nThe legitimate use of this technique is so prevalent that I would rarely expect search engines to penalize a site for using the display: none attribute. It’s just very difficult to implement an algorithm that could truly ferret out whether the particular use of display: none is meant to deceive the search engines or not.\\n  Eric Enge - Stone Temple Consulting (SEO Consultancy)\\nSource\\n\\nAnd...\\n\\nWe can flag text that appears to be hidden using CSS at Google. To date we have not algorithmically removed sites for doing that. We try hard to avoid throwing babies out with bathwater.\\n  Matt Cutts - Google Software Engineer\\nSource\\n\\nBased on the above I would tend to say that it would not be necessary to use z-index over display: none; and based on other articles I have seen by Google in the past whether you use z-index or display: none; will have much the same effect with Google, as in if it is being used for something like tabbed content then no issue but if it is being used to spam your page or artificially inflate your SERP rankings you will find yourself running afoul of the dreaded penguin.\\nupdate #1\\nThanks to OP for his reference. Google does in fact rank tabbed content lower as it is considered secondary content. Having said that I would still stand by my statement that using z-index and display: none; will be treated the same as Google does state that hiding content behind other content is also classified as hidden content and potentially could be classed as black gat SEO depending on your implementation. As such whether you use z- index for tabbed content or not I would say it would still result in similar ranking to display: none;. Based on your reference in comments if ranking is a huge issue against UX probably best if you don't use tabbing through any means and perhaps look at in page links. Perhaps look at doing AJAX based tabs as that wouldn't be classified as hidden content and Google does state now they are able to crawl AJAX based content added to the page using onclick events as long as they can crawl the JavaScript to identify the AJAX source on the server.\",\n",
       " 'Regarding the parameters setting in Webmaster tools - you should only define these if you did not have the message from google displayed: \"Currently Googlebot isn\\'t experiencing problems with coverage of your site, so you don\\'t need to configure URL parameters. (Incorrectly configuring parameters can result in pages from your site being dropped from our index, so we don\\'t recommend you use this tool unless necessary.)\". If that message is not displayed, then you can go ahead and add the parameters that don\\'t change the content. Yes, your message parameter would be classed as non-content-changing.',\n",
       " 'RFC 5785 can be paraphrased:\\n\\nIf you create a new URL such as robots.txt or favicon.ico that is expected to be at a certain place on every website, you shall henceforth make such a URL start with /.well-known/.  \\nFurthermore, you will register all such URLs with the IETF so that there is a big central list of all these URLs.\\n\\nThe registry appears to be here: https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml  You can look through the registry, decide if any of the requested URLs serve a purpose that would help your site, and create documents if some would be useful.\\nOf the URLs that you mention in your question, only assetlinks.json appears to be registered properly.\\nThere is generally no reason to create documents just because Google reports them as 404.   Google expects to get 404 errors on sites.   They even consider it a sign of a healthy site that URLs without documents return the correct status (404).',\n",
       " 'OK after looking at the stats from GA, it looks like this is actually taken into account. GA takes the total revenue from all the A/B sessions and then works out and average from all the sessions for comparison together. This includes any sessions which ended with a £0 transaction, i.e. no sale. So by default, the comparable stats take into account possible lost earnings of a higher cost of one of the experiments.\\nHopefully this is correct and will help another user with the same issue. If its not please correct me below.',\n",
       " 'Yes. You should do it. Google will \"re-index\" your website\\'s pages. Note it could take some time. While you\\'re editing your content, don\\'t forget about:\\nKEYWORD PROMINENCE\\n\\nYou should add the keywords in header, meta description, first paragraph, last paragraph, beginning of a sentence, h2 subtitles inside the text.\\n\\nKEYWORD PROXIMITY\\n\\nIf you want to focus on a group of keywords, write the each word of the group at a smaller distance each to other.\\n\\nKEYWORD DENSITY\\n\\nThis is the percentage of times a keyword or phrase appears on a web page (it is compared to the total number of words on the same page). It should be around 3%.',\n",
       " 'One of the primary reasons for using structured data is identifying/disambiguating entities. For conveying to machines that all these pages are about the same product, you have to place the structured data on all these pages, too. Not necessarily repeating all the data, but at least providing a URI for that product.\\nFor pages that contain documentation for the product, the primary entity should be TechArticle, not Product. The about property points to the specific product the documentation is for. Example in Microdata:\\n<body itemscope itemtype=\"http://schema.org/WebPage\">\\n\\n  <article itemprop=\"mainEntity\" itemscope itemtype=\"http://schema.org/TechArticle\">\\n\\n    <div itemprop=\"about\" itemscope itemtype=\"http://schema.org/Product\">\\n      <!-- providing the URI of the product, the add-to-cart button, etc. -->\\n    </div>\\n\\n  </article>\\n\\n</body>',\n",
       " 'Okay have managed to figure this one out. Turns out there is no native feature in Git to trigger a remote fetch (as in pushing code to the repository and having the repository trigger a pull origin on the web server).\\nThe way I have resolved this was to upload a PHP script to my web server under the default vhost. Within that file I have it set to run shell_exec(\"cd /path/to/my/site/root\" && sudo git pull origin master\". It is then set to send me an email with the message output through STDOUT to inform me if the pull was completed successfully or not.\\nI have then set Github up with a webhook so that every push to the repository will trigger a webhook call to the file which I simply have addressed as http://server.domain.com/github-deploy.php.\\nThere are presently some undisclosed security checks I perform to make sure that no one else can access the file and in time I will probably add IP checks to make sure that requests to the file only come from IP\\'s in Github\\'s address block.\\nAn important note that I should add here is that the only way to make this work is to allow the apache user access as a sudoer on the server but limit it to being able to run the sudo command to launch Git otherwise an access denied message will be triggered. By restricting it to git though and not passing any input from the calling script into the shel_exec command I believe I have been able to offset any concerns over security.\\nEventually I will probably implement this as a service link with Github but for now this is ample.',\n",
       " 'You can not stop this, unless you turn your entire page into an image (and even that\\'s not 100%).\\nThere are a few tricks, but those work with css (place a hidden div over it) or JavaScript (the terrible \"no right click allowed\"), but they don\\'t use a browser.\\nThey use a (scrapper)bot, which means they get the source, where those solutions aren\\'t applicable.\\nYou should start by adding a cannonical tag to your page, with little luck that\\'ll help a bit. If you dont have it, add a XML sitemap and submit that via Google Webmastertools. It might help Google index your pages first, giving your some of the visitors back.\\nYou could load a paragraph of text, and then use AJAX to load the rest of the articles. That will prevent them from downloading it, but it will also limit your SEO value as bots have a harder time finding the content as well.\\nIf you have the time, you could try to find out the USER AGENT they sent when they visit, researchfor a list user names, and block those via htaccess.\\nIf you can, add region locks. E.g.: I dont have visitors from Russia on my site, I could block IP\\'s from Russia.\\nLegal action could be an option, but I\\'m not a lawyer, I don;t know your exact rights here.',\n",
       " 'You need an A record for your domain root, pointing to the IP address of your server.\\nOnly once you have that in place (thus directing requests for example.com to your server) will your .htaccess rules take effect.\\n\\nSince your server does not have a static IP address, you will need to find out if your DNS provider supports ALIAS or ANAME records. These allow you to have CNAME-like functionality at the zone apex. If your provider does not support these records types, you should be able to move to a provider that does support them without significant effort.',\n",
       " 'There are a few different answers to your question so I\\'m going to answer it assuming 2 things are true:\\n\\nYou\\'re talking about non-visible SEO (i.e. description tags, open graph tags, Twitter card tags, etc.)\\nThe content on your mobile page is not drastically different from the content on your desktop page.\\n\\nThe answer is: yes, but not the same and not as much.\\nLet\\'s say that we want to duplicate the SEO of a responsive page with these 9 tags:\\n\\n2 canonical tags (1 generic, 1 open graph)\\n2 title tags (1 open graph, 1 Twitter card)\\n3 description tags (1 generic, 1 open graph, 1 Twitter card)\\n2 image tags (1 open graph, 1 Twitter card)\\n\\nTo do that, we need to do the following:\\nFor the desktop page:\\n\\nAdd a rel=\"alternate\" tag to the desktop page that points to the mobile page.\\n\\nFor the mobile page:\\n\\nAdd 2 canonical tags (1 generic, 1 open graph) that point to the corresponding desktop page.',\n",
       " \"Example when doing site: search if a post is displaying on page 3 of\\n  the search and another post displays on page 10, as an example, could\\n  this mean that the post displaying on page 3 has a higher potential to\\n  rank compared to post displayed on page10...?\\n\\nNot really, Google has its own logic to order pages when you make a search with the site: operator. This has nothing to do with the pages' ability to rank for certain keywords.\\nSo a page ranking at #100 in Google for site:yourdomain.com could very well rank at #1 for a keyword that Google deems the page to be the most relevant for.\",\n",
       " 'This is because what you had read is not fully true, as Google says:\\n\\nGoogle tries to associate framed content with the page containing the\\n  frames, but we don\\'t guarantee that we will.\\n\\nOr it was before 2011, when Matt Cutts from Google said:\\n\\nGooglebot keeps getting smarter. Now has the ability to execute\\n  AJAX/JS to index some dynamic comments\\n\\nDisqus comments are being indexed by Google, (Facebook comments too), I have tested myself and I can find Disqus comments in SERP.\\nFrom a previous Disqus statement they said regarding iframes:\\n\\nwe decided to completely re-implement our commenting embed inside of\\n  an iframe. This iframe is hosted on disqus.com and, as such, the\\n  browser won\\'t let your website apply styles to it using CSS\\n  statements. We did this for a few very important reasons:\\nThe iframe is part of our solution to provide full indexing of Disqus comments by Google and other search engines, without having to\\n  duplicate content on your website (e.g., through a WordPress plugin).\\n\\nAnd about Google indexing comments:\\n\\nDisqus is fully indexable for search engines out- of-the-box. We have\\n  been approved by the Google Search Quality team to be organically\\n  crawled unlike any other comment platform.\\n\\nSo Google bot has become \"intelligent\" enough to realize that the iframe content are comments related to each webpage and can be indexed, they are not just displaying content from other webpage, but at the end, Google always has the last word:\\n\\nWhile Disqus implements the necessary functionality for Google to\\n  crawl comments, indexing comments is at Google\\'s discretion.',\n",
       " \"The way Google looks at this is whether the links you've added add up to the quality of the pages. It's a myth that linking out hurts, on the contrary, linking out to relevant authority sites helps improve your topical authority. Google sees this as an attempt to provide a better user experience.\\nHowever, if you have 10 links or a hundred links going out from each of your pages, that would look unnatural to Google and might invoke a manual review and possibly follow with a penalization if they deem that you're part of a link network and trying to manipulate the ranks of the sites you're linking to. If you do absolutely need to have those links for reasons other than trying to rank those sites, nofollow them, that rules out any penalization possibilities from Google. Things get especially bad if you knowingly or unknowingly link out to a site that has a bad reputation with Google. So nofollow is your safest bet\\nBack in the days, webmasters used to redirect links with JS so as to prevent loss of link juice and PR, but since Google has gotten very efficient with parsing JS and redirect links, I think it will have the same consequences as having a non-redirect link.\\nSo my advise to you would be\\n\\nKeep the links that you absolutely have to.\\nnofollow them\\n\\nGoogle is getting more aggressive with link networks with every Penguin update and so you run a greater risk of running into hot waters if Google deems your website to be a part of a link network\",\n",
       " \"The only part of that, where the browser would work harder, is having to parse the extra few characters for each selector. Insignificantly so because the majority of the work is done on creating a CSS Object Model, finding the elements, and applying the property values for each element. \\nFor something as small as applying font families, it's far easier to do your example #2 or, better, apply it to a parent element which will cascade down to those elements; if that's possible.\\nBut there are many, many far more important things to work on to increase site speed than this.\",\n",
       " 'The question is interesting but brings back memories of a time when people would debate IF Google could read Flash. And if text in flash was - as you ask here - visible or possibly to be viewed as a technique to add (blackhat) content. The answer to the Flash story was and is \"Google reads EVERYTHING\". And TEXT made for screen readers (etc) that is USEFUL to the content is just a sign of devotion to your quality of content AND NOT, in my view, a risk.\\nThis said ... Check:\\nhttps://github.com/twbs/bootstrap/issues/10446\\nhttps://support.google.com/webmasters/answer/66353',\n",
       " 'If you have removed the Google Analytics script (which is client-side JavaScript) then it\\'s possible that your browser is loading a cached version of the page that does have the Google Analytics script, which then sets the cookies. However, GA sets _ga and _gat - which do not appear to be set.\\nWhat makes you think it is your site that is setting these (third party?) cookies? They are not for your domain.\\nHowever, your site is still reportedly using \"Local Storage\" - which is also part of the \"EU Cookie Directive\" (if that is what you are referring to).',\n",
       " 'RewriteRule ^$ /temp [L]\\n\\nIf you have a physical directory temp in the root of your filesystem... and since you have omitted the slash from the end of the RewriteRule substitution then mod_dir will attempt to \"fix\" the request by appending a slash. It does this by issueing a 301 redirect. (You are then presumably relying on the DirectoryIndex to serve the appropriate file from that directory.)\\nYou can resolve this by appending a slash to the end of the substitution, so mod_dir won\\'t try and do this by issuing a redirect. For example:\\nRewriteRule ^$ /temp/ [L]\\n\\nI notice the status code is 301, so is this equivalent to a redirect 301\\n\\nYes, that is a 301 (permanent) redirect.',\n",
       " 'I believe that I\\'ve stumbled across a solid answer.\\nInstead we can point to the default tmp directory by using $wgUploadDirectory. Then we can dump the temp files into a unique sub-directory. \\nCustom temp directory code: (Paste into LocalSettings.php)\\n$wgTmpDirectory = \"{$wgUploadDirectory}/tmp/uniquename\";',\n",
       " 'You are looking at the wrong report in Google Analytics.  You are looking at the Acquisition -> Search Console -> Queries report.\\n\\nThis report is powered by data from Google Search Console.   To get this data programatically, you would need to use the API for Search Console.\\nThe data that you are querying through the GA API is shown on the Acquisition -> All Traffic -> Channels -> Organic Search report.   \\n\\nThe data for this report is gathered by Google Analytics via search engine referrer data sent to your site as a browser header.    Google is no longer sending most keywords to your site in the referrer header, which causes \"(not provided)\" to be so high in the list.',\n",
       " \"From your reference to the noindex tag it would appear as though you don't need the student sites to be indexed. If you add all the student sites under a single directory such as www.example.com/students/student1.HTML you could then add a disallow directive in your robots.txt file for the whole students directory, this would protect your main site from any SEO issues that the student sites could cause as Google would not attempt to index any of them, only the main site but nothing in the student directory.\",\n",
       " 'Your example [http://example.com/image.jpg] is correct.\\nYou will need to enable this by setting $wgAllowExternalImages to true in your configuration file. The default is false. Optionally, you can set $wgAllowExternalImagesFrom to allow exceptions while $wgAllowExternalImages remains set to false. You can also use $wgEnableImageWhitelist to allow exceptions based upon regular expressions.\\nYou will likely need to copy the appropriate line from DefaultSettings.php to LocalSettings.php.\\nHere are some links (in order):\\n\\nhttp://www.mediawiki.org/wiki/Manual:LocalSettings.php\\nhttp://www.mediawiki.org/wiki/Manual:Configuration_settings\\nhttp://www.mediawiki.org/wiki/Manual:$wgAllowExternalImages\\nhttp://www.mediawiki.org/wiki/Manual:$wgAllowExternalImagesFrom\\nhttp://www.mediawiki.org/wiki/Manual:$wgEnableImageWhitelist',\n",
       " 'I don\\'t know what it exactly does, but I assume you want to rename the parameters. The parameters have to be POSTed anyway, so there\\'s no way around that, but if you want to rename to something else, you the following (in resources.xml):\\n<beans:bean class=\"org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter\">\\n                 <beans:property name=\"usernameParameter\" value=\"user_name\">\\n        <beans:property name=\"passwordParameter\" value=\"passwd\">\\n        < !-- other properties which you also use in the -login element -->\\n</beans:bean>',\n",
       " 'You can use the HTTP status code 410, which stands for Gone:\\n\\nThe 410 (Gone) status code indicates that access to the target resource is no longer available at the origin server and that this condition is likely to be permanent.\\n[…]\\nThe 410 response is primarily intended to assist the task of web maintenance by notifying the recipient that the resource is intentionally unavailable and that the server owners desire that remote links to that resource be removed.\\n\\nFor the human visitors you could display a message (on the 410 pages) that the site is gone and won’t come back.',\n",
       " 'I don’t think that it’s a good practice to paginate articles (unless they are so long that it would affect the performance in the browser), because having exactly one canonical URL is preferable for so many reasons.\\nI think there are two reasons why paginating articles was/is done:\\n\\nIt increases the pages views. If a visitor is reading an article split up into three pages, the visitor has to visit two more pages than if the article would be on a single page. More page views might increase ad revenue.\\nSome think that it’s better for usability. The assumptions are that visitors don’t like to scroll so much, or that it’s easier for visitors to read longer articles if they are split up, or that a long article might intimidate, etc.\\n\\nIf paginating articles and offering a \"whole article\" page, the canonical link type can be used to mitigate the SEO-related drawbacks. Each paginated page links to the \"whole article\" page, e.g. if /articles/foobar is the full article:\\n<!-- on /articles/foobar/1, which is the first page of the article -->\\n<link rel=\"canonical\" href=\"/articles/foobar\" />\\n\\n<!-- on /articles/foobar/2 -->\\n<link rel=\"canonical\" href=\"/articles/foobar\" />\\n\\n<!-- on /articles/foobar/3 -->\\n<link rel=\"canonical\" href=\"/articles/foobar\" />\\n\\nHowever, in that case search engines might prefer to offer the link to the \"whole article\" page (instead of the link to the first page) in their results, so any benefits (if there were any to begin with) of having paginated articles are gone, because visitors would start with the \"whole article\" page directly.',\n",
       " 'Googlebot only performs POST requests under very limited circumstances where it is believed by the Googlebot that it is safe and appropriate. Google takes precautions to avoid performing tasks on a site that could result in executing an unintended user action and Google making POST requests is for crawling purposes only to index what the end user would see.\\nGoogle states in its webmasters blog that to make your site better for crawling remember that Google prefers GET for fetching requests unless there is a very specific reason to use POST.\\nA good rule of thumb to follow is...\\n\\nA GET request is used to get data from the server.\\nA POST request is used for modifying data on the server',\n",
       " 'Will that increase my website rank for the US in Google?\\n\\nYes, because US people do better CTR on SERP and more conversions on your own site.\\nCase 1\\nimagine: your site offers a service for US residents. They are very interesting in it, and, if they see your site\\'s snippet in SERP, they click it wildly. But your site isn\\'t limited to visitors from US. And visitors from i.e. India, looking for such service in google.com, see in SERP your site\\'s snippet wildly too. But then, if they come to your site, the see, the service is only for US residents, and they bounce. Google realizes the big amount of bouncers and thinks, aha, some people bounce, do they probably mean, this site as a kind of shit...? High bounce rate will cause lowering of relevancy.\\nCase 2\\nimagine the same: your site offers a service for US residents. You say it definitely in the site\\'s description, so this information appears in your site\\'s SERP snippet. Visitors, form, i.e. India, are looking for such service, but they don\\'t click your site in SERP, because they read \"aha, this service is only for us residents\". On this way your site gets lower SERP CTR and, following lower relevancy.\\nCase 3\\nYour site offers a service for US residents. You say it definitely in the site\\'s description, so this information appears in your site\\'s SERP snippet. But additionally you limit the audience of your site to US. Google then shows your site only to US residents - the amount of visitors ramains pretty the same, but on this way your site gets much higher SERP CTR and much lower bounce rate. Both will result in higher relevancy.',\n",
       " 'RewriteRule portfolio portfolio.php\\nRewriteRule reportage reportage.php\\n\\nSorry, just realised what\\'s going on... you need to be more specific in your rewrites. The above rules will rewrite the URL if \"portfolio\" (or \"reportage\") appear anywhere in the requested URL - this will catch your image URLs as well (since \"portfolio\" and \"reportage\" are part of the URL path), and that\\'s the problem. (I would expect this to also generate a rewrite loop - which would also result in a broken image. In this case the server\\'s error log should give clues as to the \"rewrite loop\".)\\nMake your rules more specific, to match only the URL in the request. For example:\\n#REWRITE CONDITIONS\\nRewriteRule ^start$ index.html [L]\\nRewriteRule ^aboutMe$ about.html [L]\\nRewriteRule ^contact$ contact.html [L]\\nRewriteRule ^portfolio$ portfolio.php [L]\\nRewriteRule ^reportage$ reportage.php [L]\\n\\nBy adding anchors (^ and $) to the RewriteRule pattern it will now only match \"portfolio\" etc. exactly and also avoid a rewrite loop. The L flag ensures that no further rules will be processed in this pass.',\n",
       " 'Okay, having gone through all the documentation and having spoken to Google I have found out that this is not possible at this time. Due to the inherent insecurity of javascript Google does not recommend directly providing read access to the data through a javascript API without some high level of security. The only way Google provides access to this data through the javascript API is with an OAuth authenticated session which requires the user to be logged into Google at the time plus having at a minimum read access to the Analytics profile.\\nAs an alternate solution to this having done a range of research online I came across a good alternative named Piwik which is a PHP based website analytics engine which works much the same as Google Analytics but which runs on your own server as a PHP web application and is a lot easier to embed reports from. In Piwik I have been able to directly embed reports using <iframe> and I only need to pass a token code for a read only user to the iframe source URL to enable accessw. The beauty about this is that I am not limited to certain reports only, rather any chart, graphic, or report which I can access through the Piwik application I can embed in my site using an iframe as well as being able to directly query the data using the API.',\n",
       " 'GoDaddy SSL Certificates Help document entitled \"Rekey certificate\" may help you: https://www.godaddy.com/help/rekey-certificate-4976. Note that following this document will result in old cert being revoked and new cert being issued.',\n",
       " 'OK. I found an answer. I read from the apache doc:\\nhttps://wiki.apache.org/httpd/DirectoryListings\\nIf no file from the DirectoryIndex directive can be located in the directory, then mod_autoindex can generate a listing of the directory contents. This is turned on and off using the Options directive. \\nSo I uncomment the mod_autoindex load module line in httpd.conf and now the directory listing works.',\n",
       " 'This is a very common problem faced these days. This can be easily solved as follows:\\nIf you want to have same page /pets but also drive /pets/cats, /pets/dog to index separately, then ideally they should exist separately i.e. if opened by a http client with a separate url they should respond independently.\\nAlso the content populated via ajax or via backend should be same i.e someone opening /pets/dogs through search result or by clicking a tab on /pets (via ajax) should be same to avoid penalty of cloaking from crawler and better indexing.\\nIts just that, a more user-friendly or a single page application behavior is simulated without losing seo indexing for that content\\nFor that to happen w.r.t. crawler and for you to have only one page managing that do the following:\\nBackend Changes\\n1) At the application level, configure your server to direct calls: /pets/, /pets/dogs, /pets/cats. . .  on a single function\\n2) This function will send this required information/data to the same view(of the parsed tab) , and will set that tab active.\\nExample ,\\nif /pets/ is received then the backend should send the content of /all to view to make it crawlable and set all tab as active tab\\nif /pets/dogs is received then the backend should send the content of /all/dogs to view to make it crawlable and set dog tab as active tab\\n.\\n.\\nFront End changes\\nNow if the user selects a tab from front-end, then you should add that in history and also fire necessary analytics pixel to track separate page views(virtual page views)\\nif(history && history.pushState)\\n            {\\n                var pageUrl = $(\"#page-\"+page).attr(\"href\");\\n                pageUrl = pageUrl.replace(\"https://\",\"//\").replace(\"http://\",\"//\");\\n                history.pushState(null,null,pageUrl);\\n            }\\n\\nAnd also change the content of the page via ajax and set the tab as the selected tab.\\nNow even if the user refreshes the page, the state of the page should remain same. This is how this whole concept works.',\n",
       " 'I just want to add a note of caution.\\nYou say only that the ex-employee is refusing to hand over the login details for his account with a domain registrar.\\nThat\\'s completely reasonable.\\nIt doesn\\'t necessarily matter that this specific domain\\'s WHOIS is labelled with his email address at your company; that doesn\\'t automatically mean that his account with the registrar is any sort of \"company account\", or even that his login details relate to your company.\\nHe may have other domains on there that are none of your business. You have no right to his account.\\nWhat you need to do is ask him to transfer the domain out of his account and over to you. Until you have actually asked for this (as distinct from asking him for his username and password!!), I wouldn\\'t go down any more serious routes.',\n",
       " 'The meta name verify-v1 used to be associated with Google Webmaster Tools, it is now deprecated and has been replaced with the meta name google-site-verification, which now works across Google\\'s services and not just restricted to WMT.\\n\\nList of Webmaster Verification, Ownership and Verify Sites\\n<meta name=\"google-site-verification\" content=\"GOOGLE ID HERE\" />\\n<meta name=\"msvalidate.01\" content=\"BING ID HERE\" />\\n<meta name=\"alexaVerifyID\" content=\"ALEXA ID HERE\" />\\n<meta name=\"norton-safeweb-site-verification\" content=\"NORTON ID HERE\" />\\n<meta name=\"wot-verification\" content=\"WOT ID HERE\" />\\n<meta name=\"p:domain_verify\" content=\"PINTEREST ID HERE\" />\\n<meta name=\"yandex-verification\" content=\"YANDEX ID HERE\" />\\n<meta name=\"majestic-site-verification\" content=\"MAJESTIC ID HERE\" />\\n<meta name=\"avgthreatlabs-verification\" content=\"AVG ID HERE\" />\\n<meta name=\"baidu-site-verification\" content=\"BAIDU ID HERE\" />',\n",
       " 'The URL does not need to contain such level of detail.\\nIt is perfectly fine to have the url you propose: example.com/user/freddy-double-barrel for Freddy Double-Barrel.\\nFrom a SEO point of view, the url should cover two aspects:\\n\\nbe appealing to users\\ndescribe what the page is about\\n\\nA url like example.com/user/freddy-double--barrel or example.com/user/freddy-double_barrel does not change anything, they even look a bit unnatural to me. \\nMost of the slugs generator strip special characters from the title to create a clean and readable url. \\nConsider the following example, this is the first result in Google for guillain-barre:\\n\\nThe webpage url is: http://www.mayoclinic.org/diseases-conditions/guillain-barre-syndrome/basics/definition/con-20025832 with guillain-barre-syndrome, but its title is: Guillain-Barre syndrome\\n\\nAs long as your page title, meta description and/or content properly says Freddy Double-Barrel, it is perfectly fine to keep the current slug scheme.',\n",
       " 'It’s technically allowed:\\n\\nRFC 6596 allows relative URLs:\\n\\nSpecify a relative IRI (see [RFC3986], Section 4.2).\\n\\nRFC 3986 defines that a protocol-relative URL is some kind of relative reference:\\n\\nA relative reference that begins with two slash characters is termed a network-path reference […]\\n\\nIt can become a problem if the document is accessible from more than one scheme/protocol. So if you have a HTTP and a HTTPS version (and you don’t redirect one to the other), you end up with two different canonical URLs, which defeats the purpose of using canonical.\\nAlso note that you don’t necessarily have control over the protocols. For example, if someone downloads your document (that contains a protocol-relative canonical URL) and uses it locally (file: scheme), the canonical URL wouldn’t point to your HTTP/HTTPS URL.',\n",
       " \"Yes, you can. Either buy multi-domain certificate if it's close to the end of primary domain certificate, or buy the certificate for the secondary domain only.\\n\\nMy host for my primary domain doesn't allow me to add any more domains to my account. What can I do ?\\n\\nAnyway you have somewhere redirect configured from secondary to primary. If it is just redirect and doesn't allow you to add a certificate - you need another hosting for the secondary. The certificate should be added in the same or similar way like you did for the primary. Eventually, you can consider secondary as an empty website which contains nothing but redirect, and buy the cheapest hosting which allows you to add an certificate.\",\n",
       " 'It depends what are you looking for: loading speed it\\'s just an effect of network speed (which often is what you are looking for) and server side processing performance (hardware performance).\\nOne factor to keep in mind is where the host is located: if your audience it\\'s mostly in Europe, would be useful to get an host close to the main European backbone. Similarly for other Geographical locations.\\nAfter you have decided this, you could use stress-test tools like siege (on UNIX) to create lots of requests to a webpage and see how the host behaves under stress (using the tools you mentioned).\\nIn the case of shared hosting and virtual server you can never be 100% sure that an host is going to be consistently fast (or slow), as your test might be influenced on other site\\'s activity on the same host. It would be good to test during different times and for a relatively long period of time (for example one test every few hours for a week), in order to have a good \"rough idea\" of the host\\'s speed.\\nTo be fair, you should test the exact same site(s) and page(s) on every host (with the same frequency).',\n",
       " 'You can use structured data markup to specify which image you prefer as more meaningful to that webpage, so it is more likely to be used that image in SERP instead of the older one.\\nFor https://schema.org/WebPage you can specify it via https://schema.org/image or https://schema.org/primaryImageOfPage\\nConsider this example that contains two images, the image of a news article (main) and the image of the logo of the publisher, the image https://google.com/thumbnail1.jpg is specified as the main image of the article, while https://google.com/logo.jpg is bound to the publisher:\\n<div itemscope itemtype=\"http://schema.org/NewsArticle\">\\n  <meta itemscope itemprop=\"mainEntityOfPage\"  itemType=\"https://schema.org/WebPage\" itemid=\"https://google.com/article\"/>\\n  <span itemprop=\"description\">A wonderful article</span>\\n  <div itemprop=\"image\" itemscope itemtype=\"https://schema.org/ImageObject\">\\n    <img src=\"https://google.com/thumbnail1.jpg\"/>\\n    <meta itemprop=\"url\" content=\"https://google.com/thumbnail1.jpg\">\\n    <meta itemprop=\"width\" content=\"800\">\\n    <meta itemprop=\"height\" content=\"800\">\\n  </div>\\n  <div itemprop=\"publisher\" itemscope itemtype=\"https://schema.org/Organization\">\\n    <div itemprop=\"logo\" itemscope itemtype=\"https://schema.org/ImageObject\">\\n      <img src=\"https://google.com/logo.jpg\"/>\\n      <meta itemprop=\"url\" content=\"https://google.com/logo.jpg\">\\n      <meta itemprop=\"width\" content=\"600\">\\n      <meta itemprop=\"height\" content=\"60\">\\n    </div>\\n    <meta itemprop=\"name\" content=\"Google\">\\n  </div>\\n</div>\\n\\nThis helps Search Engines to understand your content better, and the image of the article is more likely to appear in SERP than the image of the logo of the publisher associated to the article.',\n",
       " \"NamesPro can install your DS RR in the TLD zone, although they can't host your signed zone. It doesn't sound like that would be an issue for you.\\nIt's a manual process, you have to open a ticket and supply them with the DS RR you want installed. But they did get it done pretty quick, maybe an hour on a Sunday.\",\n",
       " \"Personally I think cross domain sitemap and things like that are not for you.\\nGoogle allowed to add subdirectories, and you can manage it seperatly with separate dashboard. That means you can add these kind of property in search console.\\nexample.com\\nexample.com/blog/\\nexample.com/store/\\n\\n1) Yes, you can add both main sitemap and blog sitemap in your robots.txt, but wait, if you gonna add both sitemap in search console, then you don't have to put anything on robots.txt. Personally I don't do that, because robots.txt should be placed on root directory, so scraper use that advantages and find out your sitemap link and find out all other links. Why we should waste our bandwidth by allowing to scrape our content.\\n2) Yes, that's what you need to do.\\nFirst add your site example.com or www.example.com\\nAnd submit your root/main sitemap.\\nNow add another property with example.com/blog or www.example.com/blog\\nAnd submit your blog sitemap. That's it.\",\n",
       " 'Your problem is not that SVG images are getting indexed.  The items in your screenshot are image directories.   If you visit those URLs you will likely see a list of files that are in the directories. Since they are image directories, the generated index page uses the word \"svg\".\\nThis is a feature of web servers called \"directory index\".   If you have .htaccess you can turn that off:\\nOptions -Indexes \\n\\nOtherwise, you can upload an index.html file into each of those directories.    The index.html file will take the place of the automatically generated directory listing.    You could make the index.html file meta refresh to the home page, and put a noindex directive in it for good measure:\\n<!DOCTYPE html>\\n<html><head>\\n    <meta http-equiv=\"refresh\" content=\"0; url=/\">\\n    <meta name=\"robots\" content=\"noindex\">\\n</head></html>\\n\\nI also want to point out that svg as a content keyword is not a problem that has to be fixed.   Google shows the content keywords report so that you can check for spam.   If you saw \"viagra\" or \"escorts\" on that list you would know your site has been hacked with unsavory content injected onto it.  \\nAs long as the keywords in the content keyword report are not spammy, you don\\'t have to take any action because of it.',\n",
       " \"I agree with Simon's comments.   You are looking for a name that is brandable.\\nTo be brandable, a name should be:\\n\\nUnique -- no competition in search engines\\nAvailable -- can register domain name\\nSimple -- short, easy to remember\\nDifferent -- stand out from the names of similar products and competitors\\nPositive -- the name should evoke positive connotations (strength, speed, ease of use -- something your product has).  Avoid brand names that sound too close to something negative.  (I don't know how anybody ever got away with naming a dog breed the poodle.)\\n\\nA brand name doesn't have to be descriptive.   It is often better if it is not.   A descriptive name limits customer curiosity.   It may also prevent you from adding features or expanding in ways that don't fit the name.\\nWhen you are creating products, it is often good to pair your existing company brand with a descriptive name for the product the way that Google does: Google docs, Google fi, Google books, etc.   That way users don't have to remember so much, only your single brand.  It helps get users to use multiple products from the same company.\",\n",
       " 'If you disable CSS, you’ll see the content. So the content is hidden within CSS, without any JavaScript involved (and will likely get displayed via CSS if JavaScript is enabled).\\nEven if the content would only be shown with JavaScript, note that there are text browsers that run JS. See for example these Stack Exchange questions:\\n\\nIs there a text mode browser which supports javascript?\\nText based browser that runs JavaScript\\nText based webbrowser that supports JavaScript?\\nCommand line browser with js support',\n",
       " \"No, using display: table, display: table-row, and display: table-cell would not be as bad as using <table> tags. The reason for that is that HTML is a semantic language, meaning that the tags used should describe the content that is within. In this case, CSS can be used to describe to the browser how the data should be presented, and since the functionality of presentation that you're after is that of a table, you can go right ahead and do that.\\nI will mention, however, that they are not supported in IE7 or below.\\nP.S. You could also check out some similar questions on Stack Overflow.\",\n",
       " \"Yes, you can.\\nThe first script indicates the URL of internal site search to be shown in Google SERP between your main entry and sitelinks. The second script indicates your social media accounts to be shown in the Knowledge Graph.\\nThe scripts don't boost your rankings, they just give Google clues about what to include in the SERP if your site is strong enough to have a search box or your brand is important enough to get a Knowledge Graph box.\",\n",
       " \"WordPress shortcodes are codes that look like eg. [gallery], and are used to quickly call built-in, or plugin, functionality from within your post.\\nUsually, these will not work in places other than the main post content - by default, a shortcode entered into the page title, or a category description, or a social plugin's title and description in this case, would just render as the code itself - '[gallery]' - rather than whatever it was meant to render instead - a full featured image gallery.\\nThere may be other shortcodes that are available to you depending on what plugins you have installed.\\nThis plugin is simply giving you the option to have shortcodes work in, what I assume from the screenshot you provided is, the title and description of each social network the plugin handles.\",\n",
       " 'All the answers suggested I would do as well. If you want another level of security I would enable Apache Basic Authentication \\n\\nhttp://httpd.apache.org/docs/current/howto/auth.html\\n\\nwith a username/password on your admin URL within Joomla (make sure that this URL is https accessible only). This is done within your httpd.conf file whereby you generate a password encrypted file which is compared to the username/password credentials when the URL pattern match is accessed.\\nThis way the attacker needs to also know the Apache protected URL username/password before attempting a brute force attack. Apache will dismiss all requests with an \"Authorization required\" message and protect your joomla instance from being bombarded.\\nIn my opinion, your admin URL path should never be exposed to the internet without some additional protection.',\n",
       " 'Use custom dimensions in Google Analytics.\\n\\nFirst, navigate to the \"admin\" section of Google Analytics\\' web portal, click \"Custom Definitions\", then \"Custom Dimensions\" under the property you\\'re selecting. Click \"new custom dimension\", give the dimension a name, and save it as type \"hit\".\\n\\nNow add code similar to the following:\\nga(\\'create\\', \\'UA-XXXXX-Y\\', \\'auto\\');\\n\\nga(\\'set\\', {\\n  dimension1: __isUserLoggedIn__,\\n});\\n\\nga(\\'send\\', \\'pageview\\');\\n\\nThe bit marked as __isUserLoggedIn__ should be replaced with the relevant code. The number in dimension1 should match the number of the custom dimension created in the previous step.\\n\\nFor more details about this, see this great blog post on CSS Tricks.',\n",
       " \"From an Internet Marketing perspective it is important to 'pre-sell' products. Consumers are much more likely to buy if they are sold on the benefits of the product, not just shown the features and the price tag.\\nEvery webpage should have some goal in mind and a target audience. It should be written with highly searched for keywords in mind. These keywords should be included in the description and throughout the content to be effective. This should not detract from genuine content, however.\\nA good, easy to follow and updated guide for SEO in 2016 is The Fundamental Guide to SEO in 2016\",\n",
       " \"If you create a page using Joomla and call it 'sitemap' it should display the content of that created page in the theme framework 'on the fly' for you. Joomla does not create a sitemap.html or sitemap.php file and put it on your server.\\nIf you created a sitemap.html file separately on your own that will not display unless you save the file to your server and type the full name in the browser. eg. example.com/sitemap.html\\nYou may have a plugin that is creating the sitemap.xml file automatically for you. The plugin compiles the information dynamically for the sake of submitting it to Google Search Console.\\nAll pages are created dynamically calling on information from the MySQL database created by Joomla when you first installed it.\\nThe permalink setting determines the way the page names display. Most choose to display the names using Post name\\nCommon Settings\\n\\nPlain             http://example.com/?p=123\\nDay and name      http://example.com/2016/07/30/sample-post/\\nMonth and name    http://example.com/2016/07/sample-post/\\nNumeric           http://example.com/archives/123\\nPost name         http://example.com/sample-post/\\nCustom Structure  http://example.com/%postname%/\\n\\nThe setting you choose for permalinks will trigger Joomla to create and write code in an .htaccess file and put it on your server in the root directory such as example.com/.htaccess\\nThe .htaccess code for Post name permalink settings looks like the following:\\n# BEGIN WordPress\\n<IfModule mod_rewrite.c>\\nRewriteEngine On\\nRewriteBase /\\nRewriteRule ^index\\\\.php$ - [L]\\nRewriteCond %{REQUEST_FILENAME} !-f\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteRule . /index.php [L]\\n</IfModule>\\n\\n# END WordPress\",\n",
       " \"HTML code size is under 20 KB\\n\\nSince you mention HTML file, there is no need for range headers for HTML files. \\nFor following reasons\\n\\noften HTML files are generated by dynamic languages (PHP, JAVA,...) and can change for every single request, so making 2 requests to 'split' the range will result in errors\\nHTML files are that small that it makes no sense for range requests. \\nRange Requests are recommended for download of big files like MP3s, high res JPGs, ISO,... as they speed up download by splitting big files in chunks which are downloaded at same time.\\nFor html and assets (images, js, css) it would be an disadvantage for the server to be asked for range requests downloads.\\nNew technologies like HTTP2 actually do the opposite, they try to combine multiple html assets into ONE request (right after HTML response)\\nFor all above, you are just increasing header size and wasting your traffic if you add those headers unnecessarily\",\n",
       " \"Effect for browser:\\nThough this looks like a bit of work for web browser, but technically it does not make much of a difference. The browsers are too fast to handle these relative url structure and make a call to application server\\nEffect for application Server:\\nNone, as it needs to return the requested file (relative/absolute link ultimately maps to a web path)\\nEffect on page size:\\nYes there would be some reduction in size (again not something that would make a huge difference to your page's performance that could be achieved by something like content encoding gzip or minifying resource)\\nSo i think technically the absolute/relative urls dont make much of a difference on page speed / any weightable matrix.\\nYes it make huge difference in managing multiple environments like dev, pp, prodpp etc\\nExample :\\non your local development you might have dev.example.com\\non pre production you might have : pp.example.com\\n.\\n.\\nSo in those scenarios it would be relatively easy to manage code with relative urls (though can be managed by environment settings also)\",\n",
       " 'In the asychronous code (ga.js) this information is stored in a cookie (__utmz). \\nIn Universal Analytics this info is stored on the Google Servers and retrivied by Google via the user id (which, again, is stored in a cookie, only that Universal Analytics stores nothing but the user id). \\nWhich means that a lot of scripts that rely on information from the Google utmz cookie will fail after the migration from asychronous tracking to Universal Analytics (Google just announced that the upgrade is mandatory and that indeed old accounts will be \"auto-transfered\" to Universal Analytics).',\n",
       " 'Your assumption is correct. Schema.org types (aka. classes) are not supposed to be used as properties, and vice-versa. (Technically they could be used for anything, but it wouldn’t make sense to do this.)\\nBut the RDF UI module is doing it correctly:\\n\\nWhen editing a content type, you can select (under \"Schema.org Mappings\") a type:\\n\\nSpecify the type you want to associated to this content type e.g. Article, Blog, etc.\\n\\nThe autocomplete list correctly lists/suggests types, i.e., terms that start with an uppercase letter (like Thing).\\nWhen mapping a field (under \"Manage fields\" → \"RDF Mappings\"), you can select a property.\\nThe autocomplete list correctly lists/suggests properties, i.e., terms that start with a lowercase letter (like name).\\n\\nIf you have a content type \"Team member\", you could map it to the Schema.org type Person, and the field \"Title\" could be mapped to the Schema.org property name.\\nBackground\\nNote that it’s just a convention (followed by Schema.org and many other vocabularies) to start types with uppercase letters, and properties with lowercase letters. A vocabulary could as well use x8234 for a type and x8238 for a property. \\nIf something is supposed to be a type or a property should ideally become clear in the vocabulary’s documentation, but the canonical definition can be found in the vocabulary’s RDF.\\nIn case of Schema.org (canonical definition in RDFa), you can see that name is of type rdf:Property:\\n\\n<div typeof=\"rdf:Property\" resource=\"http://schema.org/name\"> \\n  <!-- … -->\\n</div>\\n\\nand Person is of type rdfs:Class:\\n\\n<div typeof=\"rdfs:Class\" resource=\"http://schema.org/Person\">\\n  <!-- … -->\\n</div>',\n",
       " \"configure your site at the Australian web hosting. Make sure the site is configured as this.example.extension. You'll have the IP where the site is located. Yes, you won't be able to see the site, but it's OK for now.\\nat Cloudflare configure this.example.extension.cdn.cloudflare.net to that IP\\nprofit! :)\\n\\nThe caveat is the Australian hosting may trouble you because this.example.extension doesn't point to their server. Two ways to deal with this: 1. say 'i know, do it anyway' 2. choose another hosting.\\nbtw, it's better to transfer dns to CloudFlare, if possible. their dns-es are faster than your, and geo-distributed.\",\n",
       " 'Not a Schema issue, it\\'s a website and  Google issue\\nThe problem you are experiencing is pretty common and a side effect of GEO IP detection. The issue that your encountering is occurring due to the fact that your site is using some type of IP Geolocation service, often known as GEOIP. \\nIn short your site is automatically detecting where a site visitor is located and then serving them with the correct currency, and obviously you are serving the Googlebot with US currency, because the majority of Google\\'s crawlers are located in the US.\\nGoogle\\'s Data Centres\\nGoogle\\'s crawler operates on a large volume of IP addresses, and Google operates from multiple data centres throughout the world:\\n\\nAmerica\\n\\nBerkeley County, South Carolina\\nCouncil Bluffs, Iowa\\nDouglas County, Georgia\\nJackson County, Alabama\\nLenoir, North Carolina\\nMayes County, Oklahoma\\nMontgomery County, Tennessee\\nQuilicura, Chile\\nThe Dalles, Oregon\\n\\nAsia\\n\\nChanghua County, Taiwan\\nSingapore\\n\\nEurope\\n\\nDublin, Ireland\\nEemshaven, Netherlands\\nHamina, Finland\\nSt Ghislain, Belgium\\n\\nGoogle does not crawl websites from the UK\\nAs far as I know Google does not have any UK based servers, but they do have servers in Ireland, most likely for tax haven reasons, as Ireland is know for its Double Irish Arrangement that allows big companies to pay lower taxes for its products and services, something that is constantly in the UK press, often referred to as tax avoidance by politicians. \\nAnyway...\\nGoogle crawling will never occur from a UK based server, since the majority of crawling occurs from servers located in the states, other countries outside of the UK, but besides... even if Google did have a UK crawler, Google doesn\\'t have a feature where you can select a preferred crawl location in any-case.\\nIncorrect currency May Affect Your Local Rankings\\nIf your main target region is the UK, then Google finding US dollar can and most likely affect your rankings, Since Google thinks your main region is US.\\n\\nSOURCE\\nOther signals. Other sources of clues as to the intended audience of\\n  your site can include local addresses and phone numbers on the pages,\\n  the use of local language and currency, links from other local sites,\\n  and/or the use of Google My Business (where available).\\n\\nReplicate my Findings\\nYou can replicate this issue by connecting to a VPN, or Proxy located in the US and then using a cookieless browser (important since your site is storing last used currency as a cookie) or Google Chrome incognito, you can clearly see that it is automatically assigning the currency. \\nPossible Fix\\nThe most obvious thing would be to ignore the IP addresses of Googlebot, or the user agent, however this is not recommended by Google, since it could be detected as a form of cloaking. You should always treat the Googlebot as a visitor, not a bot. \\nSo, you could always use that solution but obviously its not approved by Google. So I wouldn\\'t recommend the above solution to anyone. You can approach this by setting up your site as a Multi-regional and multilingual website.\\nOr alternatively use multiple Schema markup to determine the currency, therefore Google should understand the website has multiple currencies and return the correct results depending where the user is in the world.\\n\\nSOURCE:\\nThis would allow you to serve all currencies in your markup, hide\\n  those that weren\\'t valid for the current user/ip through CSS/JS and\\n  still have Google understand what you\\'re doing:\\n<div class=\"curr-gbp\" itemprop=\"offers\" itemscope itemtype=\"http://schema.org/Offer\">\\n  <!--price is 1000, a number, with locale-specific thousands separator\\n      and decimal mark, and the $ character is marked up with the\\n      machine-readable code \"USD\" -->\\n  <span class=\"usd\" itemprop=\"priceCurrency\" content=\"USD\">$</span>\\n  <span class=\"usd\" itemprop=\"price\" content=\"1000.00\">1,000.00</span>\\n  <span class=\"gbp\" itemprop=\"priceCurrency\" content=\"GBP\">&pound;</span>\\n  <span class=\"gbp\" itemprop=\"price\" content=\"750.00\">750.00</span>\\n  <span class=\"aud\" itemprop=\"priceCurrency\" content=\"AUD\">$</span>\\n  <span class=\"aud\" itemprop=\"price\" content=\"1500.00\">1,500.00</span>\\n</div>\\n\\nYou set the class on the containing div to the currency you\\'ve\\n  selected for the user and then hide the other options through CSS:\\n.curr-gbp .usd, .curr-gbp .aud { display: none; }\\n.curr-usd .gbp, .curr-usd .aud { display: none; }\\n.curr-aud .usd, .curr-aud .gbp { display: none; }\\n\\nGoogle should then recognise the mark-up and display it as\\n  appropriate in its listings.',\n",
       " 'IIS supports header expires, simply add them to your web.config file.\\nExamples:\\n\\nSOURCE\\n<configuration>\\n   <system.webServer>\\n      <staticContent>\\n         <clientCache cacheControlMode=\"UseExpires\"\\n            httpExpires=\"Tue, 19 Jan 2038 03:14:07 GMT\" />\\n      </staticContent>\\n   </system.webServer>\\n</configuration>\\n\\n\\xa0\\n\\nSOURCE\\nFrom .NET Daily, I successfully applied this to a PHP site on IIS. It\\n  sets the max age to 30 days from now, rather than having to specify an\\n  explicit date.\\nAdd this to your web.config file:\\n<system.webServer>\\n  <staticContent>\\n    <clientCache cacheControlMaxAge=\"30.00:00:00\" cacheControlMode=\"UseMaxAge\"/>\\n  </staticContent>\\n</system.webServer>\\n\\nThis configuration satisfies both PageSpeed \"Leverage browser caching\"\\n  and YSlow \"Add Expires headers\". YSlow requires a value greater than 7\\n  days. PageSpeed requires between 30 days and 1 year.',\n",
       " \"You can't redirect your old sites without losing 90% of their value and risking a penalty on your new site.   \\nThere usually isn't a huge cost to leaving old sites up and running.  You could use them to advertise your new site.   Put a banner about your new site on every page of your old sites.\",\n",
       " \"This sounds to me like a Google Penalty issue.\\nA sharp decline such as what you have shown that has lasted for 4 months the way your graph shows, in combination with your assertion that greater than 95% of your traffic comes from organic searches backs up that statement.\\nThere are two main penalties that you can get. The first is a manual action from the Google Spam team, and the only way to check this is to go to Google Webmaster Tools and see if you have any notification. An example of thew notification you may see could be...\\n\\nThe other option is an algorithmic penalty which is harder to diagnose. By using a site such as https://moz.com/google-algorithm-change you can see when algorithmic changes have been applied and how the relate to your drop in traffic. Based on the above report I can not see any algorithmic change that may have affected you but that is not to say that your recent change didn't trigger an algorithm alarm.\\nBacklinks can also cause substantial issues including backlimks from...\\n\\nSites that are penalized or banned from Google\\nWebsites with duplicate content\\nWebsites unrelated to your niche\\nSpammy comments and forum profiles\\nSites with thin content\\nSite wide back links\\n\\nOver 95% of Google Penalties are related to a websites backlinks.\\nFrom what you have said it has started to pick up again and so it could be a temporary ranking issue that has resolved it.\\nThere is the possibility that due to the sites inactivity for such a long time the ranking reduced automatically and that it coincidentally reduced so substantially the day after you posted a new article, but I do not believe that is what happened as the rank increase from fresh content would not take months to apply as has been the case here, this does seem more to do with a penalty.\",\n",
       " 'I believe I have a solution that will work. As this is a Google Analytics specific problem you can use UTM tagging to adjust the source so that it will not show up as referral, but as direct traffic instead. Simply append the linking URLs with:\\n ?utm_source=direct\\n\\nPlease note that the referral data will be removed from showing in GA, but the data will still be passed to the website. I have tested this and it appears to be a working solution.',\n",
       " 'Go with your first sitemap, since your second example does not tell anything to google that, this is another alternative url on different language. People use second sitemap example, when their sitemap is big and need to divide into sub parts. \\nFirst is right, second is wrong for multilangual site.',\n",
       " 'You can return a \"403 Forbidden\" if the cookie is not set with something like the following:\\nRewriteEngine On\\nRewriteCond %{HTTP_COOKIE} !\\\\bMOBILEUSER=TRUE\\\\b\\nRewriteRule ^ - [F]\\n\\nThe ! before the CondPattern simply negates the pattern. \\\\b is word boundary, so it will only match that exact name/value pair, anywhere in the cookie.\\nA single ^ (start of string anchor) for the RewriteRule pattern matches everything. A single - (hyphen) for the substitution doesn\\'t do anything, the URL is not rewritten (but the substitution is ignored anyway when using the F flag). And the F flag results in a 403 being served (this also implies an L flag - so processing stops).\\nObviously, unless you are using SSL then the cookie can be sniffed.',\n",
       " 'Yes, it is worth it. although you can use meta tags for location and language, but it seems Google treats ccTLDs (.pl, .us) differently and they get better rank in local searches than international searches.\\nI think it is worth it to have a gTLD (.xyz) for your English, or international versions.\\nYou can also provide all languages on your .xyz, then redirect the .pl domain to domain.xyz/pl/ or pl.domain.xyz.',\n",
       " \"Unless you're talking about thousands of spammy links, it definitely won't hurt your site rank.  \\nIt won't do any magic, but it can help your site to get recognized faster by crawlers.\\nMy suggestion is to think about the bounce rate:\\nIf someone actually clicked on one of these links, say from reddit, will he feel tricked and close the page right away? (high bounce rate) or will he stay and enjoy your website? (low bounce rate)\\nAs long as you won't trick people, no harm will happen.\\nIf you manage to get people to click and stay, you'll score some points.\\nAlso, even though the websites you mentioned are not directly related to your site, I bet you can find pages in them that are related, such as a dailymotion video about soil.\\nGood luck with your new website!\",\n",
       " 'A lot of it is vanity related. \\nThe benefit of using folder names instead of file names is that you can ditch file extensions on the URL, so if you put an index.html file inside of the example.com/learning/how-to-fish/directory, that index file is displayed. This gets some more intuitive site folder architecture for you and your visitors. So instead of:\\nexample.com/learning-how-to-fish.html\\nexample.com/learning-how-to-drive.html\\nexample.com/learning-excel.html\\n\\nyou could get:\\nexample.com/learning/how-to-fish/\\nexample.com/learning/how-to-drive/\\nexample.com/learning/excel/\\n\\nFolders vs. subdomains is basically up to your overall design/architecture strategy. Lots of folks use subdomains for sites/microsites/landing pages that different greatly in content and/or design to keep things organized.\\nSo ultimately it is definitely a good idea to use folders over just files, but subdomain use is up to you and your content creators.',\n",
       " 'It would seem this is just one of Google\\'s featured snippets. (from the \"About this result\" link in the bottom right hand corner of that \"snippet\".)\\n\\nWhere does the answer summary come from?\\nThe summary is a snippet extracted programmatically from a webpage. What\\'s different with a featured snippet is that it is enhanced to draw user attention on the results page. When we recognize that a query asks a question, we programmatically detect pages that answer the user\\'s question, and display a snippet as a featured snippet in the search results.\\n:\\nHow can I mark my page as a featured snippet?\\nYou can\\'t. Google programmatically determines that a page contains a likely answer to the user\\'s question, and displays the result as a featured snippet.\\n\\nAs to \"what are the parameters on which google is showing\" this - well, it\\'s all part of Google\\'s AI (black box) - so only Google really knows the answer to that.\\nYou can provide \"Feedback\" if this snippet was helpful or not.',\n",
       " 'In these days of jQuery (and other JavaScript frameworks) heavy websites then I can\\'t see how it can be a problem, since it\\'s used extensively when you use things like jQuery sliders, transitions, galleries, tickers etc. These are now commonplace and search-engines are clever enough not to blindly penalise their use.\\nA user states this in the Google Webmaster Central forum:\\n\\n\"Merely using display:none will not\\n  automatically trigger a penalty. The\\n  key is whether or not there is a\\n  mechanism - either automatic or one\\n  that is invoked by the user - to make\\n  the content visible.Google is becoming\\n  very adept at processing JavaScript to\\n  find and interpret such mechanisms.If\\n  you use valid HTML, CSS, and\\n  JavaScript you have nothing to worry\\n  about.  Good luck!\"',\n",
       " 'As you suspect, you need to internally rewrite the request, not externally redirect (301/302). No \"proxies\" are required. You are almost there with what you have, except that when you specify an absolute URL in the RewriteRule substitution Apache will implicitly trigger an external redirect (despite the docs suggesting that if the domain matches the current host it should be stripped - it doesn\\'t do this in .htaccess in my experience).\\nTry something like the following:\\nRewriteCond %{HTTP_HOST} ^example\\\\.com$ [NC]\\nRewriteRule ^foo-bar$ index.html [L]\\n\\nUnless you are serving mutliple domains then you probably don\\'t need the RewriteCond directive? (The NC flag here is simply a security measure to catch malformed requests.)\\nI appended a $ (end of string) anchor on the RewriteRule pattern to make it an exact match for \"foo-bar\" and not a URL that starts \"foo-bar\". The substitution should reference your DirectoryIndex document (index.html, index.php, or whatever document is loaded by default in the document root). I\\'ve removed the NC flag, unless you specifically need a case-insensitive match?\\n\\nUPDATE: There actually are some Wordpress ...\\n\\nAh, WordPress is the \"problem\" here. In fact, WP already rewrites all requests through index.php in the document root, so these new directives (to \"rewrite specific path to root domain but keep full path\") are entirely redundant.\\nWP then uses the URL (the visible path in the address bar) ie. \"/foo-bar\" in this case, to route the request. Unless \"/foo-bar\" is defined as a valid URL in WP itself then WP is going to generate a 404 Not Found.\\nThis is something you need to configure in WP itself, not .htaccess.',\n",
       " 'scaling larger file dimensions down can cause performance issues\\n\\nThe \"performance issue\" is because the larger image is usually a much larger file size and downloading the much larger file over HTTP is the significant bottleneck.\\nBut you\\'re saying the larger image is the smaller file size?! How does that work? If that really is the case, and this larger (but smaller file size) image is compatible with all the browsers you are concerned about then you only need the single larger image (smaller file size). Browsers/computers are speedy beasts these days; scaling a larger image down to thumbnail size is not an issue and is infinitely quicker than downloading another image over HTTP.\\nIn the past, browsers did not do a very good job of scaling images - the quality was not very good (simple pixel resize for the sake of performance). However, all remotely modern browsers use better algorithms and the quality is good.',\n",
       " \"It would be economically taxing to create all of the content ourselves\\n  so we would like an automatic way of quickly dealing with this\\n  duplication issue\\n\\ncorrect me if I am wrong but you want to automate the writing process? If my assumption is true don't ever do automated content generation. It will screw you up bigtime. \\nYou will be producing more duplicate and very low quality content. And assuming I am a potential customer browsing your site and reading the item specs and description. I would leave and won't purchase any items since I can't basically understand what I am reading on your site.\\nIn addition to @Wingit86\\ncategorize your items and review them properly. Since their might be a tendency just like wingit's example that I am searching for a Samsung monitor and a coffee cup would show then that's very irrelevant to both search engine and to the customer.\",\n",
       " 'My advice would be to use the person mark-up on the website that is about the person and the organization mark-up on the store.\\nThis way, the relevant schema will be detected for the SERPs.',\n",
       " 'You should add Cache-control headers to the 301 responses.  Consider setting a short (1 day) expiry.  Start tracking your 301 responses by site and URL to determine when the 301 response rates have stabilized.  At that point you should be able to consider switching URLs.  Remove the cache time, and wait that period.  (301 rates should increase.)\\nThis can be done as follows\\nRewriteCond %{HTTP_HOST} ^www.example.co.uk [NC]\\nRewriteRule ^(.*)$ http://www.example.com/$1 [R=301,NC,L,E=nocache:1]\\n\\n## Set the response header if the \"nocache\" environment variable is set in the RewriteRule.\\nHeader always set Cache-Control \"no-store, no-cache, must-revalidate\" env=nocache\\n\\n## Set Expiry\\nHeader always set Expires \"Thu, 01 Jan 1970 00:00:00 GMT\" env=nocache\\n\\nYour company has painted itself into a corner by stating they will never use example.com as the domain with the 301 responses.  If they haven\\'t painted themselves into the corner with www.example.com, I would use that as a stepping stone.  Browser caches can be extremely long lived, especially if clients have example.com bookmarked.\\nRedirecting to a different (never used) path should also work.  This may impact your SEO rankings.',\n",
       " \"Here is what you need to know.\\nAssuming for a moment that both sites are equal...\\nThe Google Index contains, at minimum, the domain name, the full URL, any link with a from and to relation (think relational database) to the URL, the HTML source, and analysis of the content. While there is quite a bit of conjecture over what analysis exists over the content, it is at least known that Google breaks down the HTML into the HTML DOM elements and each one is assigned an ID that identifies the order of the DOM elements, any parent-child relationship, any dependency of one element upon another, and the type of element. With that, content blocks are determined. For example, the entire content following the h1 tag is one block, any header tag is a block, the content following a header tag is a block, each paragraph is a block, etc. Each term within a block is given a position in term count from the beginning  of each block and associated with a term in the term index.\\nThis is important for two reasons. One, the content can be understood without the original HTML. And two, each part of the content can be analyzed in a variety of ways including linguistic semantics, topical semantics, and many other forms of analysis.\\nWhy did I tell you this?\\nTwo reasons. One, because this is the basic form of analysis that Google and other search engines use. And two, domain names (URL), link text, URI (path and file name separately) are also analyzed in this way.\\nEach content block and DOM element can be analyzed using semantics, primarily for topic and PoS (parts of speech - not the other one) relations in understanding the content itself, and in raw term match. The primary matching mechanism between the search query and the content is topic and term matches. Between the two are ontologies that allow many forms of analysis to know that a car is an automobile, that Rockefeller Center consists of 19 addresses, that Babe Ruth is a person or a candy bar.\\nKeep in mind that Google does not do direct term matches, even if that is what it appears to do.\\nAs each term within the content is analyzed, matrices are created to score the term according to several analysis methods including semantics. Add to that score is weight for the element as it is seen in importance. This is how a term in one thing can outweigh the same term in another thing, for example the title tag versus the h1 tag. This list is the approximate order of importance that Google assigns weight to.\\n\\nThe domain name.\\nThe URI path.\\nThe title tag.\\nThe link text.\\nThe description meta-link.\\nThe content itself.\\nThe header tags.\\nThe URI file name.\\n\\nAll things being equal, using a domain name of paper airplanes and a URI path of paper airplanes, the domain name will score higher. This is because Google believes that semantic clues in the domain name are more important/significant than in the URI path. By the same token, a URI path of paper airplanes will out perform a title tag of paper airplanes.\\nYour question is far more complicated than that however.\\nWhat is not considered are other important elements such as the title tag, the description meta-tag, and links to the page, etc., all of which are very significant in adding weight to the semantic analysis.\\nHow one will perform against another cannot simply be domain name versus URI because there are so many factors that can easily out weigh the two, however, if you were to break it down to the simplest understanding the domain name will out perform the URI path every time. That is, until Google changes it's mind.\",\n",
       " 'I deal with this type of situation using my virtual host configuration.  Under Apache, the first virtual host is the \"default\" virtual host.  I configure it to serve a 404 error with the message\\n\\n404 Not Found -- Hostname Not Recognized\\nThis server is not configured to serve documents for foo.example.com\\n\\nThen I create specific virtual hosts for each of my sites that serve the correct content when the host name is correct.\\nHere is my default virtual host configuration that uses 404.pl to handle all requests:\\n<VirtualHost *:80>\\n    Servername localhost.localdomain\\n    DocumentRoot /var/www/default\\n    <Directory /var/www/default/>\\n        Require all granted\\n        Options +ExecCGI\\n        AddHandler cgi-script .pl\\n        RewriteEngine on\\n        RewriteCond $1 !-f\\n        RewriteRule ^(.*)$ 404.pl\\n        AllowOverride None\\n    </Directory> \\n</VirtualHost>\\n\\nAnd here is the 404.pl script that prints out the \"hostname not recognized\" message as well as does redirects for domain names that are almost correct but not canonical:\\n#!/usr/bin/perl\\n\\nuse strict;\\n\\n# Put the host names you actually use in here to enable redirects\\n# The left side should be the \"main\" domain name and the right should include the TLD\\n# This enables redirects for alternate TLDs.\\nmy $hostnameredirects = {\\n  \\'example\\' => \\'example.com\\',\\n  \\'foo\\' => \\'foo.example.com\\',\\n};\\nmy $hostname = `hostname --fqdn`;\\nchomp $hostname;\\n\\nmy $server = $ENV{\\'SERVER_NAME\\'};\\n$server = \"\" if (!$server);\\n$server =~ s/[^\\\\-\\\\_\\\\.A-Za-z0-9]//g;\\n$server = lc($server);\\nmy $uri = $ENV{\\'REQUEST_URI\\'};\\n$uri = \"\" if (!$uri);\\n$uri =~ s/[ \\\\r\\\\n]+//g;\\n$uri = \"/$uri\" if ($uri !~ /^\\\\//);\\n\\n&serverNameRedirect();\\n&noVirtualHostError();\\n&show404();\\n\\nsub serverNameRedirect(){\\n    my $domain = &removeTld($server);\\n    while ($domain){\\n        if ($hostnameredirects->{$domain}){\\n            &redirect(\\'http://\\'.$hostnameredirects->{$domain}.$uri);\\n        }\\n        $domain =~ s/^[^\\\\.]*[\\\\.]?//g;\\n    }\\n}\\n\\nsub removeTld(){\\n    my ($domain) = @_;\\n    $domain =~ s/\\\\.(([^\\\\.]+)|((([A-Za-z]{2})|com|org|net)\\\\.[A-Za-z]{2}))$//g;\\n    return $domain;\\n}\\n\\nsub redirect(){\\n    my ($redirect) = @_;\\n    my $eRedirect = &escapeHTML($redirect);\\n    print \"Status: 301 Moved Permanently\\\\n\";\\n    print \"Location: $redirect\\\\n\";\\n    print \"Content-type: text/html\\\\n\";\\n    print \"\\\\n\";\\n    print \"<html><body><p>Moved permanently: <a href=\\\\\"$eRedirect\\\\\">$eRedirect</a></p></body></html>\\\\n\";\\n    exit;\\n}\\n\\nsub show404(){\\n    my $eServer = &escapeHTML($server);\\n    &errorPage(\\n        \\'404 Not Found\\',\\n        \\'404 Not Found -- Hostname Not Recognized\\',\\n        \"This server is not configured to serve documents for \\'$eServer\\'\"\\n    );\\n}\\n\\nsub noVirtualHostError(){\\n    if ($server !~ /^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+$/){\\n        return;\\n    }\\n    &errorPage(\\n        \\'400 Bad request\\',\\n        \\'400 Bad Request -- No Hostname Sent\\',\\n        \"This server only accepts requests with a domain name, not requests for an ip address such as $server\"\\n    );\\n}\\n\\nsub errorPage(){\\n    my ($status, $title, $message) = @_;\\n    print STDERR \"$title\\\\n\";\\n    print STDERR \"$message\\\\n\";\\n    print \"Status: $status\\\\n\";\\n    print \"Content-type: text/html\\\\n\";\\n    print \"\\\\n\";\\n    print \"<html>\\\\n\";\\n    print \"<head>\\\\n\";\\n    print \"<title>$title</title>\\\\n\";\\n    print \"</head>\\\\n\";\\n    print \"<body>\\\\n\";\\n    print \"<h1>$title</h1>\\\\n\";\\n    print \"ERROR: $message\\\\n\";\\n    print \"</body>\\\\n\";\\n    print \"</html>\\\\n\";\\n    exit;\\n}\\n\\n# Convert <, >, & and \" to their HTML equivalents.\\nsub escapeHTML {\\n     my $value = $_[0];\\n     $value =~ s/\\\\&/\\\\&amp;/g;\\n     $value =~ s/</\\\\&lt;/g;\\n     $value =~ s/>/\\\\&gt;/g;\\n     $value =~ s/\"/\\\\&quot;/g;\\n     return $value;\\n}',\n",
       " \"HTML META refresh is not a 301 redirect\\nWhile the outcome is the same they are technically very different because of the header status return. A meta refresh will return 200 OK header response, while a proper redirect will return the 301 Moved Permanently header status.\\nHTML Meta Refresh is Supported\\nAs far as I know Bing and Yahoo treat meta refresh similar to that of a 301 redirect. Google's @JohnMueller(G+, Pro Webmasters) recently posted a search-engine guide to 301, 302, 307, & other redirects. He mentions that JavaScript meta refresh's are discouraged, and within the guide doesn't talk about HTML based meta refesh until a user mentions it in a comment.\\n\\nDisa Johnson Apr 7, 2016+\\nInktomi treated meta refresh content=0 as\\n  301 in the old days. Too bad G & John say 'not recommended' though\\n  understandable to an extent I guess. Still, I would rather have the\\n  option in case it's needed on occasion.\\nJohn Mueller Apr 8, 2016+\\n+Disa Johnson We support meta refresh too, I don't see that going away. The W3C has been discouraging them since 2000 (\\n  https://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh ), and we\\n  regularly see sites mess up with them (eg, auto-refresh after some\\n  time to increase page-views), so if there's a choice, I'd prefer sites\\n  use one of the other supported types.\\n\\nHowever... this isn't to say that Google rewards the same, this will likely never be confirmed or denied.\",\n",
       " \"Many people appear to have this problem on Godaddy hosting and Godaddy have told one complainant this \\n\\nIt is possible that when the error is occurring that it is triggering\\n  the Mod_sec of the hosting account. In the current hosting account\\n  type that you have it isn't possible for us to disable that function\\n  for security reasons. We do have hosting accounts with cPanel that\\n  allow us to bypass the Mod_Sec rules, which could resolve the issue.\\n\\nhttp://vanillaforums.org/discussion/27446/server-fails-every-once-in-a-while\",\n",
       " \"According to: http://instagram.com/developer/endpoints/media/\\n\\nAt this time, uploading via the API is not possible. We made a\\n  conscious choice not to add this for the following reasons:\\n\\nInstagram is about your life on the go – we hope to encourage photos from within the app. However, in the future we may give whitelist access to individual apps on a case by case basis.\\nWe want to fight spam & low quality photos. Once we allow uploading from other sources, it's harder to control what comes into the Instagram ecosystem. All this being said, we're working on ways to ensure users have a consistent and high-quality experience on our platform.\\n\\nThis means that if you do this you are probably going to be breaking the service agreement and you'll need to scrape and impersonate forms if it's possible at all.\",\n",
       " \"The Webmasters FAQ outlines the nature of acceptable questions for this site: researching available solutions and providing anecdotal advice falls outside the purpose of this StackExchange site, so this type of question would be better-suited to chat or another discussion forum.\\nThat being said, there are a variety of free and open source content management systems, as well as licensed software and paid services, which may meet your requirements.\\nSome of the most popular content management systems include:\\n\\nWordPress (PHP, free and open source) – a blogging platform which has expanded to include many other features – See the wordpress stackexchange if you have any questions.\\nJoomla (PHP, free and open source)\\nDrupal (PHP, free and open source) – See the drupal stackexchange if you have any questions.\\nDotNetNuke (ASP, proprietary or free and open source option)\\nMediaWiki (PHP, free and open source) – the software that powers Wikipedia\\nUmbraco (MVC.NET, open source)\\nDokuWiki (PHP, free and open source) – a simple but powerful wiki software\\nTypo3 (PHP, Open Source (GPL)) - powerful for medium and large projects\\n\\nAll of the content management systems listed above are built around the concept of plug-ins which extend the application's functionality—there are many plug-ins, with new plug-ins authored every day.\\nSites like The CMS Matrix and WikiMatrix can help you compare CMS's (but is best suited for looking for very particular combinations of requirements).\\nIf you have researched content management systems and available plug-ins but cannot find anything that will meet your needs, you should consult a developer who can build something to your specifications either as a plug-in for an existing content management system or as a stand-alone application.\",\n",
       " 'Google doesn’t seem to document if it’s sufficient to provide this one the homepage, or if every page should have it.\\nNote that they do state this for the Sitelinks Searchbox (\"Add the markup only to the homepage of your site. It is not necessary to repeat the markup in other pages of your site.\"), so either they forgot to mention this for the Site Name, too, or they intentionally didn’t state it, which would imply that it should be on every page.\\nIt shouldn’t matter if you provide it in the head or in the body, no matter if you use JSON-LD (details) or Microdata/RDFa. Bugs aside, Google should parse it in the same way.\\nThe following is only from the perspective of Schema.org. I don’t know the state of Google’s support here.\\nRepeating the information on every page\\nFor the homepage:\\n<head itemscope itemtype=\"http://schema.org/WebSite\">\\n  <title itemprop=\"name\">Example Website</title>\\n  <link itemprop=\"url\" rel=\"canonical\" href=\"https://example.com/\" />\\n</head>\\n\\nFor other pages:\\n<head itemscope itemtype=\"http://schema.org/WebSite\">\\n  <title>Example Page · Example Website</title>\\n  <meta itemprop=\"name\" content=\"Example Website\" />\\n  <link itemprop=\"url\" rel=\"canonical\" href=\"https://example.com/\" />\\n</head>\\n\\n(Side note: see my answer about using rel together with itemprop on link)\\nBonus: Relationship to WebPage\\nIf you use WebPage for every page, you can relate the WebSite to it. For example, if you specify WebPage on html, with the isPartOf property:\\n<html itemscope itemtype=\"http://schema.org/WebPage\">\\n  <head itemprop=\"isPartOf\" itemscope itemtype=\"http://schema.org/WebSite\">\\n    <!-- the same properties as in the example above -->\\n  </head>\\n  <body>\\n    <!-- and use \"mainEntity\" here to denote the primary entity this WebPage is about -->\\n  </body>\\n</html>\\n\\nReferencing instead of repeating\\nBased on this, you could even reference the WebSite data without repeating it on every page. \\nFor that to work, you have to give the WebSite entity on the homepage a URI (in case of Microdata, with the itemid attribute):\\n<!-- on the homepage -->\\n<html itemscope itemtype=\"http://schema.org/WebPage\">\\n  <head itemprop=\"isPartOf\" itemscope itemtype=\"http://schema.org/WebSite\" itemid=\"/\">\\n\\nOn the other pages, you then reference this URI:\\n<!-- on all pages except the homepage -->\\n<html itemscope itemtype=\"http://schema.org/WebPage\">\\n  <head>\\n    <link itemprop=\"isPartOf\" href=\"/\" />\\n\\n(But in case Google expects the WebSite markup on every page, I wouldn’t get your hopes up that they support this URI referencing.)',\n",
       " \"This is not legal advice, but I know someone who runs several adult sites. He puts up the warning page with the 18 U.S.C. Section 2257 Compliance Notice and that's it.\\nI'm not sure there is a way to verify a users age when they are underage.\\nObviously this is not enough, the parents may have to employ tools to prevent access. \\nThe site owner has no other option from my experience.\",\n",
       " 'Yes, it\\'s a caching reverse proxy.\\n\"It depends\". By default only things like images and CSS are cached, but you can set the cache level per zone and set Page Rules to override what is cached or should not be cached. Static HTML can be forced to be cached by a page rule, but dynamic content obviously cannot. The exception here is if you have the Business or Enterprise plan, or sign up through certain hosting partners, where you gain access to the Railgun technology whereby CloudFlare will only fetch the information that has changed on a page, even if it is dynamic. If you\\'ve set CloudFlare to cache everything, you need to control cache timeout either with cache headers on the origin and/or setting the cache TTL on your zone in the CloudFlare UI.\\nPOSTs must always be sent to the origin, so CloudFlare just passes them straight through. You should still get an increase in performance because CloudFlare uses anycast to serve DNS and HTTP from the closest data centre.\\n\\nFull disclosure: I work for CloudFlare.',\n",
       " 'It’s perfectly fine (and useful) to provide structured data on the homepage. \\nIt’s a good place to provide a WebSite item (if you want/need one), to state what the site is about (the site’s main entity, e.g., an organization or a person), and to link to \"deeper\" entities (offering an entry place for structured data crawlers).\\nOne reason why some sites provide structured data only on other pages could be that Google didn’t support Rich Snippets on homepages. But Rich Snippets (now called rich results) are not the only search feature, based on structured data, that Google offers. For some of their features, you even have to provide the markup on the homepage, for example for the Sitelinks Searchbox:\\n\\nAdd the markup only to the homepage of your site.',\n",
       " 'From the redirect, it looks like this could be happening in the VirtualHost container for port 443.\\nFailing that, the redirect could be happening in the website code itself. (Which appears to be the case here - a WordPress plugin).',\n",
       " \"The 3 log records shown all look like legitimate traffic (both the Google and Bing IP addresses appear valid) and as closetnoc has already pointed out, only the last one references the Bingbot.\\n\\nThe pages they are trying to reach do not exist\\n\\nBut your server is returning a 200 OK status, which is potentially allowing these URLs to be indexed by the search engines. If these URLs returned a 404 Not Found then it wouldn't be such a problem.\\nIt looks like your site has been the target of a XSS-like attack to create spammy links in the SERPs for keywords that are irrelevant to your site.\\n\\nIs there something I can do to prevent any /index.php/XXXXXX requests\\n\\nYes. The additional XXXXXX in the URL after a valid filename is trailing pathname information (PATH_INFO). The default behaviour on Apache generally allows this additional path info (although it depends on the handler).\\nHowever, this can be disabled with the AcceptPathInfo directive in your server config or .htaccess file. For example:\\nAcceptPathInfo Off\\n\\nThis will result in Apache returning a 404 NOT FOUND error on such requests.\\nApache docs...\\nhttps://httpd.apache.org/docs/2.4/mod/core.html#acceptpathinfo\\n\\nDepending on your website URL structure, you could just block any direct requests to index.php. Something like the following, using mod_rewrite in the root .htaccess file:\\nRewriteEngine On\\nRewriteCond %{THE_REQUEST} ^GET\\\\ /index\\\\.php [NC]\\nRewriteRule ^index\\\\.php - [F]\\n\\nThis would need to go before any URL routing directives (eg. WordPress).\\nTHE_REQUEST contains the initial request header only, so you are still OK to internally rewrite to index.php if you are using a front controller (for example).\",\n",
       " 'If you want Google’s Article rich result, you have to provide an ImageObject value (with url/height/width) for the logo and image properties. Currently you provide a URL value.\\nIf you don’t want this feature (or you can’t get it because you can’t provide other required properties), you can keep your structured data like that.',\n",
       " \"The answer is more or less in what you've quoted from Google:\\n\\nA known site is the official website for an entity as shown in Knowledge Graph cards\\n\\nObviously, a prerequisite is Google's recognition of the entity in the first place. For most reasonably notable things that happens automatically, but it can be indirectly influenced by influencing Knowledge Graph sources, particularly Wikipedia, Wikidata (which superseded Freebase) and, for businesses, Google My Business.\\nOnce a Knowledge Graph card exists for an organisational entity, Google provides means to influence it via structured data (linked from the page you quote).\",\n",
       " 'For existing sites, switching from HTTP to HTTPS is in effect, starting over. All the ranking factors and metrics have to be rebuilt from scratch including links, trust metrics, etc. This is why all the negative hype.\\nSwitching from HTTP to HTTPS is disruptive and why people complain. It did not occur to them that there would be an additional cost to switching from HTTP to HTTPS. Conceptually, anytime there can be different content served, it is a new site. However, going beyond that, each domain name, no matter how it appears, for example, www.example.com versus example.com are stored in the index as separate sites. This includes HTTP versus HTTPS. There is a good reason for this despite the tradition where these sites wold normally be the same. Each of these sites within the index has to build trust and domain metrics all on their own. The largest factors would be trust and links.\\nIf you do not have a HTTP site and start with HTTPS, then there is no downside. You are not starting over but rather building metrics for a single site. For the record, Google may be a 900 pound gorilla, however, it does not make the rules. Each time it tries, it gets shutdown. Yes. Google wants the world to be HTTPS, however, it is fully your right to use HTTP and they will just have to deal with it. HTTP does make sense most of the time.\\nHere is some of the backlash:\\nhttp://searchengineland.com/seo-industry-tweets-reactions-googles-ssl-ranking-boost-199510\\nHere is where HTTPS helps.\\nOther than the obvious security argument which is a powerful one for both the user and web site owner, HTTPS is a trust metric that positively effects a sites Trust score. Today, there are many factors that can outweigh the effect of HTTPS, however, Google is making noises that HTTPS will have more than a the slight score increase it is today. And that makes sense.\\nConsidering the quality of the certificate, some companies do vet who they are giving certificates to, the value of the certificate can ensure to any search engine that the site ownership is actually verifiable. Google does use registration details in evaluating site trust, however, having an additional layer of vetting helps with trusting a site.\\nAs far as earnings with Adsense HTTP versus HTTPS, there is no difference. Any complaint is a result of the disruption in switching from HTTP to HTTPS.',\n",
       " \"This is because your Google search includes the domain name, or keywords that do not exist currently in your page title. Google only rewrites titles when the search doesn't have a 'search match', in this case you do not have JW Building Services in your META Title, therefore Google is rewriting your title, and obviously, its deciding that all lower case is best.\\nYou can't customise the search results in this manor, however you can adjust the real issue... Poor and over optimised SEO.\\nYour pages titles, and descriptions should be written in such as way that they are helpful to both search engines, and visitors. Currently they over optimised, which is not only not helpful to visitors, search engines dislike it too.\\nYour current home page uses: \\n\\nTitle: Electrical Services : Commercial & Domestic | Electrical Testing\\nDescription: 24 Hour Emergency Call-outs, Commercial & Domestic, Full House Rewires, Commercial Fit-outs, Electrical Testing and Fault Finding, SWA Cable Jointing, PAT, Inspection and Testing\\n\\nThe above is spammy, unreadable and Google is likely to rank you lower for over optimisation. I recommend that you have your SEO guy, or company read SEO guides dated in the last 2 years, researching Google Local search results is very important, since they differ from organics, which your site seems to disregard.\\nA particular setup would look something like this:\\nLocal Page:\\n\\nHome Page Title: Your Business Name\\nDescription: A short readable sentence mentioning your key services, and the areas you provide them too.\\n\\nOrganic pages:\\n\\nServices Page: Service Name in Area Name \\nDescription: A short readable sentence mentioning your the service name, followed by top features, and area name.\",\n",
       " \"No, you won't be able to find that info. GA doesn't make it available in your reports due to privacy issues, and any collection of it is in violation of Google Analytics' Terms of Service. Please read Google's Analytics usage guidelines for clarification.\\n[Update July 26, 2016]\\nIP collection through GA is a grey area in that there is no right or wrong. It will, however, require due diligence in the investigation with your privacy lawyers/experts or into the jurisdictions of your locale (eg. European privacy laws). Alternatively, you could always make it non-argument by setting the last octet of your addresses to 0. This would sufficiently make it anonymous.\",\n",
       " \"Well, I wasn't able to get things working following this tutorial, and many tests and variations.\\nFinally, I found a very interesting link in the uWSGI site, which actually made things work!\\nThe only issue left, is enabling https without self-signed certificates. I'll post another question, as it seems to be a separate matter.\",\n",
       " \"There is nothing to fix here.  You don't want Googlebot crawling and indexing those URLs.   Google is reporting that it tried crawling them, but got an error.\\nFix errors that happen on pages that have content you want indexed.  If Googlebot got 405 errors on normal pages, that would be a problem.\\nThese crawl errors are expected.  You have set it up in such a way that any visitor or bot that fetches the URL with a GET will see an appropriate error.  That is all that has happened:  Google got an appropriate error and listed in the appropriate report.\\nCrawl errors on non-content URLs don't hurt your site in any way.  Google expects to encounter 404 errors when crawling healthy sites.  They have even said that a million non-content 404 errors in that report won't adversely effect your site.  The same goes for other errors.  If it is on a page that doesn't have indexable content, there is no need to worry about Googlebot encountering it.\",\n",
       " \"Parse error: syntax error, unexpected '<' in /membri/thekrumbs/wp-content/themes/comicpress/functions.php on line 40\\n\\n<?php\\nif ( has_post_thumbnail()):\\n    the_post_thumbnail( 'featured-image', array( 'class' => 'featured-image' ) );\\nendif;\\n?>\\n\\nYou have a PHP code block, ie. <?php...?>, nested inside a PHP code block. This code looks out of place here.\",\n",
       " \"GTIN 8, GTIN-12, GTIN-13, GTIN-14 and MPN are all common global and local identifiers which replace a wide range of previous popular identifiers.\\n\\nSOURCE\\n\\nGTIN-14 Former name(s): DUN-14, ITF\\nGTIN-13 Former name(s): EAN·UCC-13, EAN-13, CIP\\nGTIN-12 Former name(s): EAN·UCC-12, UCC-12, UPC\\nGTIN-8 Former name(s): EAN·UCC-8, EAN-8\\n\\nIt's worth mentioning that ISBN was superseded by EAN, and EAN was superseded by GTIN-13. On the 1st of January 2005 the U.S. ISBN agency requires publishers be able to communicate ISBNs as GTIN-13. \\nHowever, Schema allows the use of product ISBN for older books, you could use productID, or ISBN, both are correct. ProductID is an less specific product identifier, you could use GTIN, MPM or another identifier that is not supported by GTIN, such as 'Product ID BRAND'. \\nYou will find that with Schema there is often times that you will find markup that can be approached in different ways, however the outcome remains the same, and correct.\",\n",
       " \"Existing Http Sitemap:\\nThis sitemap should continue to exist for time all your search results are crawled as https and even if this also redirects to https://www.example.com/sitemap.xml, then also it should contain all http links\\nReason being : the crawler when try to crawl this sitemap will crawl all existing urls and get an indication that these all have been permanently moved to https. It will update its indexes for old urls and will pass authority of the old links to new https links. If you don't add these, then crawler will get this indication only when it try to check for old http urls and will then take more time to reflect and might hurt your existing rankings a bit. The above mentioned reason lower that effect\\nNew sitemap for https : \\nHttps becomes altogether a new property for the crawler. Its better to create a new link for this https sitemap like https://www.example.com/sitemap-new.xml or whatever you want to create. You can update this / submit this via webmaster.\\nPlace all your https links here. Crawler will start crawling links from here and getting an indication of old links migration from http sitemap.\\nThis ideally takes around 20-30 days based on the crawler visit frequency on your site, for almost all links to index as https. After that you can remove your old sitemap if you want.\",\n",
       " \"To begin...\\nGoogle+ has suffered some setbacks lately. Granted. For example, 90% of those with Google+ accounts are not active. And I can see why. The benefits of Google+ seems to be less effective as a social media venue overall. Some swear by Google+. Some swear by LinkedIn. Some swear by Facebook and others swear by Twitter. Each has it's place and each suits a purpose better than the rest. As for Google+, coercing usage was not a good idea. You have to wonder why Google would do this. As it turns out, there is a good answer for this despite the low usage, the removal of some features, some features becoming products of their own, high employee turnover including key personnel, lack of focus, etc.\\nGoogle does look to social signals, however, often, the reality does not match the hype. For example, much of Facebook is not available to search users without an account or with access to private profiles where a connection is required. I imagine that LinkedIn can be much of the same. As for Twitter, while it is more open, it suffers from short posts that are short on relevancy. Twitter now allows posts and that seems to be a clue. This means, as I search for something, not being a Facebook user, many of the Facebook results found in the SERPs end up being useless. In fact, even with a public profile, Facebook answers few questions. It is not where serious people go first to post serious information.\\nAnother aspect is that while social media has it's place, it seems to be best suited for trends. And this is exactly where Google might spike a sites web page presence. While Facebook posts may not rank well and links seem to have little value, mentions of a web page suddenly gone viral, even for a short period of time, will temporarily place a page much higher in the SERPs.\\nAll well and good. But how does Google+ benefit a site?\\nWhile some, including myself, will argue that Google+ posts has little effect, one thing is clear. Google+ is more open. Posts are indexed more readily, often do not require an account, and links from these posts are said to have an effect. That said, I rather suspect whether a Google+ profile is active and focused or not makes a difference. Recently, I was poking around some Google+ accounts for the most popular SEO sites. Some were focused on the topic of the website, and others meandered around much like a personal profile. Same with Twitter. I did not look at Facebook.\\nSo let's split things into several camps.\\nFor businesses, especially for local search, Google+ for Business seems to have a real effect. To begin, a card in the knowledge graph is very possible if some simple branding signals are created on the web site. It is possible these days that smaller businesses will see their business pop up in the knowledge graph. It is also well understood that for local search, having a Google+ for Business profile is rather helpful.\\nFor blogs, especially one that is prominent or where authority can be established, Google+ has the ability to vet the author. While the author tag is dead, what was not said at the time was that any other attempts to determine authorship died with it. I believe these efforts to be as fully alive as they ever were. Tying a Google+ profile to a site and author is a step in the right direction as far as vetting the author and knowing exactly who that person is.\\nFor personal sites, I am not sure there is enough benefit that can be brought except within narrow margins. I suppose it really depends upon the person, the site, and the effort.\\nKeep in mind that establishing authorship authority on the web is nearly impossible. In the early days, Google used, and likely still does, it's ability to parse simple data such as names, addresses, phone numbers, etc. into a semantic link web that could establish some authors fairly clearly. Patterns would arise that would make authorship and authority fairly clear for some. With that, mapping social media profiles added more valuable clues. However, there was still a lot of noise in the data. Noise reduction within the data set left most authors out of the loop. Afterall, is this Steve Jones the one in South Bend or Roanoke? For example, a true story. I was a consultant within the IT industry using my personal name as it was incorporated. This worked well until just about half a block away someone with my same name, in the same field, specializing in some of the same things began to consult using his name. It caused some confusion between our customers. I sought him out. Hell of a nice guy! We had to come to an agreement. My name is not so common but not as rare as hens teeth. Until recently, there was only one of us in a whole city. If it was confusing for us in a small market, how is it for Google with a global market who knows little about anyone?\\nAnother consideration to keep in mind. Google does nothing for the heck of it. Google does everything to collect data they hope will be valuable for search. For example, GMail is primarily a link discovery tool and was created with this in mind. With Google+, the idea was to establish trusted author profiles that can be vetted and trusted. Hence why Google coerced people to join even if they do not participate. Google has vetted profiles that could be useful. While LinkedIn, Facebook, and Twitter account profiles belong to others, vetting these profiles externally was and likely is a semantic link map nightmare. For Google+, it is direct. Google knows the author because Google holds the account and can vet the author as much as they want. And they do. Ever get a letter from Google or a phone call? Likely the answer is Yes(!) to both questions.\\nSo in that respect...\\nWhere Google+ still has an advantage is in Googles ability to trust data. There is a silent push for trusted data over content lately if you haven't noticed. In the knowledge graph, all semantic data links much be corroborated. For featured snippets and rich snippets, much of the same rules apply, though less strictly. You can see how Google's SERPs can be influenced by pushing data through old forms, parsing, or new forms, schema.org markup, and by using Google+ profiles. Did you get that? Google is relying more upon data that is passed in trusted ways these days to influence the SERPs. Exactly where and how this is done, would require a longer answer.\\nSo is Google+ worth the effort?\\nFor a business? Yes. Abso-freakin-lutely!\\nFor an authoritative blog? Depending upon the authority and popularity of the blog and whether Google+ is used actively and in a focused way. But yes. Very likely.\\nFor a personal site? Perhaps. But mostly socially I suspect - specifically if you work it. Any established authority may be less serious.\",\n",
       " \"I recently happened across this issue myself. There is a difference between using RewriteRule in the VirtualHost and as part of an .htaccess file. The following rule will match (with a URL like this: example.com/fruit/apple) if it appears in a .htaccess file: \\nRewriteRule ^(fruit|fruits)/apple http://newfruitwebsite.com$1 [R=301,L]\\n\\nbut will not match in the VirtualHost context. If we check out the apache docs we can see that:\\n\\nWhen using the rewrite engine in .htaccess files the per-directory prefix is automatically removed for the RewriteRule pattern matching and automatically added after any relative (not starting with a slash or protocol name) substitution encounters the end of a rule set.\\n\\n...but what does this mean? The above rule would not match the same URL if it was placed into the VirtualHost because it doesn't allow a leading slash (/) to be at the start of the string. The apache docs go on to say:\\n\\nThe removed prefix always ends with a slash, meaning the matching occurs against a string which never has a leading slash. Therefore, a Pattern with ^/ never matches in per-directory context.\\n\\n...so if we need a slash for .htaccess and no slash for VirtualHost, how do we modify the pattern? You guessed it. We need to add /? to add the following condition in our regex (where our question mark (?) is the quantifier):\\n\\nHas between 0 and 1 / at the beginning of the string.\\n\\nNow our URL (example.com/fruit/apple) string will match because we are handling the fact that if the rule is in the .htaccess file it will have 0 slashes and if it comes from the virtualHost it will have 1 slashes.\\n\\nRelated Reading: \\n\\nApache Rewrite Directive\",\n",
       " \"I have talked with GoDaddy team, and they told that they don't support 3rd party SSL for managed WordPress account. For installing SSL on your managed WordPress account you have to buy it from GoDaddy. I have moved my WordPress site to AWS. Then installed certificate on AWS server. Its working fine now.\",\n",
       " \"No, don't do that. Look's like you misunderstand about robots.txt and sitemap.\\nVideo sitemap and other sitemap tells about your new content to Googlebot, so they come to your websites and start crawling, but before they crawl, they always check your robots.txt first. So If you disallow them, then they can't crawl anything.\\nLet me talk more about some of Google crawlers like Googlebot image and video. When Googlebot come to your site, and find out you have embed images then next time, Google image bot crawl that kind of data. \\nSo, in general Googlebot crawl anything, but when they found specific content like images and videos, then next time they crawl with their specific crawler. It makes process fast for them.\",\n",
       " \"I don't believe its bad idea to have separate social profiles for business name and founder name. You are right on forums, communities and even social media, profiles with a person name helps to connect with other people easily. You have to make sure that anything related to your business will be shared on your business social profiles and then from individual profiles you can share or like it further.\",\n",
       " 'use something like this, for Apache, to set all urls with certain parameter to noindex:\\n<IfModule mod_rewrite.c>\\nRewriteCond %{QUERY_STRING} ^utm_source*)$\\nRewriteRule .* - [E=NOINDEX_HEADER:1] \\n</IfModule>\\n\\n<IfModule mod_headers.c>\\nHeader set X-Robots-Tag \"noindex, follow\" env=NOINDEX_HEADER\\n</IfModule>\\n\\nfor nginx:\\nlocation ~*  {\\nif ($args != utm_source) {\\nadd_header X-Robots-Tag noindex;\\n}\\n}\\n\\nif NOINDEX for URL with UTM parameters will affect the original URL of\\n  its indexability\\n\\nsurely no.',\n",
       " 'You can use both properties (headline and name) for the same content.\\n<h1 itemprop=\"headline name\">Title of the post</h1>\\n\\nGoogle requires headline for their Articles search feature (and it doesn’t seem to use name for anything else). If you don’t care about this feature, you could of course ignore the error.\\nMy related answers on Stack Overflow:\\n\\nto the question News item in Schema.org\\nto the question What is the name property in schema.org?',\n",
       " 'In html5 the nav element should be used to represent a section of a page that links to\\n\\nother pages or\\nto parts within the page\\n\\nSo depending what kind of links you are specifying in the footer section you might avoid inserting them within a nav element:\\n\\nNot all groups of links on a page need to be in a nav element — the\\n  element is primarily intended for sections that consist of major\\n  navigation blocks. In particular, it is common for footers to have a\\n  short list of links to various pages of a site, such as the terms of\\n  service, the home page, and a copyright page. The footer element alone\\n  is sufficient for such cases; while a nav element can be used in such\\n  cases, it is usually unnecessary',\n",
       " 'If they have visible links to these social media profiles, it’s the best practice to mark up these links, instead of repeating the links and marking up the \"invisible\" variant:\\n<div itemscope itemtype=\"http://schema.org/Organization\">\\n\\n  <!-- … -->\\n\\n  <ul>\\n   <li><a itemprop=\"sameAs\" href=\"http://social-network-1.example.net/foo\">Social Network 1</a></li>\\n   <li><a itemprop=\"sameAs\" href=\"http://social-network-2.example.net/foo\">Social Network 2</a></li>\\n  </ul>\\n\\n</div>\\n\\nIf they don’t have visible links, or if it’s not possible to add Microdata markup to existing links (e.g., because the CMS doesn’t allow it), repeating them is better than not providing Microdata for them at all.\\nIf repeating them and needing them to be invisible, the link element should be used instead of empty a elements:\\n<div itemscope itemtype=\"http://schema.org/Organization\">\\n  <meta itemprop=\"name\" href=\"Foo\" />\\n  <link itemprop=\"url\" href=\"http://example.com/\" />\\n  <link itemprop=\"sameAs\" href=\"http://social-network-1.example.net/foo\" />\\n  <link itemprop=\"sameAs\" href=\"http://social-network-2.example.net/foo\" />\\n</div>\\n\\nEven if the page doesn’t contain any visible social media links, adding them in Microdata is still better than not adding them; but in such a case Google might decide not to use them for their Social Profile Links.',\n",
       " 'Querystrings are counted as part of a URL and variations will be considered unique URLs. So Google will see them as different pages. This also means, in your case, that you will have duplicate content issues and will need to use canonical URLs to specify which URL Google should consider the primary URL which is what will be shown in their search results.',\n",
       " 'Much of the time, there are two things to look at.\\nOne is, if you are editing the file on Windows, the CR (carriage control) LF  (line feed) [EOL] (end of line) marker needs to be changed to just LF for Linux. Some editors and tools allow for this with an option while others do not. Also keep in mind that there are 3rd. party tools that get in the way of the process such as management tools.\\nThe second is how the file is transferred. Most of the time, an FTP client is used and text is not the default transfer method, however, most FTP clients will use text for a .txt file extension at least. I have had to specifically select text over binary even for obvious text files before. Most FTP clients will handle file extensions correctly. Still, sometimes the file is not converted correctly even with the best of intentions. As well, I have seen FTP servers at fault here though rarely.\\nWhenever you see a problem like this, check to see if the uploaded file matches the original with something like WordPad where the code can be examined. This can tell you if the file is being uploaded correctly. Many control panels will allow you to open a file transparently to properly compare the two. If the upload is not handling the [EOL] marker properly, then it is likely the transfer method. For FTP, you can always force text transfers over binary. If this still does not work, then in any control panel, you can simply upload the file. Some will make the change for you. If that still does not work use SSH (shell) and edit the file, remove the contents, then cut and paste from the original. This should always work. For smaller files, this works a treat. For larger files, it can be frustratingly slow but should work fine as well.',\n",
       " 'Easy enough.  \\nCloudFront supports multiple origin servers, and uses path patterns to determine which origin server to forward the requests to... so multiple, independent backend systems, even systems that aren\\'t inside AWS, can all \"own\" one or more paths under a single hostname, with one of them being the default and owning all the paths not explicitly configured.\\nThis is probably similar to what you have read about Nginx or Apache (and can also be done with HAProxy and others), but the request routing is done by CloudFront, which connects to the appropriate back-end, sends the request, and returns (and possibly caches) the response.\\nIt does not redirect, so the browser address bar never changes.\\nThe caveats:\\nCloudFront won\\'t strip paths, so /blog* routes requests to bloghost.example.com then bloghost has to have its content under the /blog path, not in the root of the site.\\nCloudFront forwards very few headers to the origin by default.  You can configure it to forward what you need, but every header you forward will reduce your cache hit ratio.  There are very sound reasons why this is true, though beyond the scope of this answer.  You need to identify which headers wordpress needs, and whitelist them in CloudFront.\\nYou will also need to configure CloudFront to forward query strings and cookies, since, again, it doesn\\'t forward those by default.\\nSee also:\\nhttps://stackoverflow.com/a/38709097/1695906 \\nhttps://serverfault.com/a/729706/153161 \\nhttps://stackoverflow.com/a/32364600/1695906',\n",
       " \"You don't have to remove 301 redirect, if the content is relevant.\\nThere are too many webpages out there, and Google can't notify them in search console at a same time. So, I suggest either wait for a long time or just forgot that error message, since you already fix it. \\nI also liked to mention another thing. Many days ago I have also get HTML improvement messages in my seach console. One is having sort title for homepage and second is duplicate title tag for one of my blog post, So I just correct them, and after few weeks the first error was gone, because it is homepage and Google often crawl it, while second error take some time to gone.\",\n",
       " \"You should stick to a single container so that you don't need to set up different tags for each section of your site. However, in GA, I would probably set up a separate view for each section, and also a rollup view. You can easily use include filters to make sure you have one view containing only the front-end section, one for just the admin section, and finally one for everything (in addition to best practise views like Test and Raw).\",\n",
       " 'Typcal... Tried several different methods for hours, and a few minutes after I ask here, I fix it myself...\\nAnyway, correct code for getting several blogPosts is this:\\n<script type=\"application/ld+json\">\\n    {\\n        \"@context\":\"http://schema.org\",\\n        \"@type\":\"Blog\",\\n        \"blogPosts\":\\n        [\\n          {\\n              \"@type\":\"blogPosting\", \\n              \"headline\":\"Article Heading One\"\\n          },\\n          {\\n              \"@type\":\"blogPosting\",\\n              \"headline\":\"Article Heading Two\"\\n          }\\n        ]\\n    }\\n</script>\\n\\nWill use it to markup every blog like a index site, guess that is what Its for :)',\n",
       " \"This is the unique ID assigned by the registry to the domain. Each registry have different policies, and even the WHOIS response is different from registry to registry.\\nIt's reasonable to assume that the Domain ID for PIR (the .ORG registry) is equivalent to the Registry Domain ID for Verisign (the .COM registry).\\nThe ID is not really used anywhere publicly. You don't need it to manage the domain, nor to perform WHOIS lookups. Is probably used internally by the registry to represent the domain, and you can use it in external systems to uniquely identify that specific domain instance as it should be guaranteed to be unique, but beyond that is not a particularly meaningful information.\",\n",
       " \"Bing and Google are two separate entities with different set of algorithms behind their search. I have also seen some rich snippets which show only on Bing only and not on Google. It's best to submit your website to Bing Webmaster Tools and then use the Bing Markup Validator tool to diagnose the problem (if any).\\nI have also experienced that Bing does not support all the rich snippet data types.\",\n",
       " 'You should use CollectionPage instead of WebPage, not in addition to it. CollectionPage is a more specific WebPage.\\n\\nIt seems that significantLink (for URL values) as well as hasPart (for CreativeWork values) could be used. But I think hasPart is the better choice here, because significantLink could also be used for pages that don’t belong to the category (so for consumers it’s not necessarily clear that these are category items), and hasPart would allow you to provide metadata (in case you want to display it on the category page).\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"CollectionPage\",\\n  \"hasPart\": [\\n    {\\n      \"@type\": \"ItemPage\"\\n    },\\n    {\\n      \"@type\": \"ItemPage\"\\n    },\\n    {\\n      \"@type\": \"ItemPage\"\\n    }\\n  ]\\n}\\n\\nAnother option is to use mainEntity to reference an ItemList. It conveys that the list is the primary entity of the CollectionPage, so for consumers it’s probably clear that this is the collection the page is about.\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"CollectionPage\",\\n  \"mainEntity\": {\\n    \"@type\": \"ItemList\",\\n    \"itemListElement\": [\\n      {\\n        \"@type\": \"ItemPage\"\\n      },\\n      {\\n        \"@type\": \"ItemPage\"\\n      },\\n      {\\n        \"@type\": \"ItemPage\"\\n      }\\n    ]\\n  }\\n}',\n",
       " 'Most of the Hotel-related types and properties were introduced with version 3.1, which was released two weeks ago.\\nIt seems that Google didn’t update their SDTT yet. I would expect that it will recognize the new types/properties soon.',\n",
       " 'The @id keyword allows you to give a node a URI. This URI identifies the node.\\nSee Node Identifiers in the JSON-LD spec.\\n(The equivalent in Microdata is the itemid attribute, and the equivalent in RDFa Lite is the resource attribute.)\\nWhy are identifiers useful?\\n\\nYou can reference a node instead of repeating it (see my example).\\nOther authors can do the same (on external sites): when they use the URI you specified, it’s clear that they are talking about the same thing.\\nConsumers can learn that different nodes are about the same thing.\\n\\nIt’s also one of the core concepts of Linked Data and the Semantic Web. If you care about this, you might want to use URIs that differentiate between the actual thing and the page about that thing (see my explanation).\\nThis is what Apple is doing in the example. The URI http://www.apple.com/#organization represents the actual organization, not a page (and not a part on that page) about the organization. This is a Hash URL, and it’s a popular way to make the distinction between the thing and the page about the thing. If you want to say in your JSON-LD that you like Apple, you could use http://www.apple.com/#organization to identify Apple. If you would use http://www.apple.com/ instead, it would be Apple’s homepage you like.',\n",
       " 'You have to provide an ImageObject item, and you have to add the caption to this ImageObject item. Otherwise you would add the caption to the Article, which is not what you want (and caption isn’t defined for Article anyway).\\nSo in Microdata it could look like:\\n<div itemscope itemtype=\"http://schema.org/Article\">\\n\\n  <!-- Article properties here -->\\n\\n  <div itemprop=\"image\" itemscope itemtype=\"http://schema.org/ImageObject\">\\n    <!-- ImageObject properties here -->\\n    <p itemprop=\"caption\">Caption of the image</p>\\n  </div>\\n\\n  <!-- Article properties here -->\\n\\n</div>',\n",
       " 'While it is better not to have a 301 redirect, there generally is no harm in search performance to update a URL using a 301 redirect. This method has the advantage of preserving any value or most of the value from any inbound (back) link to your page.\\nKeep in mind that a redirect results in multiple requests. While the effect of this is small, as a practice, it is better not to use a 301 redirect if at all possible.\\nFor this reason, evaluate any inbound (back) links to your page. If there are valuable links, then you do want to do a 301 redirect to change the old URL to a new URL to preserve link value. However, if you find little to no value, then it is likely better to update the URL to the original post and drop the old URL resulting in a 404 Not Found. In this case, search engines will find the new URL just fine and rank it appropriately over a small amount of time and drop the old URL from the index entirely. Again, only do this if the post has link of little to no value.\\nOne small warning. Most people want to preserve any link value at all cost regardless. Most of the time, unless there is a moderate to fairly significant value to the link, it is just not worth the effort. You have to decide. Do not be nervous to break a link. Links come and go. Make sure that if you chose to preserve any link value that the value you are preserving is worth the effort. Why do I say this? Again, it is because often people try and preserve link value that is often disposable. Of course, you are the only one who can decide this. Whether you preserve link value using a 301 redirect or not is your choice. There is no real harm either way.',\n",
       " \"In general, having keywords in the URL is a plus. It is an SEO ranking factor and it is useful for users so they have an idea of what the target page will contain.\\nHaving said that, in your example it is entirely pointless to change. Your URL already has keywords, including what's apparently your most important keyword (cosplay). Changing things now will almost certainly make no difference or even harm your rankings.\",\n",
       " 'The HTML is not correct, because a span can’t have a p as child. It should probably be a div, i.e.:\\n<div itemscope itemtype=\"http://schema.org/Article\">\\n\\n  <div itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">\\n    <p itemprop=\"name\">Author of the post</p>\\n  </div>\\n\\n</div>\\n\\nThe problem you seem to have is likely the same one from your comment to my answer to your other question. \\nHow Microdata works\\nI think you have to understand how Microdata works:\\n\\nThe itemscope attribute creates an item. \\n(Here is something.)\\nThe itemtype attribute specifies the type of that item. \\n(This something is a Person.)\\nThe itemprop attribute specifies the property this item has. \\n(This something, which is a Person, has the name \"Alice\".)\\n\\nThe ìtemprop always belongs to the nearest itemscope parent! So in your example, the author property does not belong to the Person item, it belongs to the Article item.\\nIt says: Here is an Article. This Article has an author, which is a Person. This Person has a name.\\nYour first suggestion\\n\\n<p itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">\\nAuthor of the post\\n</p>\\n\\nYou provide the author property, and you say that the author is a Person (good), but then you don’t provide any property about that Person. How should a consumer know that the element content (\"Author of the post\") is the name of that person, and not the person’s address/birthday/description/etc.? (Schema.org’s Person type can have many properties.)\\nMicrodata ignores any content that is not an itemprop value. The value of the author property in your suggestion is the Person item, not the string \"Author of the post\". A Microdata parser will never see that string (unless you put it as value to an itemprop).\\nYour second suggestion\\n\\n<span itemscope itemtype=\"http://schema.org/Person\">\\n<p itemprop=\"author\">Author of the post</p>\\n</span>\\n\\nThis means: There is a Person which is authored by \"Author of the post\". \\nBut the Person has no author, of course. (And Schema.org doesn’t define an author property for Person.)\\nAnother problem: For consumers it’s not clear that this Person represents the author of the Article. You have to use an itemprop attribute to \"add\" it to the Article.\\nA possible alternative (not recommended)\\nInstead of \\n<div itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">\\n  <p itemprop=\"name\">Author of the post</p>\\n</div>\\n\\nyou could use\\n<div itemprop=\"author\">Author of the post</div>\\n\\nbut it’s not recommended/expected (by Schema.org).\\nThe key difference here is that this author property has text (instead of another item!) as value. The obvious problem of using a text value: You can’t provide more information (in the form of properties) about this author (e.g., the author’s website URL, etc.).\\nWhy it’s not recommended? Because Schema.org’s author property is defined to get an Organization or a Person value.',\n",
       " 'The URLs should be in the same language as the content. Ideally you would also translate the relevant path segments (like \"product\" in your example), not only the slug.\\nA URL that’s in a language users might not understand is useless for them. A good URL is descriptive, it gives an idea what the content is about. If it’s in a language the users don’t understand, they can’t get this idea. And they might not be able to type it (e.g., in case it’s printed / written down), they might not be able to dictate it (e.g., via phone), they might not be able to recognize it again (e.g., in case they try to remember if they already visited it), etc.\\nIt could even stop users from visiting the page, because they might assume that the page’s content will also be in this language. Language codes in the URL could help here, but not all users might recognize them.',\n",
       " 'The structured data of the website social profiles are correct.\\nFrom the tests I made it doesn\\'t have to do with the fact that it\\'s a local business, but with the google search preferences used when performing the search or a problem when Google generates the profile box.\\nFor https://www.google.com/search?q=dispute+bills it shows the full results:\\n\\nBut then there are some circumstances where the social profiles not appear:\\n\\nChanging Google Search language preference to Spanish (they only appear with Google in English)\\n\\nDoing a verbatim search also hides the profile links\\n\\nBlocking javascript also hides them:\\n\\nSo, your Social Profiles are correctly configured, there are some scenarios on each client configuration that may hide them. \\nYou can try performing the search in another browser and if you can\\'t see them report the situation using the feedback button that appears below the box as the instructions suggests in the Troubleshooting section (as you know they are already correctly configured):\\n\\nreport the missing events using the small \"Feedback\" link under the\\n  Knowledge Graph panel in Google search. Click \"Wrong?\" on your\\n  company\\'s name and explain the problem, being sure to use the term\\n  \"social profiles\" in your description of the problem.',\n",
       " \"Yes, but as you already guessed you wouldn't be able to use relative paths because that would take you outside of the web root. You you would need to use the full URL to the CSS file to reference it in a different site:\\nSo instead of ../../css/style.css you would do http://example.com/css/style.css.\",\n",
       " 'I have been working for a hosting company and I must say, yes it is pretty normal for small and middle sized hosters and usually has good reasons. If you choose a cheap webspace plan, your site is running on a server that is shared among many customers. If the hosting provider would update PHP, it would normally affect all customers on that server. And PHP-updates are not 100% backwards-compatible. Therefore, when a hosting company updates PHP on a shared server, it is quite likely to break some of the customer sites. Therefore, lots of hosting companies run linux distributions with long-term-support that offer security updates for old PHP versions, even if PHP does not officially provide security updates anymore. For example, in Debian Wheezy, PHP 5.4 is currently still provided with security updates, even though PHP itself officially provides security updates only for 5.6 and newer.\\nThat said, there are ways to install and run multiple php versions on one server. But this of course increases complexity of the whole setup drastically. It usually also means you cannot use easy automatic updates anymore but need to manually install security updates for each of the installed versions. Therefore, mostly only big hosting providers follow this approach and give you the possibility to switch between different PHP versions.\\nAnother way to provide multiple PHP versions for the hosting company is of course to run different versions on different servers and put accounts on the server that fits the requirements of the customer.\\nTo conclude: No, it is not unusual. When you decide for a webspace, you should check the php (and MySQL) versions that the hosting company provides. Also, some PHP settings and extensions might be important, like imagemagick for Typo3, which is not provided by all hosting companies.\\nEdit: If planning to run WordPress as the OP, check this site: https://wordpress.org/about/requirements/ It says clearly what to ask your hoster for.',\n",
       " 'I think it’s better to include the language code for the default language, too. Quoting the pros from my answer on Stack Overflow:\\n\\nIf you decide to change the default language, you don’t have to change your URLs.\\nConsistency.\\nIt allows you to redirect from / based on the visitor’s language preference.\\nIt’s a signal that your site is available in multiple languages.\\nIt’s easier for users that want to change the language from the URL.\\nIt allows users of external search engines to search for pages in the default language only (site:example.com/en/; where site:example.com/ would find pages in all languages).\\n\\nOn the root page you could either provide links to both sites (similar to what Wikipedia does), redirect according to the visitor’s language settings (or based on IP), or redirect to the default language.',\n",
       " \"It's considered link cloaking, the major search engines will not appreciate this behaviour, nor will users. You should always adopt a transparent links policy, that way you run no risk of annoying your users or being punished by the search engines.\\n301 Redirects\\nIf you have new pages, new products etc, then the correct method would be to use a 301 redirect from old page, to new page, this way not only do you keep search engines happy, you keep your visitors happy too, assuming that old product, is related to new product. \\nThe other great thing about a 301 redirect is that you don't need to edit code source, simply apply your rules to your .htaccess, or IIS config, and the rest would be done for you. Finally, a 301 redirect will ensure that page rankings are being passed from old to new, while a JavaScript injection most likely won't.\",\n",
       " 'This should be fine. \\nIt probably doesn’t affect SEO. Consumers would have no reason to parse the URL path segment if they could just parse the rel-alternate+hreflang markup instead, which is typically easier and more reliable. Any way, even if the URL path segment is used by some consumers, both types are valid language tags after all.\\nFor what it’s worth, Mozilla does this for their Firefox add-on site, too:\\n\\nes: https://addons.mozilla.org/es/firefox/addon/noscript/\\nde: https://addons.mozilla.org/de/firefox/addon/noscript/\\nen-GB: https://addons.mozilla.org/en-GB/firefox/addon/noscript/\\nen-US: https://addons.mozilla.org/en-US/firefox/addon/noscript/\\n(en and unknown values redirect to en-US)\\n(de-DE/de-CH redirect to de)',\n",
       " \"Sitemap for multiple folders under same domain :\\nSitemap is a list of urls for a web crawler to know the web structure and urls of your website to crawl.\\nThere are various conventions followed by developers to create a sitemap. Like:\\n1) Single sitemap with all urls\\n2) Multiple sitemaps per directory or logical grouping of urls\\nBut this does not make much of a difference as for crawler is concerned. It will pick those urls and crawl it.\\nRecommended approach:\\n\\nCreate a single file sitemap.xml\\nAdd all root links + links for directory sitemaps inside it\\nAdd directory sitemaps will have all links inside that directory +\\ndirectory links if there are directory inside directory. This can be\\nnested to any level\\nAdvantage of this is a proper organization/management of sitemaps\\nfor you.\\n\\nEven if your sub-directories are created by various tools ultimately your website is a set of links that crawler understands.\\nFor easy management, you can have a individual product's sitemap created from its used platform/tool and link it in main sitemap.\\nFor google analytics:\\nUnder the same property in google analytics you can create different views based on your requirement.\",\n",
       " 'As far as i know Youtube delivers personalized videos only if you are logged in. When i open a private browsing window and go to https://www.youtube.com/ they show me generell video categories and say \\n\\n\"You\\'re viewing YouTube in English (US). You can change this\\n  preference below.\".\\n\\nSo, if you have personalized content only after the login you should be safe.\\nIf you personalize the content for geotargeting or based on the user referral you should give the user / searchengine a link to go to the other possible results and overrule your preselection. Like in the footer:\\n\\nYou are currently watching the page optimized for user from google, want to see the normal version? Click here.\\n\\nIn that way, even Google is able the see all your content (Content for Google referral & normal content) and in my understanding nothing is cloaked. \\nMoz has a good article about whitehat cloacking: https://moz.com/blog/white-hat-cloaking-it-exists-its-permitted-its-useful',\n",
       " 'Your site has likely been hacked and a malicious script inserted to add spam-advertising. The script may show one result to the Google bot and other results to humans.  You should contact your hosting provider right away to get them to look at the site and see if they can assist in cleaning it.  If not, there are security services that can do the same thing for a fee (Sucuri, Stop the Hacker, etc).\\nIf you are running WordPress or Joomla or another common content management system, you will definitely need to get a pro involved to not only clean the infection but also harden the site against future attacks.',\n",
       " 'This code works fine\\n\\nYou must have something else... the code you have posted would result in a rewrite loop (500 Internal Server Error) for all requests?\\nBut neither do you have to do anything special to \"ignore existing directories and rewrite everything to public dir\". So again, there must be something else going on here?\\nHowever, what you do need to do in order to avoid a rewrite loop is to include a condition to avoid rewriting to the \"public\" subdirectory when you have already rewritten to the \"public\" subdirectory. For example:\\nRewriteCond %{REQUEST_URI} !^/public/\\nRewriteRule (.*) public/$1 [L]\\n\\nWithout the RewriteCond directive /something would get rewritten to /public/something to /public/public/something to /public/public/public/something, etc...\\nIncidentally, you don\\'t need anchors (^ and $) if you are matching the entire string, so ^(.*)$ and (.*) are the same.\\n\\n<IfModule mod_rewrite.c>\\n\\nAside: Unless your site is intended to \"work\" without mod_rewrite, you should remove the <IfModule...> wrapper.\\nUPDATE:\\n\\nComment: the existing .htaccess is from laravel developers:\\n<IfModule mod_rewrite.c>\\n    <IfModule mod_negotiation.c>\\n        Options -MultiViews\\n    </IfModule>\\n\\n    RewriteEngine On\\n\\n    # Redirect Trailing Slashes If Not A Folder...\\n    RewriteCond %{REQUEST_FILENAME} !-d\\n    RewriteRule ^(.*)/$ /$1 [L,R=301]\\n\\n    # Handle Front Controller...\\n    RewriteCond %{REQUEST_FILENAME} !-d\\n    RewriteCond %{REQUEST_FILENAME} !-f\\n    RewriteRule ^ index.php [L]\\n</IfModule>\\n\\nOk, this makes more sense, but it\\'s also part of the \"problem\". You can\\'t just add another directive and expect it to work. You need to work with the existing directives (if that is the intention). The order of the mod_rewrite directives is important.\\nThis is made more complex because you wish to ignore existing files and directories in the document root (as per your question). Also, the /public subdirectory presumably needs to be kept private, otherwise we could simply redirect. The \"Redirect Trailing Slashes\" then becomes a bit tricky as we can\\'t redirect to the /public directory (since that obviously exposes the hidden directory), but we need to check the existence of directories in that directory, not the document root (in which we are currently located). You also need to block direct access to the /public directory (or rather redirect back to the root).\\nTry the following (just the inner, mod_rewrite directives are included after RewriteEngine):\\n# Prevent direct access to the \"public\" folder - redirect to root\\nRewriteCond %{THE_REQUEST} ^[A-Z]{3,9}\\\\ /public/\\nRewriteRule ^public/(.*) /$1 [R=302,L]\\n\\n# Redirect Trailing Slashes If Not A Folder...\\n# - but look for the file in the \"public\" folder\\n#   (ensure we are not already in the \"public\" folder)\\nRewriteCond %{REQUEST_URI} !^/public/\\nRewriteCond %{DOCUMENT_ROOT}/public/$1 !-d\\nRewriteRule ^(.*)/$ /$1 [R=302,L]\\n\\n# Rewrite \"everything\" to the \"public\" subdirectory if not already\\n# This ignores existing files/dirs in the document root\\nRewriteCond %{REQUEST_URI} ^/(.*)\\nRewriteRule !^public/ public/%1\\n\\n# Handle Front Controller... (as before)\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteCond %{REQUEST_FILENAME} !-f\\nRewriteRule ^ index.php [L]\\n\\nMake sure you clear the browser cache before testing, since the previous (erroneous) 301 redirects will have been cached by the browser.\\nChange the 302 (temporary) redirects to 301 when you are sure it\\'s working OK. (302s aren\\'t cached by the browser, so makes testing easier.)',\n",
       " \"For the reasons you cite, you should change them to hyphenated words. Search engines are pretty good at extracting words out of run on sentences but that's a shot in the dark. I'm sure Google would give preference to separated words over run ons. Users are more likely to guess at a URL using hyphens than run on ones. So, overall they are more discoverable.\\nSo switch them out.\",\n",
       " 'Google makes this a bit cumbersome, but you can see the volume that each search query drove to a specific landing page within Search Console.\\nGo to Search Traffic > Search Analytics, drill down into a specific keyword, then click the \"Pages\" radio button above the graph. You\\'ll then be able to see what pages absorbed what amount of search traffic.\\nTo tie this to ROI, do a landing page drilldown under the Behavior > Site Content section of GA. If you look at the same page on the same date that you have set in Search Console and set your segment to just organic search traffic, you can look at revenue generated on that day, divide it by organic search sessions, and get a per-session value. This value is super useful for making business decisions (like a mini LTV).',\n",
       " \"It's no big secret that Google loves to use algorithms and crazy hard mathematical equations for its return of search results, in both organic and local results. This is no different when Google computes its review average.\\nMEAN (Average)\\nThe issue here isn't the fact that Google is returning an incorrect review average, the problem is that you are assuming they are using average calculator based on a simple formula called 'MEAN'. \\n\\nSOURCE \\nTo calculate the Mean\\nAdd the numbers together and divide by the number of numbers. (The sum\\n  of values divided by the number of values).\\nTo determine the Median\\nArrange the numbers in order, find the middle number. (The middle\\n  value when the values are ranked).\\nTo determine the Mode\\nCount how many times each value occurs the highest is the mode. (The\\n  most frequently occurring value)\\n\\nBayesian Average, or Similar\\nGoogle is using a far more complex formula to work out your average, and many people believe its a modified Bayesian average formula that predicts the probability of next few reviews, a simpler way of getting your head around this is to think, Google is predicting a negative review, It's unrealistic to assume that your business will always be 5.0, 4.6 and so on.\\nA business could 5 reviews with a average rating of 5.0, while another business may have the exact same number of reviews, and the exact score from visitors, but have a completely different average rating, this is because Google is using Bayesian average or something similar, or a modified Bayesian formula. Google for example does, can, or could take into other factors such as time, date, frequency, volume and even the niche of the business.\\nGoogle is far from being transparent when it comes to disclosing algorithm information, so sometimes you just need to accept it, or attempt to understand it the best you can. \\nIt's extremely unlikely that someone is going to pick a 4.7 over a 4.5, Google knows this... People are for more likely consider the amount of reviews a better indicator. The only thing you can do about it is keep up the good work! everyone else is in the same boat. \\nYou will find more information about Bayesian and Google Local Reviews on Google.\",\n",
       " \"I also asked this question on twitter and got this response:\\nME: Question: Is being listed in a local #ChamberofCommerce business directory good for local #SEO? #LocalSEO\\nFrank J. Kenny \\u200f@FrankKenny: A link from the chamber's website to your is always a good idea.\\nME: @FrankKenny That's what I thought, just curious and wanted to make sure Google didn't see it as a link farm.\\nTherefore with what Frank said on twitter and @closetnoc said in the comments above I am going to say yes it is a good thing.\\nFrank J. Kenny \\u200f@FrankKenny: Not at all Chad. Good, solid, link juice.\",\n",
       " 'Before the end of 2015 i would have recommended to implement the ajax crawling from google, but it\\'s deprecated: https://webmasters.googleblog.com/2015/10/deprecating-our-ajax-crawling-scheme.html\\nBut in general i don\\'t recommend you to use them:\\n\\nI\\'m not sure how google handels hashfragments and i found no\\ntrusted, fresh source \\nU can not use them with analytics (out of the box)\\nAre client side and not sent in HTTP request (for example no apache url rewrite)  \\n\\nIf you rly want you should use Hashbangs \"#!\", but if you really want to build a SEO optimized page you shouldn\\'t use them either.\\nExample for a SERP with Hashbangs: https://www.google.de/#q=site:anaavenue.com \\nOh, and if i analyse the Google itself, the query site:www.google.de inurl:\"?q\" gives me search results, but for site:www.google.de inurl:\"#q\" i don\\'t get any.\\nAnd don\\'t forget, that there are more search engines than Google..',\n",
       " 'I suggest you write them as 3 indepentent stories and (in the beginning) pretend the other 2 don\\'t exists. That gives you an answer for your H1/H2 problem, because you now create relevant titles for the stories. The H1 title is intended for the title of the page and has to be unique and descriptive for a user. By giving it the title of the story, you comply.\\nNow you have 3 stories, you can optimize:  \\n\\nCreate internal links between the articles. This is the main task as it has the most benefit  \\nImprove the content itself, make sure it reads well for your audiance  \\nEvaluate the titles, can you reword it so that the titles are connected (without repeat yourself)?  \\nYou can now create a tiny navigation/menu between the stories. You can do this like a menu, of some nevigation bar, or by adding a \"Like this? Also read ....\" at the bottom of a story.',\n",
       " \"If appearing on your mobile site, then yes, they may not rank as highly.\\nHelping users easily access content on mobile\\nYou can see with the examples they give, they don't mention if they are deceitful or malicious, simply that they obscure the content:\\n\\nShowing a popup that covers the main content, either immediately after the user navigates to a page from the search results, or while they are looking through the page.\\nDisplaying a standalone interstitial that the user has to dismiss\\n  before accessing the main content.\\nUsing a layout where the above-the-fold portion of the page appears\\n  similar to a standalone interstitial, but the original content has\\n  been inlined underneath the fold.\\n\\nTo continue to have a pop up while avioding the penalty, you will need to use small smart style banners, such as the example they give:\\n\\nBanners that use a reasonable amount of screen space and are easily dismissible. For example, the app install banners provided by Safari and Chrome are examples of banners that use a reasonable amount of screen space.\\n\\nAlthough this is only on mobile devices, you can still continue to show them on desktop. \\n(Until they decide to penalise pop ups on desktop too)\",\n",
       " \"On the response, I am not able to see this rule trigger [NewCookie is not set]\\n\\nmod_rewrite only works on the request. So you can't check for the Cookie being set on the response, if that is what you are trying to do?\\nIf your mod_rewrite directives don't appear to be doing anything against the request sent from the client then it would seem either the cookie is not being sent back, or it is failing to set the NewCookie (eg. client rejects it because of domain mismatch?)\\nYou could perhaps check to see if your directive is being processed, by setting an environment variable and checking for the existence of this in your server-side code. For example:\\n[L,CO=NewCookie:%1:our.proxy.com:1440:/,E=TRIED_TO_SET_COOKIE:YES]\\n\\nRewriteRule ^(.*)$ /$1\\n\\nNote that if you just want to set the cookie, without any URL rewriting then simply use - (hyphen) as the substitution (special feature). This can then be simplified to:\\n RewriteRule ^ -\\n\\nQuestion: Does mod_rewrite explicitly forbid access to httpOnly cookie?\\n\\nNo. mod_rewrite will see the cookie if it is sent in the request.\",\n",
       " \"The first URL should be the file that is access, while the second one is the referrer, aka the file/page that made the browser access the first URL.\\nYou can configure what shows up in your log files. Typically by modifying the  'LogFormat' lines in your /etc/apache2/apache2.conf \\nMore information about what information you can show: http://httpd.apache.org/docs/current/mod/mod_log_config.html\",\n",
       " 'From the Apache docs\\n\\nSymLinksIfOwnerMatch\\n  The server will only follow symbolic links for which the target file or directory is owned by the same user id as the link.\\n\\nSo, enabling this option (as opposed to FollowSymLinks) prevents symbolic links being followed that might point to some critical parts of the system (where the owner of the link does not match the target file).\\n\\nDo I need it? I have a plain HTML web site (though might go for Joomla one day).\\n\\nIf you have a plain HTML site and no fancy URLs then probably not.\\nHowever, it is required by mod_rewrite, which is required for URL rewriting. (And is required by Joomla if you enable \"pretty\" URLs - ie. URL rewriting.)\\n\\nSome sites specify that it replaces the FollowSymLinks\\n\\nSymLinksIfOwnerMatch is a more restriction option. The two are mutually exclusive.',\n",
       " 'I asked this same question on StackOverflow.   To get it to work properly, you have to use environment variables:\\nRewriteRule ^page$ /page.html [L,E=LOOP:1]\\nRewriteCond %{ENV:REDIRECT_LOOP} !1\\nRewriteRule ^page.html$ /page [R=301,L]\\n\\nThis is because mod_rewrite does multiple passes through your rules.    During the first pass, it sets the environment variable.   During the second pass, it prepends the variable with the REDIRECT_ prefix, so you have to read it as REDIRECT_LOOP.',\n",
       " 'There is a article on Searchengineland: http://searchengineland.com/tested-googlebot-crawls-javascript-heres-learned-220157\\nThey tested a lot of different typical JS use cases and most of them are handled fine by google. But it is important to allow Google access to your js / css assets to render your whole site.\\nThere are some rumors, the browser Chrome is only a side product of the crawler Google is using.\\nIf SEO is important for your business i would not recommend you to implement any SEO critical js (like loading content via ajax) without heavy testing on your own.\\nSome time ago i set up a test too to find out which urls are crawled by google. But i guess Google only extracted the URL from the code and does not perform a real JS action. The article is in german, but \"red\" means \"not crawled\" and \"green\" means \"crawled\": http://www.sirpauls.com/welchen-links-folgt-google-ein-experiment/\\nResults for onclick:\\n<input type=\"button\" name=\"the-button\" value=\"THE BUTTON\" onclick=\"window.location=\\'http://www.domain.com/test/target.php?id=11\\'\"/>\\n<button onclick=\"JavaScript:window.location=\\'http://www.domain.com/test/target.php?id=18\\'\">Click Me!</button>\\n\\n= Not crawled\\n<div onclick=\"window.location=\\'http://www.domain.com/test/target.php?id=12\\'\">THE DIV</div>\\n<a href=\"#\" onclick=\"window.location=\\'http://www.domain.com/test/target.php?id=3\\';\">Link</a>\\n\\n= Crawled',\n",
       " \"I think you mean a catch-all inbox: https://en.wikipedia.org/wiki/Catch-all\\nYou can make rules like:\\n*@example.net -> all@example.net \\n\\nI'm not an exchange expert, but i think this will help: https://technet.microsoft.com/en-us/library/bb691132(v=exchg.80).aspx\\nAs described you can define a pattern as transportrule, like:\\n^brian@contoso.com$\",\n",
       " 'From the perspective of Schema.org, both ways are fine, as the image property expects an ImageObject value (your second example) or a URL value (your first example).\\nAs always, a specific consumer might support only one of these two ways.\\nFor many of Google’s rich results, an ImageObject is required, because Google wants to see more data about the image, which is not possible when providing just a URL. For example, their Articles rich result requires the height/width properties for the ImageObject value.',\n",
       " 'One of the Google guys, Paul Lewis, said achieving 85 or better should be your goal. Of course those things you mention matter. Both to Google and, especially, users! \\nPage render blocking is blocking your page. Is it a problem? Maybe not for you but Google is suggesting you look into it and make sure. (Thanks!). Saving 1.8MB of bandwidth? Holy cow! Maybe my images are too big. If not, what is it? Inline styling? What a pain. Are they wrong? No!\\nAnything that makes your web site load faster is always good for the user! Thanks Google!\\nBut overall, Paul agrees with you.\\n\\nNow the thing to bear in mind about PSI is that it’s distilling down a\\n  collection of very good (and broad) set of rules and seeing how your\\n  site measures up. It doesn’t really care about context or what you\\n  chose to do or not do as a team. As such I just tend to see PSI as a\\n  checklist of things that are somewhere between “a good idea” and\\n  “crucial”, and I don’t aim for a score of 100 across the board.\\nThe reason I’m not going to pin all my hopes on a PSI score of 100 is\\n  that it may make my development more difficult. Inlining CSS and\\n  JavaScript to pages is one recommendation that it makes, but I think\\n  more context is required here.\\n\\nAnd even Google will say that.\\n\\nA higher score is better and a score of 85 or above indicates that the\\n  page is performing well.',\n",
       " 'I am worried I may be penalised by Google for content duplication\\n\\nThere\\'s not really a \"penalty\" as such, however, what you will find is that if Google does perceive these URLs as duplicate then it will only return one of the them (for any given page) in the search results. It may even favour one domain over all others - if it sees that domain as authoritative.\\nHowever, it\\'s also confusing for users that might stumble across the different sites and realise the content is the same.\\n\\nWhat way would you go about doing this?\\n\\nIf you are intending these domains to be indexed then that\\'s a tricky one. Ordinarily, in order to avoid any duplicate content issues, you\\'d pick one as the \"canonical\" domain and redirect to that. (Or, set a cross-domain rel=\"canonical\" meta tag, or HTTP response header? At least that way, the other domains would be accessible.)\\nIf you don\\'t take manual action in resolving what is potentially duplicate content then Google will simply decide for you, if it is perceived as duplicate. Possibly the page with the most quality backlinks will \"win\"? The same page on the other domains may not appear at all in the SERPs - that is the duplicate content \"penalty\". Google is trying to return quality results to its users. Returning 3 or 4 pages that contain essentially the same content (Google doesn\\'t really care about \"branding\") is not \"quality search results\" - so it is something that Google actively tries to avoid.\\nYour domains are going to be competing against each other. The only way to avoid the \"duplicate content\" issue is to vary the content.',\n",
       " 'Google’s structured data guidelines describe what you should/shouldn’t do for getting one of their search result features, not what you should/shouldn’t do with structured data on your pages.\\nGoogle, as only one of possibly many consumers, doesn’t (and shouldn’t) decide what kind of structured data is useful. It’s perfectly fine to go against their guidelines, the only \"risk\" being that you don’t get one of their search result features.\\nYour examples\\nIn your first example, the page is giving an AggregateRating for a Thing named \"Cufflinks\". Their structured data is not really expressive, but the text on the page explains that this is the average customer rating for all of their products in that cufflinks category. The structured data doesn’t seem to be misleading (it’s vague, however), so I don’t see a problem with this. However, Google would ideally not show an AggregateRating rich result for them, based on their guidelines, as this is clearly a category page and not about a specific product/service:\\n\\nRefer clearly to a specific product or service.\\n\\nProvide review and/or rating information about a specific item, not about a category or a list of items.\\n\\n(Note that you quoted the guidelines for product rich results, but the page in question isn’t using Product structured data.)\\nIn your second example, the page is giving an AggregateRating for a Product named \"car insurance\". From a quick look, the page doesn’t seem to be about a category/list; while this is also some kind of category/entry page, the focus seems to be on the car insurance. And the structured data conveys that the ratings are only for the car insurance, not the other insurances. So it seems to be fine that Google displays an AggregateRating rich result for this page.\\nReceiving stars for category pages\\n\\nIs there an ethical way of receiving stars for category pages?\\n\\nAccording to Google’s current guidelines, no. \\nBut that shouldn’t stop you from providing that structured data on category pages, if you think it can be useful for a consumer. \\nIf you don’t want to appear to be misleading (in the hope that Google doesn’t detect that your page is a category page), be expressive in your structured data, e.g., by using a CollectionPage instead of an ItemPage. Give consumers as much information as possible about what your aggregated ratings are about, so that they can better understand if it meets their own guidelines or not.',\n",
       " '1st of all, posts are displayed newest first so rel=\"next\" for\\n  previous posts is not too intuitive, to say the least. Wolud it be\\n  more useful if it were rel=\"prev\" for older entries?\\n\\nrel=\"next\" specifies the next item of a logical sequence, it does not mean \"the newer published posts\", but \"the next page of this series of pages\", which in this context is the previous published posts page\\n\\n2nd, with content being updated constantly, the content of a paginated\\n  url will change with every post published until eventually page 2\\n  becomes page 3 and so on. Do these pages even deserve an indexed link\\n  of their own?\\n\\nIf having a list of posts for each author from older to newer makes sense then their pages will be \"fixed\" and you won\\'t have the changing pages content problem.\\nIf the order of your post lists is from newest to older, your paginated lists will contain a list of post that will be changing their URLs often as you say, so Google will detect that and increase their crawling rate there, visiting them often.\\nFrom personal experience it is unlikely to visit a paginated list of pages out of synch, or even a page from that list from Google, it has only happened to me in forum posts that sometimes the page cached in Google didn\\'t contain the post I was looking for but it was on older pages in the thread. \\nIn the example you gave, looking for \"articles by X about zebras\" I think it would make more sense if Google shows directly the link to the post about zebras in SERP, which will also have the author in the page, instead of a link to the post in a list of author posts (but that will depend on the titles and description of list/post).\\n\\n3rd, a \"view all\" is certainly not an option with hundreds of posts or\\n  more.\\n\\nAs Webmaster guidelines suggests:\\n\\nLimit the number of links on a page to a reasonable number (a few\\n  thousand at most).\\n\\nMy suggestion is:\\n\\nkeep the order from newer to older, as this is what people expects from posts lists in blogs (not the other way around)\\nimplement rel=\"next\" and rel=\"prev\" links to indicate the relationship between URLs and trust Google will keep crawling them often.\\nMonitor search queries and landing pages in Search Console to see if you are receiving traffic to you paginated content or directly to posts.',\n",
       " \"This guidance (Guidance on the rules on use of cookies and similar technologies) from ICO(UK's independent body set up to uphold information rights)  has a part related to the intranet:\\n\\nHow do these rules apply to intranets?\\n  In our view the rules do not apply in the same way to intranets. The Regulations\\n  require that consent is obtained from the user or subscriber. A ‘user’ is defined\\n  as any individual using a public electronic communications service. An intranet is\\n  unlikely to be a public electronic communications service. Although the Regulations would not therefore apply in the same way to cookies that are set\\n  on an intranet it is important to remember that the requirements of the DPA are\\n  likely to apply if your use of cookies is for the purposes of monitoring\\n  performance at work, for example. Wherever an organisation collects personally\\n  identifiable information using cookies then the normal fairness requirements of\\n  the DPA will apply.\",\n",
       " 'Browser will encode input, according to the character-set used in your page.\\nI personally avoid , in url structure, because of encoding. It\\'s %2C. \\nSo your url would be like /url=keyword1%2Ckeyword2.\\nCommas are allowed in the filename part of a URL, but are reserved characters in the domain.\\n* From the URI RFC:\\n\\n2.2. Reserved Characters\\nMany URI include components consisting of or delimited by, certain\\n     special characters.  These characters are called \"reserved\", since\\n     their usage within the URI component is limited to their reserved\\n     purpose.  If the data for a URI component would conflict with the\\n     reserved purpose, then the conflicting data must be escaped before\\n     forming the URI.\\n reserved    = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\" |\\n                \"$\" | \",\"\\n\\nThe \"reserved\" syntax class above refers to those characters that are\\n     allowed within a URI, but which may not be allowed within a\\n     particular component of the generic URI syntax\\n\\nWhile it\\'s definitely possible to use commas in URLs, it\\'s not a widely used practice, nor is it recommended.\\nFurther reading: https://www.searchenginenews.com/sample/content/are-you-using-commas-in-your-urls-heres-what-you-need-to-know',\n",
       " \"Basically, you are disrupting your entire site. What ever search performance you had will be gone and will have to be rebuilt from scratch. Without creating 301 redirects at all, here is what will happen.\\nThe pages that no longer exist, will return 404 errors and will for as long as there are links to the pages that no longer exist. Google will retry these pages for a period before giving up. Occasionally, Google will retry the again because links to these deleted pages remain. That is the way things are supposed to work. Do not use Search Console and Mark any correct 404 error as Fixed.\\nGoogle will find your new pages from your home page. It is like starting over. All new pages will have to rank just like any other new page. It will take time. Once Google has visited enough pages, sees a series of 404 errors, and sees a number of new pages, it will likely reindex your site fairly aggressively. This is normal.\\nEvery new page will require time to properly index, gain performance metrics, and build links. If you are changing your entire site, the sites performance will suffer for quite a while. For this, You will want to promote your site. However, more importantly, for any old page that is important, if there is a relevant page, you should create 301 redirects even if it is just a few per day. Do this for any page that you consider to be highly valuable. That way you will redeem previously existing value. But do not go too slow on this. You do not have to redirect each page, just the ones that will help you retain your most important links and content.\\nLastly, while it is mostly not necessary, it may help to create a sitemap if that is a simple process for you to do. If not, then do not worry about it. Submit your sitemap using Search Console. While Google should be able to crawl most any site without issue, the sitemap will help Google to audit your sitemap against what pages it can find. Ignore the metrics that Search Console gives you regarding pages indexed from your sitemap. Most of the time, this is hugely misleading. Do yourself a favor and only worry about whether Google can see your sitemap okay. After that, don't look. Do not resubmit your sitemap. Google will see it is updated and read it when it cares. Leave it alone. Just update your sitemap when you update your site.\",\n",
       " 'If you want to see an aggregate view, you have to use a single tracking id.   There is no way in Google analytics to combine the stats from multiple trackers.\\nYou can view the stats from individual domains that use the same tracking ID in Google Analytics.  To do so, you can apply \"segments\".   Create a new segment.  Use the advanced conditions to set the hostname for the site you want to view individually.\\n\\nWhen only this segment is applied, all of the reports will show only the data for that particular host name.\\nComparing the different hostnames on a single graph can be done as well, but the proceedure for doing so is very convoluted.   Me website has a bunch of subdomains that I want to compare, so I wrote a blog post about how to get the graph:  http://blog.ostermiller.org/analytics-subdomain/   Comparing different domain names with the same tracker should be exactly the same as comparing different subdomains since it triggers on hostname.\\n\\n(source: ostermiller.org)',\n",
       " \"Double redirects are less efficient:\\n\\nThe browser makes a GET request for each\\nEach hostname (or subdomain) requires a separated DNS lookup.\\n\\nIn your chain you will have:\\n\\n3 GET requests\\nTwo DNS lookups (HTTP and HTTPS use the same DNS lookup for example.com)\\n\\nIf it is convenient, it would be better to configure http:// www.example.com to redirect directly to https:// example.com.\\nHow much worse is it though?   Not much.   A second redirect adds 10s of milliseconds to the request.  I've never used hosting where I pay per GET request.   If it is difficult to reconfigure it to a single redirect, I wouldn't worry about it.\\nOnce you get into longer redirect chains, there can be problems.   Pretty much everything should follow a chain of five or six redirects.   Higher than that and some browsers will throw up errors and search engines bots may give up.\",\n",
       " 'If first url string 1 contain \"tool-model-\" at the end AND does not contain \"tool\" catalog at the beginning make redirect to http://www.example.com/tool/tool-model- PLUS two digits (string 2)\\n\\nTo simplify/combine your redirects into a single ruleset I would use mod_rewrite, rather than mod_alias (Redirect or RedirectMatch) since you want a condition that says does not contain \"tool\" (which is tricky with RedirectMatch). Try something like the following in your root .htaccess file:\\nRewriteEngine On\\nRewriteCond %{REQUEST_URI} !^/tool/\\nRewriteRule /(tool-model-\\\\d\\\\d)$ /tool/$1 [R=302,L]\\n\\nThis states for any URL that ends /tool-model-NN and does not start /tool/ then redirect to /tool/tool-model-NN. $1 in the substitution is a backreference to the captured group (ie. (tool-model-\\\\d\\\\d)) in the RewriteRule pattern (a regex).\\nNB: I\\'ve used a 302 (temporary) redirect above. Change this to 301 (permanent) when you are sure it\\'s working OK. 301 redirects are cached by the browser, so can make testing awkward. (So, you will need to clear your browser cache before testing the above directives.)\\nAlso, if you are currently using mod_alias Redirect (or RedirectMatch) for other redirects then it would be advisable to also convert these to mod_rewrite RewriteRule directives. The reason being that different modules run at different times, regardless of the order in the .htaccess file. So, you can get some unexpected conflicts by combining redirects from both modules.',\n",
       " 'This feature has been greatly improved in DHIS version 2.24. I recommend that you upgrade your DHIS version and try again. In 2.24 you can simply click the file icon next to each data entry input field and upload your file.',\n",
       " 'Schema.org’s height/width properties are not for stating in which dimension the image should be displayed, but which dimension the image file has.\\nKnowing the image’s height/width, consumers (like Google Search) can then decide if to do something (and what to do) with the image. They will use their own CSS if they display the image.\\nIn the case of Google Search, they want to see pixel values, e.g., for their Article rich result:\\n\\nThe height of the image, in pixels.\\n\\nThe width of the image, in pixels.',\n",
       " '...what is the reason for doing such a nonsense thing?\\n\\nThey might...\\n\\nbe associated with a competitor and want to take traffic away from your site. An exact clone of your site on another domain can only be detrimental to your sites SEO.\\nbe trying to increase the value of their domain (an established site that receives lots of traffic), which they can then later sell on for a profit. Your site is just an unfortunate pawn in the process, selected because of its popularity.\\nbe trying to serve malware to unsuspecting visitors by injecting malicious links/code into the cloned site. (Since the cloned site is relatively easy to setup and appears to be attracting a large number of visitors.)\\n\\nIt can also depend on the website, how it is being cloned and modified. They might also:\\n\\nprofit from changing the advertising network to one of their own.',\n",
       " 'Does it negatively impact my SEO?\\n\\nNo, not at all. \\n\\nAre there way to prevent this from happening?\\n\\nYou can use image related text surrounded it, for example figcaption tag will help you here. Alt text will also help you here. Google just want to know that, this image is related to this. And there are many of way to tell it. \\nGoogle can even use your paragraphs note to know about the images, I have notice some of quora answer with pics are indexed very well in image search, without using proper image name, alt, title, and figcaption tag.',\n",
       " \"In order to serve an app with a different document root from within a subdirectory, you should use the ^~ modifier on the prefix location so that other regular expression location blocks do not cause a conflict. See this document for details.\\nIf the app uses PHP, you can use a nested location block to inherit the different root.\\nFor example (and this is just a starting point):\\nlocation ^~ /app {\\n    root /var/www/html;\\n\\n    index index.php;\\n    try_files $uri $uri/ /app/index.php;\\n\\n    location ~ \\\\.php$ {\\n        try_files $uri =404;\\n\\n        include       ...; # this is your system's fastcgi_params file\\n        fastcgi_param SCRIPT_FILENAME $request_filename;\\n        fastcgi_pass  ...; # this is a socket or IP:port for php-fpm\\n    }\\n}\\n\\nI have left some ...s in as they depend on your specific installation. Notice that the root is appended to the URI to locate the physical file, so /app/index.php will be located at /var/www/html/app/index.php.\\nFinally, the app must know that it runs in a subfolder. When it requests resource files (such as .js and .css) it must prefix the URI with /app/, otherwise the main apps resources will be loaded.\",\n",
       " \"According to GA's official support articles,\\n\\nIf you see landing page = (not set), this is generally due to a visit with no _trackPageview hit included.\\n\\nThis can happen if you have a visit with another type of interaction hit, such as _trackEvent, _trackSocial, or _trackTrans (Ecommerce), but no _trackPageview hit.\\nTo investigate, create an advanced filter that includes pageviews matching exactly 0. You can then apply this to the Content > Event reports, Traffic Sources > Social > Social Plugins report, or Conversions > Ecommerce reports. This will help you identify which type of hit is causing the issue.\\nOften, the cause is:\\n\\nA profile filter filtering out specific pageviews\\nThe omission of the _trackPageview code call on the website\",\n",
       " \"Invalid Code, Disallow, No-Index, Local Cache or Facebook Cache\\nThis problem normally occurs with invalid code, Facebook Caching or content blocking using either no-index or robots.txt. \\nNow, your code is valid, and the Cache is being refreshed using Facebook Debugging Tools, so it must be the later (no-index, or robots).\\nLocal Cache\\nUsing header expires it is possible to locally cache assests, for example you can tell browsers and crawlers to cache the content for one week,  or even a year! now, if you are using an expire on *.jpg and you happen to replace a jpeg with the same file name, all browsers, and crawlers that have a cached version the file, will not receive the update. This can be overcome by using versioning on your filenames, for example:\\n\\nexample.jpg?version2\\n\\nOr if you would like to keep things simple... just use a filename, that way... no one will have a cached version of it.\\nFacebook URL Cache\\nFacebook does cache fetch requests, otherwise Facebook would waste a lot of their bandwidth, but more so because they would be flooding people's websites with requests, especially when things trend, and go viral. \\nYou can manually check the cache by using Facebook's Sharing Debugger, see  Stack Overflow - Facebook not clearing cache.\\nHowever, using Facebook's Object Debugger, the new cache looks correct:\\n\\nTime Scraped  2 seconds ago\\nResponse Code 200\\nFetched URL   http://www.laisva.lt/\\nCanonical URL http://www.laisva.lt/ (811 likes, shares and comments More Info )\\nServer IP 79.98.28.33\\n\\nSo, it looks like Facebook is using the last used image, that it was 'Allowed to Index', see below.\\nDisallow / No-Index\\nSince you are using WordPress your media uploads as you most likely know, are stored in /wp-content/uploads/.\\nNow taking a look at your robots.txt you have:\\nUser-agent: *\\n\\nDisallow: /wp-content/\\nDisallow: /wp-*\\n\\nThose disallow values will block search engines, social media platforms and all pother platforms respect the disallow value, simply put... your telling Facebook not to index your images.\\nAlso, it's not advisable to block /wp-content/ period, because you should want Google to index your images, its another way people can visit your site.\",\n",
       " 'You only really need to set the rel=\"canonical\" header. This should be sufficient in ensuring only the canonical URL (ie. the one with no URL params) appears in the SERPs. Setting a noindex robots meta tag for such URLs would seem to be overkill (and a tad risky) IMO.\\nPresumably you are unable to set a rel=\"canonical\" meta tag in the HTML itself?\\n\\n...what if I have 20+ URL parameters that I don\\'t want indexed?\\n\\nIs it safe to say that you don\\'t want any URL with any URL params (ie. any query string) indexed? In which case you can simply change your RewriteCond directive to read:\\nRewriteCond %{QUERY_STRING} .\\n\\nThat is, there is a query string of any length.\\nIf, however, you want to exclude 20 specific URL params then you are going to have to name every one of them. For example:\\nRewriteCond %{QUERY_STRING} (?:^|&)(id|start|page|another)=\\n\\nThe (?:^|&) is a non-capturing group to ensure we only match these specific param names and not something like sid or lastpage, etc. (if they could possibly be URL param names).\\n\\nHeader set Link \\'%{HTTP_HOST}%{REQUEST_URI}e; rel=\"canonical\"\\' env=CANONICAL_HEADER\\n\\nThis is invalid. You are missing the scheme (eg. http), e symbol after the %{HTTP_HOST} variable (this would result in a 500 error) and angled brackets (<..>) around the URL. This should be of the form:\\nHeader set Link \\'<http://%{HTTP_HOST}e%{REQUEST_URI}e>; rel=\"canonical\"\\'\\n\\nReference: (Google\\'s support doc regarding canonical URLs)\\nhttps://support.google.com/webmasters/answer/139066?hl=en\\nUPDATE: However, the %{REQUEST_URI}e environment variable, when used in this context, includes the query string - which really defeats the object of this excercise. This whole block should be rewritten as:\\nRewriteCond %{QUERY_STRING} .\\nRewriteRule (.*) - [E=CANONICAL_URI:$1]\\nHeader set Link \\'<http://%{HTTP_HOST}e/%{CANONICAL_URI}e>; rel=\"canonical\"\\' env=CANONICAL_URI\\n\\nInstead of using the REQUEST_URI variable, we capture the URL-path only (which excludes the query string) using the RewriteRule directive and store this in the CANONICAL_URI variable. This is then used in the Header directive instead.\\nThere is also no need for the <IfModule> containers here. It either works or it breaks, these directives are not intended to be optional (are they?).',\n",
       " 'By \"type\" I assume you mean mime-type, in which case you should look at mod_expires and the ExpiresByType directive - this is undoubtedly the easiest/modern way to set the appropriate Expires and Cache-Control (max-age directive) HTTP response headers.\\nIt goes something like:\\nExpiresActive On\\nExpiresByType application/javascript \"access plus 1 month\"\\nExpiresByType text/css \"access plus 1 month\"\\n\\nThis is using the \"alternative syntax\". Ensure that the mime-types stated match the mime-types that your server is responding with. (2692000 seconds would seem to be close to 1 month)\\nHowever, if you need to modify other directives of the Cache-Control header then you will still need to use the Header directive.\\n\\n<FilesMatch \"\\\\\\\\.(ico|jpe?g|png|gif|swf|css|js)$\">\\n\\nAside: the double backslash at the start of the regex is superfluous, you only need to escape the literal dot. ie. \"\\\\.(ico| etc.\\nUPDATE:\\n\\nI already have the ExpiresByType info set.\\n\\nThe Header directive (mod_headers) will take priority if the corresponding FilesMatch container matches. However, you say that many of these files \"don\\'t have .css and .js extensions\", so it\\'s not going to match and the ExpiresByType directives are going to take priority in these instances.\\nYou do have a conflict of interests between your mod_expires and mod_headers directives. With mod_expires (ExpiresByType) you are trying to cache CSS and JS files for 4 and 6 months respectively, with mod_headers (Header) you state a cache time of 1 month for both?! So, without a file extension present, these files are going to be cached for 4 and 6 months, regardless of your Header directive.\\nAlso, you should note that the ExpiresByType directive also sets the Expires HTTP response header (for old browsers). This is going to conflict with the Cache-Control header that you are manually setting with the Header directive. Although all modern browsers should prioritise Cache-Control over Expires.\\nBasically, you don\\'t need to use Header set Cache-Control ... if you are using mod_expires. Or, if you do, you need to make sure they are both doing the same thing.',\n",
       " \"Analytics segmentation conditions offer much more operators, than regex. If there's no specific reason behind choosing regex, you could simply go for the following condition, which matches your described requirement.\\nFilter >> Sessions >> Exclude\\nPage >> does not start with >> /blog\\n\\nIn case you prefer to or have to use regex, please find detailed description of regex in Analytics here. Also, I'd suggest to try the following expression, based on this sample:\\n^((?!\\\\/blog).)*$\",\n",
       " 'You will need to ensure that MultiViews is disabled before this will work correctly, as this will tend to conflict with your mod_rewrite directives. Add this in your .htaccess file:\\nOptions +FollowSymLinks -MultiViews\\n\\n(FollowSymLinks needs to be enabled for mod_rewrite to work, so just to be sure.)\\nThen, something like what you already have looks reasonable:\\nRewriteEngine On\\n\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteCond %{REQUEST_FILENAME} !-f\\nRewriteCond %{DOCUMENT_ROOT}/$1.jsp -f\\nRewriteRule (.+)/$ $1.jsp [L]\\n\\nI\\'ve made the trailing slash mandatory on the URL (otherwise you potentially have two URLs accessing the same content - duplicate content).\\nUPDATE: To redirect any requests to the .jsp URL to the canonical URL (ie. without the extension and with a trailing slash) then something like the following (similar to what you had in your question) would need to go before the directives above:\\nRewriteCond %{THE_REQUEST} \\\\.jsp\\\\s\\nRewriteRule (.+)\\\\.jsp$ /$1/ [R=301,L]\\n\\nThis is only strictly necessary if the .jsp URLs had been indexed or externally linked to. If this is a new site then this step is optional.\\nIt is more efficient to match what you can with the RewriteRule pattern (ie. (.+)\\\\.jsp$), rather than have a catch-all regex here. The THE_REQUEST condition ensures that this only applies to initial requests and not rewritten requests - thus preventing a redirect loop.\\nSo, in summary:\\n# Disable MultiViews\\nOptions +FollowSymLinks -MultiViews\\n\\nRewriteEngine On\\n\\n# Remove file extension from URLs (external redirect)\\nRewriteCond %{THE_REQUEST} \\\\.jsp\\\\s\\nRewriteRule (.+)\\\\.jsp$ /$1/ [R=301,L]\\n\\n# Internally rewrite extensionless URLs back to \".jsp\"\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteCond %{REQUEST_FILENAME} !-f\\nRewriteCond %{DOCUMENT_ROOT}/$1.jsp -f\\nRewriteRule (.+)/$ $1.jsp [L]\\n\\nDEBUGGING: To help with debugging the above, add the following directive below the RewriteEngine On directive and check the environment variables (MOD_REWRITE_THE_REQUEST and MOD_REWRITE_URL_PATH) in your server-side code:\\nRewriteCond %{THE_REQUEST} (.*)\\nRewriteRule (.*) - [E=MOD_REWRITE_THE_REQUEST:%1,E=MOD_REWRITE_URL_PATH:$1]\\n\\nWhat do these environment variables contain when you access a .jsp URL?',\n",
       " 'The author\\nYour description seems to be about the person, not the page. And for creator, you would ideally provide a Person value (which then also allows you to add the description there). \\nHowever, the blog post author is not really the author of this page. Either omit the creator on this level and add it to each blog post, and/or (I’m not 100 % sure if this is appropriate) use about.\\nThe list\\nUsing hasPart for each blog post is possible, but I would prefer to use mainEntity with a Blog or an ItemList value (as suggested in your previous question).\\nThe blog posts\\nInstead of CreativeWork, you should use the more specific BlogPosting.\\nInstead of about, you should use description. (about could be used for the topic the blog post is about.)\\nIn addition to headline, you might want to use name (with the same content, just like you did with CollectionPage).\\nAs mentioned above, you might want to add creator (or author) to each BlogPosting.\\nExample\\nHere I’m using about (for the Person) and mainEntity (for the ItemList). I also gave the Person a URI (via @id), and reference this URI as value for the author property in each BlogPosting.\\nIn case the use of about is not appropriate in this context, you can omit it and provide the Person node also on the top-level (by using @graph).\\n{\\n  \"@context\": \"http://schema.org/\",\\n  \"@type\": \"CollectionPage\",\\n  \"name\": \"Articles by: Stepanie Schaefer\",\\n  \"headline\": \"Articles by: Stephanie Schaefer\",\\n\\n  \"about\": {\\n    \"@type\": \"Person\",\\n    \"@id\": \"http://www.cheapflight.com.uk/authors/stephanie-schaefer/#i\",\\n    \"name\": \"Stephanie Schaefer\",\\n    \"description\": \"Stephanie is a Boston native who loves to find ways to escape New England winters. She’s thrown a coin in the Trevi Fountain, sipped wine on a vineyard in Northern Spain and swam in the Mediterranean Sea. Although she hasn’t been everywhere, it’s definitely on her list.\"\\n    },\\n\\n  \"mainEntity\": {\\n    \"@type\": \"ItemList\",\\n    \"itemListElement\": [\\n      {\\n        \"@type\": \"BlogPosting\",\\n        \"name\": \"Top 10 booze-infused getaways\",\\n        \"headline\": \"Top 10 booze-infused getaways\",\\n        \"description\": \"Vacations are all about letting loose, so it’s no surprise that more and more travelers are opting for locations known for their libations. Whether you’re a former keg stand ..\",\\n        \"url\": \"http://www.cheapflight.com.uk/news/top-10-booze-infused-getaways/\",\\n        \"dateCreated\": \"2016-04-08\",\\n        \"author\": {\"@id\": \"http://www.cheapflight.com.uk/authors/stephanie-schaefer/#i\"}\\n      },\\n      {\\n        \"@type\": \"BlogPosting\",\\n        \"name\": \"Another post\",\\n        \"headline\": \"Another post\",\\n        \"description\": \"Another post description.\",\\n        \"url\": \"http://www.cheapflight.com.uk/news/another-post/\",\\n        \"dateCreated\": \"2016-09-08\",\\n        \"author\": {\"@id\": \"http://www.cheapflight.com.uk/authors/stephanie-schaefer/#i\"}\\n      }\\n      ]\\n    }\\n}',\n",
       " \"If its just localising the same content I'd lean towards keeping the same domain.\\nPartly becuase it helps a lot with keeping analytics much more simple, and it will reduce Backlink dilution too. To separate the pages, look into using hreflang.\\nIf its a super low competition niche though and you have limited resources, separate sites is not too bad and there are benefits to EMDs. part of that benefit is that the branded back links you get with your URL as the anchor text will provide slightly more benefit... but remember you will have to work twice as hard to generate two sets of backlinks, one for each region.\",\n",
       " 'You will find a lot of comment on this matter by doing a simple search of url shortener seo on Google.\\nYou will find that much of the information is a few years old and quote Matt Cutts who has been gone for years now. For this reason, I recommend caution. Any tactic that has existed for a period, would by now, be abused and targeted for scrutiny.\\nHere is a list from MOZ:\\nhttps://moz.com/blog/the-benefits-and-pitfalls-of-url-shorteners\\nDoFollow links of this type work like any other link with a 301 redirect. There is no difference. However, I must caution that these URL shortner links can be spam none-the-less and are subject to the same criteria that any other link is subject to.\\nIf you are intending to create a link campaign around URL shortner links, I would advise against it. Google still prefers organic links. Instead, I would advise seeking links within content as you would for any other link campaign. If some of these happen to be shortened URL links, then so be it. What is most important for a link is the context of the content surrounding the link, the link text, the relevancy of the link, etc. Seek these and do not target a specific link type, then you should be okay. However, do not over do it. Links should appear to be organic.',\n",
       " 'The framework is:\\n\\nGenerating the page internally for every request\\nUsing ETag headers with a content hash function to compare the generated page with the version sent before\\nEither sending the full page, or a \"Not Modified\" response based on the hash comparison\\n\\nBecause your content is database generated, the framework has to assume that it is dynamic and can change with every request.  Therefore it must execute all the queries and generate the page before it compare to the version the client actually has.\\nI\\'m not familiar with Express, but most frameworks have a mechanism where you can cache the results of SQL queries for some amount of time to reduce the load on the database.',\n",
       " 'Simple answer. Registrars can charge what they want.\\nWant more? Ok.\\nBecoming a registrar requires a HUGE investment in network bandwidth, equipment, backups, personnel, fees, company website creation, and so forth. Because of the requirements both from ICAAN and the public at large, any new registrar cannot be a small effort. Any new .TLD such as .gofishing (just making one up), will require recovery of expenses as quickly as possible to recover as much as possible the initial investment. Why? Businesses must become profitable quickly. For registrars, large investments become yearly expenses such as renewals with ICANN and any equipment and network upgrades along with expansion of personnel. If the initial recovery of expenses comes too slowly, then the second year expenses can tip the balance sheet for the business into a non-recoverable scenario.\\nThis means that established registrars have an advantage. While new registrars must scramble to recover initial investments within the first one or two years and not stretch too far into years three and four, established registrars have the luxury of having already paid back their initial investments and having built an infrastructure that only needs to be maintained. This allows an established registrar to lower prices attracting more customers while remaining highly profitable.\\nAlso consider the market. For example, for .com, there is higher market value for this space. Why? because .com sites have more value and most people would rather have a .com site at least for their primary site. For .TLDs such as .gofishing, the market would be much smaller. This means that the registrar with such a strong and vast market for .com sites, can offer them at a lower price due to volume and demand and .gofishing with limited demand will not find profitability as easily if at all.',\n",
       " 'It’s a good practice to make URLs browsable. But for usability, not necessarily for SEO.\\nI think there is no reason to assume that there will be any kind of SEO consequence. Most bots will probably never try to visit new URLs by removing path segments from right to left, but even if some bots try it, they can hardly expect this to work for all sites.\\nSo as long you don’t link to the non-existent page, your SEO should be fine. \\nBut you might still want to consider doing this for usability reasons. You could create /sectors (which outputs an automatically generated list of links to all sectors), and add a meta-robots element with noindex, without linking this page in the menu. If that’s not easily possible, you could consider redirecting from /sectors to / (in case the homepage is the go-to page for a list of all sectors).',\n",
       " \"You need to go sign up with Google's Webmaster Tools. Google Webmaster Tools\\nOnce signed up, click add property. You will need to confirm ownership in 1 of 2 ways. File upload to root folder of website, or hosting login via Webmaster Tools.\\nOnce confirmed. Add property, then go to the crawl tab on the left and select fetch and render. Once fetched (usually within minutes) You will see a Submit to Google Index. Click submit.\\nI would also suggest submitting a sitemap when possible.\\nIt will message you with any/all errors.\\nIt is essential to use for all developers.\\nText tutorial can be read here: Siteground\",\n",
       " 'First, parallax is a visual effect. What you\\'re asking about is a single \"infinitely\" scrolling page, or Single Page Application.\\nIf I\\'m understanding correctly, you don\\'t want to use pushState because you think you\\'ll lose analytic data. I\\'m not sure why you think that: Google Analytics describes set-up for SPAs here. \\nThat being said, if it\\'s a \"really big page\" and SEO and analytics are critically important, I\\'d question whether an SPA is the best approach. Aside from the engineering problems, search engines other than Google still struggle to crawl AJAX and similar configurations. The extent to which you care about that will of course depend on search engine market share inn the market you\\'re operating in.\\nUnless there\\'s a highly compelling, make-or-break user experience argument for SPA, why go to the trouble?',\n",
       " \"My answer doesn't directly answer your question but states why you should not be doing this.\\nGoogle considers these as low quality links and may penalize you for them. Having these links will not only not help your SEO efforts, but may hinder them. Your best SEO move might be to not do this at all.\",\n",
       " \"Why not offer an OpenID login to allow your visitors to authenticate using their Google or Yahoo! (or StackExchange) accounts?  Or perhaps integrate Facebook login, if your site is more socially based.\\nIf it's just a personal site, I'd allow anonymous posting, otherwise people are unlikely to bother with the hassle of setting up an account.  Use Recaptcha (or similar) to cut out the spam.\",\n",
       " 'Google views duplicate pages for each city as low quality \"doorway\" pages.   It gives it as en example directly in their webmaster guidelines: https://support.google.com/webmasters/answer/2721311?hl=en\\nYou can have a page for each city but you have to make it unique and usable.   It can\\'t just have a single action such as \"Click here to go to our home page\" or \"click here to contact us\".  \\nTo make it higher quality and acceptable to Google you can add:\\n\\nDelivery or travel charges specific for that city\\nPrices specific to that city\\nTurn around time that may be specific to that city\\nA map of your location relative to that city\\nStats about how often you serve the city\\nReviews or testimonials from users in that city\\n\\nThe reviews and testimonials is probably the easiest way to get a large amount unique and relevant content onto the page.',\n",
       " \"I've used GoDaddy's unlimited (and cheap) monthly plans, and a basic Wordpress site with more than about 3 users clicking around at the same time will give overload warnings (GoDaddy will email you if your server is using most of it's resources). \\nYou can always view charts of the load on your server using the apps in your cPanel at GoDaddy. Also, you can do a free load test up to 25 users at www.LoadImpact.com and watch your GoDaddy shared server likely get totally overloaded and sluggish. \\nSo having 6-8k users is definitely going to be far from any unlimited cheap plan that GoDaddy/HostGator and others may offer you. But the only way to be sure is to use scripting services that simulate load (like mentioned above) and go from there.\",\n",
       " 'RewriteRule ^dir/(.+?)(-[0-9]+)?$ oldwebsite.com/dir\\nRewriteRule (.*) newwebsite.com/$1 [R=301,L]\\n\\nNot sure what you thought these directives were doing, or why you were using two directives and rewritting to the oldwebsite in the first one?\\nIf you need to redirect to a different host, then you must specify an absolute URL, including the scheme, in the substitution. eg. http://newwebsite.com/.\\n\\nBut the last rule takes control on first one\\n\\nYes, it would, since you don\\'t include the L (LAST) flag on the first RewriteRule and the second rule matches everything. RewriteRule directives chain together (the output of the first directive is used as the input for the second, etc.), so you need to stop that from happening.\\nHowever, you presumably need these to be external Redirects, not internal rewrites.\\nTry the following in the .htaccess file in the specific subdirectory of oldwebsite.com that you want to redirect from (eg. example.com/dir/.htaccess).\\nRewriteEngine On\\nRewriteRule ^(page-.*) http://newwebsite.com/newdir/ [R=301,L]\\n\\nThis redirects all pages that start with \"page-\" in the subdirectory (eg. dir) to http://newwebsite.com/newdir/.',\n",
       " \"Google penalizes for text that is not visible to the users.  White text on a white background can be used for keyword stuffing.  In that case the keywords are put in the page source where Googlebot indexes them, but the font color makes it so that users don't see it.\\nShadow text is not cloaking because the user can see it clearly.  There is no risk of penalty of using CSS text-shadow.\",\n",
       " 'It is neither odd nor risky.  It is actually quite common practice.  \\nThe only thing about it is that it can create confusion for people who are inexperienced with the way the system works.  I have a web hosting business and encourage my customers to have everything under one roof for that very reason.\\nBut if you know what you are doing...',\n",
       " 'For wamp 2.5 on Windows, use a text editor, e.g. notepad++ to edit c:\\\\wamp\\\\bin\\\\apache\\\\apache2.4.9\\\\conf\\\\httpd.conf\\nChange DocumentRoot \"c:/wamp/www\" to DocumentRoot \"c:/my/new/path\" (Note slash direction). This will change the location where files are served from (~Line 230).\\nChange <Directory \"c:/wamp/www\"> to <Directory \"c:/my/new/path\"> (Note slash direction). This applies permissions from the old directory to the new one (~Line 252).\\nUpdate: Is the F drive a data drive, a system drive or a partition? If it is not a system drive it is pretty much impossible to install the server there, since there is no \"exe\" support milieu on a non-system, or data, drive. If it is a system drive/partition, then you may indeed need to install it on F drive and just move over your files.',\n",
       " \"wp-content and your databases is almost enough.  I would also include wp-config.php\\nBut take note that sometimes just restoring everything doesn't always work.  It should, I can't explain why it doesn't.  But sometimes things go wrong.\\nYou could also consider one of the backup plugins, but whatever your solution, be sure to get your backups offsite.  Don't leave them on the server, keep them on your home PC or something like drop box.  If your user account disappears and your backups are on the server, you may as well have never done them.\\n[UPDATE]  When doing a backup and don't plan to do a restore in the near future, it might be a good idea to back up your entire WP install.  For example if you are on the current version 4.6.1 and there is a major upgrade to say version 5.0, that includes a database change, then your restore may not work on the new WP version.\",\n",
       " 'You need to use properties (itemprop) if you want to relate items.\\nThe itemref attribute can be used if you can’t nest the elements, but you still need to use properties. In your example, it seems that you can nest the elements, so there is no need to use itemref.\\nAs described in my answer to your previous question, you could use mainEntity with an ItemList value (and in the ItemList, itemListElement for each BlogPosting).\\nAnother option is to use hasPart. It’s less expressive than the mainEntity/ItemList way, and using blogPost (in a Blog) might be preferable, but it’s very simple, so it can illustrate the way how it works in Microdata.\\nNesting (without itemref)\\n<section itemscope itemtype=\"http://schema.org/CollectionPage\">\\n\\n  <article itemprop=\"hasPart\" itemscope itemtype=\"http://schema.org/BlogPosting\">\\n  </article>\\n\\n  <article itemprop=\"hasPart\" itemscope itemtype=\"http://schema.org/BlogPosting\">\\n  </article>\\n\\n</section>\\n\\nitemref (without nesting)\\n<section itemscope itemtype=\"http://schema.org/CollectionPage\" itemref=\"blogs1 blogs2\">\\n</section>\\n\\n<!-- don’t nest this \\'article\\' inside another element with \\'itemscope\\' -->\\n<article itemprop=\"hasPart\" itemscope itemtype=\"http://schema.org/BlogPosting\" id=\"blogs1\">\\n</article>\\n\\n<!-- don’t nest this \\'article\\' inside another element with \\'itemscope\\' -->\\n<article itemprop=\"hasPart\" itemscope itemtype=\"http://schema.org/BlogPosting\" id=\"blogs2\">\\n</article>\\n\\n(Google’s SDTT will output an @id value for this example, using the id values, but this is likely a bug. You can \"overwrite\" it by giving each BlogPosting an itemid value.)',\n",
       " \"A standard SSL certificate will do the same job at encrypting communications between the server and client whether the certificate authority has provided it for a fee or for free, and either could be considered appropriate however it is worth noting that as with any SSL certificate, you will need to ensure your server and website are configured to use it properly, and that you update the certificate whenever there are updates to supported protocols and ciphers etc.\\nIf you are not already familiar with installing SSL certificates you may appreciate paying a fee so that you can receive associated support as may be required to help you get started.\\n\\nAdditionally, Let's Encrypt SSL certificates are valid for 90 days, where typically other certificate authorities provide a certificate valid for 365 days. If you are not able to automate the certificate renewal/issuance process on your server then this will mean much more manual work for you to get and keep the SSL certificate installed correctly. The 90 days validity is deliberate on their part to encourage automation since shorter certificate lifes minimise risk.\\nRef: Let's Encrypt: Why ninety-day lifetimes for certificates?\",\n",
       " \"Bit of digging turns up this old thread from Webmaster World, another thread from ServerFault. The conclusion seems to be that it's caused by a bot looking for installations of a Bluecoat product called ProxySG to attack.\\n\\nSeems a security product called ProxySG uses requests like\\n  verify-SNL_Splash. Meant to be on the LAN side but apparently it may\\n  be possible to attack the product and the external hits you are seeing\\n  may be bot's probing to find users of ProxySG.\\n\\nGoogling the term shows a lot of odd results including some sites which have dedicated sales and advertising pages to 'notified-CompliancePage' so it seems like a common enough problem that spammers are taking advantage of the confusion...\",\n",
       " \"Sounds like a typical permission problem, you need to ensure that your permissions can correctly read and write the database sock file. \\n\\nSource\\nIf your file my.cnf (usually in the /etc/mysql/ folder) is correctly\\n  configured with:\\nsocket=/var/lib/mysql/mysql.sock \\n\\nYou can check if mysql is running with the following command:\\nmysqladmin -u root -p status \\n\\nTry changing your permission to mysql folder. If you are working locally, you can try:\\nsudo chmod -R 755 /var/lib/mysql/\\n\\nIf the above doesn't fix the problem then its likely something else, Visit Can't connect to local MySQL server through socket '/var/mysql/mysql.sock' (38) on Stack Overflow for more answers.\",\n",
       " 'fixed now\\n\\nHow was it \"fixed\"? I can guess... it seems that your website is accessible by both the IP address and the domain name. My guess is that this other website accidentally pointed their domain name at your IP address. Maybe the IP address was similar; although the IP address that their domain now points to is quite different. Either way, this does look like an accidental config error.\\nYou can protect against this sort of error with a simple canonical redirect (using mod_rewrite) in .htaccess. After the Options directive:\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} !^example\\\\.com$\\nRewriteRule (.*) http://example.com/$1 [R=301,L]\\n\\nThis basically says... if the site is accessed by anything other than example.com (eg. the IP address, or another domain!) then redirect to example.com.\\n\\nHow do I prevent other websites from ever hosting my content?\\n\\nWell, that\\'s a tricky one. The above directives protect against just that one type of issue - which is probably not how a \"hacker\" would clone a site anyway (since many sites simply can\\'t be accessed by the IP address and it\\'s easy to thwart).\\nThere are many ways a determined hacker could \"clone\" a site and host your content. And the more sophisticated are pretty much impossible to protect against preemptively, unfortunately.\\n\\n...subdomains...\\n\\nIf you have additional subdomains then the above \"canonical redirect\" becomes more complex. One way would be to make an exception for these subdomains as well...\\nRewriteCond %{HTTP_HOST} !^example\\\\.com$\\nRewriteCond %{HTTP_HOST} !^(subdomain1|subdomain2|subdomain3)\\\\.example\\\\.com$\\nRewriteRule (.*) http://example.com/$1 [R=301,L]',\n",
       " 'Here are both common and practical ways of dealing with server spikes.\\n\\nInstall server-side caching – many web development platforms build content on-the-fly, such as WordPress and Joomla. This adds work to the server every time a page is requested. There are plenty of cache plugins and extensions around for these products that will store pre-built page content for a short period of time so that repeated requests for the page don’t need to go through the whole page build process. This is simple to install and can be highly effective;\\nGet a better server – you need a server that is fit for the job. Shared hosting is fine for small traffic sites (or even medium traffic sites that are well optimised), but ultimately a bigger box with more memory, better systems software, and dedicated resources will be best. Look at a VPS, a dedicated server, or a cloud hosting service that allows you to boost the resources available to your website as and when they are needed – good, professional website hosting can be achieved at a reasonable price;\\nMake sure your content is rendered quickly – this is important regardless of the volume of traffic you’re getting. Search engines and real visitors prefer websites that load quickly, so make sure you aren’t serving huge images. Simple, but perhaps time consuming to implement;\\nUse a content delivery network (CDN) – if you have lots of static resources on your site then try using a CDN to distribute the files to various servers around the globe. This will ensure your files are served from the closest point to your visitor’s location and will remove the need for your server to do the work;\\nConsider Cloudflare – this is effectively a content delivery network, but with the added advantage that your content is optimised and they can also protect your website from malicious attacks. Setup is fairly straightforward, although there are quite a few options to consider (especially with the excellent paid pro- version);\\nTry compressing your web page content – if you could store your web pages as compressed (zipped) files and leave the unzipping to the visitor’s browser, that could really reduce the load on your site AND deliver your content quicker\\nIf you have a database-driven website (which can include websites that run content management systems such as Joomla, Drupal and WordPress) make sure that the number of concurrent database connections is high enough to cope with your expected traffic. Every time someone visits a website on one of these systems it results in database calls. Usually, there are a limited number of concurrent connections allowed. If this is exceeded the visitor will get a rather brief and unfriendly ‘Too many database connections’ message. So, increase the setting for the number of concurrent database connections as much as you can. If you don’t know where to look to increase this, try asking your website hosts\\nAsk your website hosts if they can help – if you expect a traffic spike it’s polite to let your web hosts know, especially if other websites on the server could be affected, but they might also be able to change some server settings or shift resources to help you out;',\n",
       " 'If a URL has previously been crawled (and possibly indexed) then Google will continue to crawl that URL for a considerable time after its removal - when it is returning a 404 Not Found. These will naturally appear in the Google Search Console crawl error report. Webmasters make mistakes; a 404 isn\\'t necessarily saying that the page is never coming back.\\nGoogle doesn\\'t simply get the list of URLs to crawl from the current crawl of your site, if that is what you are suggesting? AFAIK Google does not necessarily crawl a website \"level-by-level\".\\nAlternatively, you can return a 410 Gone for these pages - to send a stronger signal to Google that these pages are truly gone and are never coming back. Google is then more likely to drop the page from its index quicker.\\nOr, if you have an alternative (ie. very similar / replacement) page to redirect to then redirect instead.',\n",
       " 'After a little digging and playing around with this, I\\'ve managed to figure it out and now have a working scenario for mod_pagespeed to only work on specific domains (vhosts) on the same server.\\nMy configuration is based on a CentOs 6 build Apache 2 that runs Parallels Plesk Panel.\\nCreate a separate pagespeed conf file and store it in a location that won\\'t be a) automatically overwrote or b) automatically loaded (i.e in the existing conf or conf.d directories). In my instance, I created /etc/httpd/myconf and named the file vhosts-pagespeed.conf.\\nThis configuration file will contain themod_pagespeed configuration you wish specific domains to use (you can create as many of these as you like). For example, mine is like:-\\n#https://developers.google.com/speed/pagespeed/module/configuration#virtual-hosts\\n\\n<IfModule pagespeed_module>\\nModPagespeed on\\nAddOutputFilterByType MOD_PAGESPEED_OUTPUT_FILTER text/html\\nModPagespeedFileCachePath            \"/var/cache/mod_pagespeed/\"\\nModPagespeedFileCacheInodeLimit        500000\\nModPagespeedAvoidRenamingIntrospectiveJavascript on\\nModPagespeedLibrary 105527 ltVVzzYxo0 //ajax.googleapis.com/ajax/libs/prototype/1.6.1.0/prototype.js\\nModPagespeedLibrary 92501 J8KF47pYOq //ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js\\nModPagespeedLibrary 141547 GKjMUuF4PK //ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js\\nModPagespeedLibrary 43 1o978_K0_L http://www.modpagespeed.com/rewrite_javascript.js\\n\\nModPagespeedEnableFilters extend_cache\\nModPagespeedEnableFilters rewrite_javascript\\nModPagespeedEnableFilters rewrite_css\\nModPagespeedEnableFilters combine_javascript\\nModPagespeedEnableFilters combine_css\\nModPagespeedEnableFilters inline_javascript\\nModPagespeedEnableFilters inline_css\\nModPagespeedEnableFilters insert_img_dimensions\\nModPagespeedEnableFilters move_css_to_head\\nModPagespeedEnableFilters lazyload_images\\nModPagespeedEnableFilters rewrite_images\\nModPagespeedEnableFilters outline_css\\nModPagespeedEnableFilters flatten_css_imports\\nModPagespeedEnableFilters inline_import_to_link\\nModPagespeedEnableFilters local_storage_cache\\nModPagespeedEnableFilters collapse_whitespace\\nModPagespeedEnableFilters elide_attributes\\nModPagespeedEnableFilters remove_comments\\n\\nModPagespeedDisableFilters rewrite_javascript,combine_javascript\\n\\nModPagespeedModifyCachingHeaders off\\n\\nModPagespeedDomain *\\n<Location /mod_pagespeed_beacon>\\n      SetHandler mod_pagespeed_beacon\\n</Location>\\n<Location /mod_pagespeed_statistics>\\n    Order allow,deny\\n    Allow from localhost\\n    Allow from 127.0.0.1\\n    SetHandler mod_pagespeed_statistics\\n</Location>\\n<Location /mod_pagespeed_console>\\n    Order allow,deny\\n    Allow from localhost\\n    Allow from 127.0.0.1\\n    SetHandler mod_pagespeed_console\\n</Location>\\n<Location /mod_pagespeed_message>\\n    Allow from localhost\\n    Allow from 127.0.0.1\\n    SetHandler mod_pagespeed_message\\n</Location>\\n<Location /mod_pagespeed_referer_statistics>\\n    Allow from localhost\\n    Allow from 127.0.0.1\\n    SetHandler mod_pagespeed_referer_statistics\\n</Location>\\n</IfModule>\\n\\nYou\\'ll then need to disable the global mod_pagespeed and only include the vhost specific pagespeed config on those domains which you wish to enable mod_pagespeed.\\nNavigate to /var/www/vhosts/domain.com/conf and create the file vhost.conf with the include (this is all this file needs to contain):-\\nInclude /etc/httpd/myconf/vhosts-pagespeed.conf\\n\\nDisable global /etc/httpd/conf.d/pagespeed.conf by modifying like so:-\\nModPagespeed off\\n\\nThen restart the server:-\\nservice httpd restart\\n\\nNow check that mod_pagespeed is enabled on the domain.com that you extended the vhost to include the custom pagespeed conf.\\nIf that doesn\\'t work, you may find you need to reconfigure the vhost for domain.com with something like:-\\n /usr/local/psa/admin/sbin/websrvmng --reconfigure-vhost --vhost-name=domain.com',\n",
       " 'I always go for Page Name first:\\n\\nAlmost every website these days have individual icon (favicon.ico or similar) -- this alone enough to identify which site it is (unless you opened few sites which share some generic icon or no icon at all).\\nWhat is more important (from user point of view) -- site name or which page he/she is on?\\nWhen you have few tabs of the same site opened you really would like to know in advance on which tab you need to click (just take this site as an example -- open few questions and see how its done and compare how it would look like when al of them start with the same text).\\n\\nThe above I would apply to almost all sites that I use on regular basis. But if site is small or where it is quite unlikely that user will open few pages at a time (for example: \"business card\" type of website) then it is not that important what goes first: title or site name.\\nSpecial case is the home page -- I would say do it the way suits better for you. Sites where I was working on (mostly e-commerce) -- most of them have Company name first following by some slogan (e.g. \"AwesomeSoft - UK supplier of Adobe, Microsoft ....\".)\\nI\\'m not sure which one is more beneficial for SEO (if it makes any difference at all), but I would guess Page Name first still be preferred.',\n",
       " 'Interaction Events do influence the bounce rate.\\nAnalytics defines a bounce as someone loading your site and then leaving with 0 interactions. Consider a visitor landing on your article page, clicks the \"read more\" button and then exits the browser.\\nIf you attach a GA event to clicking the \"read more\" button and set it to be an Event without interaction = true then this would count as a bounce. If Event without interaction = false is set, then this user would not count as a bounce because he had an interaction on the site. \\nA good example where you want to use non-interaction events are \"scroll depth tracking via events\". That way a user coming to your site and scrolling all the way to the bottom, then leaving without a subsequent pageview it would be recorded as a bounce as GA intends it.\\nEvent without interaction has no affect on subsequent pageloads or how the browser handles anything. Loading a second page would automatically make the session a non-bounce session since loading page #2 is considered an interaction with the site.',\n",
       " \"First verify that the nameservers are right. Then in cPanel under Addon Domains make sure it's not in there, it is then delete it and re-add it. If you still get the error you need to contact your host. There is something mis-configured in your cPanel account that they can easily fix.\",\n",
       " 'With Google Analytics, a hit is ANY request sent to the GA data collection system. This includes pageviews, events, custom variables, measurement protocol uploads, etc.\\nThis is what is defined as a pageview and I think covers your questions, ultimately.\\n\\nDoes Google Analytics count another page view for index.html?\\n\\nYES:\\nPageview -\\nA pageview is recorded every time a page is viewed. Or, more technically, a pageview is recorded every time the Google Analytics pageview tracking method is executed. When a visitor hits the back button, a pageview is recorded. When a visitor hits refresh, a pageview is recorded. Every time a page is opened in the browser, regardless of whether it has been cached, a pageview is recorded.\\n\\nI hope an external referrer is only counted once in a session, so the back-navigation will not count as another visit from google?\\n\\nCounted Once Per Session: The original referrer is the first referrer in a visitor’s first session, whether internal, external or null.\\n\\nA visit consists of a series of pageviews that a single visitor makes during a period of activity. A visit ends after the visitor either closes the browser, clears cookies, or is inactive for 30 minutes.\\n\\nBut what will be in the previous page path for that visit?\\n  Can it make the connection index→page1→index? or is the navigation path broken?\\n  Does google use direct as the source?\\n\\nSince Google Analytics cannot tell ahead of time what page you will be visiting next, it simply keeps a two page relation on every page visit. The current page you are on (Next Page Path), and the page you were on before getting to the current page (Previous Page Path).\\nFor instance if you come into a site on a home page, while you are on the home page, you will have your Next Page Path set as the home page. The value of the Previous Page Path in relation to the current page you are on, is always the page you visited before coming to the current page, or it will state that it was an entrance. This leaves us with the simpler structure below:\\nPrevious Page Path → Next Page Path\\nBy setting up a custom, Flat Table report with both Previous and Next Page Path as the dimension choices you can use the relation between these dimensions to gather some very useful information. If you’re curious which page users come from to get to a certain page or which page users go to after visiting a certain page, than this report is perfect for you.\\nBy using the advanced search function, you can set the Previous Page Path dimension to the page you’d like more information on. With the Previous Page Path set to a specific page, the Next Page Path dimension values become a report of pages that users visited after that page. Alternatively, if you set a specific page as the Next Page Path, the Previous Page Path value becomes a report of the pages users visited before coming to the specified page.\\nThe Next Page Path and Previous Page Path reports allow you to see common user paths through pages on your site. They are very powerful when understood and used properly. With a little effort, these reports can help you determine if your users are moving through your site as expected, allowing you to improve the user experience in ways that were not possible before.\\nFor all analytic definitions and effects, or causes. You can learn more here: http://www.analyticsmarket.com/blog/google-analytics-definitions',\n",
       " 'It is normal to see abnormal bot behavior, in the sense that numbers do not seem to make sense. The boards are full of questions addressing many variations of similar situations. \\n2 things to do if you have not done one, the other or both.\\n\\nSign up with Bing webmaster tools: http://www.bing.com/toolbox/webmaster\\nControl how Bing crawls your site: https://www.bing.com/webmaster/help/crawl-control-55a30302\\n\\nCrawl Control -\\nThe Crawl Control feature in the Configure My Site Section allows you to exert control over the speed at which Bingbot makes requests for pages and resources on your website. You can tell Bingbot to crawl your site faster or slower than the normal crawl rate for each of the 24 hours in the day. This way you can to limit Bingbot activity when your visitors are on your site and allow us more bandwidth during quieter hours.',\n",
       " 'Yes it does count those visitors as well. To help you better understand how they are categorized, visits are sorted into: \\n3 Basic Traffic Mediums\\nThe Google Analytics Traffic Sources section categorizes your site traffic as \\n\\nDirect traffic\\nReferring traffic\\nSearch engine traffic\\n\\nDirect traffic. represents those visitors that arrive directly and immediate on your site by: \\n\\nTyping your URL into the browser’s address bar\\nClicking on a bookmark\\nClicking on a link in an email, SMS, or chat message\\n\\nDirect traffic is a strong indicator of your brand strength and your success in email or text message marketing. \\nDirect traffic can also be an indicator of offline marketing success.\\nReferring traffic. Referring site traffic, which is sometimes called referrer traffic or referral traffic, counts those visitors that click a link on another site and land on your site. Referral traffic can be indicative of social media marketing success.\\nSearch engine traffic. Search engine traffic is that traffic that comes from visitors clicking on links on a search results page for any search engine — whether Google, Bing, Yahoo!, Blekko, or similar. This traffic source is divided into organic or non-paid search engine traffic — meaning that the visitor clicked on a so-called natural search result — and CPC or paid search engine traffic, which is the traffic you purchase (via pay-per-click ads_ from search engines. Search engine traffic usually indicates that you have good or at least reasonably good content. It also can mean that you have chosen a good software platform. Be sure to learn which keywords are driving this traffic. Multi-channel merchants, as an example, may find that their brand name is a key search term. When this is the case, offline marketing is usually the real traffic driver.',\n",
       " 'I\\'d like to have this new domain because I\\'d like to have \"awesome\" and \"page\" as keywords in the URL and not only in the name of the page.\\n\\nGoogle puts little to no weight on keywords in the domain name at this point.   Several years ago it appeared to be major ranking factor.   Google changed their algorithms.   Now it doesn\\'t help and it may even hurt.\\n\\nThe main goal should be to be present, on google SERPs, with both URLs.\\n\\nNo matter how you do it, Google is only going to index your page under a single URL.     Depending on how you host it, you can choose which URL Google indexes.   If you have the content up on both URLs, Google will detect that it is duplicate and choose only one to put into the search results.\\n\\nI\\'d like to have only the domain name and redirect the traffic on the original page\\n\\nThis sounds backwards from your chosen redirect solution.   When implementing a \"domain forwarding\" redirect at the domain registrar, the new domain will redirect to your existing page.  It sounds like you had wanted it the other way around.\\n\\nOn the redirect manager, I\\'ve been asked to choose between \"keep the URL on the browser bar\" or to redirect the user to the page with the address changed\\n\\n\"keep the URL on the browser bar\" implements a \"framed redirect\" whereas not doing so will likely issue a 301 or 302 redirect.   Framed redirects introduce problems when users click on links in the framed page.   At that point the URL in the browser bar still doesn\\'t change.   If you are going to use a framed redirect you probably want to put a <base target=_top> tag in the head of your page so that all links on the page break out of the frame set.\\nNo matter what type of redirect you choose here, Google will continue to index your page at the original URL.   Only people who type in your new domain name will be able to take advantage of it.\\nIt sounds like you might actually want to host your page under the new domain name.   As you note, you might have to buy more hosting for that.   Moving a page to a new domain name is also likely to kill your Google rankings.   Google has tools to help you move a whole site to a new domain, but doesn\\'t support moving only a page.\\n\\nDo I need to add this new domain to GWT? \\n\\nIf you are only redirecting the domain, there is no great benefit to having it in Google Search Console.',\n",
       " \"I am not sure what you are looking for, but here goes.\\nIt sounds like it comes down to content removal, what it takes to have content removed and somehow does court ordered removals play a bigger role in having Google cooperate, then the answer is NO.\\nMost reputable or ethical online businesses of all sorts especially the Googles, Yahoos etc. have a way for anybody to request removal content for a variety of reasons based on infringement.  \\nIt is called the The Digital Millennium Copyright Act (DMCA) is a United States copyright law that implements two 1996 treaties of the World Intellectual Property Organization (WIPO).\\nWhat's left is most likely some other form of either illegal activity, harassment or privacy matters, and those can be reported in most cases and the company will act.\\nYou do have to file each report at least 2 times. One to the website hosting the content and one for Google. Each complaint/report must contain a Legal contact info, seperate infringement and removal request. \\nIn the end, does the Court have the ability to order removals, sure but so can you and I and we are taken just as serious, and I have filed for copyright infringements online about 50 times.\\nWhat a court can do is subpoena records, which can do through the free information act, and all other matters, well... through the court.\",\n",
       " 'Below is my personal view of what I see/think and found out in 10 minits taking time and yes I am maybe totally wrong but...it could/should be much faster for a page with just 425KB.\\nI checked your site gtmetrix (4.87sec) which is not that bad at all imho (could be worse ugh).\\nI also checked your site with pingdom(3sec) and at webpagetest(6sec).\\n(click on the links to see the results)\\nYou are having only 4 plugins (which is awesome lol) and the landingpage is full loaded 425KB first view and only 40KB for the next views which seems to be okay. The Dynamic page is generated in 1.426 seconds and 40 HTTP requests are not that bad neither.\\nYou could check some by disabling one of the 2 plugins you use which could bite each other (and also a check with both disabled). \\nDeactivate first WP Minify and do checks, deactivate then also WP Super Cache and check.\\nEnable then WP Minify again and do another(3rd) check. What happens? (are the results good or still lousy?) \\nIf neither of the 3 checks makes it faster then 3 seconds and there are no other errors in your opinion, it could be some issues having with the server (shared hosting server it is), and/or it is showing the bad side of \"crowded\" webserver which shares with a lot of other users. Another result ugh.. you are on a server with ...  2026 domains hosted on 1 server...ouch.\\nI don\\'t want to be picky but that minifying (looking at your css code) is kinda...lousy done. (assuming it is done by that plugin you use ofcourse)\\nBut as said, I can be totally wrong.',\n",
       " \"Although I'm not 100% sure, but I'd guess, it is not possible to apply such limitation. \\nFirst of all, this support article describes user rights, and nothing is mentioned or linked about further settings on top of detailed user levels.\\nIn practice, I think it'd be quite hard to maintain such access levels, as most of the reports can have primary dimension changed, or combined with dozens of applicable secondary dimensions. This means, that narrowing access to a specific report still provides access to a wide range of metrics and dimensions.\\nBased on your business needs, you might consider creating additional views for example with subdomain filtering, traffic source filtering, customer location or technology, etc. This can introduce a limitation to relevant or restricted set of data, and permissions can be granted for specific views only.\",\n",
       " 'In the end it was a misconfiguration in Google tag manager that I found with the help of Margarita\\nFor any tag you could configure a tag that would be fired along with it.\\nWe had the page view tag also fired when a click tag was fired.\\nGoogle changed the ui three times since we created that tag, and back then it seemed like a precondition for the tag.',\n",
       " 'Im going out on a limb and since there is no word of it. I will answer based on experience.\\nSubmit a sitemap > then fetch as Google (usually takes a couple of minutes) > submit to index.\\nCreating a sitemap in Wordpress plugin\\nI have had a client ask me to use index, nofollow\\nThey were crawled, then asked me to change to index, follow and it was updated within 1 week. \\nI have had sites live for long periods, without submitting a sitemap and only the home page was indexed.\\nOf course nothing is 100% when Google is concerned but you can narrow it down rather quickly.',\n",
       " \"The general rule of thumb is you don't want to change URLs. Changing URLs means links pointing to those pages and bookmarks referencing those pages can break, leading people to an error. From an SEO standpoint, you also lose any authority assigned to that particular URL (like links referencing that URL) if you suddenly remove that URL from your website.\\nYou can get around that with redirects. Typically, though, for best SEO and simpler management, you want those redirects to be permanent (a 301 redirect). In your case it doesn't sound like permanent redirects would make sense as you need to continually have a different URL. You could use temporary redirects as you change the URL, but that could become a management nightmare.\\nSo, what pages do you need - one URL or three for each type of sale? Well, a couple questions for you here:\\n\\nDo these sales overlap or do they reach run at separate times of the year?\\nAre the sales being targeted to the same general audience (should the same people see all three sales)?\\n\\nIf the answer to all three is yes, and my guess is that the answers probably are yes, then I'd think you can go with a single sale page. The content of that page changes during the year as the sale changes. That way people know to bookmark and return to this page to see the latest sales.\\nIf the sales overlap and you can't have the same page talk about multiple sales or if the sales are targeted at different groups of people, then maybe you do need to have a separate page for each. In that case, you would want to keep the URL active for each sale, without changing the URL. Instead, you would just change which sale is linked to from elsewhere on your site when a particular sale is active. The inactive sales would still be pages on your site, with a consistent URL, but would not be heavily promoted when the sale is inactive.\\nThe first option, with a single page, is certainly a cleaner way to go and hopefully that makes sense given the audience targeted.\\nI hope that helps!\",\n",
       " \"I wouldn't say that is a blackhat practice because it depends on why you would redirect this domain to your own site.\\nYou can find types of response in this article. Extract:\\n\\n1. Buying Expired Domains: Don’t Expect Credit\\nHave you picked up a domain that was once owned by someone, not\\n  through buying it directly from them but because it had expired and\\n  went back into the common pool of domains for purchase by anyone?\\n  That’s an expired domain – and chances are, the backlinks aren’t going\\n  to pass credit according to Matt’s statement.\\n2. Buying Domains & Redirecting Links: Probably No Credit\\nDid you see a tasty domain and think it would be nice to get it, in\\n  order to obtain its links for a different site, such a redirecting\\n  them? Sounds like there’s a good chance that Google is going to notice\\n  the purchase date, take note of the redirection as well and decide\\n  those “historic” links shouldn’t count. What about if you just paid\\n  someone to keep the domain going under their name but closed down any\\n  existing content and point to another location? Might work; then\\n  again, Google might note the change, the oddity of one site to\\n  completely point at another, and it might be that the links won’t\\n  count.\\n3. Buying Domain & Running Web Site As Usual: Credit Likely\\nDid you buy a web site from someone else and are maintaining the\\n  business on that site as normal? Despite the fact that your domain\\n  name registration will have changed, since the site is carrying on as\\n  usual, there seems to be a good chance that link credit will continue\\n  as normal.\\n4. Getting Domains Through Acquisition: Credit Likely\\nHave a domain that changes hands, due to a company acquisition –\\n  company A buys company B? You should be OK, thought it’s unclear how\\n  Google tells the difference here from an ordinary transfer. And no,\\n  Google wouldn’t share more on how they can tell the difference.\",\n",
       " 'You can\\'t solve the problem with DNS records.  DNS is not capable of issuing redirects on its own.    DNS records simply associate a domain name with an IP address.   There is no way to use a DNS entry to configure an HTTP redirect.  \\nTo issue redirects, you would need to configure the web server software to do so.    This would normally be done in your .htaccess file, but I\\'m not sure what type of control Shopify gives you in this case.\\nGoogle is smart enough these days to realize that an IP address with duplicate content is not your preferred canonical URL.    When Google detects duplicate content, it simply chooses one of the two to index.   In the case of an IP address and a domain name, it will choose the domain name.   This type of duplicate content is not a big deal anymore.  Fifteen years ago, it would have been a \"must fix\".\\nYour link rel canonical meta tag is a perfectly good solution to the problem.   With that in place, there is no need to configure the server to issue redirects.',\n",
       " \"You need to check the mozrank (or whichever authority measure you like) of both websites and the individual pages. If the manufacturers page is stronger than yours, and you are linking to them, then you risk helping them out rank you.\\nOn the other hand will NOT linking to them hurt your sales/conversion rate?\\nAre you using 'boiler plate' copy across a large percentage of your websites pages? It sounds you might be, which i'd consider a higher priority to find a way around.\\nIn reality however I don't think the links will matter all that much in the grand scheme, particularly if users are likely to use Geogrpahic terms in there search phrases too.\\nWorst case scenario, you launch without them, rank track some phrases, then add the links in. Observe the difference, act accordingly.\",\n",
       " \"The real filename would be exactly the same filename you originally named it. Your only limitation is that set out by the operating system and the filesystem. \\n\\nOK, so looking at Comparison of file\\n  systems if\\n  you only care about the main players file systems: \\n\\nWindowz (FAT32, NTFS): Any Unicode except NUL and /\\nMac(HFS, HFS+): Any valid Unicode except :\\nLinux(ext[2-4]): Any byte except NUL\\n\\nso any byte except NUL, /, : and you can't have files/folders\\n  call . or ..\\n\\n(Source)\\nApplications may use CHARSETS but this isn't renaming the file locally, its simply translating the file, the original file renames unchanged.\",\n",
       " 'As closetnoc suggests in comments, the 50,000 URL limit for sitemaps refers to the number of URLs in the sitemap file itself. ie. the number of <loc> elements. This is an individual sitemap limit, not a website limit. (The file must also be no larger than 50MB*1 (uncompressed) - so whichever comes first.)\\n(*1 Previously 10MB.)\\nThen you can also have a sitemap index document which itself has a limit of 50,000 sitemaps. And Google allows you to submit upto 500 sitemap indexes - so the actual URL limit that can be submitted in sitemaps is quite enormous. Of course, you don\\'t need to submit all your URLs in sitemaps for them to be indexed.\\nReference:\\n\\nSitemap (Google) - https://support.google.com/webmasters/answer/183668?hl=en\\nSitemap Index (Google) - https://support.google.com/webmasters/answer/75712\\nSitemap Protocol - http://www.sitemaps.org/en_GB/protocol.html\\n\\nRegarding Sitemap Index documents... Note that there is an inconsistency on the Sitemap.org website itself. Whilst the Sitemap Protocol page states, \"Sitemap index files may not list more than 50,000 Sitemaps\". The FAQ page states, \"A Sitemap index file can include up to 1,000 Sitemaps\" (which is what Google apparently used to support sometime before 2010). Curious that both pages appear to have last been updated at the same time... 27 Feb 2008.',\n",
       " 'For responsive purposes, you can use the devices resolution conventions:\\n\\nBy using these dimensions, you cover a maximum of devices.',\n",
       " 'You can do this using the .htpasswd method via .htaccess file. You can see how to use it here: \\n\\nPassword Protection with htaccess\\nWith .htaccess it is very easy to password protect a folder or\\n  directory. The method is called htaccess password protection or\\n  htaccess authentication, and works by uploading two files called\\n  .htaccess and .htpasswd in the directory you want to password protect.\\n  The htaccess file should contain the following:\\nAuthType Basic\\nAuthName \"Password Protected Area\"\\nAuthUserFile /path/to/.htpasswd\\nRequire valid-user\\n\\nYou only need to change “/path/to/.htpasswd” with the full path to\\n  your .htpasswd. Take a look at my article on how to find the full path\\n  using PHP. Next you need to upload the .htpasswd file which contains\\n  the username and password to enter the password protected folder. The\\n  .htpasswd file should contain:\\ntest:dGRkPurkuWmW2\\n\\nThe above code will allow the user “test” to access the password\\n  proteced area with the password “test”. The text “dGRkPurkuWmW2″ is a\\n  encrypted version of the password. You will need to use a htpasswd\\n  generator to create another password. Each line in the .htpasswd\\n  file contains a username and password combination, so feel free to add\\n  as many combinations as you like.\\nThe automatic way – Use the generator\\nYou can also just use the htaccess authentication generator to\\n  create a htaccess file for password protection.',\n",
       " 'If you want the root URI / to quietly map to /geoserver, then a simple method is with an internal rewrite:\\nlocation = / {\\n    rewrite ^ /geoserver last;\\n}\\n\\nEDIT: If you want an external rewrite, use the same location block with a return statement:\\nlocation = / {\\n    return 302 /geoserver;\\n}\\n\\nThe location directive is documented here. The rewrite and return directives are documented here.',\n",
       " 'You dont just get a box because you markup your website, you get one when and if Google think you are worthy. That means you will need lots of traffic.  \\nI Googled beatsense and you already have several links under beatsnse.com so you are well on your way :) No searchbar yet tough',\n",
       " \"I found this on Heroku and now my domain works fine without the www. I am not sure what it does.\\nIn short:\\nRoot domains must be added in addition to any subdomains. The process for adding root domains is the same in the Heroku CLI:\\n$ heroku domains:add example.com\\nAdding example.com to example... done\\n!    Configure your app's DNS provider to point to the DNS Target example.herokuapp.com\\n!    For help, see https://devcenter.heroku.com/articles/custom-domains\",\n",
       " \"Link text on links within your own site does not appear to be a major ranking signal right now. Using keywords on links to other pages on your site does not seem to help those pages rank better for those keywords.\\nChoose anchor text link for interior links that are easy to use for your readers.  Using the same word to start each link looks spammy.   You might think that putting the word in would help each page rank better for those keywords, but it is more likely to get your site slapped with an over-optimization penalty from Google.\\nHiding the text from users while showing it to Google is called cloaking.   Using zero sized font, or white text on a white background is expressly forbidden in Google's webmaster guidelines.  Don't try to put the word in the link, but hide it from users, or your site will get slapped with a cloaking penalty from Google.\",\n",
       " \"This is a common problem when sending mail from a site on a shared hosting account that has its mail hosted offsite.\\nWhat your host needs to do is disable local mail delivery for your domain name. Depending on what control panel they offer you, you may even be able to do this yourself. For example, in cPanel (which you might have access to at http://example.com/cpanel, where example.com is your domain), you can navigate to 'Remote MX Wizard' under the E-mail heading (it might be named slightly differently depending on the version of cPanel your host uses) and there will be options in here to disable local delivery (the option may also be labelled 'use a remote mail exchanger').\\nThis will be different if your host uses a different control panel. Either way, your host should be able to help you with this. You might need to spell it out, because some hosting level 1 support teams tend to not immediately pick up on solutions to non-straight-forward problems ;)\",\n",
       " \"You can see what the file contains here: http://cdn.attracta.com/sitemap/728687/0.xml.gz\\nIf that doesn't reflect your sites structure I would delete it. However see answer 2 - you may be able to update it to make it accurate.\\nProbably.\\n\\nAn Attracta Starter Account\\n  ($120/year value) is bundled and\\n  integrated into the HostMonster\\n  Control Panel of every website\\n  account.\\n  http://www.attracta.com/partner.jsp\\n\\nIf you can find where Attracta is on the control panel you could get the sitemap updated I would think.\\n3 . It depends on how accurate that file is.\",\n",
       " 'When you moved to Wix to WordPress, your URLs likely changed.   You need to implement 301 permanent redirects between every Wix URL to every WordPress URL.\\nHere is an article about Wix to WordPress migration that suggests the CMS2CMS Wix to WordPress plugin for WordPress.  It appears that that plugin can set up the redirects for you.\\nDeleting and re-adding the site in Search Console will have no effect.   Without implementing the redirects, Google will not quickly process the new website structure.   Any rankings in Google search will go away.',\n",
       " \"FINAL ANSWER UPDATE\\nSo I just learned the reason the OP wants to use 301 redirect is get rid of the annoyance of recurring errors even though new site maps have been submitted and marked as fixed.\\nUse a 410 redirect to completely remove the URL's from index and recurring 404 error notices. \\nTo remove a directory, add something like this to you htaccess file:\\nRedirect 410 /path_to_directory\\n\\nHere is what Google has to say about 404 removals:\\n\\nMake removal permanent\\nThe Remove URLs tool is only a temporary removal. To remove content or a URL from Google search permanently you must take one or more of the following additional actions:\\n\\nRemove or update the actual content from your site (images, pages,\\n  directories) and make sure that your web server returns either a 404\\n  (Not Found) or 410 (Gone) HTTP status code. Non-HTML files (like\\n  PDFs) should be completely removed from your server. (Learn more\\n  about HTTP status codes)\\nBlock access to the content, for example by requiring a password.\\nIndicate that the page should not to be indexed using the noindex\\n  meta tag. This is less secure than the other methods.\\n\\nA 301 redirect for deleted pages doesn't make sense. \\nA redirect tells a user and search engines a page or site name has moved and you direct/point them to the new correct page or URL. \\nThat is not the case here. \\nA 404 error will work fine and what most people do not realize is, 404 error pages do not affect your ranking. Google simply asks that 404 error pages should at least have a link a user can still navigate with. A nice little description is better for the user (ie. the page you are looking for no longer exists and maybe have a link to the home page and or category that may be similar. \\nThat is what the real situation is and not this current page has permanently moved to this page. If it doesnt add up to the user it wont to the search engine as well. \\nIf you change your domain name, a 301 redirect is the right method.\\nWith custom 404 pages you can have fun or be creative and get a chance to use CTA's. It is more engaging than redirecting them from where they thought they were going, to someone imposed on them.\",\n",
       " \"With websites that have smaller amounts of content, I suggest taking advantage of the many suggested practices Google recommends and offers. \\nUsing both anchor text and page based URL's can be beneficial.\\nAnchor text\\n\\nBoth users and search engines like anchor text, especially one that is concise and descriptive. \\nUsed correctly, anchor text provides valuable content relevant internal links. \\nAnchor text provides natural flowing Call to Actions.\\n\\nPage based URLs\\n\\nPeople are less likely to share /#page style URLs\\nIncreased meta content placement. Use creativity to take advantage of meta: title, description, H1 headings etc.\\nUser friendly URLs. Even if it is a one page site, people look at URLs. I have heard some users say that anchor text URLs discomforts them.\\n\\nUsed together, it benefits a User's Experience, and good use of Google suggested practices.\\n\\nRich Snippets for your products: Increase click through rates and SEO ranking\\nI am sure by now we all have seen product images, pricing, 5 star ranking, search boxes, map directions showing up in SERPs. Those are extremely good ways to extend your content into the search results.\\nSchema.org provides rich snippets codes that are essentially universal.\\nI say, keep yourself open to new or different idea, stay well informed by researching your options and you will always make the best possible decision.\",\n",
       " \"It should listen on the IP address assigned to the PC on the LAN. When you're behind a router at home, the PC has no idea of outside world IP numbers.\\nRemember that you are port forwarding, not IP forwarding. Your router is using NAT (Network Address Translation) and assigning your PC its own IP, unrelated to the outside world.\",\n",
       " 'Google provides a simple yet great solution to effortlessly embed a subscribe button on any website.\\n\\nGo to Google\\'s Configure a Button generator.\\nEnter Channel / User name\\nStyle the button\\nCopy the code to be pasted on your website.\\n\\nUsing a text link to redirect a user to a Youtube Channel, where the visitor is prompted to Subscribe. That code is as follows.\\nhttps://www.youtube.com/subscription_center?add_user=GoProCamera\\n\\nreplace \"GoProCamera\" with your own username and you\\'re done.\\nRetrieving your Youtube User Id and Channel Id.\\nYoutube ID\\'s are located in the \" Overview > Advanced \" section of your Youtube account.\\n\\nTo address your other concerns.\\n\\nI am using the meta-refresh method but found here that is not the best practice, SEO-wise.\\n\\nThe meta-refresh syntax should not be used in this case. \\nWhat is meta-refresh good for? How about I tell you how deceptive usage has caused concern for this method. \\nYou visit a site, and 20 secs later, you are redirected to another page, and from there 3 pop ups appear. That can quickly annoy a user and the rest could be played out in your mind of all the ways to avoid using it improperly.\\n301 redirects and similar methods will serve no purpose here. \\nThey are usually used where a site has changed a name for instance Http to Https. Not applicable here for any reason.',\n",
       " 'As it states in the Apache docs for a mod-alias Redirect:\\n\\nThe old URL-path is a case-sensitive (%-decoded) path ...\\n\\nSo, assuming /%D7%9E%D7%96%D7%99%D7%9F-%D7%AA%D7%9B%D7%A0%D7%99%D7%9D is the actual request as sent from the client, then you will need to match against the literal, percent-decoded (aka URL-decoded), text in the Redirect directive. From your example this would be:\\nRedirect /מזין-תכנים /\\n\\n(Make sure your .htaccess file is UTF-8 encoded.)\\nIf you want to match the percent-encoded URL, as sent from the client, then you will need to use mod_rewrite and match against THE_REQUEST server variable (which is not percent-decoded). For example:\\nRewriteCond %{THE_REQUEST} ^[A-Z]{3,9}\\\\ /%D7%9E%D7%96%D7%99%D7%9F-%D7%AA%D7%9B%D7%A0%D7%99%D7%9D\\\\ HTTP/\\nRewriteRule ^ / [R=301,L]\\n\\nYou will need to enable the rewrite engine earlier in your code if not already ie. RewriteEngine On.\\nHowever, if you change to using mod_rewrite redirects then it is strongly recommended to change your mod_alias Redirects to use mod_rewrite as well in order to avoid unexpected conflicts.',\n",
       " 'A 302 response code is a temporary redirect rather than a permanent redirect. This is also not the best solution to be using in this scenario and nor are language cookies either.\\nExample.com will 301 redirect to the /en page as this is the default page. A href=lang tag will then be set on /fa and any other language versions to specify to Google that these are the language equivalents of the English page.\\nRather than using a cookie to save language selection and temporarily redirect them to example.com/language you should use the href=lang tag which lets Google determine which language the page should be set to based on the version of the website the user is visiting. For example if a users language is English and visits the /fa version of the web page this should be displayed in Iranian. \\nIn terms of the impact from an SEO perspective this will tell Google that there are different versions of the web page and will allow it to be indexed in different language versions.',\n",
       " 'http://example.com/page1/\\nRewriteRule ^page1$ ....\\n\\nYou are missing a trailing slash on the end of the RewriteRule pattern. Try the following in the .htaccess file at example.com\\'s document root. This will need to go before any existing WordPress rewrites.\\nRewriteEngine On\\n\\nRewriteCond %{HTTP_HOST} ^(www\\\\.)?example\\\\.com$ [NC]\\nRewriteRule ^page1/$ http://site.example.com/page1/ [R=301,L]\\n\\nI also removed the double quotes and escape slashes ...\\n\\nYes, no need for the plethora of backslash escapes. That looks like one of cPanel\\'s attempts.\\nAlso, no need for the <IfModule> wrapper, unless this is expected to \"work\" without mod_rewrite installed?\\nThe RewriteBase directive is also superfluous here. (Note that you should only have one RewriteBase directive in your .htaccess file - WordPress usually writes this already.)',\n",
       " \"HTTPS does not just provide secrecy (of which you are doubting the value, though there are good reasons for it still) but also authenticity, which is always of value. Without it, a malicious access point/router/ISP/etc. can rewrite any part of your site before displaying it to the user. This could include:\\n\\ninjecting ads for your competitors\\ninjecting ads or annoying widgets that make your site look bad and harm your reputation\\ninjecting exploits to perform drive-by downloads of malware onto the visitor's computer, who then (rightly!) blames you for it happening\\nreplacing software downloads from your site with ones that have bundled malware\\nlowering the quality of your images\\nremoving parts of your site they don't want you to see, e.g. things that compete with their own services or depict them in a bad light\\netc.\\n\\nFailure to protect your users from these things is irresponsible.\",\n",
       " \"I am going to give this a shot.\\nFrom a filesystem point of view:\\n\\nFAT32:\\n\\nMaximum number of files: 268,173,300\\nMaximum number of files per directory: 216\\xa0-\\xa01 (65,535)\\nMaximum file size: 2\\xa0GiB\\xa0-\\xa01 without LFS, 4\\xa0GiB\\xa0-\\xa01 with  \\n\\nNTFS:\\n\\nMaximum number of files: 232\\xa0-\\xa01 (4,294,967,295) \\nMaximum file size\\n  \\n  \\nImplementation: 244\\xa0-\\xa026 bytes (16\\xa0TiB\\xa0-\\xa064\\xa0KiB)\\nTheoretical: 264\\xa0-\\xa026 bytes (16\\xa0EiB\\xa0-\\xa064\\xa0KiB)\\n\\nMaximum volume size\\n  \\n  \\nImplementation: 232\\xa0-\\xa01\\xa0clusters (256\\xa0TiB\\xa0-\\xa064\\xa0KiB)\\nTheoretical: 264\\xa0-\\xa01\\xa0clusters\\n\\next2:\\n\\nMaximum number of files: 1018\\nMaximum number of files per directory: ~1.3\\xa0×\\xa01020 (performance issues past\\n  10,000)\\nMaximum file size\\n  \\n  \\n16\\xa0GiB (block size of 1\\xa0KiB)\\n256\\xa0GiB (block size of 2\\xa0KiB)\\n2\\xa0TiB (block size of 4\\xa0KiB)\\n2\\xa0TiB (block size of 8\\xa0KiB)\\n\\nMaximum volume size\\n  \\n  \\n4\\xa0TiB (block size of 1\\xa0KiB)\\n8\\xa0TiB (block size of 2\\xa0KiB)\\n16\\xa0TiB (block size of 4\\xa0KiB)\\n32\\xa0TiB (block size of 8\\xa0KiB)\\n\\next3:\\n\\nMaximum number of files: min(volumeSize\\xa0/\\xa0213,\\xa0numberOfBlocks) \\nMaximum file size: same as ext2\\nMaximum volume size: same as ext2\\n\\next4:\\n\\nMaximum number of files: 232\\xa0-\\xa01 (4,294,967,295)\\nMaximum number of files per directory: unlimited\\nMaximum file size: 244\\xa0-\\xa01\\xa0bytes (16\\xa0TiB\\xa0-\\xa01)\\nMaximum volume size: 248\\xa0-\\xa01\\xa0bytes (256\\xa0TiB\\xa0-\\xa01)\\n\\nReference:\\nhttps://stackoverflow.com/questions/466521/how-many-files-can-i-put-in-a-directory\\nFrom a functionality point of view:\\n\\nKeep in mind that on Linux if you have a directory with too many\\n  files, the shell may not be able to expand wildcards. I have this\\n  issue with a photo album hosted on Linux. It stores all the resized\\n  images in a single directory. While the file system can handle many\\n  files, the shell can't.\\n\\nFrom a server speed point of view:\\nToo many files in one directory can cause load times to increase by seconds. Having too many directories can also increase load times. (Your server specs play a role in this)\\nFrom an SEO point of view.\\nI can understand not having proper image names for security reasons (assuming a user uploads photos and you have a rewrite in place) But you really should give a content relevant (sub)directory names for better search ranking and the ability to partially remove URL's to get to another spot on your server. (ie photos/outdoor/landscape/mountains)\\nIn the end there is no one fits all but you can make a better informed decision based on the aforementioned.\",\n",
       " \"Those files are almost certainly meant to be served via Apache content negotiation. Since there does not appear to be a .var file to serve as an explicit type map, and since the file name extensions correspond to standard language and charset codes, they're probably meant to be used with the implicit MultiViews mechanism.\\nTo enable MultiViews for the directory containing these files, add the following directive into an .htaccess file in that directory (or into a matching <Directory> section in the global Apache config):\\nOptions +MultiViews\\n\\nThis will make Apache automatically resolve requests for nonexistent files by appending additional extensions based on the language, MIME type and charset preferences indicated by the HTTP Accept, Accept-Charset and Accept-Language headers. For example, a request for index.html might cause Apache to instead return the contents of  index.html.de or index.html.ja.jis or index.html.ru.utf8, depending on the preferred language(s) configured in the user's browser.\\nNote that, in addition to content negotiation, you should also provide some way for users to explicitly select the version of the page that they prefer. When using MultiViews, this can be as simple as providing direct links to the translated pages with the language code included.\",\n",
       " 'Based on the lack of mentioning other steps you took, it seems you might have missed a few leading to \"View results in table?\\nAdd a View Results in Table Listener\\nIn JMeter, listeners are used to output the results of a load test. There are a variety of listeners available, and the other listeners can be added by installing plugins. We will use the Table because it is easy to read.\\n\\nSelect Thread Group, then Right-click it\\nMouse over Add >\\nMouse over Listener >\\nClick on View Results in Table\\n\\nYou may also type in a value for Filename to output the results to a CSV file.\\nRun the Basic Test Plan\\nNow that we have our basic test plan set up, let\\'s run it and see the results.\\n\\nFirst, save the test plan by clicking on File then Save, then\\nspecify your desired file name.\\nThen select on View Results in Table in the left pane, then click\\nRun from the main menu then click Start (or just click the green\\nStart arrow below the main menu).\\n\\nYou should see the test results in the table as the test is run.\\n\\nInterpreting the Results\\nYou will probably see that the Status of all the requests is \"Success\" (indicated by a green triangle with a checkmark in it). After that, the columns that you are probably most interest in are the Sample Time (ms) and Latency (not displayed in example) columns.\\n\\nLatency: The number of milliseconds that elapsed between when JMeter\\nsent the request and when an initial response was received\\nSample Time: The number of milliseconds that the server took to fully\\nserve the request (response + latency)',\n",
       " \"I wouldn't worry too much because 404 errors are not penalized. It basically represents files that have been temporarily hidden. If removed, it will clear itself from the search. I usually suggest not blocking Google for any reasons.\\nAs you will see Google recommends 404 pages and noindex tag opposed to blocks.\\nMake removal permanent'\\nThe Remove URLs tool is only a temporary removal. To remove content or a URL from Google search permanently you must take one or more of the following additional actions:\\n\\nRemove or update the actual content from your site (images, pages,\\ndirectories) and make sure that your web server returns either a 404\\n(Not Found) or 410 (Gone) HTTP status code. Non-HTML files (like\\nPDFs) should be completely removed from your server. (Learn more\\nabout HTTP status codes)\\nBlock access to the content, for example by requiring a password.\\nIndicate that the page should not to be indexed using the noindex\\nmeta tag. This is less secure than the other methods.\",\n",
       " 'This is an question how you want to make it, there is nothing you must to have.\\nI recommend to have your template files outside of /fileadmin I use often /template for this.\\nAs next step you should create folders for an structure you understand.\\nMany customers try to place the files like the navigation structure, so they find the content belongs to them.\\nYou can also create folders like Pictures, Videos, PDFand so on...\\nyou dont need the captial letters. In TYPO3 they are use often camelCase, so it ends as example to picturesOfWedding or simplay pictures/wedding/john...',\n",
       " 'Googlebot now renders pages and views the page as a user sees it when it loads, including applying CSS and running JavaScript. Google will detect text that is hidden using either CSS or JavaScript.  \\nGoogle will penalize a site for hiding keyword rich text that cannot be viewed by users.   They call the practice cloaking: showing content to search engine bots, but not to users.\\nNote that hiding text that can be shown via user interaction will not be penalized.  There are many legitimate reasons to hide text initially but show it when the user needs it.   Popular interactive devices such as carousels, light boxes, and fly outs need to initially hide some content.\\nWhen text is initially hidden, but can be viewed by users through interaction, Google may choose not to index the words in that text.  If it does index them, it may give them less relevance weight than words shown to users on load.\\nIn the last few years as Googlebot has gained the ability render pages, Google has changed its guidelines regarding AJAX.  It used to be that if you wanted a crawlable AJAX website you would have to implement \"hash bang\" URLs with HTML snapshots.  Now Google can crawl AJAX websites that implement \"push state\" for changing URLs as the content on the page changes.  Push state is much easier to implement than HTML snapshots because it doesn\\'t require server side HTML production that mimics the client side JavaScript.\\nEven though Google can now crawl AJAX pretty well, if you want an AJAX site well indexed, you have to provide a different URL for every piece of content.   Single page applications where the URL never changes are still not SEO friendly because Google has no way of deep linking into the content that matches searches.',\n",
       " \"Okay. This will be one of those cases where I will work through the process so that it becomes clearer. It will be somewhat long, but hopefully not painfully long.\\nLet's start at the beginning shall we?\\nStarting with what we know about how Google works based originally with the research paper by Brin and Page back in 1997, we know a few things things that are very likely still in play today.\\nGoogle has a URL in it's index and fetch queue and fetches the page. The code for the page is stored within its database for various forms of processing. One of the processes would be to find new links. Any link that has been found by Google will be first located in the link index if it exists. It it does not, then the link will be added to the link table and added to the fetch queue.\\nAny link within the link table has at least these elements, the link URL, the source URL, and the link text. It is likely that there are other data elements, however, these do not advance the discussion. Any link that is added to the link table has a verified the source URL, but not necessarily the target URL. Using relational databases as an example, the source and target URLs could be a URL ID within the URL table and a join table would join the link table source URL and target URL elements using an ID back to the URL table. Confused? Don't be.\\nFor any case where the target page has not been fetched, the link within the link table is said to be a dangling link. Once the page is fetched, then the link within the link table is complete. If the target page does not exist, then the link within the link table is a broken link. Simple?\\nOnly complete links can pass value. The PageRank algorithm requires a complete link in order to calculate value. All dangling and broken links stops any calculations using the link. Previously, PR was a recursive process that would calculate link values using the link table over and over until the value that can be adjusted to any link falls within a numerical value that is so small that it is effectively not going to make a difference. I am sure this still occurs as a house keeping process. However, PR today is calculated using another method similar to hops in a network that measures distance from one page to another with relative importance. It is based upon the trust network model which is how the original PageRank model was designed to emulate. A link is a trust vote from one entity to another. While it gets more complicated than this, you get the picture. It effectively does the same thing as the recursive process using a more real-time calculation though likely less precise but precise enough to be reliable. This requires complete links since trust values (using the trust network model) cannot be passed if trust is not established. Remember that a link is a trust vote or link in the trust network model. PageRank is represented as a trust value in a trust network.\\nNow that you understand links and how important they are, lets move on.\\nFor a search engine, it does not make sense to remove any URL. If a URL does not exist within the URL table, then you cannot know anything about the URL and would be at a loss. URLs are likely not deleted generally unless it makes sense to, for example, if the URL no longer exists. However, when a page is set to NOINDEX, then the search engine has been explicitly instructed NOT TO index the page. Since a web page within the index consists of two things, a URL and the HTML source code, NOINDEX effectively removes the page at this point. Links to a NOINDEX page are at least dangling.\\nNow that you know what an indexed page looks like, lets move further.\\nThere are many ways that a search engine will penalize a web page or site. One is delisting. This is the most severe of all penalties and takes a long time to recover. This category of penalty you can evidence since the page will not and cannot be found. As well, Googles Search Console will, in a round about way, let you know that pages are being delisted. Of the remaining penalties, the penalties are applied in SERP filters.\\nWhen a search query is performed, there is actually several queries against the index at once that are then blended into a result set based upon a portion of the algorithm. The remaining algorithm, that we often refer to as a single entity, is a series of relatively simple SERP algorithms. The primary algorithms of which will reorder the result set based upon more real-time metrics such as trends. Of the algorithms, the ones that remove entries from the result set or seriously downgrade the placement of an entry within the result set are called filters. One which is applied is the filter that handles DMCA as evidenced with ...we have removed 1 result(s) from this page...\\nSo now that you know how penalties are applied, are links, PR, and DMCA filters connected?\\nWith this, we know a filter has been applied, however, this has nothing to do with the link index which is how PageRank is calculated. It is as far removed from the link/PR process as it can get. Links and PR are at the beginning of the of the indexing process whereas removing the DMCA penalized page is at the end of the query process. In fact, these are two completely separate engines. So while a page may be removed because of a DMCA complaint, it is not actually removed from the index and therefore links to and from the page are still calculated.\\nClear as mud? I hope that I explained this well. Please let me know if I can clarify something for you.\\n[Update]\\nAn exception that does not apply to the OP's scenario.\\n@StephenOstermiller brings up a good point that does not undermine the above, however, I would like to add it for completeness.\\nAs you well know, scoring a site or page within search requires many factors. While this is not as technical or mystical as you may imagine, it is still a lot or factors to weigh. I forgot about the effect of trust scores mostly because it did not apply in the OP's case. So I am adding it here.\\nClearly there are sites that are up to no good such as, spam sites. Within this classification of sites are sites that are habitual abusers of copyright content. This was a huge problem many years ago where content scrapers would build sites off of your hard work. For a long time, nothing was done. The sites with original content would lose out to the scraper sites fairly consistently. I should know. I had two PR 8 sites that lost nearly all of it's traffic due to scraper sites with absolutely no recourse.\\nHowever things have changed. And it has only largely been about four years since the significant changes have begun.\\nFor these special classification of sites, the sites trust score can be significantly reduced. This is well known. It takes years to rebuild trust scores and for some sites, this may never happen. Why, for example, do you think domain monetizers are so willing to thoroughly trash a site with hundreds of thousands waiting in the wings for the same abuse? It is because the reality is that a domain can ruin it's value beyond redemption.\\nThere are many factors that go into establishing trust. I will not get into that here. However, do know that trust is a major component of building rank for any site.\\nThat said, for any site that is a serious violator of the DMCA with a fairly extensive track record, would see a serious knock in it's trust score. This is not the scenario the OP is describing. However, it is the scenario I am assuming here.\\nLinks and establishing PageRank have more than one component. One is PageRank (authority) of the page itself. For highly authoritative pages, there is an authority cap. A PR 8 page will not share a value of 8 amongst the links on that page. This is part of the original PageRank algorithm intended to put a more natural curve into PR. Otherwise, it would be nearly impossible for a new page to compete against a page with high authority even after a long period of time. The value of the link itself is scored using several factors including the semantic value of the link text, link URL, location of the link (prominence), the semantic value of the content block that contains the link if it applies, etc. All links are scored from 0 to .9. The calculation of the authority and link score is the value passed by any link.\\nWell and good. So how does this effect a site that is a significant violator of the DMCA?\\nThe value of any inbound link would not necessarily be effected by the trust score of the target site since the links value comes from the source site. However, any outbound link could be. The authority of any site that is a significant DMCA abuser would be effected by the trust score. Afterall, authority comes from trust. So in this way, the value of an inbound link would not be passed through outbound links without being degraded depending upon the trust score.\\nThis changes the answer somewhat.\\nWhile it does not apply to the OP's scenario, there is a scenario where an inbound link value is not completely passed through the site with a DMCA violation. However, this is a hard case and therefore the threshold before this happens is significant.\",\n",
       " 'You seem to have a few issues with your current config file:\\n\\nYou are trying to use both mod_expires and mod_headers to set the same cache headers (but with different values?). mod_expires sets the max-age parameter of the Cache-Control header, as well as the older Expires header. This is likely to result in conflict (particularly since you are trying to set different values with each). mod_headers is likely to win. Using mod_expires is preferable. However, if you need to set specific Cache-Control headers then you will need to use mod_headers.\\nExpiresByType text/js \"access plus 7 days\"\\nYour server is unlikely responding with a text/js mime-type for JavaScript files. So, this probably isn\\'t doing anything. Check what mime-type your server is sending and specify that. It\\'s probably application/javascript.\\nThe <IfModule mod_headers.c> container is inside the <IfModule mod_deflate.c> container?\\nRemove the <IfModule> wrappers. Any errors? The <IfModule> wrappers are only required if your site should function correctly without these modules installed (if you are moving your site from server to server), or if certain directives from another module are dependent on a particular module being installed. If you use <IfModule> wrappers around everything then your system simply fails silently when the module is not installed, rather than alerting you to an error. So, it might simply be failing silently. An error is usually preferable!\\nAs Stephen pointed out in comments, if you have all these directives within the # BEGIN..END WordPress comments then they are likely to be overwritten with the next WP update.',\n",
       " 'The Expires header is the simplest. If you\\'re using Apache you can set this by mime type using mod_expires. In your .htaccess:\\n<IfModule mod_expires.c>\\n    ExpiresActive On\\n    ExpiresByType text/css \"access plus 1 day\"\\n    ExpiresByType image/png \"access plus 1 day\"\\n</IfModule>',\n",
       " 'Apologies if I cover anything you already know below. :-)\\n\\nDo I have a dedicated, private server, or not? How can [I] tell definitively? \\n\\n(Re-) Ask your hosting provider. Specifically, ask them if there is a dedicated hardware-based server that you alone exclusively control and do not share in any way with anyone else. If they say no, then you are not using a (truly) dedicated server.\\n\"Private Server\" could mean anything but often this is a term used in lieu of \"Virtual Private Server\" (VPS). \\nThese differ from \"shared\" hosting in the sense that shared hosting simply jams all the resources and process onto a single hardware device with one OS and likely little resource management. Virtual Private Servers use virtual machines to emulate dedicated hardware - so each \"fake\" server occupies the same physical server (and it\\'s resources) but acts like it is a totally separate device (complete with its own OS, memory limits, etc.)\\nVPS hosting can be better than shared in the sense that resources are potentially better allocated than with shared hosting and physical servers that are used for virtual hosts tend to be more powerful simply because hosting multiple virtual machines is resource intensive. That said, it still is generally not the same as having a truly dedicated server.\\nRegarding whether you have a VPS or shared hosting -- again, ask your hosting provider. Assuming they are honest, they should be able to tell you exactly how things are managed.\\n\\nI was told that I should code the database host as server76.XXX.net - which sounds like a sub-domain to me[.]\\n\\nRegarding the sub-domain, these are simply resource pointers. \\nThe resource itself could be a folder on a system, a web application, a single machine or a group of machines. Sub-domains in no way indicate that a resource is a \"shared\" host in the sense we are discussing here. \\nSub-domains are mapped to resources via a combination of hardware (e.g. routers, etc.) and software (e.g. Apache web server, etc.)... which why a sub-domain can point to both a single machine (e.g. somename.dynamic-home-dns.net) or multiple machines (mail.yahoo.com, translate.google.com - \\'cause we all know those services run off exactly one shared server... :-D )\\nThat said, if it is a single machine, it still says nothing about how that machine operates or whether it is physical or virtual. Likewise, it could be running all kinds of processes, it could be a dedicated database machine you share with others or it could be totally dedicated to the task of processing your database alone.\\n\\nThey won\\'t even let me telnet in (\"in case you mess things up\"), just create databases[.]\\n\\nThis isn\\'t totally out of the ordinary and unfortunately doesn\\'t give any clues about the situation either, since not all hosting providers give Telnet/SSH access to machines regardless of being dedicated servers, VPS machines or shared hosting. \\nThat said, as comments from the peanut gallery, this isn\\'t an unreasonable thing to expect from VPS or dedicated hosting and I would be sorely tempted to look at other providers which would allow this.',\n",
       " \"There are lots of websites that provide free SSL certificates (like cloudflare, letsencrypt, google firebase and blogger.com (Only for their blogs), Wordpress.com (Only for their blogs)...)  and people using it widely. And I think Google already know about that. But they still haven't announced anything on their blog about ssl certificate choices.\\nIn old days, domain sellers were telling people to renew their domain for 4-5 years so that google knows that you are serious about your business. They were telling people that they would get better rankings, so maybe this is also like a marketing scheme, notice their statement starts with we believe.\\nSome of these questions we won't be able to answer unless we hear official statements from google. But SSL is just one of the factors along with 200+ oher factors. Personally I advise that if your site is not an e-commerce site, then you don't need that kind of SSL, instead you can use that money to buy a better host or cloud and CDN to improve your website speed and user experience.\",\n",
       " 'The \"content keywords\" report is intended to monitor your site for spam.  When hackers hack a site, they often add pages for \"Viagra\" or \"Casino\" related keywords.\\nIf you see unexpected keywords in this report, then you have a problem with a hacked site and you need to take action.   If you were the one that created the content with the keywords, there is no action you need to take.\\nTo appear higher on this report, a keyword has to appear on a large number of pages on your site.   The report takes into account how common words are on sites in general so that common words such as \"the\" do not generally appear.\\nIf you want a keyword to appear higher, use it on more pages.   If you want it to appear lower, use it on fewer pages.   It does not appear to matter how many times on each page a keyword is used.  I have tested this fairly straightforward relationship that is powering this report with my own websites.\\nOptimizing your keywords based on this report will not improve your site\\'s search engine rankings.  I\\'ve tested that as well.   Even when I use a term on more pages and get it higher in this list, rankings for that term have not improved.  Google uses other factors for ranking.\\nBased on my experiments, this report is only good for spam detection.  Having words that you made up in this report should not negatively effect your site in any way.',\n",
       " 'Are they actually uploading empty files? Since they\\'re using 301 redirects, the blank pages might as well not exist. They\\'ll never get called anyway. Doing this is basically implementing bit.ly yourself with the added advantage of being able to choose your own URLs and keeping your own domain. There is nothing terrible about this at all.\\n301 redirects barely lose any \"link juice\", so links to these URLs are basically the same as links to the landing page. I\\'m not sure how Google will react to these pages in a sitemap, but again, because of the 301s I really wouldn\\'t expect any problems. Every link is telling Google it moved permanently to the new page, so only the new page should get indexed.',\n",
       " 'The definition of Event makes clear that the event has to happen \"at a certain time\", and it recommends:\\n\\nRepeated events may be structured as separate Event objects.\\n\\nIf you would use a single Event item for multiple events, it wouldn’t be possible to use many of the properties, or it wouldn’t be clear what they mean:\\n\\nattendee: which of the multiple events did the person attend?\\neventStatus: what if one showtime was cancelled?\\nisAccessibleForFree: what if one of the multiple events is free?\\noffers: for which showtime is this offer?\\nrecordedIn: from which showtime is this recording?\\netc.\\n\\ntl;dr: You have to provide multiple Event items.',\n",
       " \"I believe that FTP doesn't actually have a recursive delete operation. The options I can think of are as follows:\\n\\nUse the ncftp program. This is another FTP program with additional options. However, I think it just works on the command line and doesn't have a GUI.\\nIf you have SSH access to the server then use that instead of FTP and use the rm -r command.\\nWrite a PHP or other script to do the deletion for you.\",\n",
       " 'It seems like you did it backwards and missed a few steps. Here are the full instructions from start to finish.\\nIn Google Domains:\\n\\nClick My domains in the navbar and then click the domain name.\\nClick the DNS tab .\\nScroll down to Custom resource records.\\nCreate a new CNAME record:\\nName: The subdomain you are directing to the site (do not enter \"@\").\\n\\nType: CNAME\\nTTL: 1h\\nData: GHS.GOOGLEHOSTED.COM\\n\\nClick Add to save the record.\\n\\nIn Google Sites:\\n\\nLog in to Google Sites, and go to the site you are integrating with your domain.\\nClick on the More Actions menu in the top right corner.\\nSelect Manage Site.\\nOn the left side of the Manage Site page, click Web Address.\\nUnder Add a web address enter the address of the subdomain you are using, for example examplesite.documentation.example.\\nClick Add at the top of the page to add your address.\\nYour address should display in a list below the Google Sites address.\\n\\nIF YOU STILL GET ERRORS:\\nThe reason you get this message is that, for example,  when you sign up for Google Apps, we automatically create a site at http://www.example.com mapped to http://sites.google.com/a/example.com/www. When you try to map http://sites.google.com/site/mysite/ to http://www.example.com, this doesn\\'t work, because we\\'ve already mapped it for you.\\nHere\\'s how you can solve this problem:\\n\\nSign in to the control panel at www.google.com/a/example.com.\\nClick the Settings tab and then select Sites in the left column.\\nClick the Web Address Mapping tab, where you\\'ll see the www mapping.\\nSelect the checkbox and click the Delete mapping button.\\nClick Domain Settings tab.\\nClick the Domain names tab.\\nUnder the Primary Domain section, click the Advanced DNS Settings\\nlink, and read instructions on how to sign in to the DNS registrar\\'s\\nwebsite (either GoDaddy or eNom).\\nSign in to the DNS registrar, and map the CNAME of www to\\nghs.google.com.\\nTry to set the web address mapping of\\nhttp://sites.google.com/site/mysite/ to http://www.example.com.',\n",
       " \"Response from ICANN.\\nUnfortunately I can not.\\n\\nThank you for contacting the ICANN Global Support Center. \\nThe application window for the current round of new gTLDs closed in\\n  2012. At this time, we are still processing applications from this round and dates for the next round have not yet been set. \\nYou can stay informed of the latest program news--including\\n  announcements of future application rounds--by visiting our website,\\n  http://newgtlds.icann.org. Additionally, you can create a MyICANN\\n  account and initiate personalized up to date notifications and\\n  reminders. To create a MyICANN account, please visit the following\\n  page https://myicann.org/user/register. \\nThrough the new Generic Top-Level (gTLD) program, any established\\n  public or private organization anywhere in the world can apply to\\n  create and operate a new gTLD registry. Please note, however, that\\n  applying for a new gTLD is not the same as buying a domain name. An\\n  applicant for a new gTLD is, in fact, applying to create and operate a\\n  registry business supporting the Internet's domain name system. This\\n  involves a number of significant responsibilities, as the operator of\\n  a new gTLD is running a piece of visible Internet infrastructure.\\n  Applicants will need to demonstrate the operational, technical and\\n  financial capability to run a registry and comply with additional\\n  specific requirements. \\nFor the current round, the evaluation fee for each new gTLD\\n  application was $185,000 (USD), and additional fees may have been\\n  incurred based on the application path taken. \\nFor detailed information about the application process, including\\n  requirements and fees, see the gTLD Applicant Guidebook:\\n  http://newgtlds.icann.org/en/applicants/agb. (Note that requirements\\n  and fees are subject to change in future rounds.) \\nYou may also visit the new gTLD Frequently Asked Questions page for\\n  additional information.\\n  https://newgtlds.icann.org/en/applicants/global-support/faqs/faqs-en\\nIf you like to know more about ICANN, please visit\\n  http://learn.icann.org.  ICANN Learn is an online learning platform\\n  requested by and built for the global ICANN community.  Courses cover\\n  the basics of what ICANN does, basic web skills, how to get involved\\n  with ICANN, and more. \\nWe hope this information was of assistance to you, this case will now\\n  be resolved. Please contact us if you have additional questions.\",\n",
       " 'To answer your specific questions...\\n\\nIs it best practise to redirect everything to the home page\\n\\nNo. This generally gives a bad user experience. The user expected to see content related to X, but instead are seeing your home page. Bounce rate will be high. Consequently Google will not see this as a good thing either and generally treats (mass) redirects to the home page as a soft-404.\\nA redirect is telling users and search engines that the content has moved and is now located \"here\". A redirect to the home page is not conveying that same message.\\n\\nor do we need to redirect each page to the correct page on the new domain?\\n\\nYes. Specifically a 301 redirect to the new URL on the new domain. It\\'s the only way to maintain as much of the search engine ranking as possible.',\n",
       " 'You have to wait until Google re-indexes your website. This can usually take a few weeks to complete. Once that happens, you will see the updated URLs on Google.',\n",
       " 'Primarily I see 2 core issues you are facing,\\n1. I receive the links for the old site that aren\\'t valid.\\nThere are couple of ways you can go past this. Assuming, you have limited number of pages, performing (permanent) redirections using nginx/apache using pattern patching (or exact url). Other could be, handle the links at application level using JavaScript.\\n2. How can I reset Google Search Console (parameter url, index,...)?\\nTwo points, first, there is no \"one click remove all links\" method for Google Search. Second, through Google Search Console is a way for you to provide indication to Google about your website. The more specific you are , the better it is. In you case, if you have revamped your web app and old links aren\\'t valid then point the old links to new ones through permanent redirects. If they aren\\'t valid anymore, throw 404 errors. Over a period of time, the errors will start popping up in console, for you to track and make amends are required.\\nLastly, by \"reset\" if you meant start fresh then one possible, crude way could be to delete the property and create new one.',\n",
       " \"The truth is Google wants you to succeed.\\nLuckily, Google in most cases is fair and accurate in its ranking of those who get penalized and at the same time rewarding those who are not using any deceptive practices. \\nEverything affects SEO and Traffic Exchange programs.\\nBut how you are involved in it makes all the difference. Are you condoning misbehavior, using tactics that are obviously deceptive and manipulated or is it things out of your control. (Things out of your control still should be paid attention to as you will see further down.)\\nShould I be worried about how people link to or enter my site?\\nIf you know for a fact that you are not giving incentives for people visiting your site, not exchanging irrelavent content links and keeping your content relevant to links you publish in your content than you will be fine.\\nWhat are some things I can do if I see sites pointing to my website and I never asked them to?\\nFirst locate your backlinks (you can do that in Google Webmaster Tools) and determine if they are quality links. If they are links from trusted sites and relevant to your content then great! If not you can contact the site owners and ask them to remove the backlinks and if not, Google does offer a way to disavow a link to your site. So in turn you are telling Google that hey, I do not want the link to be associated with me.\\nI only share links with other site owners and not traffic exchange programmes, is that ok?\\nIF the site is relevant to your content or users do in fact find relevance in sites that appear to be not relevant.\\n\\nYou should not have a site about Tires and exchange/post links to a site about Pets. (Unless it's in an AD form and you make it clear that it is such. IE. Support Your Local Pet Rescue)\\nThe same principles apply to those linking to your site.  \\nA good example of links appearing to be irrelevant, would be a site A, a store that sells Jewelry and has a back link to a Site B, a Floral shop. That is perfectly find because people do tend to buy flowers along with jewelry for instance an upcoming wedding.\\n\\nI do not practice deceptive marketing, what else can I do?\\nOnly thing at that point to have concern over is things you can't control, for instance you have been hacked and someone has injected redirects, in the form of hidden spam links or malware of any sort. \\nIt seems you have GA and so they will spot and notify you if that is the case. Also you want to check on your backlinks, which can be done on your GWT, and those you spot, take action on. You can do this by contacting the site owner or blocking incoming clicks from the site.\\nIn the End as in the beginning - Google wants you to succeed.\\nThey have many things in place to spot site owners willfully involved in user behavior manipulation and will penalize those sites and I don't mean Call to Actions, that is good marketing!\\nKeep this in mind, you can easily spot when a website's link has tricked you upon entrance of their site and so it should be easy for to know if what you are doing is not benefiting a user's desire.\",\n",
       " \"1) Facebook, for example, used to give you a numeric (i.e. 1000000001) id, now i guess they automatically establish you one (alphanumeric) related to your name/etc at first, and don't change. The old (numeric) ones, after first changed, becomes final. The old URL probably has a 301 redirect to the new (and final) one. I don't think is reasonable for any DB recording of your old nicks (except for management/log purposes) to redirect to your new ones, it's basically duplicate urls.\\n2) In that case, username should be free. Depends on business logic too. But they also limit it by X changes per Y time.\\n3) The redirect will be to whatever the code sends it to.\",\n",
       " 'The mistake here is not understanding the what A and CNAME records support:\\n\\nA records will only support IP addresses.\\nCNAME records will only support valid domains.\\n\\nTherefore using a full URL path within the CNAME Points to field is not correct, since a URL is not a domain.\\nIn this instance the correct setting is: subdomain.firebaseapp.com. \\nDNS points to servers not paths\\nIt is a common mistake by many webmasters that they believe the DNS hosting is responsible for pointing to the content, this is not true. \\nThe DNS is responsible for pointing to the server that hosts the content, nothing more. It is then the responsible of your server to negotiate where your content is located, this is normally done through server side settings, generally referred and known as a virtual host file.\\nAdding your custom domain within the FireBase Console will add this information to their server-side virtual host file.\\nQuick Summary of DNS\\n\\nYour DNS points to the IP address or cname domain of the server hosting your content.\\nYour server translates the domain to where physically your files are stored through the virtual host file.\\n\\nRedirect are server-side not DNS side\\nThe other issue you have is that you want the domain to point to admin.html, this can not be set by the domain DNS and will need to be controlled by FireBase. Firebase supports redirects using json, so if you want users to automatically be redirected to admin.html you need to use that method, or another simpler method would be to rename admin.html to index.html, therefore no redirect would be required.',\n",
       " 'From Vanessa Fox, an ex-Google employee:\\n\\nGoogle is no longer treating subdomains (blog.widgets.com versus widgets.com) independently, instead attaching some association between them. The ranking algorithms have been tweaked so that pages from multiple subdomains have a much higher relevance bar to clear in order to be shown.\\nIt’s not that the “two page limit” now means from any domain and its associated subdomains in total. It’s simply a bit harder than it used to be for multiple subdomains to rank in a set of 10 results. If multiple subdomains are highly relevant for a query, it’s still possible for all of them to rank well.\\n\\nIt\\'s unclear what the \"some association\" part is -- but the parent and child domain like a.example.com and b.example.com obviously have a relationship of some kind, far beyond what two websites named example-a.com and example-b.com would have.\\nI don\\'t think any Google penalties can go upstream to the parent, otherwise a lot of web hosting services would quickly find themselves delisted, by this (sensible, IMO) logic:\\n\\nIn this instance I’m talking about the exception given to protect free hosts from penalties, particularly those who give their users subdomains such as Hypermart, Xoom, Wordpress.com, Blogger, Tripod etc. This exemption can’t only cover the popular free hosts otherwise no new freehosts would ever stand a chance. As soon as they got a single spammy user their whole site could get banned and poof goes their legit business.\\nLikewise algorithmically it can’t cover all free hosts because then the big ones like Wordpress.com and Typepad would all be penalized. On a foresight this would also include profile based social sites such as Myspace and outbound linking social sites such as Delicious.\\n\\nTherefore penalties can only go downstream to the child subdomains from the parent.\\n\\nPrimary domains can pass a penalty to subdomains. Subdomains can’t pass a penalty to a main domain unless the main domain holds a clear relation to the subdomain.\\n\\nIt\\'s not entirely correct to say that a.example.com and example.com are unrelated (as far as Google is concerned). But it does appear that only penalties, not bonuses, are ever transferred across.',\n",
       " 'I think this is a (long-standing) bug in Google’s SDTT. \\nRelevant Stack Overflow question:\\nWhy does Google Testing Tool use the “id” attribute to generate a URL for the Microdata item?\\nIn Microdata, only the itemid attribute should be able to give a URI for the item, while the id attribute is only used for referencing elements on the same page (with the itemref attribute), not for providing URIs. So the SDTT should never do anything with id values after following all itemref references.\\nIf you provide a value for the itemid attribute, it overwrites the value the SDTT extracts from the id attribute. That would be the only way to fix this problem from our side.\\n<article id=\"post-29\" itemscope itemtype=\"http://schema.org/Service\" itemid=\"/acme-service#this\">\\n\\nThe itemid attribute takes a URI as value. It’s a good practice to offer a relevant document under this URI, but it’s not required by Microdata. itemid serves the same purpose as @id in JSON-LD and resource in RDFa Lite.',\n",
       " 'As closetnoc recommended it is best to wait. Sometimes I have seen it happen within minutes and a good way to see how your site is propagating is to track it with:\\nhttps://www.whatsmydns.net/',\n",
       " 'http://zhanzhang.baidu.com/sitemap/index is the (sparse) documentation about sitemaps for Baidu. It only describes that it may help to submit your sitemap, and it shows a screenshot of their webmaster tool, listing several sitemaps and their status. However, it seems that they intend this feature only for \"high quality\" sites, on invitation basis.\\nHere is also a FAQ about sitemaps: http://help.baidu.com/question?prod_en=master&class=477. They say, for example, that \\n\\nthey follow the sitemaps.org standard,\\nthey support XML and TXT,\\nin the general case it should be placed in the root as sitemap.xml, and\\na single sitemap must not be larger than 10 MB or 50000 URLs.\\n\\nSo you’d need to register an Baidu account, login to their webmaster tool, add and verify your site(s) and then … wait and hope that you are invited to upload your sitemap. I don’t know their process, but I wouldn’t assume that this will happen for your sites.\\nAlternatives?\\nhttp://zhanzhang.baidu.com/sitesubmit/index is their site submission tool, which even works anonymously. It’s probably intended to submit your website’s homepage only (e.g., example.com), but you also get a success message (\"URL提交成功\" = something like \"URL successfully submitted\") when entering the URL to your sitemap. However, this probably doesn’t mean that they’ll use your sitemap that way.\\nFor blogs, they also offer a ping service http://ping.baidu.com/ping.html (XML RPC: http://ping.baidu.com/ping/RPC2) (documentation). Entering your feed there is also possible.\\nAnd special cases: For WordPress, there seems to be an official Baidu plugin that pushes any new content to Baidu and allows them to show updates in real time. For Discuz! forums, there is also a Baidu sitemap submission plugin.',\n",
       " \"No you don't need it. \\nIf you have already blocked your subdomain from search engine, then search console can't find, any data about your site, so what's the purpose of that property?\\nSince, Google allowed up to 100 property in search console, so feel free to test it, and delete it later. But I am 100% sure, you will not going to get any data on all of properties, since you blocked it on robots.txt\",\n",
       " 'what do you see in the console is the site after it is rendered by browser. if the site is properly coded, and all javascripts are loaded fast, you can hope, that the whole site will be indexed. if the time gap for loading of javascripts is more then, say, 4 seconds after onLoad event, it could happens that Googlebot will not wait so long and the site remains not fully indexed.',\n",
       " 'I narrowed it down to a simple test case:\\n<div style=\"position:relative;\">\\n<script src=\"http://www.dupure.com/scripts/plugins/modernizr.min.js\"></script>\\nHELLO WORLD\\n\\nThe div with the relative position is inserted by Google cache and ends up enclosing both the modernizr script and the page contents.  The modernizr script does something that prevents all the sibling content from displaying when in this container.  Other people have encountered this same issue:  Google Cache snapshot Issue #1086\\nYou appear to be using modernizr version 2.5.3.  Upgrading to the latest version of modernizr fixes the problem.  \"Hello World\" shows up on the page in this case:\\n<div style=\"position:relative;\">\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.min.js\"></script>\\nHELLO WORLD',\n",
       " 'If you have already chosen the best possible region for your target customers then latency based routing will simply increase the cost of what you have already achieved. Additionally latency based routing is designed to work when you have servers in multiple availability zones or regions so as to route the user to the most appropriate availability zone based on the lowest latency from the end users location.\\nAs for CloudFront this is a slightly different kettle of fish. CloudFront, being a content distribution network, has edge servers on a large number of different public and private network and while AWS does identify the location of most of its CloudFront edges this list does not necessarily include instances where an edge has been colocated at an ISP\\'s network to bring the content as close to the end user as possible. It also allows to shift some load from your app servers to some specially designed static content servers which AWS keeps running fast. At this point the only way to answer your question is to ask yourself \"Is the cost of using CloudFront going to be worth the decreased load on my servers?\" The other thing to bear in mind is that by shifting static content to CloudFront you may find that overall load time is faster for your site not specifically due to server load, but rather due to parallell loading of static content due to the different domain being used for cloudfront (app.domain.com, cdn.domain.com).\\nHope this helps.',\n",
       " 'Here is your updated function.\\nfunction OnButtonClick() {\\n\\n        // Save the value of the first one for future reference\\n        var select1alueTemp = select.options[select.selectedIndex].value;\\n        // Find the second list selected value and assign it to the first one\\n        select.value = select2.options[select2.selectedIndex].value;\\n        // Then assign your previously saved value to select2\\n        select2.value = select1alueTemp;\\n\\n}\\n\\nYou should add some validations as well if this is going to be on production.',\n",
       " \"Dmoz is dead so you're wasting your time.\\nThere is no timetable to get listed. It could be a day, or more likely, never. And anything in between. It's whenever the editor decides they're going to approve your listing if they ever do at all. But there are a shortage of editors, many editors are slow or absent, and just as many are corrupt and won't list competitor's sites. So getting into Dmoz is very difficult.\\n\\nTo summarize, if you bother to submit your site, which may not be a good use of your time, just submit it and forget it. There's really nothing else you can do.\",\n",
       " \"It is almost never correct to run a DNS server on a virtual private server and set up your DNS records there.\\n\\nYou are required to to enter TWO name servers into the DNS settings at your domain registrar.   Unless you have two VPS boxes, you cannot do so.\\nA DNS server opens more ports on your VPS that hackers can use to get in.   Unless you have experience running a DNS server and are willing to do the work to keep it secure and up to date, I don't recommend running one yourself.\\nSetting up self hosted DNS is more complicated than setting it up on somebody else's server.   If you point your nameserver records to be within your own domain name there is a chicken and egg problem that has to be solved with SOA records.   I've always found them to be confusing and error prone.\\nDNS hosting is cheap.   Good stand alone DNS hosting is under $2 per month.   Many domain registrars offer DNS services for FREE with a domain registration.   Many web hosting companies offer DNS services for FREE with web hosting.   It sounds like you might be able to use Site5.\",\n",
       " 'You are talking about a branded \"card\" which comes from the knowledge graph.\\nIn essence, you have to think in terms of RCS (real company [****] stuff) which is something Google says they looks for. Google will not take your word for it. In otherwords, what you do on your site is not enough.\\nAt one point, I counted 46 branding signals. You do not need all 46, or even a majority, however, you do have to have enough of them for Google to feel confident that you are a real business with a brand. I will not list them all, however, I will give you a few of the major highlights.\\nSchema Mark-Up: You will need to properly mark-up your site using schema.org mark-up. You will need NAP (name, address, phone) at a minimum. This should be easily found just on click away on the About or Contact page. It can also appear in the sites header or footer.\\nAdditional Contact Information: Often this is an e-mail address and/or contact form. This can also be additional names, addresses, and phone numbers with indications such as sales, marketing, etc. Again, this should be easily found one click away on the About or Contact page.\\nA Business Listing: Your company should be found on one of the respected business listing sites. While there are many, the trusted authority sites are ones that derive their data from other trusted sources primarily telephone listings from the local telephone company or one of the major sites such as yp.com which aggregates these listings. What is important to remember is that not all sites vet their data or derive them from valid sources. Stay away from these. They are not useful. Often, an ad in the yellow pages and ordinary business listing in the phone book will do the trick.\\nGoogle+ for Business: This is Google\\'s opportunity to vet your company. By creating a Google+ business profile, you are giving Google a chance to ensure that you are who you say you are. Google will often, one way or another, validate any e-mail address, physical street address, and phone number.\\nGoogle Maps: Make sure that your business can be found on Google Maps. This requires a \"walk-up\" address and not a P.O. Box. Some have claimed that residential addresses no longer work. While an at home business is perfectly valid, Google may be using other metrics to ensure that a location of a business is legitimate.\\nOther Social Media: While I hate social media, it has become an unfortunate way of life. Make sure that you have some other form of social media account that can verify your NAP data. Facebook or LinkedIn are excellent for this.\\nDomain Name Registration: Google prefers that a business domain name registration be public with valid contact information. This is not a requirement, but is a significant factor. If your domain name is private, you can leave as it is. This is explained in the next section.\\nConsistent NAP Data: Google will check to make sure that all contact information found is reasonably consistent. This includes the NAP data found on the website, business listings, social media, registration information, etc. Enough consistent information will override a private domain name registration. If your contact information is not reasonably consistent, this will stop branding cold in its tracks.\\nBranded Links: It is one thing to have relevant link text and URL, however, some reasonable portion of your links should be branded with your business brand or name within the link text.\\nBranded Citations: Links are not enough. On other sites, articles, press releases, reviews, and other information should exist mentioning your brand or company name within the content and not within a header, footer, or sidebar.\\nBranded Co-occurrences: Citations are not enough. On other sites, articles, press releases, reviews and other information should mention your brand or company name as a comparison to another recognized brands within the content and not within a header, footer, or sidebar.\\nBranded Searches: Search users must search for your brand either as a Primary and Secondary search. A primary search is the first search entered by a user. A secondary search is where the primary search results are inadequate and a second more refined search is used. For example, athletic shoes and athletic shoes nike are an example of a primary and secondary search. However, athletic shoes and cheap shoes are not because these are two separate searches and not a refinement of the first search.\\nWikipedia: While this is not necessary, an article in Wikipedia is certainly a significant factor. The editors will validate your business article to enough of an extent that Google feels comfortable with the listing. This also serves another purpose. Since Wikipedia is available via an ontology that Google relies upon on as a primary source of information within the knowledge graph, any article you create that is approved and published will immediately get picked up and used by Google. Wikipedia is a significant signal for branding.\\nThese are the major factors. Do enough of this and you will get your card.',\n",
       " 'productionCompany is a property, not a type.\\nIn Schema.org, a property always starts with a lowercase letter, while types always start with an uppercase letter.\\nIt seems that you want to say something about a company. In that case, you should use Organization or one of its sub-types, e.g., LocalBusiness:\\n<div itemscope itemtype=\"http://schema.org/LocalBusiness\">\\n  <!-- … -->\\n</div>\\n\\nThe productionCompany property takes Organization (or one of its sub-types) as value. So you could for example say that a certain Movie was produced by this company:\\n<div itemscope itemtype=\"http://schema.org/Movie\">\\n  <div itemprop=\"productionCompany\" itemscope itemtype=\"http://schema.org/LocalBusiness\">\\n    <!-- … -->\\n  </div>\\n</div>',\n",
       " 'There are several reasons to remove extensions from URLs:\\n\\nTo make the URLs look cleaner\\nTo make URLs easier to type\\nTo make URLs easier to remember\\nTo make URLs more SEO keyword friendly\\nTo be able to change technologies -- if you ever want to move your site from one technology to the other, its easiest to do so without users even knowing if there are no extensions on the URLs\\n\\nKeep in mind that many sites are generated by a content management system (CMS) that would make URLs look like this: /index.php?page=this-is-the-widget-page. That is particularly ugly and has far more cruft than just an extension. Rewriting to remove index.php?page= makes it much better.\\nExtensions are not needed on the web because servers send the type of document as a header.     Web pages are served as text/html, images as image/png or image/jpeg.  This lets browsers know how to render the content without using an extension to figure out that the URL contains text, HTML, PDF or image (for more information see the Wikipedia article Internet media type).\\nSome webmasters choose to use an extension on their URLs that matches this content type.  So any text/html document would have a .html extension and any image/png document would have a .png extension.    That can help when the URLs are saved to the file system where the meta data about their content type is lost.  In most file systems, the program that opens the file is chosen by the extension.  So even if a page is served by PHP, some webmasters remove the .php extension, and some replace it by .html.\\nThere is also the question as to whether URLs might be better ending in trailing slash (/) when they have no extension which has a lot of discussion on Stack Overflow.',\n",
       " \"how long does it take for google to stop giving these errors?\\n\\nThere is really no fixed time. For some of the more 'active' websites, I have noticed Google to make alterations to search index at a fast pace as compared to less visited (by Google bot). As correctly pointed out in comment section, wait it out.\\n\\nwhat can i do meanwhile?\\n\\nIf all the urls being shown as 404 are indeed not available then skip the following suggestion. If not, try an redirect old url to new ones, this will help you keep traffic. Additionally, you can keep a check on indexed links and see if Google is removing them (or not).\\nP.S - If you have a relatively young website, this can take some time.\",\n",
       " 'Use Alexa website or any Punycode converter tools to find normal domain name for this type of domain names.\\nYou can use alexa.com/siteinfo/[Non-Enlgish domain name]. In your case, you can use This URL.\\nThen disavow the new displayed domain name(xn----7sbbbh0au1bathasd.xn--p1ai).',\n",
       " 'Since these two pages have been deleted (from public view at least) and they have not moved somewhere else - so there is no equivalent page to redirect to - then the appropriate response is a 404 (as you are already doing).\\nHowever, this should be a customised 404 with an appropriate link to the parent page (user profile), so users remain engaged on the site. The user now understands that the URL they followed no longer exists, but are guided to the nearest available.\\nGoogle sees the 404 and eventually drops the page from the SERPs.\\n\\nCan I redirect or not?\\n\\nRedirecting to a \"different\" page is likely to be seen as a soft-404 by Google - this is likely to be the only \"penalty\". As with a regular 404, Google is likely to drop the URL from the SERPs. Since the page doesn\\'t exist anyway, you can\\'t really argue with that.\\nFor the user... The user is redirected to a different page to what they were expecting. This could lead to confusion and an increased bounce rate.\\nThere is also another issue with redirecting... how quickly do you want these pages removed from the index? The cached version of the page might still be available. A redirect (soft-404) might take longer than a proper 404 (which is still subject to some delay). A 410 Gone would be even quicker.\\nIn summary... A customised 404 would be preferable. A redirect is unlikely to see an additional \"penalty\" with search engines, but could potentially result in some confusion.',\n",
       " 'Quick starting point, using categories, product within url and on website is more of keeping things organized than anything else (IMHO). Moreover use case varies depending on what you are trying to achieve.\\nNow, onto your questions in particular,\\n\\nThe URL for the above becomes www.domain.com/category/greeting-cards/?Occasion=Birthday. This can produce really messy URLs if you throw other filters into the mix, for example, relationships etc.\\n\\nBy messy, you mean long? Because there are some really long, ugly links on the internet, and honestly, as a user you really don\\'t care (unless you are fiddling with links).\\n\\n...does using /category/<< name of category >> and /product/<< name of product >> have a negative impact?\\n\\nAnswer is yes and no. \\nYes, because, in terms of SEO, the real part of the link i.e. containing terms related to page will be pushed back and general word i.e. category or product are show ahead (which really bring no value). For example, a search for \"nike shoes montreal\" throws this link amongst others,\\n\\nNotice, the highlighted words nike, shoe, and montreal in url.\\nNo, because, leveraging url structure for SEO is not the only thing you can do to improvise overall SEO.\\n\\nWould it be beneficial to get rid of \\'category\\' and \\'product\\'?\\n\\nApart from what\\'s stated above. The decision will be more of cost vs benefit. If you are spending xxxx dollars to get this working, then you\\'d be better off with existing structure. If said changes are coming in cheap, then give it a shot, nothing to lose (provided things are done correctly).\\n\\nI have asked my developers why we need this. They say the SEO agencies they\\'ve worked with previously say this is not a problem and has no side effects.\\n\\nThat\\'s not an answer. You don\\'t do something just because others are doing it. Sloppy answer.\\n\\nWould this be better as www.domain.com/greeting-cards/birthday-cards?\\n\\nIf \"greeting-cards\" has other sub-categories then it is better approach.\\n\\nMy developer is asking for a significant investment to make this change...\\n\\nIf you are using Wordpress then this should not involve significant investment. Just saying ;)\\nAnyhow, good luck :)',\n",
       " 'I don\\'t know of a way of seeing the percentage over time, but you can get the absolute numbers over time.\\nLike other \"compare stats over time\", Google Analytics hides it under \"motion charts\".  \\n\\nChoose the date range for which you are interested\\nNavigate to \"Acquisition\" -> \"All Traffic\" -> \"Channels\"\\nClick on \"motion charts\" (icon with three black circles top right of the graph)\\nChange the metric of the graph from \"% new sessions\" to \"sessions\" (sideways drop down to the left of the graph)\\nChange the graph to a line chart (small gray line chart in a tab over the graph as opposed to the black line chart icon next to the motion charts icon)',\n",
       " '.net .org and .com are considered as top level domain(TLD) and are public TLD so anyone can register easily from any domain register company.\\nPeople sometimes use .net when .com are not available for registration. While people use .org when they provide some information or service for free.\\nLink from Wikipedia and Google article might be helpful.',\n",
       " \"Both rel=canonical and noindex has their own purposes.\\nrel=canonical should be used when you want to give weightage of the page to a page from where the content originated. The seo juice in this case would be passed to the actual content owner.\\nSo in case you are copying content from other sources, it is a good practice to give them weightage via canonical tag\\nrel=noindex should be used on pages which you don't want crawler to index like logged in user profiles or search result pages etc.\",\n",
       " '...they might be able to access the main root also\\n\\nYes, that is how cPanel\\'s Addon domains work. They are simply additional (ie. \"add-on\") domains on the main account. You can\\'t restrict access.\\nIn order to restrict access, you would need to create another \"account\". eg. As a \"reseller\".\\nRelated question:\\nwith PHP: How to restrict access from one Addon domain to another?',\n",
       " 'Image filename is not very important. Because google know perfectly this truth that some Content Management Systems and many custom-made websites do not allow to edit image filenames and many images uploaded with unrelated filenames.\\nAlso many Web.2 and social websites, controlled by their users (Not admin) and images uploaded with dirty filenames. So Google can not count this factor as an important factor.\\nJust focus on these main and powerful factors: \\n\\nThe alt attribute\\nThe title attribute\\nText around the image \\nThe title attribute of Anchor text for image links.\\nUsing images in related page with related H1 and title tag.\\nImage quality and Image size.\\nSharing images in Social network websites specially Pinterest.\\nSharing images in other websites by using your image URLs.\\nCreating natural backlinks pointed to your image URLs.\\nAll Page-Level ranking factors(Pages that include your images).\\nAll Domain-Level ranking factors.\\n\\nSee Domain-Level and Page-Level ranking factors here.',\n",
       " \"No, it does not affect in SEO. Google own IP is changing from time to time also with location. It is mainly because of load balancing. \\nMany of website using cloud based hosting, and their bucket location is various based on user location. Choosing best bucket region can help developer to save money, because ingress and egress traffic are depend on cloud server location. Also when same region bucket cached by CDN then rate are different. So there are some valid reason to use different bucket region (And so one multiple IP address) to host website.\\nAnd if you're using IP checker tools, then you should note that, they are returning their server IP address based on location. So if you try to ping same website from other location then server IP may be different, so don't focus on third party tools, because their location is fixed.\",\n",
       " 'When it comes to the \"main domain\" (ie. no apparent URL path) there is really only one URL. That is, the one with a trailing slash:\\n//example.com/\\n\\nHowever, Google \"understands\" this and essentially \"fixes\" any URL where the slash is omitted. So, //example.com and //example.com/ are really the same. So, yes, it doesn\\'t much matter.\\nGoogle always shows a trailing slash on the main domain in the SERPs.\\nAny URL you see in the browsers address bar where the trailing slash on the domain is omitted is purely cosmetic - in the browser. These days browsers are a bit notorious for making the visible URL look more friendly (omitting the scheme and even the query string in some browsers by default). However, if you look at the underlying request the browser is making, it is the same, regardless of whether you type the trailing slash or not. The trailing slash is present in the request (as the URL-path), because the HTTP request is simply not valid with an entirely \"empty\" URL-path.\\n\\nhttps://domain.com (https://domain.com/ changes to https://domain.com)\\n\\nTo be clear, it is the browser that removes the trailing slash in the visible URL in the address bar, before the request is even sent to your server. As mentioned, this is purely cosmetic. Google Chrome and Firefox remove the trailing slash, IE11 does not. Opera removes the trailing slash (as well as the query string!) by default, however, this can be changed in Settings > User interface > \"Show full URL in combined search and address bar\" - you then get to see everything... the scheme, trailing slash on the domain and query string!\\nYou\\'ve quoted your .htaccess code... However, your code is not doing anything with regards to slashes on the main domain. And nor can you influence the trailing slash on the main domain with server-side code.\\nTo answer your specific questions...\\n\\nis it ok that my main page remains without trailing slash? From what I\\'ve observed it\\'s typical behaviour of 99% domains.\\n\\nThis has nothing to do with the domain, or even your server. In order for the user agent to construct a valid HTTP request there must be a slash present in the URL-path. But yes, it\\'s \"OK\".\\n\\nshould I use in Google search console https://example.com or https://example.com/ ? Currently I got https://domain.com/ and I wonder if it\\'s a huge problem.\\n\\nI don\\'t think you really have a choice. GSC automatically appends a trailing slash on the domain.\\n\\nWhich one should I indicate in a sitemap? Currently I got https://example.com.\\n\\nStrictly speaking you should include the trailing slash. The Google references I\\'ve seen, and the Sitemaps Protocol site itself all reference the main domain URL with a trailing slash. However, in reality it\\'s not really going to matter. Any URL processing engine will \"fix it\".\\nRelated question:\\nIs trailing slash automagically added on click of home page URL in browser?',\n",
       " 'dynamically generate marketing websites for my company\\'s brand partners.\\n\\nAlthough, I understand your question, I need to clarify this since this is usually not a common approach. Are you generating a page for each individual domain or generating a page for different section of the website?\\n\\nMost of the images, site layout, variable amounts of copy, etc will be the same.\\n\\nFor different domains, if majority of stuff (including parts of content, if applicable) remains same then you aren\\'t going in right direction with SEO if not \"shooting ourselves in the foot\".\\n\\nShould I be architecting this with subdomains instead, then pointing domains like \"www.FlyBar.com\" to something like \"flybar.baz.com\".\\n\\nSub domains won\\'t necessarily be of any benefit from escaping a possible plagiarism penalty (or content duplication). The core point being (and as you already understand), all the domains (or sub domains) essentially have same content in form of images, layout, etc.\\nAlternate approach:\\nConsider it as one possible approach. If your domains are spitting out same stuff for all domains and you are willing to stick on one parent domain (as suggested by possibility of using sub domains), why not throw in filters at www.FlyFoo.com and load appropriate filter using maybe JavaScript on pageload depending on what the user requests. You can replicate the specific request in url using parameters.',\n",
       " \"This has been discussed by Google's Andrey Lipattsev most recently in June 2016 when he stated that Google does not have a duplicate content penalty.\\nAs Andrey states \\n\\nIt is not a penalty if Google discovers your content is not unique and doesn't rank your page above a competitor's page.\\n\\nIt is further outlined by an SEO Audit company...\\n\\nFor SEO, it is not necessarily te abundance of duplicate content on a website that is the real issue. It is the lack of positive signals that no unique content or added value provides that will fail to help you rank faster and better on Google.\\n\\nIf your desire is to rank higher than other directory services then you should endeavour to have a fair bit of unique content and value added content on your site, however based on the information from Google you won't be penalised by having clients duplicate their descriptions from their own sites to their business listing on your directory service.\",\n",
       " 'What you are trying to do is incredibly difficult to do using WHM as cPanel/WHM is simply not designed with this use-case in mind. The easiest solution would be adding a second IP address which has been identified in the comments however if that is out of the question then in this case it would almost be easier for you if you where to remove the WordPress multi-site from the cPanel server and move it onto its own virtual server under the Apache default vhost. Then you can provide the servers IP address for all DNS entries for sites that are hosted through the WordPress multi-site. This would have the added benefit of not requiring configuration changes to the server and so not needing service restarts every time a new site is added to the WordPress multi-site installation.',\n",
       " 'You cannot apply filters retroactively in Google Analytics.   Filters are applied on incoming data only.   They permanently and irrevocably change the data as it comes in.   You cannot apply a filter retroactively nor can you remove a filter and reset the already filtered data.\\nThe alternative is to use segments.   Segments allow you to view a portion of your historical data.  They can be applied retroactively, or they can be removed at any time without destroying data.\\nHere is a good reference about this with more information on getting started with segments: http://msp-c.com/Stuff-We-Like/March-2015/Do-Google-Analytics-Filters-Apply-Retroactively',\n",
       " \"It's deprecated for all TLD not only to .zone. Only old result are available in serp\\nGoogle does not want to create another database for link operator. It is mainly used by spammer/link buyer in past. So Google does not updating from many years. My website is one year old with .com tld, and it does not display any links when I do search with link operator, while there is some hundred backlinks when I check through Google search console. Only old website will return some of their result. \\nFor your own website Use Google search console tools to check who links to you. For other website use third party tools, normally it's called backlinks checker or analyzer.\",\n",
       " 'This is easy. Add this in your <head>\\n<meta name=viewport content=\\'width=device-width, initial-scale=1\\'>\\n\\nThis tells browsers to scale the page to something other than their default view.\\nFor more information, Google for \"viewport\" as their are other settings but what I show is most likely all you should use.\\nHowever, there is more to coding for mobile than just that. You\\'ll want to look into \"media queries\" and \"mobile first\" programming.',\n",
       " \"I don't have comment privileges, or I would have left this as a comment on an earlier answer.\\nDO NOT, I repeat, DO NOT escape an apostrophe in HTML using \\n&apos;\\nThis is not a valid HTML character entity reference.  It is an XML character entity reference.  While Firefox and Chrome, at least, will render the above as an apostrophe in an HTML document, Internet Explorer will not.  And it is following the standard when it refuses to do so.\\nYou may escape an apostrophe in HTML using\\n&#39;\\nBut I don't believe it is, in general, necessary.\\nhttp://fishbowl.pastiche.org/2003/07/01/the_curse_of_apos/\\nhttp://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references\",\n",
       " 'Apart from the obvious security issue of someone else being able to physically see what password you are typing - which the user would be aware of anyway - there are a few security \"concerns\" that could arise with how it is implemented and what (trustworthy) software is on the users system, that could potentially expose the password to third party apps/tools unnecessarily.\\nIf the \"show password\" option simply changes the type of the input element to text then any browser plugins (or third party apps) that check spelling of text fields will now be active on the element. In Google Chrome this could involve sending the text to Google to \"Ask Google for suggestions\". \\nI also have a third party dictionary app installed that will lookup text under the mouse cursor. This doesn\\'t work on password fields, but does on text fields.\\nText fields are also copyable (potentially by a third party app), password fields are not.\\nLogin forms shouldn\\'t be autocomplete enabled anyway, but changing the type of the INPUT to text does potentially allow the password to be saved in the browser\\'s autocomplete database (which is not necessarily secure), unless the INPUT is changed back to password before submission.\\nAn alternative to changing the type of the password field is to \"show the password\" in a non-form element. This might be \"safer\".\\nIn summary... it should be safe, but I think there are potential risks that the end user might not be aware of. But it is the end user that has to make the choice to \"show password\".',\n",
       " 'What is needed in this matter after fixing the problems on your site is what Google calls a \"Reconsideration Request\".\\nTo begin with a reconsideration request is a request to have Google review a manual action they have taken against your site for a violation of Google\\'s webmaster terms after you have fixed the offending issues.\\nTimeline: Many times I have been asked how long this process will take and how long until the site is back in the index and the simple answer is that it can vary. The actual decision by Google once you have completed all the required fixes could take anywhere from a few days to a week and a half. A good rule of thumb to follow is that once the request has been approved wait until the flag is removed from the Search Console, then allow a similar amount of time to how fast your site is normally crawled and reindexed into the Google SERP and that is an approximate timeline for you. There is nothing that Google is able to do to speed up the process nor is there anything else you can do if you have done everything else in this guide to speed it up either.\\n\\nWhat Do I Do To Submit a Reconsideration Request\\nTo submit a reconsideration request the following steps should be followed...\\n\\nSign into your Search Console account\\nVerify all versions of your site to ensure you have complete and accurate data on your web properties\\nCheck the manual actions section to verify that Google has in fact taken any manual actions on your site\\nCheck the Security Issues section in your Search Console for any other issues which may be affecting your site\\nClick on the \"Request a Review\" link to ask Google to reconsider your site\\n\\nWhat Is The Reconsideration Process\\nThe reconsideration process starts from the time a manual action notice is sent to the webmaster until the point that Google completes its assessment of if the issues with the site have been addressed sufficiently. The process can vary in minor ways depending on the type of manual action and the reason for the manual action but the general process Google follows is...\\n\\nYou receive a manual action notification and fix the issues mentioned.\\nYou document your reconsideration request (see documentation tips below).\\nYou address any additional issues (see common mistakes below).\\nYou submit the reconsideration request in Search Console.\\nYou receive an acknowledgement from Google (it may take a few days for your request to be processed).\\nYour request is either rejected or approved. \\n\\nIf your request is approved by Google then the manual action will be removed from your site.\\nYou should bear in mind that reconsideration requests are handled by real people so good documentation will help the process along and help the reviewer understand the steps you have taken to address the issues.\\nA good reconsideration request should do three things...\\n\\nExplain the exact quality issues affecting your site\\nDescribe the steps you have taken to rectify the issues\\nDocument the outcome of your efforts\\n\\nYou should also provide links to pages on your site that show your cleanup efforts. Some tips for specific issues are...\\n\\nManipulation of Backlinks - Provide a list of the links that you have taken action on. You should make every effort to remove the offending links before using the disavow tool. Simply using the disavow tool on the offending backlinks with no effort to remove un-needed ones may lead to a rejection of your request.\\nSelling Links on Your Site - You should provide examples of pages where the rel=nofollow tag has been added to the violating links or the links have been removed. It is important to note that if this is the issue you should make every effort not only to clean up the reported offending links but all the other same types of links on your site.\\nScraped Content - You should provide examples of the bad pages that you have removed and good pages with useful and genuine content that you have added to your site.\\nNewly Purchased Domain - If you have purchased a domain that belonged to another site previously and you believe that the manual flag was caused by the previous site then you should use the reconsideration request form to advise Google that you have recently purchased the domain and that it now adheres to the webmaster guidelines. Make sure that when you do this you actually have a site accessible by the domain and that it does in fact meet the guidelines otherwise the review may be rejected.\\n\\nCommon Errors in Reconsideration Requests\\nThe following is an extensive but not exhaustive list of some of the more common errors when filing a reconsideration request.\\n\\nNot Using the Disavow Tool Correctly - The disavow tool should be used only where appropriate. If you can remove an offending backlink then you should do so. Adding all backlinks to the disavow tool is not considered in line with Google\\'s good faith removal guideline and will not be enough to to make the request successful. Your first point of call should be to contact the webmaster of the backlinking site and request the removal of the offending backlink. If that fails then use the disavow tool and be sure to document in the review request the attempts made to have the offending backlink removed. If there are multiple links on the offending backlinking site and you can not get them to remove the links then when using the disavow tool you should use the domain: operator to disavow the whole domain as a convenience. You should also make sure that if the disavow tool does have to be used not to disavow organic backlinks as this can affect the SERP ranking if your site negatively.\\nNot Using Fetch as Google While Cleaning Hacked Content - Often there is cloaked content when a site has been hacked, which means that search engine spiders see different content that what is being shown to the sites visitors. One way to see if the site has been compromised is to use the fetch as google tool to see the same content that Google crawlers see. Often review requests have been rejected in the past as the underlying issue has not been resolved and the Google crawler still sees spammy content even if the developer using a browser can not see the spammy content.\\nSubmitting Reconsideration Requests for Empty Sites - This would seem like a given but a large number of reconsideration requests are rejected as the site in question is an empty site of some description. Sites that are not ready for reconsideration are sites that are still under development, such as empty sites, blank pages, or many pages with little to no content, parked domains that don\\'t actually have any content and are simply acting as a placeholder or redirect to a different domain, and sites that are inaccessible due to network or server errors. The general rule of thumb to follow is that you should not lodge a reconsideration request if your site is not ready for production and ready for the world at large to get use of it.\\n\\nAs a catch all rule for all reconsideration requests you should make sure that you are not violating Googe\\'s quality guidelines before you submit a reconsideration request.\\nIf your reconsideration request is rejected by Google that doesn\\'t mean that it will always be rejected, it simply means that there is still an issue with your site. You should go back to the manual action notice and make sure to fix all of the issues identified through the notice, not just on any sample links documented, but on all pages throughout your whole site, before submitting another reconsideration request.\\nNB: I have seen circumstances before where webmasters, for whatever reason, have been creating or redeveloping their site on the production server under the main domain name and have continued to allow access to the domain during the work. In some cases that I have noted a manual action was placed against the domain as the site effectively violated the Google Quality Guidelines even though the site was under development and wasn\\'t being targeted to end users. In this situation the webmasters had to go through the whole Reconsideration Request process before the site could be added back to the Google index after development work was done. A good practice in this situation is to replace your site with a coming soon page, or password protect the site, and if those options are unavailable, add a robots.txt file to block all crawlers until your site is finished. It is always best not to do development work on the publicly accessible production site while members of the public can access the pages being developed. Remember, if a member of the public can access a page so to can the Google crawler.',\n",
       " \"When you buy a domain name you own that domain name for the duration of the agreed period, whether it be a 1 year, 2 year, etc domain registration. During that time only you are allowed to use the domain name or make changes to the underlying details of the domain name.\\nWhen it comes time to renewing the domain, while there is strictly speaking no requirement to allow the renewal, there is a requirement to allow domain transfers to another registrar, as such practically every registrar out there lets the domain owner renew before expire. If your domain has expired then how long after the expire until it can be bought by someone else depends on the gTLD in question. Some have a 30 day grace period, others it is straight away, and listing each one is beyond the scope of this question.\\nBasically it is something that, while unwritten, has become a right due to the supporting right of transfer which would negate any effort by a registrar to prevent you from renewing your own domain name. Most gTLD's also have dispute processes, which vary from gTLD to gTLD. If you encounter this sort of issue and wish to raise it as a dispute check the registry for the gTLD you are using and find out what their disputes process is and raise a dispute.\\nSources:\\n\\nCan a registrar ransom a domain name which has become popular?\\nhttps://au.godaddy.com/help/can-i-renew-my-domain-name-after-it-expires-609\\nhttps://www.icann.org/resources/pages/name-holder-faqs-2012-02-25-en\",\n",
       " \"First things first, not all 404 errors are bad, the fact that a 404 error is coming up means that your server is doing the right thing and returning a 404 page not found error for the request.\\nWhy does this happen...\\nOld Links Still Pointing To Your Site\\nThe internet is like an elephant, long memory, and many sites will still have links to your site even after your site no longer exists and the domain is not registered anywhere. 404 errors from someone clicking on one of these links is very common and nothing to be worried about at all. If the content has just been moved to another location you should try and add a 301 permanently moved header and redirect the user to the new location so as to maintain the page rank from that link as well as to maintain the user experience.\\nMis-Spelt URL's\\nOnce again this is very common, the internet is by and large used by humans and humans are fallible, we often spell things incorrectly, and so if a URL has been typed it incorrectly it will return a 404 error to the end user which in many cases will encourage the user to check the address and try again making sure to spell it right or use the site search feature if there is one to try and find the page they where looking for. Once again nothing to worry about if you see these sorts of 404 records in your server log.\\nLarge Number of Requests to the Same Page\\nThis is often the result of a mis-configured web crawler. This can either be a genuine crawler from a legitimate search engine website such as Google, or it can be a private crawler used by a private party working on their own search private search engine. If the load it is imparting on your server is of a concern then you can block the user agent string or IP address but often this is not needed as returning a 404 error to the end user does not impart a huge load on the server and many web servers are designed to handle a larger number of connections that those which are imparted by a mis-configured innocent crawler.\\nAttempts to Breach the Website\\nNow this is where you are more likely to need to sit up and maybe do something. If your server logs indicate traffic which you know should not exist, and it appears as though the connections resemble a penetration test of your website (such as URL's which vary by only a small amount and are increment an ID number of small number of letters each time). In this case it is a possibility that whoever is controlling the connections on the other end is trying to see if they can break into your website, sometimes just for the crowing rights of saying they managed it, sometimes for more nefarious purposes. The first step when you see this would be to verify, check the IP address and do a lookup, does the IP belong to a business that may have a genuine need or is unlikely to be doing this sort of thing intentionally, if so try and let them know as they may have malware on their network or something may be mis-configured. If however the IP address appears to be coming from an internet service provider then there is a real chance that there is no reason for that to be happening, in which case you can temporarily block the address space from connecting to your server and notify the ISP's abuse email about the attempted breaches on your server and the IP address which was causing the breaches. No guarantee that anything will be done but it is a possibility.\\nBasically the point is that excepting extreme circumstances or protracted issues that may be occurring there is no need to be concerned from an initial small or medium number of 404 errors, mainly when the errors get into a large number of they are starting to affect your sites performance do you need to dig deeper and find out what is going on.\",\n",
       " \"From Google's Matt Cutts...\\nhttp://yourseosucks.com/2009/09/5-common-seo-questions-answered-by-googles-matt-cutts/(Point 4)\\n\\nUm. We really don't care that much... Any time we see white space, we'll separate stuff. And we can ignore white space, so it doesn't really cause us a lot of harm either way... As long as you're doing normal, reasonable stuff, I wouldn't worry about it that much.\\n\\nBased on this I would tend so say not a big deal SEO wise. Certainly clean it up if you can as one of the pagespeed recommendations Google makes it to minify HTML and eliminate extraneous white space, but it would appear as though it doesn't make a different to your ranking.\",\n",
       " 'If you take a look at the linked schema.org page firstly you see that the ClaimReview schema is pending which means that it may be up for change as it develops and they are still finalising examples but at the bottom it provides an example json-ld block which can be embedded in your page head.\\nThe page also provides a description of what each tag means which is a little beyond the scope of this question and answer but basically all you have to do is add the script block to the head of your page and set the appropriate data is the json and then when Google indexes the page it will detect it. Functionally it is similar to adding OpenGraph data to the head of a page but rather than adding it all as separate meta tags the ClaimReview schema opts at the present time to do it as a single json-ld block.',\n",
       " 'As @John Conde states simply placing keywords throughout the page will not only make it difficult for your readers but will also not help your PageRank at all. Google hasn\\'t based ranking on specified keywords for a long time since the problem with keyword stuffing and even back in the day what people would class as keywords where not what Google depended on anyway. When Google indexes a page it extracts context specific keywords from the page to identify what the keywords should be.\\nThese days when Google indexes a page it effectively does so by asking \"what would a human think this page is about\". I understand from your question the specific part where you say not to say that keywords don\\'t matter and only content matters but the fact is that while I wouldn\\'t say keywords don\\'t matter I would say that with modern SEO you should actually be calling it HRO (Human Reader Optimisation) in that the keywords for your page should be a natural part of the flow of the page contents and more specifically the topic being covered in the page. I doing this crawlers which pull your page and then assess it for indexing will automatically pick up on the appropriate keywords much the same as a human reader would.\\nYou talk about the fact that you have read \"SEO Fitness Workbook, 2016 Edition\" and that is great however the fact remains that SEO is not really something that can be easily taught in a book as a hard and fast thing, SEO is something which is dependant on the site in question and involves a great deal of experimentation to find the right content to achieve the keywords that you are wanting to target. By all means you should decide on the keywords that you would like to see associated with your page and then work on the content to cover those keywords in a natural flowing way but you should also be prepared for disappointment and a lot of experimentation as the keyword choices and ranking Google uses is based on a huge number or variables and it is not as simple as making sure all of your keywords exist on the page somewhere. Google can and quite frequently does ignore certain keywords or give them less weight if the assessment of the page shows that other keywords are more appropriate.',\n",
       " 'Juice only flows in and from pages that are indexable.\\n\\nThere is no secret squirrel code to change this outcome.\\n\\nSome people and some sites may suggest to use: \\n\\n<META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, FOLLOW\"> \\n\\nThe follow command used within the meta will only inform Google to follow links to discover new pages, it does not imply to pass juice. \\nSelectively Select Quality Profiles\\nIf you want to be rewarded then you can selectively enable index on those pages that are considered high quality.\\nNot all links are born equal\\nYou should also note that even if the profile page is considered unique content, the backlinks created for this page may have a:\\n\\nBig Effect\\nSome Effect\\nNegative Effect\\nLittle Effect\\nNo Effect\\n\\nDue to the high volume of backlink abuse over the years Google has fine tuned what type of links it rewards, some links may help the site, some may not at all. If the site has too many incoming backlinks that are not relevant to the topic of the site then you should expect this to work in a negative way. \\nFor example, if your site is about IT hardware reviews, and you have one user profile with a 100 backlinks from various sites about animals and cats, then you should expect that to either have no effect, or a negative effect.',\n",
       " 'I think it’s not relevant if it’s a physical item or a digital item. \\nAn example that makes clear that the difference shouldn’t matter when deciding if and how availability should be used: You could sell concert tickets, from your online shop and from your retail store, in digital form and in paper form (from both places).\\nOnlineOnly means that the item can only be purchased from the web shop, while InStoreOnly is for the opposite case. Both can be used for digital and/or physical items. (I guess they are intended for cases where an organization has both, a web shop and a physical store, but using them in cases where only one is available shouldn’t hurt.)\\nIf you want to convey that it’s a digital item (for download), you could use the availableDeliveryMethod property with the value \\nhttp://purl.org/goodrelations/v1#DeliveryModeDirectDownload\\n\\nSo for a product that can be downloaded after purchase from your web shop, it could look like:\\n<div vocab=\"http://schema.org/\" typeof=\"Offer\">\\n  <link property=\"availability\" href=\"http://schema.org/InStock\" />\\n  <link property=\"availability\" href=\"http://schema.org/OnlineOnly\" />\\n  <link property=\"availableDeliveryMethod\" href=\"http://purl.org/goodrelations/v1#DeliveryModeDirectDownload\" />\\n</div>',\n",
       " 'Some URLs have changed on a site several weeks back and no 301 redirects were added to let search engines know that the content has moved locations.\\n\\nLink unavailability will result in Google encountering 404 pages and if these issues aren\\'t fixed (or redirect isn\\'t done), then eventually those links will start disappearing from search results. Important thing to note here is the word \"eventually\", its because it really depends how frequently Google bots visit (crawl) your website. The more active they are, the more quickly they discover such errors and so on.\\n\\nIs there a time period that the 301 redirects need to be added in to preserve the site\\'s ranking on search engine?\\n\\nI believe, there is an indirect answer in the above mentioned reply. I am not aware of any hard limits and instead know that crawl rate determines or rather indicates an approximate time period. Either way, if you are aware of this metric or not, you can always add a redirect to a) preserve traffic, and/or b) fix 404 errors in Google webmaster to let Google know that there is indeed a redirection and link has just moved (and not removed).',\n",
       " \"The type of tag used when adding a piece of text to a page does not in and of itself affect the SERP ranking for the page in question. You should always use HTML tags for the correct purposes for situations where users are using an assisted device such as a screen reader which depends on the correct usage of HTML tags to work right.\\nIn this instance you may be better served to list each service, and then beneath each service add a short couple of sentences which describe the service in question and make the service name a link to the service page details.\\nThe whole point of SEO is to improve the quality of the page for the end user, and by doing things that will improve the end user's value derived from the site you will naturally be improving the sites quality for SEO and SERP ranking as well.\",\n",
       " 'The GA function is in fact an async wrapper function. It provides a single entry point to the analytics.js code to do everything. The first value is a method, the second is a value, and subsequent ones are parameters.\\nIt is not so much a queue per-sey as each trigger is done straight away but done in an async manner so they can get done in parallel and not block page functionality or loading which overall improves the end user experience.',\n",
       " \"In a search for company contacts, I found that Arvixe was a reseller for another registrar. I appealed to that registrar who provided me with a company contact that was not publicly available. That contact responded to my requests. A month later, when I went to transfer out a client's domains, Arvixe had redone the domain management page so that privacy and auth code links were again available. \\nTo respond to suggestions to try phone contact, my question was posted soon after Arvixe was acquired by Endurance International Group and was just not answering its help line.\",\n",
       " 'This is due to the number of people who embed these HTTP libraries in their own software for the purpose of scraping content from other sites, which is often done for the purposes of copyright infringement. Well-made crawlers which are legitimate and designed for a specific purpose (like archiver bots and search bots) have their own custom user agent strings to uniquely identify them. Based on this a general feeling held by many who apply these sorts of restrictions to their own sites is that any connections using the default user agent string from these libraries have not been made for a legitimate purpose and any which are caught out by mistake would probably result in the developer contacting the webmaster.',\n",
       " \"This can be due to a wide range of factors including the number of plugins that you are running. Some factors that need to be taken into consideration are...\\n\\nIs Caching Enabled\\nHow Many Images Are Being Served\\nIs the Output GZiped\\n\\nBasically everything that wordpress is doing will consume memory for each request. In addition to that WordPress does not scale well without some sort of caching plugin such as W3 Total Cache and WP Super Cache. Any WordPress installation should make use of some sort of caching plugin as this will esentially minimize the dynamic PHP compilation and SQL hits for each request made and instead serve cached copies of the content.\\nWithout having access to the command line of your server to see what is consuming the most memory anyone here is only going to be able to give you reasonable guesses but the first steps would be to take the actions above.\\nAs a side note when working with shared hosting providers I tend to calculate the average PHP application as 2MB of memory per session due to all of the added modules that many shared providers enable within Apache.\\nFurther after checking the GoDaddy help page I found that the number of simultaneous connections is the entry process count (haven't encountered that before with other providers). You don't indicate which resource you are running out of so if it is the entry process resource you are maxing out then the only solution would be to either change to an alternate provider who doesn't apply that sort of resource limit, or to increase the number of entry processes your account can support. Additionally CRON jobs and SSH sessions count towards the entry process count as well. The entry process you provided of 125 seems unusually low for GoDaddy as they advise their delux plan has at least 300 entry processes included and in the next plan up of premium, unlimited, and ultimate, that count goes up to 600.\\nTry checking these out first and if none of these are the reduced resources please advise of which specific resource is being maxed out and I will post an update.\",\n",
       " 'What you are trying to do is no problem at all and won\\'t result in any issues with Google.\\nI will use the PHP website as an example. It is a major website ranking extremely highly. It has a very large number of mirror sites, more for capacity planning and lower latency than for dealing with an inaccessible main site. On each mirrored site there is a list of every single other mirror as well as the main canonical site. The way this is dealt with is not by restricting crawling, rather it is by making sure that every page has a rel=\"canonical\" tag added to the head pointing to the relevant page on the main canonical site.\\nIE: \\nau1.php.net/mirrors.php\\nau2.php.net/mirrors.php\\nphp.net/mirrors.php\\n\\nall have the same line in the header...\\n<link rel=\"canonical\" href=\"php.net/mirrors.php\" />\\n\\nBy doing this PHP avoids duplication issues, makes sure that Google is aware that the main php.net site is the canonical source of the page, but still lets all of the other pages be indexed which would allow someone to access a local mirror using a Google search rather than needing to type in the address manually.',\n",
       " 'Offering multiple feeds for the same entries is perfectly fine. \\nThis is commonly done to provide different formats, or, like in your case, to offer a feed with the full content and a feed with excerpts only. Another reason might be to offer feeds with different entry count (e.g., the newest 10 items, and the newest 50 items).\\nFull content or excerpt?\\nThat said, I don’t agree that a feed with the full content \"isn\\'t normal practice\", and I don’t agree that only an excerpt feed \"meets standard criteria\". \\nIt’s typically more useful to provide the full content:\\n\\nfeed reader users can read your content within their feed reader (without having to visit your site in their browser), and \\nthey can use filters (and the search function) in their feed readers that work with the full text.\\n\\nIn case of Atom, you could even provide both in the same feed: a content element for the full content, and a summary element for the excerpt. (I guess other feed formats allows this, too.) Sophisticated feed readers might offer the user the choice what to display.\\nReasons for providing only the excerpt (typically not in the user’s interest) include: \\n\\nthe author wants the users to visit the website (e.g., because of advertisements)\\nthe author is afraid that a full-content feed makes it easier for scrapers to \"steal\" the content\\n\\nMarkup\\nIf providing multiple feeds, make sure\\n\\nto have the most useful feed as the first one (because of auto-discovery), and\\nto use the title attribute to describe the feed purpose (see example).',\n",
       " 'You can use Options -Indexes directly in the VirtualHost container. However, whether it will have the desired effect when defined here or not will depend on whether Options have been defined anywhere else in the server config. For instance, any <Directory> sections defined anywhere else (that relate to the directory being accessed) are likely to override this, since they are processed later in the request.\\nOptions defined directly in the VirtualHost container will only override Options defined directly in the server config.\\nSince the Options directive \"controls which server features are available in a particular directory\", it is far more common to see this in <Directory> containers. Defining Options directly in the server config / VirtualHost is only suitable for defining a base default. However, it is common to see defaults set in a <Directory /> container in the server config which will control the entire server - an Options directive here would override any Options declared directly in the VirtualHost. (Note that \"server defaults\" should not be declared with the +/- prefixes, so will not be merged.)\\nThe Apache documentation linked to in the question refers to Apache 2.4. On Apache 2.4 Indexes are not enabled by default (default: Options FollowSymLinks), so for this to be enabled on the server at all it must be explicitly defined somewhere.\\nOn Apache 2.2 Indexes are enabled by default (default: Options All), but this can be overridden in the VirtualHost container (providing it is not defined elsewhere).',\n",
       " 'If the old sub domain now redirects to a new domain then you have already told Google the canonical version by doing so. \\nGoogle often takes several days and even weeks to drop a previous ranked site. The time can vary from site to site, from niche to niche. So, seeing both versions of the site is common and will resolve itself in time, Google does this intentionally  to avoid webmasters losing sites from a temporary error with their site.',\n",
       " \"Hitting F12 on IE8 it should start the Developer Tools that allows you to emulate IE7 (not IE6) using the Browser Mode.\\n\\nI suppose on IE9 you will be able to emulate back at least until IE7, but I'm only supposing because now it irritates me the simple idea to have to buy W7 in order to test this.\\n\\nUPDATE: as specified by Jeff Atwood: IE9 emulates back until IE7 too.\\n\\nUPDATE 2: as suggested by Nick in comments below, if you want to be 100% sure to emulate the old IE browsers you can use VMs provided by Microsoft (incredibly they are also provided for free). As a side note, keep in mind that testing on VMs is more time consuming than using IE Developer Tools, IMHO VMs testing might be worth when testing something that's JS/CSS greedy like a web app or a complex website, not for simple sites that use jQuery and some CSS.\",\n",
       " 'Since you mention WordPress then only the first (mod_rewrite) solution will work reliably. WordPress uses mod_rewrite for internal routing, so you should also use mod_rewrite for external redirects. Redirect is a mod_alias directive and executes later in the request, so you could end up with a confusing redirect.\\nHowever, the redirect posted only redirects the home page of the blog. Presumably you also want to redirect all the internal pages as well? In which case you will need something like the following in .htaccess:\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} ^www\\\\.example\\\\.com$\\nRewriteRule ^blog/(.*)$ http://blog.example.com/$1 [L,R=301]\\n\\nHowever, it might also depend on where the main domain and subdomain are pointing. Do they point to the same place on the file system? A subdirectory? Somewhere entirely different?',\n",
       " \"Count of Sessions as you mentioned is a dimension not a metrics as we may expect it to be, it is incremented in the cookie with each session and is passed in GA as such. To elaborate :\\nSuppose we have 3 users, following is their visit log :\\nUser A - Visit's 1st time | Count Of Session =1 \\nUser A - Visit's 2nd time | Count Of Session =2 \\nUser A - Visit's 3rd time | Count Of Session =3\\nUser B - Visit's 1st time | Count Of Session =1\\nUser B - Visit's 2nd time | Count Of Session =2\\nUser C - Visit's 1st time | Count Of Session =1\\n\\nNow if we make a report of Users group by Count Of Sessions, we may have something like this :\\nCount Of Sessions |  Users\\n    1             |    3 \\n    2             |    2 \\n    3             |    1 \\n\\nFor your problem it would be difficult to present this in a single value, data would make more sense if you could show a tabular view, I've tried to address your case with following widget :\\n\\nIt's important to note that the number of users you see against the Count Of Sessions is not exclusive, to have the exclusive users against each Count Of Sessions we could simply :\\n\\nUsers having n only sessions = (Users with n Count Of Sessions - Users with (n+1) Count Of Sessions\\n\\nSo, Users having only 3 sessions would be 1,062,191 (3,962,517 - 2,900,326)\",\n",
       " \"There are so many factors that contribute to your ranking, that we can't possibly tell what exactly is wrong.\\nI can tell you that in this case it isnt speed or responsiveness that makes the difference (because, well, you just said so). However, those are two important factors. This means that something more significant is happening, which might be one of the following:\\n\\nYour competitor has an older website, giving it more authority\\nYour competitor has other flows/backlinks which are worth more\\nYou have made an update, give it a few weeks to balance out\\nYour responsive version has mistakes, maybe htmlerrors, which don't help\\nTheir content might simply be better, more up-to-date, better written\\n\\nKeep checking your WebmasterTools and Google Analytics, see where you can improve. And again, give it some time.\",\n",
       " \"Single page websites are bad for SEO. Period. \\nSince search engines rank web pages and not websites you need to make sure each web page is tailored towards  a specific topic so it optimized to rank well for that topic. Putting more then one topic on a page dilutes that page's value for each topic and thus hurts its chances of ranking for both topics. It gets worse as you add more topics to that page. It also isn't useful for users since they have to sift through content they're not interested in to find the content they are looking for.\\nAdditonally, internal links are valuable tools for SEO. They're almost as good incoming links from external sites and their anchor text does have value. Having a single page website means you are missing out on them.\\n\\nI was thinking of something like having google index the different sections as different pages. Is something like that possible?\\n\\nNo.\",\n",
       " \"There's no right or wrong answer to this.\\nFrom a marketing perspective, it really depends on if you have enough room on your website for the Google+ button and how many people use it.\\nFor example, if 99% of people who click on your social buttons are just using Twitter and FaceBook, it may be better to just have those and make them more predominant - otherwise, you're just wasting space.\",\n",
       " 'This is too much opinion but I\\'ll answer anyway. You are doing too much description in the URL. A restaurant is a category so no need to prefix it with \"category\". restaurant-tavern-bistro ... we get it ... don\\'t throw the kitchen sink in there, too. example.be/restaurants, we get it.\\nBut are all tavern\\'s restaurants? Will you be cluttering drink-only taverns in that category, too? And aren\\'t all bistros restaurants?\\nSo, example.com/restaurants is good enough. If a tavern serves food, put it under restaurants and also under bars or taverns.\\nAnd don\\'t go with example.be/company/company. See the redundancy? Why not example.be/company/details? Or example.be/restaurant/company/details?\\nWith URLs, you are mainly trying to help search engines find their way around your site. Using these steps help them find the bottom. Most people won\\'t be typing those urls into the address bar so length doesn\\'t always play into it.\\nWith theatre companies I built, one does more than just theatre. They also do radio plays. So theatre.com/theatre/name-of-play and theatre.com/radio/name-of-radio-play. But you can also find the play with theatre.com/name-of-play. \\nYou can also do theatre.com/theatre/play/cast and theatre.com/play/cast-member but this can drive you crazy.',\n",
       " \"What can prevent Google from Indexing Pages?\\n\\nLow quality (low interaction, thin or duplicate)\\nRobots.txt\\nIncorrect header responses \\nBlocked resources\\n\\nIt's about the quality of articles\\nWhile an article made up of 250-500 unique words may not be treated as Google as 'thin' doesn't necessary mean the content is high quality. It used to be that size matters but now SEO has shifted and is more about quality than ever. I very much doubt you have 5 million quality articles!\\nWhat is a quality page or article?\\nA quality page is not one that is unique but one that people expect to find and want to see, a page that receives little, or no interaction is a page that will likely be poorly ranked. Sites that have an insane amount of content with little natural interaction makes Google believe that the site is not important and will allocate less time for crawling per a visit.\\nIt's about quality not quantity\\nNowadays it is considered better SEO to publish less often and with higher quality content, the type of content people want to see, as interaction is key for rankings, authority and crawl time allocate.\\nCrawl time allocation\\nIf you bothered to read the previous paragraphs you will notice that I've mentioned crawl time allocation, this is key... and most likely the cause of your index issues. \\nYou have an insane amount of pages, Google has limited resources and it will only ever crawl a site for a certain amount of time before it stops and goes to another site. The allocate time that each site is allocated varies depending on how important Google believes your site is, so the chances. You also need to factor in the 'return' rate, of which Google's bots decide to revisit your site between visits.\\nWhat effects Google's Crawl Time and how often Googlebot will return to the site\\nDomain and site authority is a huge factor for Google in which it decides how often it will return to your site and the duration it does, to put this into respective a site like Stack Overflow will have Googlebot pretty much visiting every few minutes a day, maybe even sooner, but a site with just as much content with low interaction and low authority will be at best a few times a day.\\nEnsuring Googlebot can crawl as much as possible every visit\\n\\nWebsite Speed\\n\\nNot only does Google reward SEO value for sites that are fast you can also have more pages crawled between visits. Use website speed tests from multiple locations, and ensure that your website responds fast for both Google and your main target region. I recommend WebpageTest, aim for below 1.5seconds on first visit as a good guide.\\n\\nServer Uptime Availability\\n\\nIf your DNS or server that fails to respond for just a few seconds each day can mean that you missed your return crawl, so its important to monitor your website and ensure that its performing well, Pingdom, and other providers can provide this service for you.\\n\\nRobots.txt\\n\\nEnsure that you have a good robots.txt, most sites don't need to worry about robots.txt since they only have a low volume of URLS, but sites with high volume and low authority, then the time spent on your site is critical and having the Googlebot, crawl, not index things like login pages, and pages with noindex will only prevent Google from spending more on pages it can index. Use robots in conjunction with noindex header responses.\\n\\nClean up 404's with 301 or 410's\\n\\nGoogle loves to crawl, and re-crawl pages that return a 404 status, for most webmasters this is not an issue. However since you have a high volume of pages, it falls back on time is critical. Ensure that your 404's redirect to pages that are on topic, or they return the 410 Gone status. Google will learn by this and should stop attempting to crawl those pages, giving you more index time on pages not crawled.\\n\\nRemove duplicate pages and avoid canonicals\\n\\nNowadays most SEO savvy webmasters will use canonical links to avoid duplication, this for most webmasters is AWESOME! it let Google know what is duplicate, and what is not, but the major problem with canonicals is that they too are a page, and that page needs to be crawled, if you have a page that can be accessed via www, non-www, tag pages and all other type of pages then your simply losing crawl time that could be spent on discovering new pages. So something to bare in mind.\\n\\nCompile pages\\n\\nIf you have a high volume of pages then its extremely likely that you have similar pages, or pages that can be merged. Google loves long pages! so do users, if you have pages 1 of 5, merge them, if you have relevant pages to one another, merge them.\\n\\nCrawl errors\\n\\nActively monitor your crawl errors, these use crawl time up and you need to keep on track.\\n\\nTracking Google\\n\\nRecord when Google visits your site and for how often, keep a track on it and see if you can improve it, doing the above will certainly help.\",\n",
       " \"There is no way to redirect an individual post on a wordpress.com site to an external site for the reason you mention. This is one of the (many) limitations on wordpress.com and it's worth it to really do the research before making a decision on whether to use .com or self-host WordPress on your own (not a dig at you, just putting this here for future readers).\\nWhat you can do is the ever-popular Poor Man's Redirect:\\nGut the content on the page/post in wordpress.com and replace it with a link to the desired page.\",\n",
       " 'One way to do this is to simply define a separate ErrorDocument for each directory. Either create a .htaccess file in each subdirectory with the appropriate directive, or use <Directory> containers in your server config.\\nFor example, in htdocs/clientName/workDivision1/.htaccess:\\nErrorDocument 404 /clientName/workDivision1/errors/404.php\\n\\netc.',\n",
       " 'When a connection to a back-end server generates and error (default is any 5xx code) Apache marks that connection as in an Error State. Apache will then not use that connection for a period of time controlled by the retry parameter to ProxyPass. This defaults to 60 seconds.\\nThis means that unless you change it, once your back-end generates a 5xx return code, Apache will not send any more request to it for 60 seconds, but will immediately return a 503 to the client browser.\\nYou can change this by setting the retry to a lower number. For example, to wait only 5 seconds before re-trying:\\nProxyPass / http://0.0.0.0:3000/ retry=5\\n\\nDetails of this options, and many others, are in the ProxyPass documentation here: https://httpd.apache.org/docs/current/mod/mod_proxy.html#proxypass',\n",
       " 'Google includes uncrawlable pages in the index when they are linked from other sites. \\nThat means that a link to the website like<a href=\"domain.com/en\">[CHINESE] - Hey nighthawk</a> can show up in the search results.\\nSome have suggested that such occurrences are temporary.  They aren\\'t always.   Google indexes uncrawlable pages because sometimes important pages are blocked by robots.txt.  Matt Cutts explains:\\n\\nYou might wonder why Google will sometimes return an uncrawled url reference, even if Googlebot was forbidden from crawling that url by a robots.txt file. There’s a pretty good reason for that: back when I started at Google in 2000, several useful websites (eBay, the New York Times, the California DMV) had robots.txt files that forbade any page fetches whatsoever. Now I ask you, what are we supposed to return as a search result when someone does the query [california dmv]? We’d look pretty sad if we didn’t return www.dmv.ca.gov as the first result. But remember: we weren’t allowed to fetch pages from www.dmv.ca.gov at that point. The solution was to show the uncrawled link when we had a high level of confidence that it was the correct link. \\n\\nYou are unlikely to see this page from your search result except for the site: query that you did.   Otherwise somebody would have to search for [CHINESE] Hey nighthawk or some portion thereof.',\n",
       " \"From what you are describing it almost sounds similar to the way a site is referenced when you add a URL to a Facebook post in that at the end of the post an image from the site, the page title, the URL, and a short extract is added. If this is the case then you will not have any issues as a small extract like that and a single image won't make your page appear as duplicate content. The only way you will encounter SEO issues from duplicate content penalties would be if you copied the entire body or most of the body of the actual content into your own page without defining the canonical link to the originating article, but if you are creating your own content and it simply references the article with a Facebook style link then you will have no issues.\",\n",
       " \"I thought I should answer my own question because I think I've solved the puzzle.  \\nWe are on a new ISP provider with dynamically assigned IP addresses (where previously we had our own static IP address which did not change) and it turns out that the IP addresses we are currently being assigned are on a blacklist - so we are being seen as insecure by FB.\",\n",
       " \"In my opinion each client should have it's own account within an MCC. If you have 1 campaign per client in 1 account i'd suggest researching about the MCC and splitting your client campaigns out into individual accounts. Unfortunately you will not be able to grant access for individual campaigns.\\nIf you are using an MCC you will be able to grant read only access (or different levels of access) for your client for that particular account.\\nIt will also mean that you will be able to create and optimise multiple campaigns at ease for individual clients and will allow for better and more efficient reporting.\",\n",
       " 'Having any word, be it of a country or anything in the title, will positively impact the SEO ranking of your website for that term/word.\\nThe title is very important for higher SEO ranking and therefore should be wisely created/chosen and used.\\nReasons:\\n1) It gives a good indication to a crawler (Search Engine like Google) about the content of page/site        2) It showcases in a rich snippet of search       results and serves as the entry point for your user. If the content is    relevant, the chances of a user clicking your results are higher.\\nSo, if you think that putting Australia in the title would help user reach your site or improves the search result rich snippet in any way, shape or form, then go ahead and use it. Otherwise please ignore and use more relevant words/terms.\\nWhile putting a word like Australia or any other country name for that matter may not hurt your SEO ranking, as i already explained, when a user sees Australia, he/she might get a negative impact if user is not from Australia.',\n",
       " \"Page URL is full url (including protocol) , try contains instead of equals.\\n \\nRef : https://support.google.com/tagmanager/answer/6106965?hl=en\\nTo check value of each built-in variable's on your page follow below listed steps :\\n1. Enabling Preview & Debug mode on your GTM web container\\n\\n2. Open your website, you'll see a Quick Preview panel on bottom of your website.\\n3. Go to the event (Window Loaded/DOM Ready/Page View) you want your variable on and then navigate to variables tab, there you'll find all user-defined and built-in variables. Search for the variable you're looking for and there you'll find it's value listed.\",\n",
       " 'Fixed the issue. Only needed to set the Nameservers to ns1.domain.com and ns2.domain.com and add the Gluerecords. Another (dumb) problem was that I was visiting the website at http://domain.com which was not set (only www one).',\n",
       " '...do I put it after the # END Wordpress in the htaccess or am I missing something?\\n\\nNo. Any external redirect should come before the # BEGIN WordPress code block. ie. Before WP internally rewrites the request.\\nIf you placed it after the WP code block it will simply be ignored, since WP will have already routed the request.\\nOtherwise your mod_rewrite redirect looks OK, assuming your old /blog/ URL always has a trailing slash (to the \"blog root\"), the blog subdomain points to the same place on the filesystem as your main domain and you have already canonicalised the domain (ie. it can only be accessed via the www subdomain).\\n\\n Redirect 301 /blog ....\\n\\nYou should avoid mixing RewriteRule (ie. mod_rewrite) and Redirect (ie. mod_alias) directives. Since these two directives belong to different modules they execute at different times during the request and you can end up with confusing conflicts if you are not careful. So, since you are already using mod_rewrite (WordPress directives) then you should stick to using mod_rewrite for any redirects.\\nIt wouldn\\'t matter were you placed the Redirect in your config file - it would still execute after the mod_rewrite directives.',\n",
       " 'Links like this are often used to bypass spam filters.   They are obscuring the final spammy destination with a URL that appears to point to your site. The forum software likely has anti-spam measures that are fooled by this technique.  \\nThis is bad for your site because this will ultimately get your site added to the list of spammy sites.   When you implement a redirect, it is best to whitelist the domains to which you can redirect.   I typically allow redirects to my own domain name, and allow relative URLs.',\n",
       " 'For now it is possible to revert.   Click on the hamburger menu (top left) and then  at the bottom of the menu there is \"Back to previous AdSense\".  At some point Google will force you to use the new only.\\n\\nGoogle designed the new AdSense interface according to material design principles.  \\nOn the plus side, the new interface is much better on mobile devices. I like the feature on the home page where you can hide the info you don\\'t want to see on a daily basis.\\nI don\\'t like several things about it:  They recently change the homepage numbers to be like \"8.19K\" impressions which I find hard to digest.   They also took away the 28 day comparison which is the thing I look at most frequently.',\n",
       " 'When you request a report, Analytics looks up each metric in the pre-aggregated data tables and serves those results to your reports. If you adjust the date range from August 1st - August 31st to August 1st - September 1st, Analytics looks up each metric in the September 1st pre-aggregated data table and adds the new data to the existing total.\\nThis works well for most metrics. Many metrics, like Pageviews or Screenviews, are simple additive counts over days. However, Users is based on more complicated calculations. Instead of simply adding (or subtracting) processed data from the pre-aggregated tables, Analytics must recalculate Users for each date range that you select in a report. \\nFor example, if a user visits a website on August 31 and on September 1, Analytics recognizes this user as a single user over the course of these two days. If you change your date range from August 1st - August 31st to August 1st - September 1st, Analytics can’t simply add the difference to the value of Users you see in your reports because this number is based on a complicated calculation, and not just added to the running total in the pre-aggregated data tables. Instead, the metric has to be calculated on the fly each time you request it in your reports.\\nCheck out this link for more in-depth info and how to address this challenge. \\nhttps://support.google.com/analytics/answer/2992042?hl=en',\n",
       " \"Set up your mail server(s) to listen at mail.mydomain.com\\nMake sure your firewall doesn't block your mail server(s)'s ports\\nSet an A record on mail.mydomain.com pointing to your IP address\\nSet a MX record on mydomain.com pointing to mail.mydomain.com\\nIt should be working, but you probably wanna set up DKIM, DMARC and SPF\",\n",
       " 'Official Matt Cutts answer might be helpful for you.\\nFooter link is similar to widget link, and Google strongly suggest to add nofollow tag on such a links, because indirectly or directly you\\'re manipulate their pagerank algorithm.\\nActually there are some websites like Wordpress.com and blogger.com add their brand name in footer links when people create a blog like example.blogspot.com or example.wordpress.com, and Google might does not consider them in pagerank calculation. Normally they did not get any penalty, because they have some quality backlinks, and also they are not using keyword rich anchor text in link, like your friends using \"Website Design Company in cityname\" or something like that.',\n",
       " \"The direct answer to the Bounce rate concept has been answered before, here.\\nFrom Google Analytics Help Center:\\n\\nBounce rate is the percentage of single-page visits or visits in which\\n  the person left your site from the entrance (landing) page.\\nThere are a number of factors that contribute to a high bounce rate. For example, users might leave your site from the entrance page if there are site design or usability issues. Alternatively, users might also leave the site after viewing a single page if they've found the information they need on that one page, and had no need or interest in going to other pages.\\n\\nIn reply to your question, yes, it's calculated separately because in fact it has nothing to do with those 2 other metrics you point: it's just an independent percentage of how many users arrive and leave your site without seeing more than one page.\\nThe time a user stays doesn't matter, a user can open your page and allow the session to timeout (30min by GA default) and will still be counted as a Bounce if he only sees one page.\",\n",
       " \"The IP address has no impact on how Google assigns its resources. Google treats each domain and sub domain as separate regardless if they are shared on servers with 10,000 websites or just 1. Googlebot resources are assigned based on authority of the site nothing to do with the server apart from page speed which can increase rankings.\\nA dedicated IP has many benefits but does not directly improve Google's crawl duration or the rate that Google returns to the site.\",\n",
       " 'There are very few restrictions on registering .com domains.   101domain.com has a page listing them.  The only restrictions are:\\n\\nViolating rights of third parties, illegal activities including viruses and hacking tools, obscene and abusive materials, and any names contrary to the government of the United States.\\n...\\nDomain Names must:\\n\\nhave minimum of 3 and a maximum of 63 characters;\\nbegin with a letter or a number and end with a letter or a number;\\nuse the English character set and may contain letters (i.e. , a-z, A-Z), numbers (i.e. 0-9) and dashes (-) or a combination of these;\\nneither begin with, nor end with a dash;\\nnot contain a dash in the third and fourth positions (e.g. www.ab--cd.com); and\\nnot include a space (e.g. www.ab cd.com).',\n",
       " \"An inode limit is effectively a file count limit. Based on the quote from the terms you have provided it appears as though your host is Hostgator, if that is the case then the inode limit means the following for you...\\nThe soft limit of 100'000 is a limit in the sense that once you have more than 100'000 files on your hosting account (across all domains and sub domains) then your site will no longer be included in the weekly backups. You can continue to upload files past this limit but there will be no backup maintained by Hostgator. This is not so much of an issue if you use cPanel's built in backup feature to download your backup archives regularly but if you don't then you need to be aware that backups won't be available after you have exceeded this file count.\\nThe hard limit of 250'000 is a total limit in that if you exceed 250'000 files on your shared hosting account then your account may be flagged for review and/or suspension.\\nAs for how to avoid exceeding this limit an average site suited for shared hosting should not come close to this file limit. Even very large application based sites with a ridiculously large number of library files and plugins still would be under 10'000 files (one Joomla installation I performed for a client with nearly 50 extensions and over 100 plugins reached an inode count of 15'000). cPanel has an inode count indicator on the control panel that you can check whenever you need. It is good to check this figure after a large site refactor or a large number of new files being uploaded, as well as regularly if your site uses file based caching or provides an option for end users to upload files to the site.\\nIf you come close to hiting a 250'000 inode limit then chances are your site would be better suited to a VPS server instead of a shared server.\",\n",
       " \"As has been stated in the comments above Google treats sites differently based on protocol as well as if they have different sub domains (including www and non www sites). what this means is that...\\nhttp://example.com\\nhttps://example.com\\nhttp://www.example.com\\nhttps://www.example.com\\nhttp://some-other-sub.example.com\\nAre all treated as different sites for crawling and indexing purposes.\\nAs a side note there was an unconfirmed update to Google's algorithm on the 10th May 2016, an update I add that Google has not announced or confirmed either, and so this error may have been due to that update as well. The message appears to have hit many other users as well around the same date.\\nDue to Google's lack of details on this incident there is no real way to know for certain if this was a temporary bug or if this was due to a change in the indexing algorithm. If it was a temporary bug then I would expect your sites index to return to normal, if not then chances are it has been adjusted by an algorithm change.\\nAdditionally I note from your comment that you have canonical links on your site. You should ensure that your canonical links are an absolute URL not a relative path and that the root protocol and domain are the main ones that you want appearing in the Google index and that you want customers to access predominantly. In other words if you want the Google SERP to show your site as https://www.domain.com/whatever then you should make sure that all of your canonical links start with https://www.domain.com/.\",\n",
       " 'If your page is recognized as an authority on the subject of XYZ, it will rank for it, regardless of whether that keyword appears on the page.\\nOn-Screen SEO\\nIs your page still “about” XYZ? If so, just removing the keyword might not have much effect.\\nOff-Screen SEO\\nOn-screen optimization of keywords is only part of Google’s (or any major search engine’s) ranking algorithm.\\nIf other websites link to yours, that “juice” will pass to the page. That might include:\\n\\nLinks to your page that include XYZ in their anchor text.\\n\\nPages about XYZ that link to your page, especially if these pages\\nare respected.\\n\\nWhat to do about unwanted off-screen SEO factors.\\nCheck your backlinks, and use your judgement on which links may be driving the unwanted traffic.\\n\\nDisavow the links in Google Webmaster Tools.\\nContact the other sites, requesting they remove links to your page.\\nContact the other sites (where relevant) requesting they remove the problematic keyword from the anchor text of links to your page.\\n\\nCAVEAT: The first two techniques above will make your page generally less attractive to Google, not just for the particular, unwanted keyword.\\nRemoving a given page entirely from search results\\nIn the last line of your question, you sort of veer away from “how to I reduce SEO for XYZ” and just ask how to get the particular page out of search results.\\nTo ensure the page does not appear in any search results, you can add a no-index meta tag to the page.',\n",
       " \"The first thing to remember is that what may seem like a slow crawl rate may not necessarily be slow and that when it comes to crawl-rates concepts like slow become meaningless.\\nMy Site Hasn't Been Crawled Yet\\nFirst things first, before Google can crawl a new site they need to know it exists. Google gets this information from a number of sources including...\\n\\nExisting sites with links to your new site\\nSubmissions of the fetch as Google form\\nUploading a sitemap for your new site using Google Webmaster Tools\\n\\nWhen the new site is identified it is placed into a queue to be crawled by one of Google's Googlebot web crawlers. How long it takes from this point can vary from a few hours to a few days or more depending on a number of proprietary factors that Google doesn't publish.\\nOnce the site has had its existing crawl it is now in the Google index and can be searched, it will also be regularly re-crawled.\\n\\nMy Site Hasn't Been Re-Crawled For (XX) Number of Days\\nThis statement is often made when a site hasn't been re-crawled for anywhere from a few days to a few weeks. Your site not being re-crawled yet doesn't mean that Google has forgotten about you. For newer sites, sites that don't change a huge amount, and sites that have not been deemed by Google to be of high authority the crawl rate is often up to every 2+ weeks. The purpose of this is to not waste resources on re-crawling a site that may or may not have had new content added in the mean time, as well as to not cause an undue load on a website where that load may not be required. The frequency of re-crawls ramps up and ramps down based on the results of previous crawls. If Google does a re-crawl after two weeks and finds a large number of new content and the new content is assessed as being of a high quality then it is more likely that the next crawl interval will be sooner. This is not a hard and fast rule however and as with most things to do with Google and other search engines quality is everything, as such a site that may have been getting crawled every week at one point may find the crawl frequency reduced if the quality of the content on the site is assessed as not being reasonably high or the change frequency of the site is relatively low.\\n\\nWhat Can I Do To Speed This Up?\\nThe first thing I will say it to not try to do anything with the intention of speeding up the crawl process for your site. Realistically focusing your energy on improving your site from the point of view of the end user and continuing to increase the valuable content on your site will do more to increase the crawl rate than any tricks or black-hat techniques.\\n\\nHow Do I Know Something Hasn't Gone Wrong?\\nThe first step is to sign up for Google Webmaster Tools. Using GWT if Google attempts to crawl your site and can't for some reason, or there is an issue with your site that needs to be addressed this is the first place that a notice will be issued. Using this tool you will also be able to see when your site was last indexed, how many pages from your site have been indexed, and other metrics such as those that can be useful in performing your duties as a webmaster.\",\n",
       " \"So you would need to create a segment that only includes 'Operating System Version contains XP' - Give it an appropriate name aswell. \\n\\nThen when you go back to your reporting, remove the 'All Users' segment. \\nClick Audience > Technology > Browser & OS \\n\\nFollowing this, you should be able to use the first Dimension as Browser Version, then add a second dimension which is 'Browser' - Click Internet Explorer when all browsers are displayed. \\nThis will segment all your users to only include the XP OS\\nAnd you can see the data with two dimension of version and Browser.\",\n",
       " \"It's used for sharing links easily. There are no SEO benefits.\\nHere in stackexchange, you can see h1 tags and hyperlink together, because it is easiest to copy paste when someone want to reference another question from SE. You can still copy paste from browser address bar, but this one, make copy paste faster.\\nLink with better anchor text is good practice for SEO.\\nExtra note: Most of the people don't do that, because Googlebot follow the link structure and crawl all the links. Suppose Google can crawl 500 URLs in one day, and you place header link, then your 1 crawl will be waste in header link, because Google already is crawling it. There are no official statements on how Google crawl links, so I can't add more value on this. Generally I don't do that, unless I have specific reason which does not related to SEO.\",\n",
       " 'Simple answer is no... however additional information appearing in the serps can indirectly improve your SEO because people are more inclined to click things that shows additional information, particularly review stars as a good example.\\nObviously the more traffic your site receives the higher the chance of someone linking to the site or sharing it on social media which effectively increases your SEO directly, and naturally. \\n\\nSOURCE\\nWhether structured data affects rankings has been the subject of much\\n  discussion and many experiments. As of yet, there is no conclusive\\n  evidence that this markup improves rankings. But there are some\\n  indications that search results with more extensive rich snippets\\n  (like those created using Schema) will have a better click-through\\n  rate. For best results, experiment with Schema markup to see how your\\n  audience responds to the resulting rich snippets.',\n",
       " 'Aloha Simon, I actually follow you. :)\\nAnyhow, there are a lot of he said, she said, Google said in regards to SEO.\\nWhat I have noticed from experience.\\nAdding an area code has worked small wonders in the meta description. \\nThe reason being people are actually searching IE. \"custom auto body shops 209\"\\nThey tend to use it more as an extended search opposed to saying the county after extending it past a city search.\\nAdding a complete phone number.\\nAs you mentioned there are sites that scrape the info and post it. This is ok as they are only posting numbers and general business info. Google knows how your site/number is being presented and/or backlinked. I have never noticed a drop or increase based on that alone. A phone number is something you should already have and if one searches it for whatever reason, you will show up and so will the number scraped sites. Again that\\'s fine because it only validates the business is whom they claim to be.\\nYour number is on the site already\\nI assume every site should have a number listing already so it won\\'t stop other sites scraping your site and again should not hurt you.\\nAs for the address\\nLike numbers I do not think the full details should be used. Except for a contact page and as an address.\\nHow I have done it with best results is this.\\n\\nUse schema coded cards and use full details in there ( IE, person/business, position, number, address etc.\\nIn the meta I use something like this\\n\\nTitle: Custom Auto Body Shop San Jose, CA 95136 | (408) 555-1212\\nDescription: ABC is a professional custom auto body shop located in San Jose, CA 95136. Call (408) 555-1212 Free Estimates.\\nI have gotten incredible results from this method and use the same concept when creating multiple city page listings.\\nBelieve it or not some of the sites I have done this way are listed up to 5 times on the search listings first page.\\nNote: As you can see adding a number creates a CTA on the search results and for some businesses that info is enough for them to call you without visiting your page. So it is something to keep in mind with Analytics if you are heavy into that info as well.',\n",
       " 'Pricing is very dependent on which services exactly you plan to use. There is a pretty decent pricing calculator for google cloud services here:\\nhttps://cloud.google.com/products/calculator/\\nDetails about app engine specific service fees can also be found here:\\nhttps://cloud.google.com/appengine/pricing\\nGenerally you would pay for the number of instances selected to host your app. Then you have to pay for any network traffic generated by them and any cloud storage you require (as I said before there are other services you might use, which will add costs).\\nAs far as I know you can easily define a \"spending limit\" for google\\'s app engine, which makes it very easy to stop spending money at a certain point. There are also a minimum number of resources provided to you before you have to pay anything (so you can for example host a low traffic website for free). If traffic suddenly picks up, the site may eventually go offline after you reached your max. resource limit, though.\\nThe main difference between google (or amazon) cloud services and the dedicated hosting providers is that with google (and amazon) you pay for what you use, while with the others you generally pay for what you signed up for (to be honest I\\'m not too familiar with their business model - so correct me if I\\'m wrong).\\nThe general idea when using google (or amazon) cloud services is that you can always easily scale up (and down!) as required.',\n",
       " 'Your invalid certificate authority error is due to the fact that CloudFlare issued it, not because of how you were routing traffic. This is because you used a type of certificate meant only to secure communication between your origin server and CloudFlare\\'s network. It is issued via what they call their \"Origin Certificate Authority\" explained here.\\nWhen you use CloudFlare\\'s \"Universal SSL\", they will create a certificate from a legitimate Certificate Authority that is trusted by most browsers and they will serve your website\\'s content from their servers using that real certificate.\\nRemember that for end-to-end TLS encryption you cannot use CloudFlare because they have access to your decrypted traffic after it arrives from your origin but before they re-encrypt it for their CDN. They are literally MITM\\'ing your encrypted traffic. That\\'s acceptable if you are aware of it and choose to use it anyway, but you should know this is happening.\\nFor a fuller explanation see this website describing the problem in greater detail. Not the prettiest looking site, but their technical arguments are irrefutable. ',\n",
       " \"According to Google itself, this is what they have to say about the meta tag description, here.\\n\\nThe description attribute within the  tag is a good way to\\n  provide a concise, human-readable summary of each page’s content.\\n  Google will sometimes use the meta description of a page in search\\n  results snippets, if we think it gives users a more accurate\\n  description than would be possible purely from the on-page content.\\n  Accurate meta descriptions can help improve your clickthrough;\\n\\nThis means your description is used as an indication to Google of what is inside the page and also as the text used in the snippets, providing information to the user that might lead him to click in your page, if it appears in the Search Engine results page.\\nWhen developing a website, make sure you write for people, not bots. My advice is to don't care if the description is the same as the content as long as it's the appropriate text for your users.\",\n",
       " \"The registrar carries the domain name and it is there where you can tell them where the nameserver(s) are. You cannot have a domain name that is not registered at one registrar point to nameservers not registered with it. It is a registrars main job to do this correctly.\\nSince you can't point the main domain with any other registrar, you can't point subdomains either.\\nTo possibly help clear things up, you should look into tools for discovery like nslookup, dig, drill, and following information from there with whois. \\nThere are more tools but let's see how far that gets you.\",\n",
       " 'You domain is using the following nameservers which you have set at the registrar (1&1):\\nns1.allmaint.arvixevps.com\\nns1.allmaint.arvixevps.com\\n\\nTherefore the domain is using the Arvixe DNS. So out of the two pictures you have shown the top one is the DNS records actually being used and the one in control.\\nThe settings in cPanel have no effect at all because the domain is not pointed to the cPanel nameservers. If you changed the nameserver at the domain registrar (1&1 in your case) to your cPanel nameservers then the cPanel DNS would be used.\\ncPanel has a build in DNS and by default is expecting users to use it. For ease of use to the user if they were to use the cPanel DNS it will auto create records for any new domains you add or sub domains; while in your case you will have to manage your records at the registrar.',\n",
       " \"Google will occasionally rewrite titles and descriptions to give people a more varied search result. It is true that the video you mention does not mention 'domain' in the title or the description within the source.\\nHowever, YouTube as you know brings up similar videos in the sidebar, these may be very relevant, or slightly relevant. Google can rewrite titles and descriptions based on everything found on the page, in this case it is rewriting the description and using content from other videos in the side bar.\\nGoogle in some point of time found these videos in the sidebar:\\n\\nDomain Re-animator Review: Building a PBN on a Budget\\nAmok Time Re-Animator Herbert West\\n\\nThese may appear in the title, or description, or both. If the Video is not relevant to what a user is searching for then they are likely to find what they are after in the sidebar, so its not a major issue. \\nThis is not Google misbehaving and in fact is intended by Google. Rewriting happens more often than you must likely realise, its not an issue and actually improves the user experience.\",\n",
       " \"It's not possible without some form of batman and robin cloak\\nThere is absolutely no way without some form of cloak to prevent Google seeing that pop-up. Google expects you to treat Googlebot as new visitor every time. Just because Google doesn't store a cookie doesn't mean you should treat them as a returnee. \\nGoogle has to consider both new visitors and the returned\\nGoogle has to factor new visitors since the majority of your visitors coming from Google will be 'new' visitors, why should they care about returnees that may not even use Google to view your site.\\nVisiting your favourite site to find some obscure content is a like putting on your favourite t-shirt only to find out its still damp\\nPurposely obscure content dampens users experience and one of the reasons why preventing Google seeing that is definitely cloaking, regardless if its once, or a hundred times, Google has new visitors to factor. However it should be noted that Google has not officially said that such overlays, modals and popups will result in lower rankings on the desktop environment, this does not however insist that they don't already have some form of unconfirmed algorithm, or add it later.\\nMobile Only: Google has stated that in 2017 it will rank such sites lower\\n\\nSOURCE\\nPages that show intrusive interstitials provide a poorer experience\\n  to users than other pages where content is immediately accessible.\\n  This can be problematic on mobile devices where screens are often\\n  smaller. To improve the mobile search experience, after January 10,\\n  2017, pages where content is not easily accessible to a user on the\\n  transition from the mobile search results may not rank as highly. \\n\\nYour two choices are:\\n\\nCloak it using user agent, hostname or IP\\nRemove it for both users and search engines\",\n",
       " 'I think we figured out how to diagnose the issue. All this time we were focusing on error logs because we consider a 500 server crash an error, and we specified errors should be logged to error_log in the configuration files.\\nOut of desperation, we grep\\'ed the entire file system for *\"Internal Server Error\". Lo and behold, mod_security is not honoring the configuration settings:\\n# grep -IR \\'Internal Server Error\\' /var/log\\n...\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log:HTTP/1.1 500 Internal Server Error\\n...\\n\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161024:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161016:HTTP/1.0 500 Internal Server Error\\n/var/log/httpd/modsec_audit.log-20161016:HTTP/1.0 500 Internal Server Error     \\n...\\n\\n<no error_log entries>\\n\\nI\\'m OK with mod_security logging to their own file. I take exception that they did not log to where we told them to through php.ini, and it has wasted months of man hours and trouble shooting. All they had to do was write an occasional message into error_log and this would have been fixed months ago.',\n",
       " 'Here are stats about webmail usage of the top web mail providers:\\n\\nGmail: 16%\\nOutlook.com 5%\\nYahoo! Mail 3%\\n\\nGiven that these are the top 3, it makes sense to create links specifically for them.   All of them have URLs that will open a compose email message in their system.\\n\\nGmail: https://mail.google.com/mail/?view=cm&fs=1&to=TO@EXAMPLE.COM&su=SUBJECT&body=BODY\\nOutlook.com: https://outlook.live.com/owa/#subject=SUBJECT&body=BODY&to=TO@EXAMPLE.COM&path=%2fmail%2faction%2fcompose\\nYahoo: http://compose.mail.yahoo.com/?to=TO@EXAMPLE.COM&subj=SUBJECT&body=BODY\\n\\nAlong with the classic mailto: link:\\n\\nmailto:TO@EXAMPLE.COM?subject=SUBJECT&body=BODY\\n\\nPlanetScale Networks suggests in his answer that craigslist does something similar.  They include AOL (presumably because it has been bigger historically).   They also call the Outlook option \"Hotmail, Outlook, Live mail\", presumably because it has had other names over the year to which users still refer to it.\\n\\nFor completeness, here is the AOL link for compose: http://mail.aol.com/mail/compose-message.aspx?to=TO@EXAMPLE.COM&subject=SUBJECT&body=BODY\\nWhen you use these URLs, parameters must be URL encoded (percent encoding) so THE SUBJECT would need to be THE%20SUBJECT to be put into one of the links.\\nSources: Hotmail/Live.com mailto: URL no longer working. Is there a new format?, What is the format of compose links to web mail clients such as Gmail, Yahoo Mail, Hotmail and AOL?',\n",
       " \"Google handles special chars, and it seems they don't affect rankings:\\nhttps://www.google.es/search?q=Google%C2%AE&oq=Google%C2%AE\\nhttps://www.google.es/search?q=Google*&oq=Google*\\nhttps://www.google.es/search?q=Google*%2F&oq=Google*%2F\\n\\n; but as Matt said, you should avoid to make thinks harder for robots, google's or any others:\\nHow does Google handle special characters? by Matt Cutts\",\n",
       " \"I've never heard anything about a penalty due to this. At worst you're wasting the spider's time, but that's part of why we have computers in the first place: doing tedious repetitive things. Still, you should ideally be addressing the issue.\\nThis...\\n\\nMy solution would be to only change the entry only if the new imported product data differs from the previous data.\\n\\n...is what you should be doing in the first place, regardless of external considerations like sitemaps, etc. If your content isn't different(and I would include deleting and replacing with identical information in that description), then your lastmod date shouldn't be modified. Here you're wasting your own resources. You haven't said how many products are involved, but there's going to be a point where this process is going to get slow and computationally expensive.\",\n",
       " 'As you suggest, neither of those two rules can be redirecting /alfred-victoria-blah-blah. So, either there is something else that is performing this redirect, or a previous (erroneous) 301 redirect has been cached - clear the browser cache etc.\\nAlso, confirm whether this is a 301 redirect and not a 302 or some other 3xx (could help indicate where this is happening).\\nIt would seem that an old redirect was cached on this occasion.',\n",
       " 'Generally speaking, Google does not like directory sites too much. Some perhaps. However, most are generally either low quality or spammy or even replicated information that can be found everywhere. For back link building, using directory sites is not recommended. This was all the rage way back when. But not today and should be generally avoided.\\nCreate organic inbound links (back links) from higher quality content sites with links preferably in the content itself or immediately following the content. Do not go for the low hanging fruit that anyone can do for themselves. You will simply be another sheep in the herd. Do not be a sheep. Be a wolf.\\nPlease remember that many SEO sites are junk. Why? Because too many are parroting content with no real research. They are designed as \"me too\" sites trying to carve out a segment of the SEO pie. Many repeat bad advice and too many have old advice that were trends that will get you into trouble. Keep in mind that most SEOs are not technical people and do not understand search and the technologies behind search and therefore cannot understand the nature of a search engine. Always look for the best information out there, not advice, that is within the past year or so. Not older posts and not sites that repeat what everyone else says. These are the guys that tell you to create links using directories. Stay away from these sites. Please.\\nWhen in doubt, ask here before doing anything. We are real experts and can point you in the right direction.',\n",
       " \"If an user looks for a keyword in Russian, Google will display results using that alphabet. Using the example you provided:\\n\\nGoogling Рефера́т, look how it makes Рефера́т bold, but not it's equivalent in the latin alphabet. \\n\\nGoogling the other way around, it only highlight the relevant search, but not it transcription in Russian.\\n\\nThe same happens in the URL, I think a Russian user would make more sense to look for words using the Cyrillic alphabet instead of the Latin one.\",\n",
       " \"Leaving a domain parked at the domain registrar after registration won't hurt your eventual rankings in any way.   Leaving a domain name with a default page at the domain registrar is very common.  Pretty much all domains are in this state for at least a short amount of time.   Many are left there for months or years.\\nGoogle doesn't want to index the site while it is parked at the domain registrar.  Google says:\\n\\nParked domains\\nParked domains are placeholder sites with little unique content, so Google doesn’t typically include them in search results. \\n\\nHowever, once you put your own content on that domain, Google will crawl it again and immediately notice that it is no longer parked.   At that point, Google will index your content just fine.\\nGooglebot has special code to help it identify parked domains.  It will choose not to index the domain while parked, but no penalties are applied against the domain.  You will be able to get it ranked again at any point you choose to unpark it.\",\n",
       " \"There are privileges which bureaucrats do not have by default, and it is possible to give those to them (or some other group or a specific user if you prefer), e.g.\\n$wgGroupPermissions['bureaucrat'] += array_fill_keys( $wgAvailableRights, true );\\n\\nSee the manual for details.\\nIt won't help you, though. Submitting a page edit works the same way no matter what privileges you have. You really should debug and fix your problem instead of trying to find weird workarounds.\",\n",
       " \"It's depend on what kind of ad network you're using. If you're using Google Adsense, then it is good choice to go with responsive ad unit. \\nYour marketing team said, 300*250 pays more than responsive ad, then it's not right 100%, you might get more or less clicks on that ads. There are thousand of people (Advertiser) don't know how to create banner ads on Google Desktop Network (GDN for advertiser - Adsense for Publisher), so GDN automatically pick best free stockphotos for their advertiser, and said them to put your banner ads title, which look so good and professional. \\nWhen advertiser use GDN, Google strongly recommended to use responsive ads unit, because it is fit on most of website, and many of advertiser go with them. And so on Google also recommended publisher to use Responsive ad unit, because there are many of advertiser use that option while serving ads. \\nCPC is Mostly depend on targeting option, like male, female, age, interest, topic, geo etc . Well, you can use media queries to choose which ad size you want to display for specific device size. \\nAds type, ad space, size, cpc is not same for all publisher, you should do A/B test yourself.\",\n",
       " 'What does each of this characters mean in context of an URI?\\n\\nNote that a URI consists of several different \"contexts\" or parts. eg. the scheme, host, URL-path, query string and fragment identifier. Whether a reserved character has special meaning is dependent on where in the URL that character is used. These \"reserved characters\" are often used to delimit parts of the URI. You only need to encode these characters if they conflict with the \"special meaning\" (if any).\\nSome characters are simply defined as possible alternatives to others, so might not carry any special meaning (on the server) where it is processed.\\nI\\'ll start the ball rolling, some I\\'m not sure about:\\n\\n! (Exclamation Mark) - not sure. JavaScript does not encode this character. It has historically been used to represent AJAX URLs in the fragment identifier. eg. #! (hash-bang) - but this was Google, not a \"standard\".\\n# (Hash / Pound) - Delimits the fragment identifier. (Last part of the URL, client-side only, not passed to the server.)\\n$ (Dollar) - Not sure. No special meaning in the URL-path or query string that I am aware of. However, JavaScript\\'s encodeURIComponent() will encode this character (whereas encodeURI() does not) which implies it might have special meaning in the query string.\\n& (Ampersand) - Used to delimit query string parameters.\\n\\' (Apostrophe / Single quote) - Not sure. No special meaning in the URL-path or query string. JavaScript does not encode this character.\\n( and ) (Parentheses / Brackets) - Not sure. No special meaning in the URL-path or query string.  JavaScript does not encode these characters.\\n* (Asterisk) - Not sure. No special meaning in the URL-path or query string. JavaScript does not encode this character.\\n+ (Plus) - Can be used to encode a space (alternative to %20) in the query string only. A literal + when used in the URL-path.\\n, (Comma) - An alternative to ;.\\n/ (Slash) - Delimits path segments in the URL-path.\\n: (Colon) - Delimits the scheme from the host and host from the port.\\n; (Semicolon) - Delimits URL parameters in the query string.\\n= (Equals) - Delimits name/value pairs in the query string.\\n? (Question Mark) - Delimits the start of the query string.\\n@ (At) - Delimits userinfo in the authority part of the URL.\\n[ and ] (Square Brackets) - Note sure. These are used by PHP to allow array-like parameter names in the query string - although I don\\'t think this is the \"official\" use.\\n\\nJust to note the differences between PHP (urlencode() and rawurlencode()) and JavaScript (encodeURIComponent() and encodeURI() respectively) when using the built-in functions to encode parts of the URL... PHP encodes all \"reserved\" characters, however, JavaScript (in the browser) is far more selective.\\nFurther reference:\\nRFC 3986 - Uniform Resource Identifier (URI): Generic Syntax',\n",
       " 'This is a Joomla 0 Day Attack. Information found here: https://blog.sucuri.net/2015/12/remote-command-execution-vulnerability-in-joomla.html\\nThis is not a vulnerability test despite the __test. It is an attack.\\nMake sure that any Joomla install is as up to date as possible.\\nAnother option is to simply use .htaccess to intercept this exploit by looking for a common string, \"__test\" would work, and redirect to some other place.',\n",
       " 'Your HTML is incorrect, as you seem to realise. HTML entities inside attributes should be correctly encoded, so:\\n<meta property=\"og:title\" content=\"14&quot; Lamp Stand\"/>\\n\\nthen you won\\'t have any problems.\\nSearch Engines may well \\'figure it out\\', but you\\'re giving them bad data and hoping they interpret it correctly. Much better to just fix the problem if you can.',\n",
       " 'The site you are referring to does not have a description meta tag. The description meta-tag is often where SERP snippets come from assuming that they are crafted well.\\nIt appears that Google is taking the SERP snippet from two places, one is an image alt tag alt=\"compass-cruises-612x340\" and the other is from the first paragraph tag. If the description meta-tag is missing, as it is in this case, or not crafted well, Google will attempt to create a snippet from information found on the page or from another source (rarely). One thing that can effect this are the search terms used.\\nThis should NOT happen. This is a Google error.\\nHowever, this would not happen if the site had crafted a proper description meta-tag. This is a failure with both the site itself and Google.',\n",
       " \"While I am not too critical of Google, it appears they dropped the ball on this. It appears that in the beginning, when Google started adding restaurant menus to it's search results, it relied upon using 3rd party sources for the menu data. This makes sense of course, however, Google, at the time and now, cannot know if the data they are presenting is accurate.\\nGoogle stated at the time We get all of our menu data from a partner, similarly to how we show other types of answers, like weather. without citing what sites they are getting the data from. Google has a history of secrecy that does not always serve them well. This is one case. Some have speculated that this data came from allmenus.com and viewmenu.com. It is possible that today there are other sources.\\nThere have been some rumbles about how Google has not added the menus to their knowledge graph, however, I believe this is only half true. I am sure that Google was always using the knowledge graph, however, not populating it directly from the restaurant sites, but from niche aggregator sites. All well and good assuming these aggregator sites are up to date.\\nIt is possible, and rightly it should be so, that Google now takes this data directly from restaurant sites or perhaps sometimes does assuming that the data is not available somewhere else. There is no information on this.\\nIf this is so, if Google takes the data from the restaurant site directly, then there is only one course of action you have. Use Fetch and Render to fetch the menu pages and Submit to Index using Googles Search Console. Keep in mind that this step is not immediate, however, the regular googlebot will visit your pages in 1-2 days and make the update official. This does not directly update the knowledge graph of course. However, it may have an influence. Who knows? Otherwise, you would have to wait for Google to get it's act together.\\nIf Google is still using aggregator sites for the source of data, then you can simply search for your restaurant name and something false or out of date in your menu in an attempt to find the source or sources. It may be possible to update your menu on these sites. Even then, if you update your menu data on these sites quickly, you are dependent upon Google to update their knowledge graph. This does not always happen quickly. Sheesh. Damned if you do and damned if you don't.\\nWhile I like the feature, the data itself is rarely static and should be updated relatively quickly where ever it exists. Google is in the best position to ensure that the data they present is as up to date as is reasonable providing they are willing to take on the task properly.\\n<rant>In googles zeal to expand it's offerings and algorithms, complexity, stove piping, and other issues have made quite a few Google fails of late. Google has a long list of fails throughout their history. So what else is new? I just answered another question tonight with another colossal Google fail. This is clearly a weakness in what is an otherwise outstanding feature that can really benefit their user pool and websites alike.</rant>\",\n",
       " 'Rule number 1... never ever share passwords\\nSharing username and passwords is considered a bad practice, should it be for internet banking, social media or SQL databases, never use the same password and preferably use a different username. Sharing one password means they only need to hack one password and the vault is open.\\nInvestigate how they got in...\\nYou should first investigate how they got into your server... its much easier to exploit vulnerabilities rather than brute force a password assuming your password does not contain a directory word or a short random password. You should be able to scan through access and error logs to identify the method used. It\\'s pointless using secure passwords if your site is open to attack by other means. \\nLocalhost SQL access only\\nFurthermore unless your site uses a external cloud based SQL service then it should be impossible for external hackers to brute force their way into the SQL database, since SQL users should be set to \"LOCALHOST\" only. If they have access to your database then its more likely they have done so through your content management system and not directly to the database itself. \\nShort Passwords can take be cracked easily \\nUsing a short password could take a few days to crack while a complex password with a good character length can take years. I recommend that you visit Stephen Ostermiller\\'s website: Password Creator, not only will it help you create a complex password, it will estimate how long it would take someone to crack.\\nWordPress sites\\nIf your site(s) use WordPress then I recommend that you checkout my blog post 10+ ways to stop WordPress Brute Force Attacks.\\nSummary...\\nDon\\'t make it easy for hackers, use a different complex password to login to your content management system, a different complex password for the database and a complex password to your hosting package, everything should be complex and everything should be unique and not similar.',\n",
       " 'Yes, Google still crawl webpages that have noindex tag.\\nBut if you have same content on two different webpages and one URL contain noindex tag, while second does not, then you should not worry about it, because out of all duplicate content only one webpage is indexed by Google. Rest of webpages are crawlable but not indexed in Google search result, so that is fine. ',\n",
       " 'I managed to solve my problem with this video : http://www.tisuchi.com/upload-laravel-projecy-cpanel/',\n",
       " 'I assume you mean two Offer items for one Product (like your example suggests).\\nInstead of repeating the offers property, you have to use one offers property with an array value (in [ and ]):\\n\"offers\": \\n[\\n  {\\n    \"@type\": \"Offer\"\\n  },\\n  {\\n    \"@type\": \"Offer\"\\n  }\\n]',\n",
       " 'The title tag can be ignored completely as long as you fill in the alt tag as far as image SEO is concerned.\\nAs far as I have been able to figure out, Google has four sources for the text it associates with images:\\n\\nThe alt image attribute\\nThe title image attribute\\nThe image URL file name\\nText within the div that contains the image (eg <div><img src=\\'...\\'> Text associated with the image.</div>\\n\\nOf the four, the text in the containing div seems to be the most powerful.  Google tends to like to index text that users can see.    I have had the best luck getting an image associated with text by enclosing both the image and the text directly in a div.\\nThen to get images to rank well, you need \"high quality\" images.   Google pretty much defines \"high quality\" as \"large\".   Generally, the more pixels in the image file, the better it will rank.',\n",
       " \"Browsers (including Firefox) sometimes cache the fact that a URL redirects. If you have changed your rewrite rules, make sure you clear the cache in the browser before re-testing. (Although you wouldn't usually see evidence in your server log in that case.) \\nAs for how to investigate, I use Live HTTP Headers extension for Firefox to figure out what HTTP(S) requests are actually being made by Firefox in situations like this.\\nI think in your case you may have had a 301 redirect configured at one point from /wiki/Main_Page to /wiki/.   Firefox got that permanent redirect into it's cache.   Then later when it got /wiki/ it found that was a 302 redirect to /wiki/Main_Page.  302 temporary redirects are not cached, so it was doing:\\n\\nFetch from server /wiki/, 302 to /wiki/Main_Page\\nFetch from cache /wiki/Main_Page, 301 to /wiki/\\nFetch from server /wiki/, 302 to /wiki/Main_Page\\nFetch from cache /wiki/Main_Page, 301 to /wiki/\\nFetch from server /wiki/, 302 to /wiki/Main_Page\\nFetch from cache /wiki/Main_Page, 301 to /wiki/\\n...\\nError, infinite redirect\",\n",
       " 'Check the search console - it will show which pages are linking to these 404 pages. \\nIf these links are all external than there is probably nothing to worry about - just hacked sites linking to your domain. It could be that somebody tried to hack your site as well but did not succeed\\nIf part of the links are coming from pages on your site, you site was probably hacked.\\nTo be on the save side - change all passwords (cms/server) and check if you are using the most recent version of all the plugins you are using - if not update them.\\nYou might want to check these resources as well\\nMy site has been hacked and\\nGoogle help for hacked sites',\n",
       " 'If you want to block all users that trying to access your website with other way than your domain name, you can do this :\\n<VirtualHost example.com:80>\\n   ServerName example.com\\n   ServerAdmin webmaster@example.com\\n   UseCanonicalName Off\\n   <If \"tolower(%{SERVER_NAME}) != \\'example.com\\'\">\\n      AllowOverride None\\n      Require all denied\\n   </If>\\n</VirtualHost>\\n\\nAll access trying in other way than example.com will result on :\\n\\n403 Forbidden\\nYou don\\'t have permission to access / on this server.\\n\\nOTHER WAY, DIFFERENT RESULT :\\nIf you want to make a rewrite rule, so all access trying in other way than example.com will result on a redirect to example.com, you can do a simple redirect rule like this :\\nRewriteCond %{HTTP_HOST} !^example.com\\nRewriteRule ^(.*)$ http://example.com/$1 [R=301,L]\\n\\nFor www :\\nRewriteCond %{HTTP_HOST} !^www\\\\.example.com\\nRewriteRule ^(.*)$ http://www.example.com/$1 [R=301,L]',\n",
       " \"As you know China's notorious firewall has blocked thousands of websites in the mainland of China which include the likes of Yahoo, YouTube and even Google but generally the likes of the blocks are because these sites fail to comply with their internet censorship program.\\nUnderstanding China's Censorship Policies\\nTo understand what sites China blocks, or may block in the future I recommend that you read how China allows government criticism but silences collective expression. China doesn't block encryption it blocks: \\n\\nSOURCE\\n\\nIP Blocking\\nDNS hijacking\\nKeyword content inspection/filtering\\n\\nChina's firewall is far from perfect and they will only be able fully understand packets of data that are not encrypted, they do not currently blocked HTTPS SSL encrypted connections unless they have blocked the website by IP, or by DNS hijacking.\\nOnline China Website Firewall Checkers\\nThere are many online checkers that will physically test a website as a real visitor from someone behind the Great Firewall of China.\\nUsing my own SSL enabled website you can see I get status 200 OK:\\nHTTP/1.1 200 OK\\nDate: Sun, 06 Nov 2016 17:52:36 GMT\\nServer: Apache/2.4.18 (Ubuntu)\\nX-Pingback: https://www.bybe.net/xmlrpc.php\\nLink: <https://www.bybe.net/wp-json/>\\nLink: <https://www.bybe.net/>\\nVary: Accept-Encoding\\nTransfer-Encoding: chunked\\nContent-Type: text/html\",\n",
       " 'I would like to know if the current setup is going to give me trouble in the future.\\n\\nYou\\'ll need to compare your setup with the filesystem the server uses and what you already have stored on the server. Some file systems may have a limit on the number of directories that can be made in one folder, but if you are only doing a few folders, this should not be an issue. A work-around to this can be to map your URL\\'s to a script, and this can easily be done with apache\\'s mod_rewrite module. For example, this code for mod_rewrite works:\\nRewriteRule ^(.*)$ /runscript.php?spec=$1\\n\\nWhat this rule does is executes /runscript.php on your server passing the folder name as the parameter value for \"spec\". then you can use your script to determine the value and display the appropriate content all from one file. That way, you won\\'t need to worry about creating separate folders or files, plus it\\'s easier to maintain.\\nIn your PHP file, you can have the following:\\n<?php\\n$spec=$_GET[\\'spec\\'];\\n\\nif ($spec==\"486_machine\"){\\necho \"This is the page about the 486 PC.\";\\n}\\n\\nif ($spec==\"pentium\"){\\necho \"This is the page about the pentium processor\";\\n}\\n\\n?>\\n\\nIf you try my script and you go to the URL http://example.com/pentium, then you\\'ll see \"This is the page about the pentium processor\". Tweak the script well enough and you\\'ll be able to share info about all processors.',\n",
       " \"Initial Connection\\nYou will find that the initial connection includes negotiating the SSL, so since the handshake is high, its a good indicator that something is seriously wrong with the way you have setup the SSL.\\n\\nGoogle Chrome: Understanding Resource Timing\\nTime it took to establish a connection, including TCP handshakes/retries and negotiating a SSL.\\n\\nSSL Handshake and TTFB\\nYou have two major issues, the Time spent completing a SSL handshake and the servers waiting TTFB (time to first byte).\\n\\nTTFB: 4079ms (should be less than 1000ms)\\nSSL handshake 11830ms (should be less than 100ms)\\n\\nIt should also be noted that when testing with 3G/4G devices it can cause longer first bytes due to the fact that phone signals vary in strength... this can cause intermittent connection issues and varying latency times.\\nStep 1: Investigating the SSL issue\\nIt's pretty obvious that you have a serious SSL issue and most likely due to a faulty install of OpenSSL or similar. Start by testing your SSL cert using SSL Labs and then correcting any issues or warnings it suggests. \\nIf the SSL is still operating slowly then you most likely have a overloaded server or a server fault. If its the later then you will need to try and narrow down where the fault lies. Use the Server Fault stack should you need further assistance on this matter, one user reported that creating new keys resolved a slow SSL issue that he/she was encountering that may, or may not be relevant.\\nLoad balancers can help if its a server resource issue.\\nStep 2: Investigating the TTFB\\nOnce you have investigated resolved the issue of the SSL and you still have an increased TTFB then you should test your server by ensuring that it has enough resources.\\nThe first byte time is influenced by but not limited to:\\n\\nDistance from user to data centre hosting the server can increase TTFB \\nUncached GZIP can increase TTFB \\nCongested networks can increase TTFB \\nCongested servers can increase TTFB\\n\\nSometimes increasing the CPU's and RAM isn't always the best option. Sometimes its better to introduce a load balancer because not only does it mean you can easily run multiple servers side by side but it actually offloads caching and SSL requests. Some other benefits include:\\n\\nSOURCE\\n\\nCaching: The appliance can store content that does not change (such as images) and serve them directly to the client without sending traffic to the web server.\\nCompression: Reduces that amount of traffic for HTTP objects by compressing files before they are sent.\\nSSL Offloading: Processing SSL traffic is demanding on a web server's CPU, so a load balancer can perform this processing instead. \\nHigh availability: Two load balancing appliances can be used in case one fails.\\n\\nTips for lowering your TTFB:\\n\\nEnsure your database is on the same network, or a quality SQL cloud.\\nEnsure your database is read from memory and NEVER EVER the SWAP file!\\nMake use of a content delivery network, it offloads server requests and compression tasks.\\nMake use of Varnish Cache to reduce load on the database by caching pages\\nBenchmark your static files on the hard disk using HDParm\\nBenchmark your server using Apache HTTP server benchmarking tool\\nBenchmark the website with 10 passes with multiple remote locations using WebPageTest\",\n",
       " \"There can be several reasons causing structured data not appearing in Search Console, besides having them validated. The fact that Google will crawl your page doesn't assure that they will show you the structured data as the other answer suggests. There is no an estimated time as this is a process that Google doesn't specify in detail how they handle and validate them internally.\\n\\n\\xa0Google does not guarantee that Rich\\n  Snippets will show up for search results from a particular site. Here are some reasons that marked-up\\n  pages might not be shown with Rich Snippets:The marked-up structured\\n  data is not representative of the main content of the page or\\n  potentially misleading.Marked-up data is incorrect in a way that\\n  the\\xa0testing tool\\xa0was not able to catch.Marked-up content is hidden\\n  from the user.The site has very few pages (or very few pages with\\n  marked-up structured data) and may not be picked up by Google's Rich\\n  Snippets system.\\nQ: How long does it take for rich snippets to be visible? A:\\xa0Once\\n  you've marked up your site's content, Google will discover it the next\\n  time we crawl your site (although it may take some time for rich\\n  snippets to appear in search results, if we do choose to display rich\\n  snippets for your site). If you're marking up your content for rich\\n  snippets, you can\\xa0let us know. Google won't be able to individually\\n  reply to your message, but we may use the information you supply to\\n  improve our detection and display of marked-up content\\n\\nIf you have validated your structured data, and you are sure they are relevant you only need to wait, check for it in few weeks.\",\n",
       " \"The sitemap URL has to be absolute.   In your case it is missing the https://.   Change the line to:\\nSitemap: https://sumki5.ru/sitemap-shop.xml\\n\\nAnd Google will stop complaining in search console.\\nIf you don't fix the problem, Google will not be able to access your sitemap.  That isn't necessarily a disaster.   Sitemap files are not needed for good SEO.  In fact, they really don't help at all.  What they do is give you additional insight into your site in Google Search Console.  See: The Sitemap Paradox\",\n",
       " \"Sad truth is that google adsense is an advertising service. They rent a window of your webpage for advertisements. These advertisements can contain any number of requests to other third party helper URLs and may even contain unoptimized content. The content is what the advertisers produce, and you really have no control over it.\\nAll I can suggest if you really want to improve (a.k.a. speed up) loading time is to disable all the fancy offerings with google ads such as animated ads, video ads, ads that expand, etc. Just stick with the basic text, and basic graphic ads, and if that isn't fast enough, then disable everything but text-based ads.\\nNow, having said all that, taking my advice MIGHT cost you money, and I say this because people are visual creatures. They like to see videos and graphics and advertisers understand this. If you remove graphical/animated based ads, then people will look for other graphics and there may be fewer advertisers bidding on your ad slots which can result in less revenue for you.\",\n",
       " 'No, that’s exactly the intended purpose of AggregateRating.\\nIf the video has a rating, there is no point in not providing the rating in structured data, too.\\nBy using the itemReviewed property, it’s clear to which item the rating belongs, and consumers (like search engines) can decide if to do something with it.  For example, Google Search displays a ratings rich snippet only¹ for local businesses, movies, books, music, and products. But that doesn’t mean that you shouldn’t use the markup in your case; it means that you shouldn’t expect to see a rich result for it.\\n\\n¹ At least according to their Review snippets documentation. I think there are several cases where the snippet gets displayed for items in other categories, too.',\n",
       " 'Currently, the meta data for the organisation is drawn from the Desktop version of the site. Add the schema data for the Organisation thing there and it will appear for other versions.',\n",
       " 'It has been widely accepted by the SEO community that both Google and Bing reward for pages that receive minor or major updates to the page. It would not make sense for search engines not to reward sites without some form of influence. Information can naturally overtime and refreshs most often increase user experience which we know is something Google particularly cares about the most or at least thats what they keep feeding us with.\\nGoogle owns thousands of patents and some of them allow us to speculate how Google\\'s algorithm works. Here\\'s a patent that Google owns about scoring a page that changes over time:\\n\\nSOURCE\\nA system may determine a measure of how a content of a document\\n  changes over time, generate a score for the document based, at least\\n  in part, on the measure of how the content of the document changes\\n  over time, and rank the document with regard to at least one other\\n  document based, at least in part, on the score.\\n\\nBut as Matt Cutts often says \"just because Google has patent X doesn\\'t mean they use patent X\", however you should be able to find various articles supporting what I\\'ve said to be somewhat true. However much Google or Bing rewards for such updates is left to speculation because they never reveal the extent of how much weight is put into each factor of there complex algorithm.\\nI recommend that you check out:\\n\\nMoz: Why Google Rewards Re-Publishing - Whiteboard Friday\\nMoz: 10 Illustrations of How Fresh Content May Influence Google Rankings',\n",
       " \"Google's Matt Cutts has addressed this via video:\\n\\nIt’s a fair question. I think we can handle it either way, so we should be able to process it. But if we see a lot of pages or a lot of things ranking on a site all of a sudden, then we might take a look at it from the manual webspam team. So if it doesn’t make any difference whatsoever to you in terms of the timing of the roll out, I might stage it a little bit and do it in steps. That way it’s not as if you’ve suddenly dropped 5 million pages on the web.\\nAnd it’s relatively rare to be able to drop hundreds of thousands of pages on the web and have them be really high quality. An archive of a newspaper is a great example of that.\\nBut if it’s all the same to you, and it doesn’t make that much of a difference, I might tend to do it more in stages and do more of a gradual roll out. And you could still roll them out in large blocks, but just break that up a little bit.\",\n",
       " \"We talked about Alexa here on Webmasters a lot.\\nAlexa is useless and relies on users using their toolbar. It has junk metrics and unreliable data.\\nI advise you not to consider Alexa ranking in any terms. Still, I'm not sure why do people use it, it's total gibberish.\",\n",
       " 'Yes that is how Google know it.\\nBut use only one line out of these two lines, \\n<link rel=\"alternate\" href=\"http://example.com/about\" hreflang=\"en-US\" />\\n<link rel=\"alternate\" href=\"http://example.com/about\" hreflang=\"en\" />\\n\\nbecause it is pointing to same URL. I can\\'t say weather it is harmful or not for SEO but if Google have any testing tool for hreflang then it will trigger some error on that.  So If your webpage completely written in US English language then keep it first line otherwise choose second one. \\nSame apply to Portuguese hreflang links. On this Google official article you can see one example about that.\\n\\nIt\\'s a good idea to provide a generic URL for geographically\\n  unspecified users if you have several alternate URLs targeted at users\\n  with the same language, but in different locales. For example, you may\\n  have specific URLs for English speakers in Ireland (en-ie), Canada\\n  (en-ca), and Australia (en-au), but want all other English speakers to\\n  see your generic English (en) page, and everyone else to see the\\n  homepage. In this case you should specify the generic English-language\\n  (en) page for searchers in, say, the UK. You can annotate this cluster\\n  of pages using a Sitemap file or using HTML link tags like this:\\n\\n<link rel=\"alternate\" href=\"http://example.com/en-ie\" hreflang=\"en-ie\" />\\n<link rel=\"alternate\" href=\"http://example.com/en-ca\" hreflang=\"en-ca\" />\\n<link rel=\"alternate\" href=\"http://example.com/en-au\" hreflang=\"en-au\" />\\n<link rel=\"alternate\" href=\"http://example.com/en\" hreflang=\"en\" />',\n",
       " \"Periodically releasing new content is great if you have subscribers.  You will get more traffic from an email list or RSS feed if you release 60 videos one per day rather than 60 at once.\\nFor SEO, it doesn't matter how often you put new content on your website.   In fact, Google often rewards updating and expanding existing content more than creating new pages with new content.\\nYou just have to have fresh enough content to keep users happy.  A website that looks stale isn't going to do well with users.   News gets stale very fast.  If you have last week's headlines on your home page, users will notice.   For most topics, you can get by without updating for months or even years before users notice.  It is only when users notice that the content is stale and start bouncing back to Google when searching that Google starts de-ranking.\",\n",
       " \"Things are changing.\\n\\nThere's quite a few ccTLD that Google is now treating as gTLDs. And this list will keep growing IMO.\\nYou can also set country-target on Google WebMaster Tools (gTLD only feature... hopefully will get improved).\\nAlso make sure your page clearly states the language (html lang=). Google will not treat Asian languages (Japanese) as Belgian content. They are not that dumb. Buy the .jp domain and 301 it to the .be.\\n\\nAt the end of the day, it's mostly about your incoming links sources and anchor texts for localization. Get enough .jp links and it should work.\\nBut the best/safest thing would be to use a .jp domain and redirect the .be one to it. Not gonna be easy if the boss doesn't want it. You can use the .be one in your stationary and such and just redirect the traffic to the real website.\",\n",
       " \"The favicon request is the browser that WebPagetest uses requesting the favicon to show on the tab or elsewhere. Regardless of whether you actually have a favicon, the request will still be made because you might have a favicon which should be shown to the user if it was a real user accessing your website.\\n‘/cdn-cgi/pe/bag2’ is Cloudflare's Rocket Loader, which combines multiple ‘script src’ into one asynchronous request which returns all the JavaScript from all the individual requests in one go, amongst other things. The query parameter ‘r’ lists the URLs to the scripts that your page requests.\",\n",
       " 'Your service provider suggests the following:\\n\\nFree accounts use the eWedding URL you chose at signup while upgraded\\n  Premium accounts may have chosen a custom .com. Even if you are using\\n  a custom domain name, your original eWedding URL will still work.\\n  Source\\n\\nHowever there is a way as you have described to purchase a domain name example.com and set up a webpage containing a frame redirect to https://app.ewedding.com/example.com. In order to do that you \\'ll need to create a webpage at example.com/index.html with this code here:\\n<html><head><title>Title of your webpage</title></head>\\n<frameset cols=\"*\">\\n <frame name=\"main\" src=\"https://app.ewedding.com/example.com \" scrolling=\"auto\" noresize>\\n <noframes>\\n <body>\\n\\n Your browser does not support frames\\n\\n </body>\\n </noframes>\\n</frameset>\\n</html>\\n\\nThis method is called Frame Forwarding or Cloaked / Masked / Stealth Web Redirection. Many domain name registrars provide this option without necessity of obtaining an actual hosting and having the above coding done. \\nYou should also carefully read terms and conditions of this ewedding app service in case such a way of using it is not legally allowed.',\n",
       " 'Google Analytics has a limit of 20 goals per view.  If you are using six of those at a time, that means that you can only create three funnels.\\nIt is usually best to put the goal as the last step in the funnel.',\n",
       " \"Setting the required HTTP response headers before the redirect (which sets the HTTP status and Location headers) should work.\\nHowever, you will need to make sure that all local caches are cleared before testing. The earlier permanent redirect (that didn't include the CORS headers) will have been cached, so any new requests will not hit your server and see the update until the cache is cleared.\\nThe local cache would seem to have been the problem in this instance.\",\n",
       " 'You should switch to Service Provider view interface at Server Managment --> Tools and settings --> Plesk Appearance --> Interface views. Then go to Hosting Services --> Subscriptions, click on the needed subscription, click on Customize link at the top of the page, scroll down Resources tab and make sure there is an Ulimited tick against Additional FTP accounts field or set a number manually.\\nTo set up a ftp account for a subdomain only you would need to set Home directory to /subdomain.domain.tld when at Power User view interface at Websites & Domains --> FTP Access --> Edit Additional FTP Account.',\n",
       " 'I believe \"recommended\" (as in \"recommended products\") would be a suitable short term for this section/feature. The tool for creating such a feature would seem to be a \"recommendation engine\" (or \"recommendation system\").\\nReference:\\nhttps://stackoverflow.com/questions/1994642/what-do-i-need-in-a-database-for-customers-who-bought-this-item-also-bought',\n",
       " 'If I understand your question, I think should try to send to Google Analytics the user identifier (aka \"User ID\"), as specified on their documentation:\\nhttps://support.google.com/analytics/answer/3123662?hl=en\\n\\nThe User ID enables the association of one or more sessions (and any\\n  activity within those sessions) with a unique and persistent ID that\\n  you send to Analytics.\\nTo implement the User ID, you must be able to generate your own unique\\n  IDs, consistently assign IDs to users, and include these IDs wherever\\n  you send data to Analytics.\\nFor example, you could send the unique IDs generated by your own\\n  authentication system to Analytics as values for the User ID. Any\\n  engagement, like link clicks and page or screen navigation, that\\n  happen while a unique ID is assigned can be sent and connected in\\n  Analytics via the User ID.',\n",
       " 'There are a variety of data sources used by RBLS\\'s to identify potential spam sources...\\nHoneypots - In these instances fake email addresses are created and posted in order to receive spam emails. When spam emails are received then the sender is added to blacklists.\\nSpam Filters - Many spam filters provide regular feeds to blacklist providers based on automated spam detection algorithms. When a spam email is detected by the spam filter not only is it flagged as spam in the email client but the details of the email sender and email server are also sent through to various blacklists.\\nManual Reports - Many manual spam report buttons result in spam reports being sent through to blacklist service providers as well as flagging the email as spam. This works very much like the spam filters process but rather than the spam being flagged automatically a user has seen the email and has manually flagged it as spam using the \"this is spam\" button.\\nReputable Email Providers - Many free and commercial email providers such as Live Mail, Gmail, Yahoo Mail, etc, have their own in house spam filters and feed spam data through to blacklists.\\nThere are a large number of other methods that blacklist service providers have for detecting spam on the internet and these methods are often in house proprietary methods which do not get released. The reason why it is so hard to find out how emails are detected as spam and reported is that providing that sort of information would make it easier for spammers to implement blackhat techniques to bypass and avoid detection and reporting on blacklists.\\nAs for what the SORBS feeder servers are basically \"feeder server\" is a generic term used to describe the source servers of data which are often either a receiving email server or a spam filtering services main database.',\n",
       " 'The methods you have already outlined are the best and most practical way to identify the country that a user is in however in the interest of a complete answer I will add that no method currently available for identifying the country of a user is foolproof and I will use Netflix for this example. Netflix, which has a relatively large budget for network operations, has struggled for years with restricting access to content based on geographic area. IP filtering works to a degree but bypassing IP filtering is as simple as paying a few dollars per month for a VPN service that will allow you to appear as though you are in the allowed geographic region. Asking the user to select their country is useless as all it takes is for the user to select a different country and there is no way to validate this. As for GPS validation this only works on mobile devices and before you can access that data the user needs to agree to grant your app access to their location data and the user can quite easily deny this or even download a free browser to their computer or mobile device that allows them to manually specify the location thereby bypassing all the filters that have been implemented.\\nBasically no matter how much you try to implement location filters they will be bypassed very quickly and the harder you try to restrict access based on geography the more you will get users from other locations accessing your services. Practically speaking it is better to limit the use of the application you are developing to the geographic area you are targeting and simply make it useless outside that area rather than trying to restrict it to a specific area.',\n",
       " 'Putting aside the legitimacy and legality of what you suggest, ad serving is more intelligent these days than it was even just a few years ago. These days most ad networks use crawlers to identify the content on your site and better target ads to the end users based on the subject matter of your site. In doing this most of these crawlers also check to see placement of ads, competing ads, visibility of ads, etc. With this sort of check they can also see if javascript or CSS is being used to either make the block invisible to the end user or positioning it under other content or off the page. When this sort of coding is detected the ad networks have all sorts of methods at their disposal to deal with it, with most if not all simply deactivating ad serving on your site, and depending on the nature of the ad serving agreement and the country you are in and the site is targeted to they may even take legal action to recoup any money they have already paid to you under fraud laws. Basically there is no way to do this without being caught and the penalties if when caught can be very high.',\n",
       " \"When you buy a domain name, you are given access to the registrar's tools to set which IP address your domain name should point to. You can also use the registrar's name servers or have them point to another, including your own.\\nI can't think of a reason why, or how, a VPS could/would prevent you from running your own name server or bind but I see all kinds of strange things by bad hosts. It's best just to ask them if you're allowed to do what you want.\",\n",
       " 'What information do I need to have from my domain host\\n\\n\"domain host\" is really called the domain registrar - the organisation through which the domain is registered.\\nYou need to set the NAMESERVERS at your domain registrar to point to your webhost. Your webhost will be able to tell you what the required NAMESERVERS are. There should be at least two.\\nOnce you have set the NAMESERVERS it can take some time (up to 48 hours) for the DNS to propagate. Before everyone can reliably type in your domain name to get to your site.\\nNote that you must also set your domain name at your webhost, otherwise your host will not accept the requests.\\nOnce the NAMESERVERS are pointing at your webhost then any DNS changes are made at your webhost, not your domain registrar. Your webhost now holds the master DNS records. Since you are using cPanel this is beneficial.',\n",
       " \"There's a lot of factors in play:\\nUser scope only covers a maximum of 90 days no matter what. Beyond that, it'll count as a new user. The cohort analysis covers 1 Month intervals. If you're not stitching sessions (see extended privacy agreement check in your Admin panel) then Google won't be able to tell you when a session comes from the same User.\\nWith all that in mind, the stats you're looking at in Cohorts are Users alone. The second is showing Sessions initiated by Users. \\nIf you had 10 users, all started 2 sessions each in the first month, you'd see 0% on the 1 month Cohort. But 50/50 on the returning user sessions. Now 1 of those users came back after 1 month. You'll now get higher returning stats (52/48) and 10% on the 1 month cohort. The stats you're showing mean that a larger number of users are using the site a few times in the first month then leaving.\",\n",
       " \"AdSense has a full set of content guidelines that you site must adhere to:\\n\\nSites with Google ads may not include or link to:\\n\\nPornography, adult or mature content\\nViolent content\\nHate speech (including content that incites hatred or promotes violence against individuals or groups based on race or ethnic origin, religion, disability, gender, age, veteran status, or sexual orientation/gender identity), harassment, bullying, or similar content that advocates harm against an individual or group.\\nExcessive profanity\\nHacking/cracking content\\nIllicit drugs and drug paraphernalia content\\nContent that promotes, sells, or advertises products obtained from endangered or threatened species.\\nSales of beer or hard alcohol\\nSales of tobacco or tobacco-related products\\nSales of prescription drugs\\nSales of weapons or ammunition (e.g., firearms, firearm components, fighting knives, stun guns)\\nSales or distribution of coursework or student essays\\nContent regarding programs which compensate users for clicking ads or offers, performing searches, surfing websites or reading emails\\nAny other content that is illegal, promotes illegal activity or infringes on the legal rights of others\\n\\nPublishers are also not permitted to place AdSense code on pages with content primarily in an unsupported language.\\n\\nWhen you apply for AdSense, they review your content to make sure that it doesn't violate their policies. If you don't have any content, they can't review it and turn down your application for that reason.\\nThe minimum sufficient content is about 50 pages.  That is not an insurmountable hurdle for new websites.  If you can write 5 pages a day, you could get there in a couple weeks.  In that time, you would miss out on a few cents in ad revenue.  Ads only make about five dollars per 1000 visitors.\\nIt is also a good idea to wait 6 months after launching your website before applying for AdSense.  If you live in some countries like India, Google requires this waiting period.  Even if you live in the US or Europe, owning the website for 6 months before applying makes acceptance of your application easier.   The amount of ad revenue that a new site can make in 6 months is likely only a few dollars anyway.\",\n",
       " 'I do not think you can do want you are hoping to do. It needs to be an actual translation with its own directory at minimum. It is all there in the link you shared.\\nSo even if you were able to make a second translation, if generated it will not be effective. Pseudo Translations come with far too many errors and is also not considered a good version of the language.\\nOn top of that. From Google.\\n\\nQ: Can I use automated translations?\\nA: Yes, but they must be blocked from indexing with the “noindex”\\n  robots meta tag. We consider automated translations to be\\n  auto-generated content, so allowing them to be indexed would be a\\n  violation of our Webmaster Guidelines.\\n\\nI am sorry.',\n",
       " \"If you're not getting any error in search console, then you don't need to change rss format or something like that. If you set any attribute like dates in Rss, then Google will simply ignore it, because many of people misused everyday.\\nAny dates attribute on rss/sitemap/article does not change crawling rate for specific webpage. Once Google recrawl that webpage then Google will update that dates in their search result as snippet.\\nCrawlers are not real time, it take few days to crawl your webpages. You have to wait it for few days. Sites like wikipedia crawl often because many of people link them (It means if you have more links(PageRank) the more times crawler will come to your website). \\nOther websites also crawl automatically without using fetch and render tools, because people really enjoy that stuff. Google somehow find out those webpages automatically even those pages are not linked from anywhere. \\nYou should read these answers as well:\\nhttps://webmasters.stackexchange.com/a/52820/58259\\nhttps://webmasters.stackexchange.com/a/49560/58259\",\n",
       " 'As far as we know for now, this seems to be a bug in Google’s SDTT.\\nEven Google’s own example gives this error, and there is no error when omitting the contactPoint, although the url value stays the same.\\nStack Overflow questions about the same issue:\\n\\nGoogle SDTT error: “All values provided for http://www.example.com/ must have the same domain.”\\nGoogle structured data error: “All values provided for http://www.example.com/ must have the same domain.”',\n",
       " 'If all you are using on your website is basic tracking then yes, this would count as a bounce. \\nIt is possible to set up event tracking on links so that if someone clicks on a high engagement linkthat takes them offsite, then this would trigger an additional hit and therefore they wouldn\\'t count as a bounce. Event tracking can be implemented with the following syntax for Google Universal Analytics\\n\\nga(\\'send\\', \\'event\\', [eventCategory], [eventAction], [eventLabel],\\n  [eventValue], [fieldsObject]);\\n\\nThis could be implemented on your site as:\\n> <a href=\"coolprogram.mysite.com\" onclick=\"ga(\\'send\\', \\'event\\', \\'Click\\',\\n> \\'External Link Click\\', \\'Login coolprogram.mysite.com\\');\">',\n",
       " 'This question is very subjective, but anyway...\\n\\nIs it possible to rank higher than Google for the keywords \"webmaster forum\".\\n\\nYes, I imagine it\\'s possible, but extremely difficult. You\\'re going to have to build and market something that\\'s bigger and better.\\nMillions of people use Google products, millions of people have queries regarding those products and productforums.google.com have become a go to resource.\\n\\n\"webmaster forums\"\\n\\nBut step back for a moment. This is an incredibly generic search phrase - who/what are you thinking of targeting exactly? For example, search for HTML webmaster forum and Google has now gone from the first page of results.\\nOften users find forums when they are trying to solve specific problems. So, they are searching for specific phrases, not a generic catch-all phrase.',\n",
       " 'Each top level domain registry makes a dump of all whois data available. You can either download it yourself periodically, or subscribe to a service that does so and alerts you to changes.\\nMarketers subscribe to be notified of all newly registered domain names so they can send you spam.\\nSee: Possible to download entire whois database / list of registered domains?',\n",
       " 'I\\'m not sure that you can do this for the apex (bare) domain (unless your DNS provider specifically supports this), however, you could create a CNAME record for a subdomain that points to your DynDNS domain name.\\nhome.example.com CNAME mocksite.dyndnsprov.com\\n\\nYour DynDNS domain \"contains\" an A record that points to your (dynamic) IP address (which is automatically updated using some software on the client).\\nSo, http://home.example.com will now direct traffic to your home server. (You still need to configure your home network and server to accept/route such requests.)',\n",
       " \"I've got lot's of H1 tags styled directly by css on lots of websites and pages rank on the first page of the SERPs without issue. I've had an SEO professional I respect tell me he styles his H1 tags to ensure they're readable for the user, despite being long because of keyword phrases he's targetting. His site ranks exceptionally well.\\nIf you think about it, why would Google insist on something that flies in the face of logical clean web coding? Properly constructed html actually adds a great deal to a page's ability rank. A H1 inside a P would be confusing for the Goog-bot trying to 'make sense' of the page surely, so undesirable.\\nJust one thing... if you intend to 'hide' text in your H1 (ie: make it the same colour as the background) then that will be 'black-hat' and likely to get your site in to trouble. It MUST be readable for users.\",\n",
       " \"Google user can use caches link to see old/deleted webpages for any website. There is no any specific time duration for how much time, the cache files will be index on Google server, but if content is useful for user then Google might don't delete it for many years, otherwise they will delete it in few months. \\nIf you own content then you can request cache to removal from Google search console, they just down it temporary for 90 days. so webmaster can use noindex, robots.txt and 410 http error together to remove it permanently. More information you can find out here.\\nBut there are other cralwers as well, for example web archive machine, which crawl webpages and save it on their server, so any user can see how site was looking/have content in old days. To block such a cralwer find out their spider/bot/crawler name, and block it on your robots.txt\",\n",
       " \"Google generally removes pages from the index for one of three reasons:\\n\\nIt can't crawl them properly\\nIt found that they duplicate something else\\nNobody searched for them and they don't have many external links\\n\\nFor crawling, check Google Search Console and make sure that there are not unexpected crawl errors.   Check your server logs and see that Googlebot is fetching your pages with a 200 status.\\nFor duplicate content, search for a sentence from each page in quotes on Google.   You may find that there is a scraper site out there that has stolen your content.   Google occasionally indexes a scraper site instead of the original source of content.\\nFor lack of interest, there isn't much you can do.   I've observed that Google will often crawl and index a page for the first couple weeks before deciding that it isn't high enough quality to continue including in the search index.\",\n",
       " 'Apache server can be configured to show the error page at the error URL, or it can redirect to the error page.   It is almost better to show the error page directly at the URL rather than redirecting to it.\\nThe Apache ErrorDocument directive explains how to implement it both ways:\\n\\nURLs can begin with a slash (/) for local web-paths (relative to the DocumentRoot), or be a full URL which the client can resolve.\\n\\nIn practical terms, that means if you specify the error document as an absolute URL it will cause a redirect to the error page:\\nErrorDocument 404 http://www.example.com/404.html\\n\\nbut if you specify the error document as a relative URL starting with a slash, it will show the error document at the original URL where the error occurred:\\nErrorDocument 404 /404.html\\n\\nMy guess is that you have your ErrorDocument directive configured as an absolute URL either in your .htaccess file or your httpd.conf file.  You need to edit it to change it to a relative URL.',\n",
       " \"Indeed you can. For instance some do not realize a www. url is an actual sub-domain. So it is happening all the time. \\nYou can go to your server settings and choose your sub-domain as the main domain name or add some code to create a redirect.\\nMany people use Apache servers for hosting websites and when you have used a domain name with traffic to it, a good idea is to use a 301 redirect in the .htaccess file located in the root folder of your website. The purpose is you are telling search engines, bookmarks etc, that hey we are the same and we moved here. That's the short version.\\nYou would add some code like this to the htaccess file.\\nRewriteEngine on\\nRewriteCond %{HTTP_HOST} ^example.com [NC]\\nRewriteRule ^(.*)$ http://subdomain.example.com/$1 [L,R=301,NC]\",\n",
       " \"Are my search results reflected in the Search Analytics?\\nYes in Search Console\\nNo in Google Analytics > Aquisition > Search Console. (This needs confirmation)\\nThis is also the answer for a custom search (Not 100% sure I've understood you here though).\\nTest it out if you want to see for yourself. Find a really distinctive phrase on one of your webpages that would be highly unlikely to be used to search with by anyone else. Use it. Check your page is listed, so the search was successful. Then wait a couple of days (Google console is at least 2 days behind). Check for your very distinctive term. It is possible GA won't show it, thinking it's an 'infrequent' term, but it's worth a punt.\\nOf course, if you have a whole heap of privacy settings on, then the term won't appear. The impression will be there, but not the term.\\nAn alternative test would be to search for a page you know gets hardly any impressions - if you have a page like that.\\n\\nMy own question is related: Use site:mysite.com search without adding impressions to Google Search Console\",\n",
       " 'No such thing as dofollow:\\nThe dofollow attribute does NOT EXIST in either rel or meta, this is due to the fact that by default both pages and links are considered dofollow unless you use nofollow.\\nMeta follow is not to be confused with dofollow <meta name=\"robots\" content=\"noindex, follow\">\\nDespite MASS belief follow and dofollow have nothing in common other than the fact they share a similar name, it has absolutely no bearing on passing of page rank. Meta Follow it simply informs search engines to discover content on the receiving end of links... \\nA page that is not indexed by Google or Bing will not pass juice period. Also, the usage of content=\"noindex, follow\" is pointless because simply using content=\"noindex\" informs search engines to follow links unless you use content=\"noindex, nofollow\".\\n\\nRel dofollow is invalid markup:\\n\\nSOURCE\\n<a href=\"#\" rel=\"dofollow\">test</a>\\nBad value dofollow for attribute rel on element a: The string dofollow is not a registered keyword.d.\\n\\nUsing duplicate REL is invalid markup:\\n\\nSOURCE\\n<a href=\"#\" rel=\"nofollow\" rel=\"rel\">test</a>\\nError: Duplicate attribute rel.\\n\\nExample of valid markup using multiple rel values:\\n\\nSOURCE\\n<a href=\"#\" rel=\"nofollow me\">test</a> \\nDocument checking completed. No errors or warnings to show.',\n",
       " \"With toys in their millions, All under one roof, It's Called Toys'R'Us! \\nUnless you have good specific reasons why not to host your website under one roof then generally speaking it is far better to use sub folders rather than sub domains or other domains. Using  sub folders such as /fr/ and /ch/ will give your domain more authority because your SEO WILL NOT be spread across two sites.\\nhreflang and canonical\\nUsing both hreflang and canonical links you can help inform Google and other search engines the preferred content based on the language customers therefore the necessary pages are returned in the search results without having to redirect your customers.\\nNever treat Google or Bing as a bot... consider them users\\nIn your particular case you are using a forced redirect unless they perform X action and then a cookie is stored, since Googlebot does not perform actions and store cookies by default it will be unable to crawl one domain. It's also worth mentioning that selectively TURNING OFF THE REDIRECT for Googlebot based on either user agent or IP address would be considered cloaking as search bots should never be treated anything more than users.\\nGEO User Experience and SEO  practices\\nIt is considered a good practice for both SEO and User Experience to prompt users to change region based on GEO. For example, your site detects they are in X region but they are surfing Z region content, they then get a pop-up asking them if they would like to change region, then the cookie is stored. Meaning Google and Bing can freely index both sites without issue. Many sites take this approach, for example the history channel just one of millions on the top of my head.\",\n",
       " \"That is really tricky.\\n\\nBecause a 404 status code would be returned before the 404 page is produced.\\nA 404 says a link is no longer available or temporarily unavailable.\\nA 301 tells it the page you are looking has moved here, which it has not.\\nA 301 redirect forces a user elsewhere, possibly not relevant and you will most likely have higher drop off rates, and poor user experience.\\nLink juice carries weight through 200s and 301s. \\n\\n404's in general are not bad at all. It's honest to say the least. \\nNow, I do not suggest having a lot of 404s or 301s but if you want to use them as intended, then a 301 is not the real way to go (as user I would think 301 was not useful or honest if you will. \\nI would think a catchy 404 page with some good links or call to actions well give the user a good starting choice and in turn keep them there, which leads to better experience and SEO.\\nI know if I am looking for shoes and then redirected to a main page of clothes, I am not happy at that point and will leave. \\nKeep in mind search engines want to show results that users tend to benefit from and if they don't like the method neither will the search engines.\\nHope that helps.\",\n",
       " 'Sorry to be the bearer of bad news but redirecting like to like is the only way that you will be able to keep your SEO in tact. So, your 3 options are:\\n\\n301 redirects from all old pages to the home page and face the music that your site may encounter some form of automated negative seo algorithm by Google or Bing.\\nYou use 404 or 410 statuses and lose any SEO that those pages gained over the months or years. This can reduce domain authorithy and rankings on other pages.\\nYou want your SEO kepted in tact and face the music that you need to spend several hours remapping old urls to new urls.',\n",
       " '@closetnoc was right.\\nI have contacted Digital Ocean and they responded with:\\n\\nIt looks like these IPs are related to a monitoring service, are you\\n  currently subscribed to one? If not, it\\'s entirely possible the person\\n  who had this IP address before you was the one subscribed, and simply\\n  hasn\\'t updated their information. \\nIf it\\'s not your monitoring, it\\'s nothing to worry about and will\\n  likely stop in the near future. If it is yours, then you know they\\'re\\n  doing their job!\\n\\nI was wrong with saying \"4 same GET requests\", as this is obviously just a single request to the landing page and the other resources are loaded in by default.\\nSo for now I will manually block those IP\\'s (7 so far) in Google Analytics.',\n",
       " \"Don't be suspicious just yet. Just ask them what data they want, and use Google Analytic's export feature to send them any specific reports.\\nMarketers & consultants need as much as information as possible about their client's customer and business in order to come up with a relevant plan & strategy.\\nAnd once you both agree on a contract, NDA, or whatever. I would give them direct access. Let the guy dig deeper into the data and allow him to give you regular updates about your site's performance.\\nBut don't give him your login & password! That's a big NO! If he ask, just end it.\\nGA has a user management feature to give other people full or limited access to your analytics. So this sort of request is very common.\",\n",
       " 'Yes, this is duplicate content. The same content is accessible from two different URLs and there is no canonicalisation.\\nBasically, this means that the search engines will pick one or the other to return in the SERPs. Ranking is essentially split between the two URLs.\\n\\nboth URLs are used for linking.\\n\\nYou need to decide which is the canonical/preferred URL and link only to that one URL.\\nFor simplicity, we\\'ll consider just the two URLs you\\'ve listed. The preferred URL would seem to be the one that goes via your download script (ie. download.php), otherwise you aren\\'t going to be tracking the IPs of users downloading the file.\\nTo resolve any already indexed URLs, you can externally redirect the direct link to your script. Assuming Apache, then you can do something like the following in your root .htaccess file:\\nRewriteEngine On\\nRewriteCond %{REQUEST_FILENAME} -f\\nRewriteRule ^([^/]+\\\\.pdf)$ /download.php?file=$1 [R=302,L]\\n\\nThis will redirect a request for /Document.pdf (only if it exists as a physical file on the file system) to /download.php?file=Document.pdf.\\n$1 is a backreference to the first captured group in the RewriteRule pattern (ie. ([^/]+\\\\.pdf)).\\nChange the 302 (temporary) redirect to a 301 (permanent) when you are sure it\\'s working OK. 301s are cached by the browser so can make testing problematic.\\n\\nA more \"user friendly\" URL (UPDATED)\\nYou could take it one step further and create a more \"user friendly\" URL like /download/Document.pdf. This would then become the canonical URL - the URL that you link to.\\nIn this case, since you have a file whose basename is also \"download\" (ie. download.php vs /download), you need to make sure that MultiViews is disabled. Otherwise mod_negotiation is likely to make an internal subrequest for download.php (depending on the request) before we\\'ve rewritten the URL. So, at the top of .htaccess:\\nOptions -MultiViews\\n\\nAny direct requests for /Document.pdf or /download.php?file=Document.pdf  should be externally redirected to the canonical URL. For example:\\nRewriteCond %{REQUEST_FILENAME} -f\\nRewriteRule ^([^/]+\\\\.pdf)$ /download/$1 [R=301,L]\\n\\nRewriteCond %{THE_REQUEST} GET\\\\ /download\\\\.php\\\\ HTTP\\nRewriteCond %{QUERY_STRING} ^file=(.+\\\\.pdf)$\\nRewriteRule ^download\\\\.php$ /download/%1 [R=301,L]\\n\\n%1 (as opposed $1, mentioned above) is a backreference to the last matched RewriteCond CondPattern (ie. (.+\\\\.pdf)).\\nThe additional RewriteCond (condition) that checks against THE_REQUEST is necessary in order to prevent a redirect loop. (THE_REQUEST contains the original request header and does not change when the URL is rewritten.)\\n/download/Document.pdf would then be internally rewritten to the \"real\" URL. ie. /download.php?file=Document.pdf. An internal rewrite, as it suggests is internal to the server. There is no external HTTP request. The URL in the address bar does not change. It is completely hidden from the end user.\\nRewriteRule ^download/([^/]+\\\\.pdf)$ download.php?file=$1 [L]\\n\\nNote that there is no R (redirect) flag on this directive that would otherwise trigger an external redirect.\\nIdeally, you would make the regex as restrictive as possible. For example, in the above regex, .+ matches any characters (1 or more). However, if your filenames consist of only upper and lowercase letters then it would be preferable to change the regex to match only letters. eg. [a-zA-Z]+.',\n",
       " \"The rule for indexing is to ask 'Where is the content?' and 'What's the fastest route to it?'\\nCross domain canonicalisation is treated like a soft redirect. You state that content is served from webapp.com/page but the URI remains domain.com/page. If you're using an iFrame pointing to webapp.com/page, Google will see it as a link and index the linked content's address. If you use a cross domain canonical tag, it might index domain.com/page. \\nYou can canonicalise a page to another but it remains a suggestion - not a directive. If the content on each page is similar, Google may decide to merge their indexes and 'think' of them as the same page. If the content is very different then it will ignore your suggestion completely.\\nThe only way to force it to index another page is a 301 redirect. This acts as a harder directive. It also changes the request URI.\\nHere's a simple flow for you to follow:\\n\\nIf you ask a question of Google, where is the content that will answer it?\\nIs the URI for that content the one that is going to show up as indexed?\\nWill the server mess with that URI if a user clicks on it?\\nIs the content going to appear dynamic (different every time)?\\n\\nThese are the questions Google's algorithm will ask in order to find the shortest path from it's index to the content. Any attempt to subvert that in a way that lengthens that path or diverts it, and Google will either adapt or drop the index link.\\nIP addresses make no difference if you're attempting to index everything under a single domain. Google was originally trained to see subdomains as entirely separate sites and it still has issues interpreting if a subdomain is part of the main site or not. Think xxx.wordpress.com - where each hosted site has it's own subdomain. www.wordpress.com has no impact on these subdomains whatsoever.\\nQuick edit: Where IP address would come into SEO is if you wanted different domains to benefit each other without being seen as part of the same C-block (indicates a lazy PBN running from the same Network). This isn't relevant as you're using the same domain name - which categorically tells Google all the sites are from the same network.\",\n",
       " 'Bing has now deleted its malware warnings from Bing Webmaster Tools.   I think at this point it must have been a false positive.\\n\\nI never did find any way to dig into what the issue was.  Bing does not appear to publish any in depth analysis of what malware it actually found or what triggered their report.',\n",
       " 'My website was hacked which is now recovered but the hacker indexed 5000 URLs in Google and now I get error 404\\n\\nA 404 is probably preferable to blocking with robots.txt if you want these URLs dropped from the search engines (ie. Google). If you block crawling then the URL could still remain indexed. (Note that robots.txt primarily blocks crawling, not indexing.)\\nIf you want to \"speed up\" the de-indexing of these URLs then you could perhaps serve a \"410 Gone\" instead of the usual \"404 Not Found\". You could do something like the following with mod_rewrite (Apache) in your root .htaccess file:\\nRewriteEngine On\\nRewriteRule ^\\\\+ - [G]',\n",
       " \"It's actually not dependant on what Google understands but what users type. This is a really common issue for things like Dutch language optimisation. Many users will do partial word searches that may be relevant and Google will offer similar searches at the bottom of the page (Users also searched for:) but unfortunately gives results based on the individual nouns used.\\nGoogle's language understanding is done from the query perspective. If users are concatenating for the noun commonly, then it's better to join it. In fact, if this is the common way to do it - then Google will even autocorrect. \\nIf it's a concatenated noun rarely searched for - then you will need to add space errors. This is why you'll see it all over the place. Sometimes it's better safe than sorry.\\nI realise it's inconvenient to be researching every individual noun combination but once you understand your demographics language use this can become a lot easier. A technique that might assist you is studying your users social media and the Latent Dirichlet Allocation of nouns. There are guides on how to perform this study with a program called KNIME online.\",\n",
       " 'Don\\'t block them from being crawled - this doesn\\'t remove them from the index. It only stops Googlebot from looking at them.\\nNormally, the fastest way is to use the Search Console removal tool. For the numbers you\\'re talking that doesn\\'t sound possible as they have to be entered one-by-one.\\nThe next fastest in my experience would be to create a sitemap that does an alt language mapping - Sitemaps are crawled and processed very soon after being submitted. If you tell Google each of the bad URIs are Chinese language (rel=\"alternate\" hreflang=\"zh-Hans\") and then put in real URIs as the \"en\" alternatives - this will replace them in English language engines. You can use the same URI multiple times.\\nExample:\\n<url>\\n    <loc>http://www.example.com/bad-chinese-page/</loc>\\n    <xhtml:link\\n                 rel=\"alternate\"    \\n                 hreflang=\"zh-Hans\"    \\n                 href=\"http://www.example.com/bad-chinese-page/\"    \\n                 />\\n    <xhtml:link     \\n                 rel=\"alternate\"    \\n                 hreflang=\"en\"    \\n                 href=\"http://www.example.com/good-page/\"    \\n                 />\\n</url>\\n\\nMake sure each of these pages is returning a 410 error. This doesn\\'t just tell Google the server can\\'t find the content - it categorically says it\\'s no longer there. They\\'ll be dropped faster from the index.',\n",
       " \"Google does not like links generated from widgets and will penalize sites that use them.\\n\\nHowever, some widgets add links to a site that a webmaster did not editorially place and contain anchor text that the webmaster does not control. Because these links are not naturally placed, they're considered a violation of Google Webmaster Guidelines.\\n\\nSo, if by placing this widget on other site you somehow generate incoming links, you run the risk of being penalized by Google. If this does not generate links to your site through the widget it really won't make a difference either way. \\nTo summarize, if your site is just an iframe with no provided by <link to your site> you should be okay. But being the content is in an iframe I wouldn't expect any kind of SEO boost.\",\n",
       " 'You need to change your logic. Your conditions are implicitly AND\\'d (the default), they should be OR\\'d instead. So, currently the RewriteRule is only processed if the HTTP_HOST is \"example.com\" AND the SERVER_PORT is 80. You need it to trigger on either of these conditions, so you need to add an OR flag on the first condition.\\nFor example:\\nRewriteCond %{HTTP_HOST} ^example\\\\.com$ [NC,OR]\\nRewriteCond %{SERVER_PORT} 80 \\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\n(No need for string anchors ie. ^ and $ if you are capturing the whole pattern.)\\nUPDATE: Alternatively, check the HTTPS server variable, instead of SERVER_PORT. For example:\\nRewriteCond %{HTTPS} !on [OR]\\nRewriteCond %{HTTP_HOST} =example.com [NC]\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\nWhat that says is... for every request that is not HTTPS or is for example.com (ie. no www) then redirect to https://www.example.com.\\nAlternatively, you can implement this as two separate rules:\\nRewriteCond %{HTTPS} !on\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\nRewriteCond %{HTTP_HOST} =example.com [NC]\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\nDEBUGGING: Test what these server variables contain on your server. You can (temporarily) replace the above redirect with the following:\\nRewriteCond %{SERVER_PORT} (.*)\\nRewriteRule ^ - [E=ENV_SERVER_PORT:%1]\\n\\nRewriteCond %{HTTPS} (.*)\\nRewriteRule ^ - [E=ENV_HTTPS:%1]\\n\\nThis will create two environment variables (ENV_SERVER_PORT and ENV_HTTPS) that should be available to your server-side script (eg. PHP). Check what these contain. (eg. in PHP echo getenv(\\'ENV_SERVER_PORT\\');)\\nUnder HTTP you would expect... ENV_SERVER_PORT to be 80 and ENV_HTTPS to be \"off\".\\nUnder HTTPS you would expect... ENV_SERVER_PORT to be 443 (default) and ENV_HTTPS to be \"on\".\\nSolution\\nIt would seem that the SERVER_PORT and HTTPS Apache server variables are not available. This suggests that you have a front end proxy (or load balancer) that is handling the request.\\nThe related (but not exactly the same - despite having the same name) PHP superglobals SERVER_PORT and HTTPS (specifically $_SERVER[\\'SERVER_PORT\\'] and $_SERVER[\\'HTTPS\\']) are, however, being set (by PHP). But SERVER_PORT is always 80 when served over HTTP and HTTPS (this also suggests a front-end proxy). The HTTPS PHP superglobal is, however, correctly set to \"on\" when served over HTTPS. But we want to avoid doing this redirection in our code (less efficient, more chance of error).\\nFrom your server output, it looks like there is an X-Proto HTTP request header (identified by the HTTP_X_PROTO index in the $_SERVER superglobal array) being set (most probably by the proxy server) when served over HTTPS (it is set to the string \"SSL\"), but when served over a plain HTTP connection this header is not set. We should be able to check for this in .htaccess. Try the following:\\nRewriteCond %{HTTP:X-Proto} !SSL [OR]\\nRewriteCond %{HTTP_HOST} =example.com [NC]\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\nFor all requests that do not have an X-Proto HTTP request header set to the string \"SSL\" or the apex domain is requested then redirect to HTTPS with the www subdomain.',\n",
       " \"RewriteRule \\\\/marques-voiture\\\\/\\\\s(.*) https://www.example.com/marques-voiture/$1 [R=301,L]\\n\\nYour problem is probably the slash prefix on the RewriteRule pattern. In per-directory .htaccess files the directory-prefix is first removed from the URL-path that is matched. The directory-prefix always ends with a slash, so the URL-path that is matched never starts with a slash. So, the above rule never matches, not because of the space, but because of the slash prefix!\\nSo, try the following instead:\\nRewriteRule ^marques-voiture/\\\\s(.*) https://www.example.com/marques-voiture/$1 [R=301,L]\\n\\nYou also don't need the first RewriteCond directive that checks against the REQUEST_URI - this is doing the same thing as the RewriteRule pattern.\\nAnd you don't need to escape slashes in most Apache regex (there are no delimiters).\\nMake sure you clear your browser cache, as any (erroneous) 301s will have been cached by the browser.\",\n",
       " 'Have you checked \"Crawl this URL and its direct links\" on submit option? \\n\\nYou can do that only 10 times per month. And additionally you can submit 500 single URL in fetch and render tools. \\nWhat I want to say is, if there are more links on that page, then Google will not crawl and index all the pages on same time/day, They will schedule their works, your links are queue in cralwer pipeline, you just have to wait, there are thousand of people submit URLs to Google and crawler have to crawl only specific number of pages on certain time.',\n",
       " \"There is no penalty in how you send events on your site. What you do with your data, how you manipulate it, is up to you (so long as you are not passing along PII data to Google). But really, in the end all you are doing is impacting those who need to read and interpret your reports. Google doesn't dock you for that. Many (poorly designed) sites may naturally have a near 0% bounce rate.\\nFrom what you describe with the video tracking, you are doing what makes most sense in terms of tracking user engagement with your site - video interactions and progress. I don't see that as a way of 'manipulating' your metrics.\",\n",
       " \"What you are trying to do could technically be classified as cloaking which is a violation of Google's terms and can result in your site being removed from the Google index. Google is very strict in what they class as cloaking and basically the rule is whatever the end user sees the crawler has to see as well. If you are trying to block malicious bots then the easiest thing to do is simply block their user agent strings using .htaccess but if you try cloaking with a legitimate crawler such as Google it will be detected and will result in severe penalties and manual action notices which can severely affect your SERP ranking.\\nGoogle not only uses the known Googlebot user agent but also uses other bots which have the user agent string of real browsers on IP addresses not affiliated with Google as a way to detect this on websites so there is no way to prevent yourself from being caught out doing this.\\nNow having given that warning...\\nYou mention Facebook crawler specifically. Facebook has three different user agents for crawling. facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php) and facebookexternalhit/1.1 which are used when a user shares your website to their wall and Facebot which is used to help improve advertising performance. Out of all of them only Facebot respects the robots.txt rule as the other ones are only triggered by a user action and so are treated the same as a web browser in effect. If you want to block any Facebook crawling simply add a .htaccess rule to detect these user agent strings and if they are detected either block them or return an error page that crawlers are not permitted. Trying to forward them to an alternate site with different content will simply complicate matters and could have the potential of reducing your SERP ranking due to not having context appropriate content on the pages that the Bots can access.\",\n",
       " \"Sorry to be the bearer of bad news but the issue is with your site and there's no way to prevent a obviously intended notification. The notification informs users that the site is not fully secure, which its not if you are using 3rd party scripts over HTTP. If you want to solve the problem then you must use HTTPS:// on all local and external resources.\\nSo if you want users knowing that your site is fully secure and you want Google to reward a bit of SEO value to the site because of it, then you must either have them upgrade to SSL or ditch the widget all together... there is no workaround for this issue... Also rather than using https:// you should opt to use //example.com/image.jpgif the site is accessible via both protocals.\\nYour three options are:\\n\\nRemove the widget.\\nAccept the notification\\nAsk them to upgrade the API service to HTTPS.\",\n",
       " 'RewriteRule ^5014466727/(.*)$ jabez-darla.php [QSA,L]\\n\\n...this rule is looking for a match on a URL that literally has 5014466727/\\n\\nYes, as closetnoc stated in comments, this is simply a literal string of (seemingly random) digits at the start of the URL.\\nSince the captured group (ie. (.*)) does not appear to be used in the substitution (or in any preceding RewriteCond directives), the pattern ^5014466727/(.*)$ is equivalent to ^5014466727/.\\nThe seemingly random number (which might be unique per site/hack) is probably just to make this harder to find in the search engines.',\n",
       " 'Switch to a default theme and see if that permalink structure works or not. Outdated themes can cause issues with newer WordPress versions.\\nThen after this, try to deactivate one plugin at the time and check the links. One or more plugins are in conflict with structure.\\nStructure where you have just %postname% will work.',\n",
       " 'Browsers don’t convert /servicepage to /ServicePage unless you tell them to (e.g., with a server-side redirect). The URL path is case-sensitive, so these are different resources, and they could serve different documents (see example).\\nWhich one to use typically doesn’t matter for SEO. What is important is that you use one version. When you change it, you should redirect from the old to the new one (with 301).\\n(As search engines have to index your pages again when you redirect, you might notice some ranking changes, but after some time everything should be the same again.)',\n",
       " 'Without linking them\\nYou could of course simply use a LocalBusiness for the taxi company on the taxi site, and a LocalBusiness for the tour company on the tour site, without linking them in any way.\\n<!-- on the taxi site -->\\n<script type=\"application/ld+json\">\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"LocalBusiness\",\\n  \"name\": \"Taxi company\",\\n  \"address\": {\\n    \"@type\": \"PostalAddress\",\\n    \"streetAddress\": \"Example street 1\"\\n  }\\n}\\n</script>\\n\\n<!-- on the tour site -->\\n<script type=\"application/ld+json\">\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"LocalBusiness\",\\n  \"name\": \"Tour company\",\\n  \"address\": {\\n    \"@type\": \"PostalAddress\",\\n    \"streetAddress\": \"Example street 1\"\\n  }\\n}\\n</script>\\n\\nLinking them\\nBut if you want to convey that the taxi company is the parent company, and the tour company is its child company, you can use the parentOrganization/subOrganization properties.\\nYou can do this without repeating the taxi information on the tour site and vice-versa. Simply give each LocalBusiness a URI (via @id in JSON-LD) and reference it as the value of the properties.\\n<!-- on the taxi site -->\\n<script type=\"application/ld+json\">\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"LocalBusiness\",\\n  \"@id\": \"http://taxi.example.com/#company\",\\n  \"name\": \"Taxi company\",\\n  \"address\": {\\n    \"@type\": \"PostalAddress\",\\n    \"streetAddress\": \"Example street 1\"\\n  },\\n  \"subOrganization\": {\"@id\": \"http://tours.example.com/#company\"}\\n}\\n</script>\\n\\n<!-- on the tour site -->\\n<script type=\"application/ld+json\">\\n{\\n  \"@context\": \"http://schema.org\",\\n  \"@type\": \"LocalBusiness\",\\n  \"@id\": \"http://tours.example.com/#company\",\\n  \"name\": \"Tour company\",\\n  \"address\": {\\n    \"@type\": \"PostalAddress\",\\n    \"streetAddress\": \"Example street 1\"\\n  },\\n  \"parentOrganization\": {\"@id\": \"http://taxi.example.com/#company\"}\\n}\\n</script>',\n",
       " \"AdSense will ban you, if traffic quality is not good at all, that's simple reason is apply to all website. \\nThey have solid algorithm to identify such a quality traffic, for example if I visit your website A, then I will redirect or open pop up on my browser, then AdSense will see this is some kind of referral traffic from website A. If I do nothing and just closing the webpage, then such a webpage is consider as low quality webpage. \\nWhen you use pop up, AdSense will know that the traffic come from some mini window tab, and when you use redirects script, then they will get information from browser header tags.\\nIf you buy cheap traffic then AdSense might consider those clicks are invalid because they have no more data to identify weather clicks are valid or invalid, so AdSense generally works great for organic traffic.\",\n",
       " 'The errors should be logged to /usr/local/psa/admin/logs/panel.log \\nBut most of the times it\\'s difficult to get real issue from error message.\\nEnabling debug in /usr/local/psa/admin/conf/panel.ini provides content of generated temporary configs so you may inspect them:\\n[debug]   \\nenabled = on\\n\\n[log]\\nfilter.priority = 7\\nshow.util_exec = on\\nshow.util_exec_io = on\\n\\nDon\\'t forget to restart sw-engine service like: # service sw-engine restart\\nIt may be:\\n\\nwrong directives in domain\\'s \"Apache & nginx Settings\"\\nselinux, you can disable it for testing with command:# setenforce 0\\nmissed IP addresses, check Tools&Settings>IP addresses>Reread\\nsome web server issues, check for apache and nginx error logs\\ninconsistency in Plesk database, check for # plesk db repair',\n",
       " 'You should add the URLs of your pages which you want to get crawled and indexed.\\nDon’t you want your password recovery page to get crawled/indexed? Then don’t add it. (And if you want to disallow crawling of your password recovery page, add it to robots.txt; if you want to disallow indexing of your password recovery page, use noindex; don’t use both).\\nShould you want your password recovery page to get crawled/indexed? I would say yes. Some users might want to use a search engine to find this page (instead of visiting your site and finding a link to the page). But this is primarily a usability question, it doesn’t affect SEO much.\\nThe same goes for the sign up page. \\nYour \"Thank you\" pages ideally wouldn’t exist in the first place (you’d typically display the message on the same page, or on the otherwise existing page you get redirected to after submitting the form), but if you need them, they would be good candidates for pages that shouldn’t get added to the sitemap. And you should noindex them, too.',\n",
       " 'Google Analytics Spam is linking to THIS discussion thread now. If you got to this thread because you saw a referral in your Google Analytics, YOU HAVE BEEN FOOLED. Check the Language for those referrals (\"...Vote for Trump\"), then read this article for the details and a filtering solution:\\nhttp://www.analyticsedge.com/2016/11/heres-a-secret-%C9%A2oogle-com-is-not-google-com/\\nThe updated article now links to this solution:\\nhttp://help.analyticsedge.com/spam-filter/definitive-guide-to-removing-google-analytics-spam/',\n",
       " \"No it isn't. Also, it's impossible due to privacy and protection of personal data.\\nIf your collegue thinks it can be done, ask him how. I want to know also how to get so much e-mails from people that only liked my page(s). So either your collegue is a great and valuable asset to your company or he is a complete douche because he's putting you into this task that you can't solve.\\nSo, your answer is: NO.\",\n",
       " 'Is this correct?\\n\\nBasically, yes.\\n\\nredirect them to the file...\\n\\nIt\\'s more usual to refer to this as a URL rewrite (or internal rewrite), rather than a \"redirect\" - which is more commonly used to refer to an (external) HTTP redirect. Although it is still a redirect of sorts and the Apache docs do often refer to it as an \"internal redirect\".\\n\\nwhich is good in terms of SEO.\\n\\nStrictly speaking, these are \"user\"-friendly URLs, not necessarily SEO-friendly. Search engines understand the URL parameters perfectly well - so to the search engines it makes little difference. However, users find them easier to deal with, which can improve click-through rates. So, indirectly, they can help draw traffic to your site, but they do nothing to improve your ranking directly.',\n",
       " \"You've been hacked at a server level. You will need to wipe your server clean and reinstall to be sure you've caught everything.\\nCurrently, a normal page is served to most people - but try fetching it with Google and you'll be given a page of spam links:\\nhttps://developers.google.com/speed/pagespeed/insights/?hl=en&utm_source=wmx&utm_campaign=wmx_otherlinks&url=https%3A%2F%2Faxemplate.com%2F\\nThese are used by hackers to rank pages in search engines. By continuing to serve a normal site to everyone else they were hoping you wouldn't notice. Sorry there's no easy answer for you!\",\n",
       " 'The last record (started by User-agent: *) will be followed by all polite bots that don’t identify themselves as \"googlebot\", \"google\", \"bingbot\" or \"bing\".\\nAnd yes, it means that they are not allowed to crawl anything.\\nYou might want to omit the * in /bedven/bedrijf/*.\\nIn the original robots.txt specification, * has no special meaning, it’s just a character like any other. So it would only disallow crawling of pages that literally have the character * in their URL.\\nWhile Google doesn’t follow the robots.txt specification in that regard, because they use * as a wildcard for \"any sequence of characters\", it’s not needed for them in this case: /bedven/bedrijf/* and /bedven/bedrijf/ would mean exactly the same: block all URLs whose path begins with /bedven/bedrijf/.\\nAnd finally, you could reduce your robots.txt to two records, because a record can have multiple User-agent lines:\\nUser-agent: googlebot\\nUser-agent: google\\nUser-agent: bingbot\\nUser-agent: bing\\nDisallow: /bedven/bedrijf/\\nCrawl-delay: 10\\n\\nUser-agent: *\\nDisallow: /',\n",
       " 'given your question I have to make the following assumptions:\\n\\nyour website is offering a section for install-guides for a product\\nguides differ from product version but stay relevant for customers using older product versions\\nyour customers are aware of that and know exactly which version they may need\\n\\nFirst: this indeed is not the use-case for a rel=\"canonical\".\\nOne approach of dealing with situations like yours is structure:\\nYou can implement a static guide page for your product that always contains the most recent guide. This page is linked from the product or category page, so it gains a lot of importance.\\nScheme:\\nexapmle.com/product/guide/\\nexapmle.com/product/guide/guide-v1\\nexapmle.com/product/guide/guide-v2\\nexapmle.com/product/guide/guide-v3\\n\\nEach time a new guide is released it gets published to the static page and the old one moves down to a new URL deeper in the URL structure. Those \"old guide\" URLs are linked to from the static page, but not from the category pages. Therefor they will be seen \"less important\"\\nMake sure, the v1, 2, 3, … n versions are only linked from the main guides page and each to each. The main page must be well linked across your whole site.\\nCompared to your existing URL structure this give more relevance to the guide published on exapmle.com/product/guide/.\\n\\n/v1/install-guide\\n/v2/install-guide\\n/v3/install-guide\\n\\nIn this structure each URL is \"worth\" the same and Google needs to rely on other signals to decide which URL is the most relevant.\\nThe next step is optimizing the guide pages in a way search engines quickly understand the difference between the individual versions:\\n\\nindividualize the page\\'s title, e.g.: <title>Version 1.0 install Guide for Product XY</title>\\nindividualize the page\\'s main heading, e.g.: <h1>How to Install Version 1.0 of Product XY</h1>\\nindividualize the page\\'s description, e.g.: <meta name=\"description\" content\"Here you can find your guide to install version 1.0 of Product XY […]\" />\\nmake use of the anchor texts to point out different versions when linking to the guides <a href=\"http://example.com/product/guide/guide-v2\" >Install guide for V 2.0</a>\\n\\nEach to point out the version it was designed for and maybe the year/month/date.\\nHint: it may be helpful to point out the differences at the top of the individual pages. So customers can easily see why they need different guides ans search engines get additional information an unique content for each page lowering your risk of \"near duplicate content\" issues.\\nThis way you make sure that the most recent guide is always present on example.com/product/guide/ and will served as result for generic searches like \"install guide product\". Search engines will recognize that content on this page gets updated frequently and also will recognize that sub-items of this pages exist and serve them for more specific search terms like \"install guide Product V 2.0\".\\nBy optimizing the pages for the specific versions they represent you make sure clients find their version by searching for it.',\n",
       " 'Ideally, websites should be hosted on their own shared web hosting account.\\nWhen one website is compromised in a shared web hosting account, then the others will also likely become compromised.\\nWebsites sharing the same web hosting account all need to be maintained to keep the websites secure.\\nOnce one website is compromised, you should scan the whole account to find and fix vulnerabilities.\\nUpdate Joomla, WordPress and all third party extensions / add-ons to the latest versions on all websites on a regular basis to keep all websites secure.',\n",
       " \"Good question! I'd never thought of attempting to redefine/rename the root.\\nThe root directory is assumed to be the domain itself. By defining it with a link, you've caused it to create a new directory 'Home' that is assuming the same position as the root. This has created both the assumed root and the 'Home' directory in the structure and the situation you've outlined in your question. \\nExtrapolating from this - it would appear that not defining your root directory is the best practice. Instead, define only the sub directories/categories/topics after the root. Where these are different language sites dynamically created on the main domain using, say, a query string: Then best practice would be to do what you've done above and re-define the root depending on the region targeted (usa+google.com, UK+google.co.uk, etc.).\",\n",
       " \"If you use hreflang tags it makes it clear they're US/UK mirrors.\\nPair them up and copy them to each page. It will tell search engines that they ARE identical (or similar) content but to serve them to the appropriate geolocation.\\nIt's also possible to manually set a geotarget for a (sub)domain using Search Console. Add the subdomain and go to the interntational targeting panel.\",\n",
       " 'Showing random post from your site is a pretty common practice, adding rel=\"nofollow\" to such links makes no sense at all because it goes against what nofollow really means.\\nnofollow just indicates that what you are posting is not endorsed by you (or that webpage), or there is a some kind of commercial relationship between that webpage and the link it specifies.\\nIt would be better to clearly group that section with an html tag like the aside element.',\n",
       " 'No it has nothing to do with network traffic. The company I work for has a GoDaddy hosting account, disk IO and network caps are similar in GoDaddy but they are not the same. If you have a PHP script that prints out a simple \"hello world\" but before printing the hello world, it needs to perform operations internally inside the server that triggers the movement of 20 MB worth of files, then that script will take 20 seconds to load. And that is because of the disk IO limit, it has nothing to do with network(for a realistic test try to echo hello world in a Symfony 2 project on GoDaddy and watch the load time...)\\nIt basically means what it says, they are limiting your disk operations to 1 MB/s, even if those disk operations are not doing any network tasks. A more realistic example is if you are lets say performing a search by filename in a large directory of file, you will not be able to go through more than 1 MB worth of data per second.\\nGoDaddy limitations are hidden in the terms and conditions',\n",
       " 'A 302 to the home page is likely to be seen as a soft-404 anyway, so that\\'s unlikely to be of any benefit.\\nA 404 can be perceived as temporary (a 410 is more permanent) and if the page doesn\\'t exist then a 404 is certainly valid. There\\'s not a lot you can do; if the page doesn\\'t exist then it doesn\\'t exist. \\nHowever, if this is really a \"temporary\" thing and its absence is planned. Then a 503 \"Service Unavailable\" status might be appropriate. If the period of time is known then this can be stated in a Retry-After header. The search engines are likely to hold on to the resource for as along as possible. But a \"few weeks\" might be too long?\\nIf the original page is not available for a lengthy period of time then the search engines will likely drop the page from the SERPs regardless of status code.',\n",
       " 'Email works intermittently because of the multiple records set for the root domain name which causes a conflict. There are two possible scenarios here:\\n\\nRemove NS records, and leave MX records as mail.yourdomain.com (which in it turn will point to the IP address  149.210.230.250). This way the settings should take effect within 30 minutes or so.\\nPoint your domain name to the DNS ns1.deziweb.com. and ns1.deziweb.com. (if you want to have your domain name hosted with them) and set up all these records, including MX, on the nameservers ns1.deziweb.com. and ns1.deziweb.com.\\nIn this case you will need to wait 24-48 hours for the changes to take effect.\\n\\nHope that helps! Let me know if something is not clear.',\n",
       " 'There\\'s exactly a few ways this can be done but I recommend that you use method 1 and 2.\\n\\nIncrease Organic Rankings: \\nThe main reason the international site is ranking better is due to the authority it has vs the other domain. Generally to outrank a brand you need to out rank the other site by improving the organic rankings. You can however increase your local rankings and it will contribute some what to the organics by adding authority and possible leads to more organic SEO leads. \\nHreflang:\\nUsing hreflang you can try to force Google\\'s hand by informing them to return a different page if the user is in X location. \\nGenerally this is normally performed on sub domains or sub folders but I am unable to find any evidence to remotely suggest that it CAN NOT be used on an external site. Using the following code on your international site will inform it to return UK version for people in the UK:\\n\\n<link rel=\"alternate\" href=\"http://example-uk.com/\" hreflang=\"en-gb\" />\\n<link rel=\"alternate\" href=\"http://example-us.com/\" hreflang=\"x-default\" />\\n\\nYou should also have a think if you want both brands divided on separate domains... nowadays in terms of SEO its far better to use one domain... only massive sites use multiple domains for administrating purposes. If you would rather keep them jailed from one another then you can setup a reverse proxy which will allow you to host two different hosting accounts under one domain and alias.',\n",
       " '...create example.com/blog as a masking redirect to ...\\n\\nYes, in theory, you can do exactly that. Note that by \"masking redirect\" we are not talking about \"framed forwarding\" or similar (which won\\'t help at all). The \"redirect\" is completely \"masked\" from the user. The address in the browser\\'s address bar shows example.com/blog but the server actually retrieves the content from blog.example.com/ (an additional / proxied request).\\nOn Apache you can use mod_proxy (and optionally mod_rewrite) to proxy the request from example.com/blog to blog.example.com (a reverse proxy). However, if your current host is too restrictive (after all, it doesn\\'t support PHP?) then this might not be possible. This is likely to require some additional config on your blog, since the base URL is now example.com/blog and not blog.example.com/.\\n\\nmove my blog.example.com into blog.example.com/info\\n\\nNot sure why you would want to do this seemingly \"additional\" step? (Why /info?) You don\\'t need to move your blog anywhere. example.com/blog would go straight to blog.example.com.\\n\\ncreate a robots.txt file for blog.example.com to tell search engines to not index the subdomain.\\n\\nYou definitely must not do this! If you suddenly block the search engines from crawling the old URLs then bang goes your SEO! You will essentially be starting from scratch. Moving from blog.example.com to example.com/blog is a URL change/migration. Like any URL change you would need to setup 301 (permanent) redirects from the old to new URLs - this allows search engines to discover the new URLs where the old URLs have already been indexed. And like any URL change you might experience a dip in ranking initially. There is always a risk.\\nSo, the basic steps involved would be:\\n\\nProxy all the requests from example.com/blog/ to blog.example.com/.\\nChange all your internal links to your new blog URL.\\nSetup external (301) redirects from the old to new URLs. ie. blog.example.com/ to example.com/blog/.\\n\\nHowever, I\\'m not convinced that \"changing\" the existing URL structure is going to be \"worth it\". As mentioned above, there is always a risk in changing the URL structure and having the blog under the same host (as opposed to a separate subdomain) may not see the SEO advantage you are seeking.',\n",
       " \"You can't target another country with same ccTLD.\\nWhen you add your website(for example .com.au) in search console then by default in targeting option you will see that is targeting australia country, you can't change that location because it is ccTLD. Google is stickt about ccTLD for targeting only one country.\\nI will suggest go with another ccTLD for New Zealand. Just by adding little different content on that site, you will not get any duplicate issue. Many of websites using different ccTLD for their eCommerce website, and they only change the currency price on their website, and rest of most content are same, and still they did not face any duplicate content issue, and it is because they don't have similar content for same location. They have similar content by targeting different geo.\",\n",
       " 'There are no penalties or preferences given for TLDs in google.com. All TLDs are treated equally in a google.com web search. \\nHowever, TLDs do affect your search results for country specific searches. For example, a .me or .tv domain will not rank as well as a .us website in a google.us web search. Also, a .co.uk site will rank better then a .us site in a google.co.uk search. This is because country specific searches give precedence to local websites. \\nSo if you are targeting a specific country or region, try to use the TLD for that country or region. If not then the TLD does not make a difference.',\n",
       " \"It looks like you want the canonical site to have no www.   Github publishes step by step instructions for that case.  If you do want the www, those instructions are here.\\nIt basically boils down to pointing the domain name to github's IP address.   You can set up an A record with 192.30.252.153 or 192.30.252.154.\\nInstructions for adding an A record to GoDaddy's DNS can be found here: https://www.godaddy.com/help/add-an-a-record-19238\\nYou can't use a CNAME without the www because it will break email for the domain.   MX records get ignored if there is a CNAME record at the apex.\\nI don't think that GoDaddy supports ALIAS records. Those are basically proxy records where you put in domain name to treat like a CNAME, but the DNS provider looks up the IP address, caches it for a few minutes, and serves it as an A record.   If you want to use an ALIAS record, you would have to switch to a DNS host that supports them such as Amazon Route 53.\",\n",
       " 'Your Website is not monitored:\\nThey do not monitor your website traffic, they simply estimate your website traffic, exactly the same way Alexa estimates it. This is mentioned on both SimilarWeb and Alexa.\\n\\nAlex mentions the traffic is estimated:\\n\\nSimilarWeb mentions the traffic is estimated:\\n\\nHow do they estimate my website traffic then?\\nThey estimate traffic from data obtained from their Toolbar Browser plugins, for example:\\n\\nAlexa Toolbar Firefox Plugin\\nSimilarWeb Firebox Plugin\\n\\nThe reliability of their estimated data:\\nThe reliability of the estimates is often low because too few people install those plugins and generally the reliability of data is often variable because certain niches will have more people install the toolbar than others. \\nIf you want more reliable data you need to add your site with Alexa or SimilarWeb to their metrics system so they can monitor your true traffics, the same way Google Analytics does.',\n",
       " \"The simple answer is that most of the PHP functionallity is in the basic setup and you probally don't need to worry about this.\\nBut, extensions are exactly as they sound like, they extend PHP functionallity. You have a MYSQL extension which allows you to connect to a database with premade functions (this extension is mostly on by default, unless you have a bad hoster).  \\nIf you do phpinfo(); in a php file, there will be a section called 'modules loaded', which lists all of them. Most of the usefull ones are already included.\\nTurning these extensions on is possible via various methods, often in the php.ini (the settings file for PHP). If you want to change this, you'll need root access to the server, which you often dont have with shared hosting (but, again, you don't really need this when you just begin).\",\n",
       " \"You have no control over them because they are hosted by another provider. And honestly speaking you should not worry about them, it's up to Google, Facebook, etc to handle the caching accordingly to their need.\\nYou could potentially proxy the URLs or download the files locally, but I don't encourage you to follow that route. In fact, you may potentially end up with a configuration that do not reflect the changes when the upstream provider publish an update, or simply conflict with the caching already applied by Google, Facebook, etc.\",\n",
       " 'There\\'s not enough information in your question to say whether they will influence your SEO or not. But my guess is \"probably not\".\\nIf these are regular 3xx redirects then yes, they will pass rank.\\nHowever, if the redirect URL/script is blocked with robots.txt or robots meta tag etc. (as is often the case with such redirect scripts) then \"no\", the links will not be crawled and no rank will be passed. This would make the link essentially a \"nofollow\" link, similar to when rel=\"nofollow\" is used directly on the anchor.\\n\\nTheir systems(text editor) only creates links with ...\\n\\nIf this site allows any users to sign up and write arbitrary content then any links should be \"nofollow\" - in the interests of the site (and everyone really). The links are \"untrusted\".\\nOnly if the user posting the content has been a member for a while and shown to be trusted might the site allow links posted by that user to be \"dofollow\". But stress \"might\".',\n",
       " \"I assume you are using cPanel's Addon domains? These indeed must point to a subdomain.\\n\\nit's a default behavior for shared hosting with Cpanel\\n\\nYes, it's default behaviour.\\n\\nDo you think Google wont index the subdomains until I don't link them, therefore it's not a problem?\\n\\nIf you never link to the subdomain then it's unlikely that Google will index it - but it is possible. (But even if Google did index it, whether it would rank higher than the Addon domain is another matter.)\\n\\nthe only way to avoid this is upgrading to a VPS\\n\\nNo, it's not the only way. In fact, it's relatively trivial to 301 redirect the subdomain to the Addon domain using .htaccess on Apache (per-directory Apache config file). This will resolve any possibility of the subdomain being indexed. However, (shared) hosting support rarely deal with .htaccess issues/solutions.\\nIn the .htaccess file in the root of the Addon domain (often a subdirectory of the main domain's document root, when the subdomain is configured as a subdirectory - but this isn't necessarily the case), try the following:\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} ^(.+)\\\\.example\\\\.com [NC]\\nRewriteRule (.*) http://www.%1.com/$1 [R=301,L]\\n\\nWhere example.com is your main domain. Include/omit the www subdomain in the substitution as required.\",\n",
       " 'Okay. This is actually simple.\\nThis is what I am seeing.\\n\\nThis image comes from http://www.pinoyexchange.com/, more specifically, http://www.pinoyexchange.com/forums/showthread.php?t=714586 where this image exists http://i60.tinypic.com/2zyhetx.png.\\n\\nWhile this answer and your question do not line-up exactly, this is probably a good example of what is going on. This image is coming up for searches based upon the search query. Other search queries will likely have different results.\\nSo what do you do about it?\\nThe primary problem is that image search is different from textual search.\\nTwo things.\\n1] Use the image prominently on the website such as in the header or footer taking full advantage of the img tag alt text. I suggest taking your present logo and splitting it to separate the actual logo from the text and using how you want to be known in search as the alt text.\\n2] Is to use schema.org mark-up for your school and be sure to use \"logo\" found here: https://schema.org/logo. Of course you should use the logo image without the attached text. This should be on your Contact Page and/or About Page and/or in the header or footer. I would suggest taking full advantage of schema mark-up. The advantage here is that schema mark-up fuels the knowledge graph.\\nFor what it is worth, an image search using your search phrase does show the correct image first. The bad news is that the image is on Wikipedia. The good news is that the image is on Wikipedia. [insert cheese eating grin here] Wikipedia is used directly in the knowledge graph. That is the good news. Now you simply have to create positive image search results that returns your logo for your site more consistently than the pinoyexchange.com site and possibly even Wikipedia.\\nWhat is often missed.\\nSite owners often forget about site reputation which is not just short-circuiting negative results. It can also be about sculpting search results with positive results that return a user to their site. This is the nature of SEO. Images are often ignored. So is the knowledge graph. Optimizing for image search and properly using schema mark-up are significant tools for any site and positive search results.\\nKeep in mind that you will need to explore how your site appears for a variety of search queries and determine which ones are most important for your users. Not you. From there, you can sculpt your sites presentation to optimize toward these searches. Also keep in mind, that search is not a perfect world and some compromises may be needed toward how people search the most.',\n",
       " 'You don\\'t need a \"wildcard\" certificate to secure subdirectories. All SSL certs will secure subdirectories. SSL certs secure hosts (domains). A \"wildcard cert\" will ordinarily secure subdomains eg. <anysubdomain>.example.com - but this should be made clear when you purchase the cert.',\n",
       " \"This post is 4 years old, hope the status in not Pending anymore. But this helped me.\\nAfter 14 days of pending status, I found this link, to PING bing with your sitemap. It's was a kind of awake call for my sitemap. Few hours later, my site was indexed.\\n\\nUpload your sitemap (but you already did hence the pending status)\\nTyp in your browser:  http://www.bing.com/webmaster/ping.aspx?siteMap={full url to your sitemap)\\n\\nYou receive a response with something like this:\\nThanks for submitting your Sitemap. Join the Bing Webmaster Tools to see your Sitemaps status and more reports on how you are doing on Bing.\",\n",
       " 'There appear to be two things you need to check.   \\nFirst edit /opt/bitnami/apache2/conf/httpd.conf and make sure that the line loading mod_expires is not commented out (comments start with #).   If it is, uncomment it and restart Apache.  (source)\\nIf you are trying to set the expires headers in .htaccess you will also have to allow overrides in your Apache configuration.   Edit httpd.conf and make sure that allowoverride all is specified.  (source)',\n",
       " 'If your User ID is user scoped, then whatever the last value was set at, will be the value applied to all hits for that user, across all sessions. If it changes in the next session, then the value, again across all hits, across all sessions, will be changed to that latest value.\\nSo what you probably shouldn\\'t be doing is setting the value when the user logs out. Put in some logic to only define the user ID custom dimension when the User ID value is valid (ie. not \"logged out\" or \"undefined\" or whatever).',\n",
       " 'Deleting or blocking the images is a necessary step, but only the first step to get your images de-indexed. Especially if you want the images to be removed quickly. \\nI would recommend you sign up for a google webmasters account and then report that the images no longer exist and that they should be taken off the index. Google has put up a form where you can report images and URLs – but it can be a bit hard to find: Webmasters > Tools > Removal.\\nIn my experience the images should disappear within 24–48 hours. Some time ago I have written a short beginners guide about the process with some more details over here.',\n",
       " ':\\nRewriteCond %{HTTP_USER_AGENT} ^Yahoo!\\\\ Slurp[OR]\\nRewriteCond %{HTTP_USER_AGENT} ^Zeus [OR]\\nRewriteCond %{HTTP_USER_AGENT} ^Sogou\\\\web\\\\Spider [OR]\\nRewriteCond %{HTTP_USER_AGENT} ^360Spider [OR]\\nRewriteRule ^.* - [F,L]\\n\\nThis code is actually \"broken\" in several places and will never work as intended. In fact, it won\\'t block anything in its current state, which explains your access log.\\n\\nYou need to remove the OR flag on the last RewriteCond directive. This additional OR flag would ordinarily cause all traffic to be blocked!\\n(But since you have further errors - see #2 - this does not happen!)\\nRewriteCond %{HTTP_USER_AGENT} ^Bingbot[OR]\\nYou are missing a space between the CondPattern (^Bingbot) and the flags argument ([OR]). (It should be ^Bingbot [OR].) This won\\'t match \"Bingbot\". But, crucially, the condition is now an implicit AND - so your rule block will never succeed and no bot will be blocked! I count at least 7 directives in your code above where the space is missing!\\nAs Stephen has already pointed out in comments, the regex used to match these bots do not necessarily seem to be correct. For example, a pattern such as ^Bingbot matches the exact string \"Bingbot\" (capital \"B\") at the start of the user-agent (^ being a start-of-string anchor). But the log entry you\\'ve shown contains \"bingbot\" (all lowercase) in the middle of the user-agent string. This will not match. You probably need a condition like the following, without a ^ prefix and with the NC flag for a case-insensitive match:\\nRewriteCond %{HTTP_USER_AGENT} bingbot [NC,OR]\\n\\nYou\\'ll need to check the other regex, whether they match the User-Agent you are trying to target. Are you matching at the start of the UA (^)? Should it be case-insensitive (NC)?\\nMinor point... Given the following two directives, the first one is superfluous. However, the second one looks like an error.\\nRewriteCond %{HTTP_USER_AGENT} ^Baiduspider [OR]\\nRewriteCond %{HTTP_USER_AGENT} ^Baiduspider* [OR]\\n\\nHowever, I still see them listed daily in the AWSTATS file on my sever.\\n\\nYes, even if you block the bots (once your code is working), they will still \"hit\" your server and be logged in the server\\'s access log from which AWStats builds its reports.\\nHowever, check your raw access log and you should see a 403 (Forbidden) in the response status for these requests (this is probably reported in AWStats as well). If not, then something is wrong.\\nThe RewriteRule can also be simplified:\\nRewriteRule ^ - [F]\\n\\nThe L flag is implied when you use the F flag.',\n",
       " \"This is what I understand:\\nYou have a github page foo.github.io\\nYour domain example.com is redirected to foo.github.io via an A Record with github-provided IP Address. By this, when using URL example.com, the address bar shows example.com, but data is from foo.github.io. Essentially, example.com & foo.github.io show same page.\\nNow, github's nature is that it supports both http and https; means if you request http on example.com, it will serve http://example.com, and if you request https, it will serve https://example.com (provided your SSL certificate keys are there in github).\\nWhat you are asking is that no matter if http was requested or https, only https should always be served.\\nNow, in dynamic languages and servers like ASP.NET or PHP or LAMP, it would be done server side, but as github support only static html javasçipt css files only, your only option is to use a javascript location.href.replace by detecting if protocol is http, redirect to https.\\nAs I am on mobile and about to board a flight, I will quote javascript code from s similar question's this answer:\\nif (location.protocol != 'https:')\\n{\\n      location.href = 'https:' + window.location.href.substring(window.location.protocol.length);\\n}\\n\\nNote: If you have never setup SSL Private and public keys for example.com in 1&1's system, it means you first need to do that, otherwise when https://example.com will be requested, browsers will show an error. I will add some steps for that when I am back from vacation..\",\n",
       " 'I have a website and I want to promote it on different social media platform. However, I don\\'t want to be treated like spammer...\\n\\nDon\\'t make your website look like something terrible to people and then you won\\'t be treated like one\\n\\nCan I use another domain to promote my website? ... website is about product reviews such as gadgets and other accessories. \\n\\nIf you had some wonderful website with different major sections, then I\\'d say yes have multiple domain names pointing to quality content.\\n\\nFor example, I will use other domain to redirect to my original website\\'s landing page. That page is specially designed to attract users.  Are such redirects wrong?\\n\\nTake a look at: https://support.google.com/webmasters/answer/2721311?hl=en\\nThey describe your intention of new domains as \"doorway\" pages because the first page (which is the new domain) offers no useful content, only a way to get to the next page (a.k.a. doorway). It is nice that you cut part of the inconvenience to the users by not making them click a lonely link for them to get to the real content but there is still inconvenience because users have to first wait for content (the redirection) to load from the new domain then they have for the real page to load. Why not just ditch the inconvenience and have users wait for only ONE page rich in content to load?\\n\\nIf so is there any other way I can promote my site?\\n\\nTalk to people? Make rich quality content that people want to see, and use search analytics in google search console to see what queries people use and how many enter your site as a result.',\n",
       " 'Actually this was definitely possible (and totally free) with a lot of hacking, and trial and error. I did stand up a reverse proxy on a VPS, which worked well, but I still didn\\'t like the dependency it created on the VPS having to be up (since the two servers I have are totally doing unrelated things). \\nI initially tried CloudFlare and its advanced url rewriting engine, and it was able to rewrite url requests from 80 ()http) to 443 (HTTPS). The problem is that it tries to communicate with the A record on port 80 (see https://www.lowendtalk.com/discussion/50106/cloudflare-proxy-origin-other-than-port-80). Also, the free version of CloudFlare only grants you five total advanced dns options. I have about 8 subdomains, so this wouldn\\'t work either.\\nI happened to be fiddling with my Namecheap domain and realized the DNS settings offer a URL rewrite, so I tried redirecting sub.example.com to https://sub.example.com. I didn\\'t get an error message instantly, which happened with dynu.com and afraid.com. Those won\\'t let you save this config. In fact, afraid tells you that it is not possible because it would create an \"infinite loop\", which is true to some extent because the A record (or CNAME record) is technically ambigious to the url to be rewritten. \\nSomething with Namecheap (the special A+ dynamic record, maybe?) allows you to do this kind of redirect. I couldn\\'t find anything in their docs that mentions this. The interesting thing is that a traceroute to your dns record gets you nowhere, and if you ping your dns name, it doesn\\'t give you your server ip, but instead a Namecheap server ip, so I think it must be some sort of reverse proxy setup on their end. Keep this in mind if you\\'re going to do something like WoL (you\\'ll need a separate A record). So far I haven\\'t ran into any issues. \\nAnyway, Namecheap offers free DNS management services, so I grabbed that and set up DNS-O-MATIC and had a perfectly working site hosted at home. Now anytime a visitor attempts to go to http:/sub.example.com/xyz they\\'re always redirected to https:/sub.example.com/xyz totally seamlessly and without any port garbage in the url. The ports seem minor but if I give someone a link verbally, it\\'s a lot easier. \\nHoping this saves someone else from spending as much time as I did on it. \\nUPDATE: \\nThe namecheap solution no longer works. I believe they applied a patch or altered the rewrite rules within the last six months which no longer allow for this configuration due to A record/URL rewrite conflicts like other DNS providers. I\\'ve abandoned hope--at this time there\\'s no free solution out there without implementing a reverse proxy, or until CloudFlare allows you to specify a port other than 80. \\nIf anyone knows an advanced DNS provider that can do what namecheap used to do, please let me know.\\nFor anyone interested, there\\'s an interesting article on how to create a reverse proxy for free using an Azure Web Apps free trial but it appears there\\'s a 165 MB quota per day. Still though for most people this would work. The link to this article is in my below comment since I don\\'t have enough reputation to post.',\n",
       " 'You will not going to face any SEO problem if your address, price, about us and contact us pages are different, because there are lot\\'s of websites who have similar design and have similar content. For example I provide same service as you, so Google does not consider them as duplicate content. And you said \"under different owner\", which is enough reason to don\\'t consider your site as duplicate.\\nThere are lot\\'s of aggregated website who fetch many website details/content into their website, it means the content is purely copied, but because they provide such a value to users, they did not get any duplicate penalty. \\nGoogle penalize duplicate content, because those newly created pages initially get some value, which you can transfer to any webpage by using links. Another reason is, they don\\'t want to store that kind of data on their server, because it is not good for any user, but in your case it is useful if you\\'re targeting different cities.\\nI have seen many of eCommerce website provide same kind of content on two different URL\\'s but the price value and it\\'s currency is different for example in USD, or EURO, so they did not face any penalty.\\nMost of eCommerce website have same product for example any smart phone XYZ, and when you see that webpage, then you will see the product features, prices are all same but still they did not face any duplicate penalty, because some of content are slightly different for example title and reviews. So if you also change your title slightly differently and if possible, then also change some body text, then you will not going to face any problem.\\nDon\\'t use canonical link tag if you want to index you\\'r all 5 websites. If any webpage contain any canonical link tag, and it does not same as your browser address URL, then Google will consider them as duplicate content, and it will index only the canonical link tag which you\\'re pointing on your webpage.',\n",
       " \"There are a couple of issues that I immediately see:\\n\\nSubdomain is not the same as subdirectory. A subdomain looks like company.domain.com or customer.domain.com. A subdirectory looks like www.domain.com/customer or www.domain.com/company. \\nIf you are talking about subdomains, then you should use a regex match on a hostname of (customer|company)\\\\.domain\\\\.com. If you are talking about subdirectories, then you should use a regex match on a page path of /(customer|company)/.*.\\n\\nNote that you can't use the begins with match type with a regular expression as you have in your example.\",\n",
       " 'http://accounts.fishyname.google.asdf.com\\n\\nYou do need to have some knowledge of URL/domain structure in order to assess whether this is the real deal or not just by looking at it. In this case the domain is clearly asdf.com - which is obviously not Google.\\nAs you mentioned in comments, the \"hierarchy\" is right to left. .com is the top-level-domain (TLD) under which anyone can register a domain name. Once someone has registered a domain then they can create any number of (suspicious looking) subdomains for that domain they like (as in your example). Your example shows 3 additional subdomains: accounts.fishyname.google.\\nIf you look up asdf.com in a WhoIs database then it will report the registered owner of that domain (although this could be protected with a private registration service).\\nYou also have to be careful of domains which might look very similar to the real thing - due to the use of unicode lettering. See IDN homograph attack (Wikipedia). For instance, there was a recent case where a Russian spammer registered ɢoogle.com (that\\'s a small-cap G, not g at the beginning) to try to dupe the unwary. Reference: https://www.bleepingcomputer.com/news/security/russian-spammer-uses-fake-google-domain-to-tell-webmasters-to-vote-trump/\\n\\nI know it should have the secure (lock) symbol\\n\\nSimply having the lock symbol (ie. HTTPS) doesn\\'t tell you anything about who you are connecting to. You might simply be connecting to a malicious website... securely!\\nOnly if the SSL cert is an extended validation certificate (the green bar in the address bar) and you take the time to check the name, can you be sure who you are connecting to.',\n",
       " \"During the high traffic period your server should be able to handle all requests made by visitors to your website.\\nBut there are some limits in concurrent connections handled by the server. So it's best to serve the page requests as fast as possible.\\nHere are some suggestions to consider in these situations,\\nApplication level improvements:\\n1. Minimize HTTP Requests to Speed Up Page Load Times.\\na) Combine all JS files together in a single combined JS file, and all CSS files in a single combined CSS file.\\nb) Minify JS, and CSS files, so the file size will be reduced and it will be downloaded faster.\\nc) Use CSS Sprites -  When you combine most or all of your images into a sprite, you turn multiple images requests into just one. Then you just use the background-image CSS property to display the section of the image you need.\\nd) Delay image download with lazy-loading, this will be helpful to reduce the http requests.\\n2. Prepare lightweight pages which are expecting more visits:\\na)Exclude decorative elements like images or Flash wherever possible; use text instead of images in the site navigation and chrome, and put most of the content in HTML.\\nb)Use static HTML pages rather than dynamic ones; the latter place more load on your servers. You can also cache the static output of dynamic pages to reduce server load.\\n\\nServer level improvements:\\n1. Reduce server timeout values by consulting your hosting provider (shouldn't be too low).\\nWhen timeouts are lower the connection will be released soon, so the server will be able to handle more connections.\\n2. Use third party services like CloudFlare for static data caching, and to protect your website from malicious users and attacks like DDOS.\\n3. Upgrade your server hardware - Upgrade physical and Virtual memories, increase I/O and Entry processes limits, if required. Your hosting provider will be able to help you better.\\n4. Cache dynamic code - Use APC to cache PHP opcode.\\n5. Load Balancing - Distribute load across multiple load balancing servers.\\n\\nWhen all required actions are taken, now it's time to check if the website is ready for a huge traffic spike.\\nThere are some third party services like loadimpact.com who provide load testing with simulated traffic. The analysis will help you to understand how much load your website can handle and what can be improved.\\n\\nAlso, during the traffic spike period, avoid high CPU usage operations like website backup cronjobs etc.\",\n",
       " \"The short answer is NO. Google's PageSpeed tool does not monitor SPAs properly. It only downloads all the resources, but, for example if you were using Angular, there is a delay from when the assets got loaded to the when the app is actually running. This process is called bootstrapping. This is the main reason why the Angular team is working on Angular Universal, a server side Angular rendering engine that pre-compiles Angular apps on the server. Only then can your project be observed properly with PageSpeed. This will also give the possibility for your SPA to be crawled for SEO.\",\n",
       " \"The default URL setting has no bearing on what is tracked on your site. It's merely a setting for your reports to help remove redundant pages:\\n\\nThe Default URL field is used to help correct an issue with our website where we can load the same exact page with two different URLs.\\n\\nReference: http://www.lunametrics.com/blog/2015/09/04/google-analytics-default-page/#what-is-the-default-url\",\n",
       " \"To be honest, I think this is really just going to boil down to personal opinion.\\n\\ne.g. example.com/~about. This is my favourite at the moment.\\n\\nHowever using a tilde (~) in such a way could end up conflicting with Apache's per-user web directories which is common on shared Apache servers.\\n\\nexample.com/pages/about\\n\\nWith using an additional path segment, would users expect to be able to request example.com/pages (or example.com/pages/)? This, of course, doesn't need to be a valid resource.\\nOther than that, it would seem to be just making the URL a little bit longer/complex than it needs to be, which probably isn't a big deal.\",\n",
       " 'I think you could do it without any major problems.   The only things I can think of that could go wrong are:\\n\\nIf you need a directory with that name on your server, it may be difficult to create and work with.   Command line programs use the dash (-) to specify flags and special arguments.   They don\\'t like working with files and directories that start with a dash.  The tilde (~) often means your home directory and it may get expanded to that by your shell instead of being treated as literal.\\nThat isn\\'t a \"normal\" practice and some users may find that it looks odd enough that they might not use your site.\\nThose are not keywords, so there is no SEO benifit',\n",
       " \"It is against the Google webmaster guidelines to copy pages and change the keywords.   They call this practice doorway pages.   \\nSuch pages are not likely to be able to satisfy users because they do not have enough specific information about each product.   Those pages only purpose would be give Google a page to rank for they keywords.\\nGoogle will penalize sites that do not adhere to the webmaster guidelines.   Because of doorway pages, Google could remove your site from all search results.\\nHaving a page for each product is fine as long as you have something specific to say about that product.   Having a section on the page that is filled in with a template as you suggest would be fine.   Such a section can't be the only content on the page.\",\n",
       " 'If you want to provide multiple addresses, you have to specify one address property with an array value, not an array of address properties.\\nIf you click at the error in the SDTT, the tool correctly highlights the line where the error begins (line 7, i.e., the opening [).',\n",
       " \"Google can probably interpret these tags correctly when they are set by JavaScript.   In 2014 Google announced that it executes JavaScript before indexing pages.  \\nI don't know a good way of testing these tags specifically, however I've seen some testing that indicates Googlebot pays attention to other meta tags when inserted dynamically by JavaScript.     \\nWe Tested How Googlebot Crawls Javascript And Here’s What We Learned\\n\\nWe dynamically inserted in the DOM various tags that are critical for SEO:\\n\\nTitle elements\\nMeta descriptions\\nMeta robots\\nCanonical tags\\n\\nResult: In all cases the tags were crawled respected, behaving exactly as HTML elements in source code should.\\n\\nDynamically Added Meta Data Indexed By Google Crawlers\\n\\nThe Meta Description in the SERP result above has been injected with Google Tag Manager. So it IS true:\\nGoogle’s crawlers index dynamically injected meta data as well.\\n\\nI don't see any reason that Googlebot would treat link meta elements for pagination any differently than other dynamically generated meta elements that have been tested.\",\n",
       " 'As Stephen points out in comments, geo-IP redirects are generally bad for SEO (it prohibits the site from being crawled naturally and can at times hinder users through being redirected incorrectly). However, specialized bikes would appear to get around the SEO issues with sitemaps containing the different language versions (hreflang). (And only redirecting when no locale is included in the URL.)\\n\\n301 or 302 redirect?\\n\\nThe referenced site would appear to use a 301 (permanent) redirect. 301s are cached hard by the browser. Once redirected; always redirected (from cache). This can result in marginally less traffic from repeat visitors.\\nHowever, I would tend to go for a 302 (temporary) redirect - which is not cached - for the following reasons:\\n\\nA 301 is a one-way trip. If you needed to change your geo-ip-redirect-logic then it\\'s a struggle. Users who have already visited your site might still get redirected the old way.\\nUsers are not necessarily stationary. Mobile/tablet/laptop users can travel between regions, but a 301 redirect can tie them to the first region in which they visited the site.\\nA geo-IP redirect is not necessarily permanent.\\n\\nA 302 would not suffer these \"problems\" (albeit \"edge cases\").',\n",
       " 'Custom Reports will only display dimensions which have values. The reason why the event is not showing is that is has nothing to \\'show\\'. \\nMy assumption is that the event of interest has no event value, and consequently does not show in the report. The reason why it shows in the report under the Behavior tab is that there are other fields where the event has values (e.g. total events, unique events).\\nThe easiest fix is to include \"Total Event\" or \"Unique Events\" in your custom report.',\n",
       " 'You would think you thinking is correct, but it actually it isn\\'t.\\nI have worked on many, many sites and some URL that do not have any physical links to them (or none we were aware of) always managed to get indexed in Google.\\nWho knows where Google finds the links, but invariably it does. So this is something you should definitely fix.  \\nIF you can 301 redirect the duplicate pages to a single URL that would be the best fix, or if you need the duplicate URL to be live for what ever reason, set a canonical tag on the duplicate URL referencing a single URL.\\n<link rel=\"canonical\" href=\"http://example.com/special/path/post-name\" />\\n\\nIf for some reason you cannot set a canonical tag, you can set the robots meta tag to noindex them.\\nIn the header section of the page:\\n<META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, FOLLOW\">\\n\\nOr in the HTTP header\\nHTTP/1.1 200 OK\\nDate: Tue, 25 May 2010 21:42:43 GMT\\n(…)\\nX-Robots-Tag: noindex\\n(…)\\n\\nAnd as a very last resort, if you could not implement any of the above, you can block them in your robots.txt file, using something like:\\nDisallow: /category/',\n",
       " 'You can remove query strings from incoming hits either\\n\\nin the view settings (if you have a finite list of parameter names you want to remove); enter the names into the \"exlude query parameters\" textbox as comma separated list\\nvia view filters (if you need to remove multiple query strings, or all query strings)\\n\\nLunametrics has an article that explains the various possible filter settings.\\nYou should keep an unfiltered view as backup, though; filter change data in a view permanently and there is no way to recover data that has accidentally been filtered out.\\nThere is not much you can do for urls that has already been collected.',\n",
       " \"Do a traceroute on the IP address in question and you will more than likely find that the IP address traces to Europe somewhere. The whois record for the IP address in question is simply returning the registered owner of the IP address range and their organisational details. It will show up as the US as that is where GoDaddy is registered and where they will have registered their IP address range but just because it is registered there does not mean that it is assigned to a US server.\\nAs an example my company website is hosted by Amazon Web Services out of their Sydney data center, when I do a whois lookup on the EIP for my instance it returns as being registered in the US to Amazon Technologies. That doesn't mean my instance is in the US, it just means that my hosting provider's company is registered at that location (usually the head office address is used).\",\n",
       " 'When changing the domain for a site there is always a risk. There is a chance you will see a dip in rankings initially. All you can do is to try to minimise any damage:\\n\\nSetup the site on the new domain, whilst keeping the site active on the old domain (temporarily).\\nVerify both domains (properties) in Google Search Console (GSC / formerly Google Webmaster Tools). Ensure that the canonical (www or non-www) versions are verified.\\nSubmit an XML sitemap for the new domain in GSC.\\nSetup 301 (permanent) redirects from the old to new domains. (At this point, you can shut down the old site, whilst keeping the redirect in place.)\\nWhere possible, change any prominent backlinks to the new domain.\\nUse the Change of Address tool in GSC to submit a change of address request.\\n\\nReference:\\nGoogle Search Console Help - Use the change of address tool\\nHowever, there are still other SEO/business considerations, particularly if this is your company website. Customer awareness/trust etc.',\n",
       " \"Not long ago I was in the same boat when using WordPress plugin Yoast SEO it produced several sitemaps and I was curious to know which one to load to Google's Webmaster Tools Search Console I ran across this article on Google regarding Search Console Help:\\nSimplify multiple sitemap management\\nIf you have many sitemaps, you can use a sitemaps index file as a way to submit them at once. The XML format of a sitemap index file is very similar to the XML format of a sitemap file. The sitemap index file uses the following XML tags:\\n\\nsitemapindex - the parent tag surrounds the file.\\nsitemap - the parent tag for each sitemap listed in the file (a child of sitemapindex)\\nloc - the location of the sitemap (a child of sitemap)\\nlastmod - the last modified date of the sitemap (optional)\\n\\nOnce you’ve made and saved your index file, you can submit your index file to Google as long as you upload and save all your sitemaps to the same location on your host server. You can submit up to 500 sitemap index files for each site in your account.\\nThe article location: Simplify multiple sitemap management\",\n",
       " 'Reviewing your website as it is now, I am not too sure if this is a problem any longer / currently.\\nThe issue is not internal linkage on your website with the inclusion of UTM parameters (as another question suggests).\\nIt seems more like some process you have to share your website content on social media is leaving the UTM parameters in the URLs and sharing those URLs which has, at some point, led to them being indexed.\\nIt is rare that this happens, but it has happened to many other sites before. The fact that it is only three pages indexed with these parameters is indicative that this is neither a serious issue nor a sitewide one.\\nHere are the steps you can take to help eradicate this from happening:-\\n1. Specify a canonical URL on your pages\\nYou are already doing this and the implementation is correct. This will ensure only the specified canonical URL will be given weight in search engines. Presumably this has always been in place but if not, then this could explain why there are some old instances of pages still indexed with UTM parameters.\\n\\n2. Instruct Google not to index the UTM parameters in Search Console\\nIn the event that some URLs are being indexed with the UTM parameters (like your case), the URL parameter should appear as a detected one from within the \\'Crawl > URL Parameters\\' section of Google Search Console for your domain (see below).\\n\\nEven if the UTM parameters do not appear, you can \\'Add Parameter\\' to create them.\\nSimply select No: Doesn\\'t affect page content (ex: tracks usage) (known as \\'Passive Parameters\\') and Google will then usually only crawl just one URL with a specific parameter value.\\n3 Disallow the URL parameters in your robots.txt\\nThis will block Google from indexing the content of these URLs but not the actual URLs themselves (they could still display in the search results but will just omit the description like below).\\n\\nSimply adding something like the following would handle this from robots.txt:-\\nDisallow: /*?utm=*\\n\\nConclusion\\nSteps #1 and #2 should be carried out as a matter of precaution and \"best practice\" anyhow and step #3 in addition to steps #1 and #2 perhaps (as won\\'t be effective on its own).\\nWithin Google Search Console, there is also the ability to (temporarily) remove URLs. This is particularly useful if there are some stubborn pages still indexed but you know the root source of the issue has been resolved and this facility should be enough to rid of them once and for all from the search results.\\nI have not included this as a step above as, despite having researched this before, I cannot recall whether it will support URLs with parameters [citation needed]. I once knew the answer but my memory fails me on this particular occasion.\\nMore reading on the removal of URLs from Google.',\n",
       " 'Browsers don’t further access the server when the certificate is not trusted/valid, so the .htaccess redirect can’t work (it should work as soon as the user adds the certificate as an exception).\\nThe best solution is to get a certificate for the hostname with www, too. This does not only help for redirecting users to the correct hostname when following links from search results or bookmarks, it also helps those users that type your hostname with www (which is commonly done, even if it’s advertised without www), which might still happen long after search engines have removed the hostname with www from their indexes.\\nIf that’s not possible, you can at least signal search engines that they should prefer the hostname without www. A 301 redirect is the best way here. If that’s not possible, the second best is the canonical link type. Of course both can only work for search engines that ignore the bad certificate (I guess most do).\\nYou could also try to use the search engine’s webmaster tools to set the preferred hostname. For Google, see Set your preferred domain (www or non-www) (but I don’t know if this works for hosts without a trusted certificate).\\nIn any case, you have to wait. After some time, the hostname with www should disappear from search results.',\n",
       " \"Try right-clicking (or command-click on a Mac) over the element with Firefox and selecting 'Inspect element' from the menu. This reveals that it is an unordered list with a background image styling the list on line 255 of style.css.\\n.widget ul li {\\n    background: url(images/bullet.png) no-repeat 0 10px;\\n    padding: 5px 0 8px 18px;\\n    color: #262626;\\n}\\n\\nThere is also a style removing the list style (to be replaced with the background image) in frontend.css on line 1 - probably minified CSS all on one line.\\n.rp4wp-related-job_listing > ul li.job_listing a .meta li, .rp4wp-related-job_listing > ul li.no_job_listings_found a .meta li, ul.job_listings li.job_listing a .meta li, ul.job_listings li.no_job_listings_found a .meta li {\\n    list-style: none outside;\\n    display: block;\\n    margin: 0;\\n}\",\n",
       " 'I was able to go through and change the view settings, filters, and referral exclusion list and it has fixed the problem.\\nIf I was more patient I could have made the changes one at a time, but I can confirm that in this case (even though this is a WordPress site) it was all on the Google Analytics side (and not the website side). Not 100% positive which of the above fixed it, though.\\nIf anyone else has this issue, here\\'s exactly what I did:\\n\\nView Settings (View): I emptied the \"Default Page\" field. When creating a new view, this is empty to begin with, but at some point somebody put their domain in there.\\nFilters (View): They had a find/replace filter in place (Find \"^www.\" in the hostname and replace with nothing), but the verification said it wouldn\\'t affect any traffic, so I\\'m thinking it was a red herring. I removed it along with some other improper filters.\\nReferral Exclusion List (Property): It looks like they were attempting to block ghost spambots using this, so I removed the \"spambot\" domains from the list. They were also missing their own domain. Basically, I emptied this list, and then added their domain to it (which is how it should be by default).\\n\\nFor reference, I added a \"Valid Hostname\" include filter on this view to only allow data from traffic to their domain (and acceptable others).',\n",
       " 'As I came across the same problem I did some research and found the following:\\n\\nCommon name in the CSR code needs to be of a certain format. General requirements are latin alphanumeric characters and no special symbols like ! @ # $ % ^ ( ) ~ ? > < & / \\\\ , . \" \\' _ More peculiarities are described here for your reference. IDN (International Domain Names) common names should be first converted into the punycode, and then indicated in the CSR.\\n\\nSource: http://helpdesk.ssls.com/hc/en-us/articles/204299792-How-to-make-sure-domain-is-correct-in-the-CSR-\\n\\nStep 1: Convert your International Domain Name (IDN)\\nUsing the IDN Conversion Tool, convert your International Domain Name (IDN) into ASCII characters\\n\\nSource: https://search.thawte.com/support/ssl-digital-certificates/index?page=content&id=INFO3118\\nThough I didn\\'t see any explicit mentioning, my guess is if your browser supports IDN Domains for URLs, the certficates should work as well.\\n;TLDR - use the ascii encoded domain like: xn--cjs.com',\n",
       " 'Try using more VirtualHosts and redirecting instead of doing rewrites.\\n<VirtualHost *:80>\\n\\n  ServerName example.com\\n  ServerAlias www.example.com\\n  Redirect permanent / https://www.example.com/\\n\\n</VirtualHost>\\n\\n<VirtualHost *:443>\\n\\n  ServerName www.example.com\\n  Redirect permanent / https://example.com/\\n  ...\\n  (put your HTTPS config & path to certs here for the redirect to work)\\n\\n</VirtualHost>\\n\\n<VirtualHost *:443>\\n\\n  ServerName example.com\\n  ...\\n  (This is your destination VirtualHost)\\n\\n</VirtualHost>',\n",
       " \"Your .htaccess actually looks OK, and you say the redirection is working OK for you. The only possibility is that if Google is requesting the www subdomain? In this case, Google would not see the redirect since you are specifically checking for the bare domain.\\nSince your old-domain is a separate hosting account then your directives can be simplified (ie. no need to check the requested HOST, since it can only be old-domain anyway), and this will also capture the www subdomain (if that is indeed the problem)...\\nAll that's required is:\\nRewriteRule (.*) http://new-domain.dom/$1 [R=301,L]\",\n",
       " \"There are some people use robots.txt to block css and javascript things from Googlebot and here you gonna hide it from other techniques (May be by checking their user agent or IP address), but Google don't like that. It surely affect on SEO.\\nBootstrap contain too many CSS codes which you might not using at all, I have used recently bootstrap and when I do site audits via chrome dev tools, it says 90% css is not used at all in my website, so why should I load them? Then I remove those unnecessary css codes from my bootstrap file. Same thing will apply to Jquery as well. \\nSo optimize your framework, remove unnecessary things and host it on your own server, If your site is minimal and don't require more css then you can directly put your css in head section which will help you to save some time to request another HTTP request to server.\",\n",
       " \"You can use the API to export all the text content, with something like action=query&generator=allpages&export. Files you'll have to scrape via some script, such as pywikibot. You can see what extensions are installed via Special:Version if you want to set up an identical wiki; some of the configuration settings are available via the siteinfo API, most you'll have to guess. There is no way to bulk clone user accounts, but you can use the MediaWikiAuth extension to transfer them when they log in.\",\n",
       " \"You don't need a domain name to access a website. You just need a web server with a static IP and every time you want to access that server you should remember that number. That is what the domain name mainly does, translating those sequences of numbers into a readable, easy to memorize name.\\nTo put it easy, you don't register an IP address, you use the IP address that a web server has.\\nSo, don't register a domain name, just use some type of web hosting service and note down its IP address.\\nI would like to point out also that in case of a shared server or if using subdomains, you would need to have a domain to help the server locate your desired website, the above answer applies to a dedicated server or if the IP resolves directly to that website, that is, it has just one website.\",\n",
       " 'Redirect 301 keyboardchart.php keyboard-chart.php\\n\\nThe 500 error is because the target URL is relative. That is not allowed with a mod_alias Redirect. The target URL must either be absolute (with a scheme and hostname) or must start with a slash (ie. root-relative).\\nBut also, the source URL-path will not match either. You must specify a root-relative URL-path, starting with a slash.\\nRegardless of where the .htaccess file is located, the mod_alias Redirect directive is the same. Unlike mod_rewrite (RewriteRule) that has the concept of a \"directory-prefix\".\\nSo, like VladShundalov suggests, you would need a directive of the form:\\nRedirect 301 /keyboard/keyboardchart.php /keyboard/keyboard-chart.php\\n\\nNote that this matches any query string. The query string is automatically passed through to the target. You can\\'t actually match the query string with a mod_alias Redirect.\\nIf you need to match the specific query string then you must use mod_rewrite instead. For example, in the example.com/keyboard/.htaccess file you could write something like:\\nRewriteEngine On\\nRewriteCond %{QUERY_STRING} ^gam=7&sty=15&lay=1$\\nRewriteRule ^keyboardchart\\\\.php$ /keyboard/keyboard-chart.php [R=301,L]\\n\\nNote that you don\\'t state the subdirectory on the RewriteRule pattern in this case, however, you do still need a root-relative path on the substitution (unless you specify the path with a RewriteBase directive). The query string is passed through to the substitution automatically by default.',\n",
       " 'It’s perfectly fine to offer multiple feeds. You could even offer multiple feeds for the same items, e.g., one for the full content and one for excerpts.\\nThere is nothing that would have to be changed in a feed just because there exist other feeds. So an RSS feed’s channel element would contain the same content as if the feed were the only one.\\nLinking multiple feeds in the head (example) works exactly like linking one feed in the head. Best practices when linking multiple feeds:\\n\\nThe first feed link should be the default feed, as this is the feed that will be used for autodiscovery. An example where this might be relevant: a feed reader that allows users to provide a webpage URL (instead of a feed URL) might add the default feed.\\nLiferea does this, for example.\\nEach feed should get a title, as this allows users to understand what the feeds contain. An example where this might be relevant: a client that lists the linked feeds might show their titles.\\nFirefox does this, for example:',\n",
       " 'It is fine to index your paginated content in Google search result, you can set different title for that, for example\\nfor www.myshop.com/category-list?p=2 you can set title as \"Page 2 - product category\"\\nfor www.myshop.com/category-list?p=3 you can set title as \"Page 3 - product category\\n\\nBut if you really don\\'t want to index other categories paginated webpages then simply use the noindex meta tags.\\nFor example if you implement below meta tags in <head> section of any webpage.\\n<meta name=\"robots\" content=\"noindex\">\\n\\nThen that page will not going to appear in search result. But Google will still crawl that URL and pass the all the ranking benefits as like normal, it\\'s just used for to prevent indexing. \\nI don\\'t know have idea about Prestashop, so i don\\'t know how you gonna implement it, but I just want to say, you have to add that meta tags on those pages only which you don\\'t want to index. I mean don\\'t implement that tags on your all webpages othewise your all webpages will be deindex by Google. I am saying that because may be your Prestashop use parent heading section on all webpages, so you have to add some if else code first.\\nSo above Meta tags is easiest way to implement from client side, but if you\\'re server side fan, then you can also use x-robots-tag.',\n",
       " 'Our site has lost substantial traffic after moving to HTTPS.\\n\\nKeyword here is moving.\\nIf you just changed everything over to HTTPS from HTTP (and got rid of the HTTP version), then many robots that had access to your site before the change will think you don\\'t have a site anymore since they will come across a series of \"404 not found\" based pages generated from your server which is bad news.\\nWhat you need to do is create redirects for every HTTP page you previously made available to the public so that users who try to access the old pages will be automatically redirected to the new pages. To ensure the redirect is perfect, ensure the HTTP header response for the old page contains HTTP code 301 (moved permanently) and that a location header exists pointing to the HTTPS version of the same page. Redbot.org is a good tool to verify that your headers are correct.\\n\\nShould we go back to our last without HTTPS version?\\n\\nIf you do that, you might throw people off all the more because they may have one version in their bookmarks and if you switch back again with no regards to implementing redirects, then you\\'ll frustrate the new batch of people since they will get the dreaded 404 error.',\n",
       " 'PHPRC is a replacement for php.ini, ... The sites seem to tell how to create one.\\n\\nI think that is something a bit different (regarding creating a \"phprc\" file - a local PHP config file - something unique to DreamHost AFAIK)?\\nThe PHPRC (PHP Runtime Configuration) environment variable is one way to tell PHP where to find the main php.ini file. It can sometimes be used to change the location of the main php.ini file.\\nReference: http://php.net/configuration.file\\nOn some hosts, the PHPRC environment variable can also be used to set a local (per site) php.ini PHP config file. (Otherwise, trying to change the location of php.ini on a shared host will not do anything, apart from setting the environment variable.) This can also be achieved using a local .user.ini file and/or .htaccess - but this can vary from host to host.\\nThe Apache directive SetEnv (part of mod_env) simply sets this environment variable. PHPRC is then available to any scripting language that reads these environment variables.\\nWhy is this required for NinjaFirewall?\\nThe NinjaFirewall docs state:\\n\\nInstallation... NinjaFirewall will need to add some instructions to your system files (php.ini, .htaccess). In most cases, it will be able to detect your configuration and to make those changes for you\\n\\nMy guess is that NinjaFirewall needs a local (php.ini in your case) config file in which to make changes. Or, less likely, needs the PHPRC env var set in order to be able find the location of the php.ini file, which can then be edited. (The specific path that is being set will tell you this... is it a path within your webspace? Or somewhere else on the file system?)\\nIt\\'s also stated in this WordPress.org thread, that:\\n\\nThe SetEnv PHPRC directive in the .htaccess is only needed when you are using a php.ini that is not recursive, i.e., that does not apply to subdirectories.\\n\\nAlthough that doesn\\'t seem make much sense to me? (If php.ini is not \"recursive\", then I don\\'t think you would be using php.ini to begin with?)\\nFurther reading:\\n\\nTroubleshoot NinjaFirewall installation problems - discusses php.ini and other config files.',\n",
       " 'The document that recommends JSON-LD is more recent and it clearly states that Google recommends using JSON-LD where possible (over Microdata) specifically for the following reasons:\\n\\n\"The markup does not have to be interleaved with the user-visible text, which makes nested data items easier to express, such as the Country of a PostalAddress of a MusicVenue of an Event.\"\\n\"Google can read JSON-LD data when it is dynamically injected into the page\\'s contents, such as by JavaScript code or embedded widgets in your content management system.\"',\n",
       " 'Working from this: ...dynamic clock or meteo widget on the page could boost the SEO... ...has been updated with new content.\\nShort answer? No. Not even close.\\nSearch engines, using more than one page, can determine patterns within the HTML DOM model that separates the header, footer, sidebar, and any other page element that is truly not content. The idea is to evaluate the content and not things that do not add value to the content directly.\\nLet\\'s assume for a moment, that the \"clock\" is within the content itself.\\nSearch engines do not look at content in a linear way. It can\\'t. Computers do not read and cannot evaluate what they see in a way that humans can. For this reason, the evaluation of the content uses semantic analysis including semantic topic scores, semantic linguistics, and others to evaluate the content itself. What Google, in particular, sees would be small changes in content scoring if at all. These methods adequately handle restructuring and reordering of content, spinning content, keyword stuffing, content gibberish, and any other trick that was designed to deceive search engines.\\nContent has to have meaning. Adding erroneous elements that do not add meaning do not add value. It is that simple.\\nFor content to be fresh, there are several factors that are in play. One is whether or not new content is update routinely. Another is whether older content is updated periodically as appropriate. Another still is how factual a page is as compared to other content on the web. There are quite a few metrics that determine freshness. Not every page has to be updated to be considered fresh. Relevance and search engine result page (SERP) performance is a factor too. Freshness is not simply updating a page. It is a series of metrics site wide that determines if content is fresh.\\nAs a warning, please do not follow the advice of junk SEO sites. There are too many of them. There are no tricks that work. Any SEO advice that is even hinting at an advantage outside of creating compelling, well written, sought after, and well structured content is very likely leading you down a very foolish path.',\n",
       " 'As long as your PDF files are hosted on the same server, they should appear in the server logs.   The first step to diagnosing the problem would be to ensure that PDF views are being logged properly.\\nOnce you have verified that they are in the log file, you can configure AWStats to record them as page views.  According to the AWStats documention, the NotPageList controls the extensions that are not counted as pageviews.  The default is \\nNotPageList=\"css js class gif jpg jpeg png bmp rss xml swf\" \\n\\nYou need to make sure that \"pdf\" has not been added to that list.',\n",
       " 'Jeff Atwood is one of the founders of this site.   He since left StackExchange to create a forum product called Discourse.  I found a thread on his site with comments from him that are relevant:\\n\\n... \"all data must be migrated\" is the Vietnam of forum software, leading to massive retention of ancient forum versions across the web and a huge black eye for forum software in general. I heard an untold number of horror stories from companies that do nothing but specialize in forum migrations. I could see it in their eyes.. I remember.. I remember everything..\\n\\nBased on his comments, it appears that there is no standard for forum content storage that would facilitate forum software migrations.',\n",
       " \"I've seen several places that produce color names based on a color code.   Given that there are 16 million+ hex colors, it would be pretty hard to have unique names for each and every one.  Whatever you do will have to accept approximate values by matching them to the nearest name.\\nOn StackOverflow: Function that converts hex color values to an approximate color name?\\nOpen source Javascript library to name colors released under Creative Commons License 2.5: http://chir.ag/projects/ntc/\",\n",
       " 'You seem to have a good understanding of why you should and shouldn\\'t write your own forum software so I\\'m going to focus on my opinion instead of hashing out facts you already know.\\nForums are favorite places for spammers to spam. They\\'re right up there with blogs. Even existing forum software have a hard time keeping up with them. If you roll your own forum software you may find keeping up with them a full time job and not worth the time and effort.\\nExisting forums software offer a familiarity for users. There\\'s really just a handful of popular forum software out there and most people who are active in communities have seen most of them. If you use one of them in your site your users with be comfortable using them right away.\\nExisting forum software offers you a chance to get your community up and running quickly. They also offer you a chance to add a lot of features quickly.\\nThe cons would be a lack of \"perfect\" integration with the rest of your site. But the question is, \"How much is that offset by the positives listed above\"?',\n",
       " \"Can you explain why Google would need to have itself appear as a result for the phrase 'search engines' when someone is already searching for search engines while using Google? One would think that if someone is searching for something on Google, they would know that Google is a search engine.\\nAs for other search engines, Google is their biggest competitor. They don't have any business reasons to place Google first.\\nFinally, Google doesn't have the phrase 'search engine' on their home page. And if someone is going to link to Google's homepage, they are far more likely to do it using the anchor text 'Google' than 'search engine'. Google has transcended being simply a noun or a brand and has become a verb. People now google things, just as they use a kleenex rather than a tissue.\",\n",
       " 'Here is the full correct targeting:\\n<!-- UK -->\\n<link rel=\"alternate\" href=\"http://blah.com/stuff\" hreflang=\"en-gb\" />\\n\\n<!-- EU Targeting-->\\n<link rel=\"alternate\" href=\"http://blah.com/eu/stuff\" hreflang=\"en-fr\" />\\n<link rel=\"alternate\" href=\"http://blah.com/eu/stuff\" hreflang=\"en-de\" />\\n\\n<!-- American Targeting-->\\n<link rel=\"alternate\" href=\"http://blah.com/us/stuff\" hreflang=\"en-us\" />\\n<link rel=\"alternate\" href=\"http://blah.com/us/stuff\" hreflang=\"en-ca\" />\\n\\nOne page can have multiple annotations for multiple countries, but the page cannot be annotated in multiple languages.\\nhttp://blah.com/us/stuff should only be written in one language, presumably English (EN)\\nIf you want to try and force* http://blah.com/us/stuff to only show up in Canada and the US:\\n<link rel=\"alternate\" href=\"http://blah.com/us/stuff\" hreflang=\"en-us\" />\\n<link rel=\"alternate\" href=\"http://blah.com/us/stuff\" hreflang=\"en-ca\" />\\n\\nHowever, since the page is only in English, we can\\'t tell Google it\\'s also in other languages. According to your example, the following statements are incorrect:\\n<link rel=\"alternate\" href=\"http://blah.com/eu/stuff\" hreflang=\"fr-fr\" />\\n<link rel=\"alternate\" href=\"http://blah.com/eu/stuff\" hreflang=\"de-de\" />\\n<link rel=\"alternate\" href=\"http://blah.com/us/stuff\" hreflang=\"fr-ca\" />\\n\\n(* = Google can override your configurations, nothing is an absolute directive with hreflang)',\n",
       " 'The objective measure is in money:  How much does it cost to maintain the browser support vs how much it costs to turn away users because of browser compatibility.\\nCosts of turning away users\\n\\nImmediate revenue lost from sales or advertising\\n\"Bad will\" where users remember that your site doesn\\'t work and are less likely to use it in the future\\nWasted customer acquisition costs\\nImpacts on SEO from bad usability or \"mobile friendly\"  scores\\n\\nCosts of adding browser support\\n\\nDeveloper time (which may be difficult to estimate ahead of time)\\nQuality assurance time \\nEquipment and access to browsers\\nOpportunity cost (you could have been working on something else that makes more money)\\nNot being able to use technology because it is unsupported by some specific browser.  (This may make your site look or behave worse for everybody.)\\n\\nHaving evaluated these factors myself, I usually use a 2% threshold.  If the browser has at least 2% market share, it is worth supporting.\\nFor the last 10 years, old versions of IE have been the most costly to support.  They typically require more workarounds and support fewer features than other browsers.   They also tend to be used far longer than old versions of other browsers.   The costs of supporting IE 8 (3%) could be twenty times the cost of adding support for Opera (1%).  You sometimes have to take it on an individual case by case basis and see how badly your website is broken in that browser.',\n",
       " 'If a page has both X-Robots-Tag HTTP header and robots meta tag, can this cause problems?\\n\\nThis should not cause \"problems\". Providing you make sure to provide the same values for both. eg. noindex, as in your example.\\nFor search engines that support the X-Robots-Tag HTTP response header, then this should always take priority. Where there is an equivalent HTTP response header for an in-page meta element, the HTTP header always wins.\\n\\nWhich one is better? X-Robots-Tag HTTP header or robots meta tag?\\n\\nWell, you can\\'t use the robots meta tag on non-HTML resources, such as PDFs and images etc. This is primarily why the X-Robots-Tag header was invented.\\nBut whilst all the main search engines support the X-Robots-Tag header, it probably doesn\\'t have as wide support as the robots meta tag, which has been around a lot longer.\\n\\nOn a frame redirect/masked page, does it count the inner robots tags?\\n\\nYes. The document contained inside the frame is an entirely separate request and consequently, it\\'s fetched just like any other single request.\\n\\nDoes it count the meta tags of example1.com/masked-url or example2.com/original-url?\\n\\nBoth. They are two separate requests.',\n",
       " 'Segments are always limited in how much of them you can use in analysis.\\nThe only way I think to tackle this problem is always creating custom reports for whatever analysis you want to do and then exporting this data in excel to Do further analysis. \\nPlease note that based on traffic to these websites the custom reports may be sampled and you can request insampled if you are a premium user.',\n",
       " 'SEO-wise, no. Speed is a very small factor in ranking so unless your current load time is 20s+ it won\\'t make a difference. \\nFor your visitors however, it is a worthwhile goal. Slow load times may cause users to go elsewhere.\\nWith regard to your specific method, it will increase speed, but only if you do it in a non-blocking way, e.g. load the images after window.onload. That way the whole page can load and be usable while images load. Make sure to specify the width/height of the images so that you don\\'t get \"repaints\" (janky changes of layout) as the images load.',\n",
       " \"To begin, please consider the terms influencer marketing are marketing terms and not SEO terms.\\nWhat you are seeing is not necessarily from the knowledge graph. It is a featured snippet. More on that at the end. I will explain what is required for the knowledge graph then explain the featured snippet and how they are related.\\nThere are several things that can help to populate the knowledge graph.\\nThe first thing you have to know is that no knowledge graph card will be created without being vetted directly or having data taken from a highly trusted source. The second thing to know is that all data not directly vetted within the knowledge graph is checked for validity using methods of comparing content across many sites. Using a fact link semantic database, the knowledge graph will not incorporate information from a source that is not vetted without comparing the semantic fact links.\\nFor example, George Washington was the first president of the United States creates, at minimum, George Washington -> President -> United States where the name George Washington is linked to President and President is linked to United States. As well, President can be ordered sequentially through fact links to completely represent the data. While this is an over simplification, it is clear that fact linking creates an opportunity to compare the data to other sources. In that way, if one site makes a claim using a factual statement as determined using semantic analysis, that fact can be compared to other sites with high trust scores making the same claim. Sites with factual statements that are trusted and vetted in this way can also contribute to the knowledge graph using a similar trust score mechanism as the domain trust score.\\nThese are some of the primary factors in creating a knowledge graph card.\\nGoogle+ for business. This is Google's opportunity to vet your business directly. As a result of a Google+ account, any information you provide to Google will be vetted and may be directly used within the knowledge graph to build a card. Google+ is a trusted source that directly populates the knowledge graph.\\nWikipedia A Wikipedia page is vetted by peers and checks made by content editors. It is the original source of trusted information and the first to be incorporated into the knowledge graph. Any Wikipedia page about a business or topic can appear in a knowledge graph card. Wikipedia ranks just slightly lower than Google+ as a trusted source.\\nTelephone directories. While there are a lot of business posting sites on the web, sites with original business listing content from the telecoms are also trusted as a source for the knowledge graph. While these sites rank lower than many sites, this data can also appear within a knowledge graph card. Keep in mind that simply posting a business on a business posting site is not enough. The site has to be trusted as an originator of the content, that is, from the phone company directly.\\nData driven sites. Trusted data driven sites, such as the SEC (securities exchange commission), NASDAQ, DOW, Dunn and Bradstreet, weather sites, etc. can contribute data to the knowledge graph. However, from these sites, no card will be created as a result without one of the elements listed above.\\nThese are only examples. There is more required.\\nYou must also consider that the site must have certain trust elements. For example, valid NAP (name address, phone number) data often using schema.org mark-up. The NAP data must be relatively consistent spanning several sources including domain name registration, business listings, Wikipedia, the site itself, and others. As well, the NAP data must make sense. For example, listing a business address as a P.O. Box is not enough. Google requires a physical location as a walk-up location. The site registration information must be valid. Remember that Google is a registrar and can see the proper contact information for a site. All of this data must be valid and not contain contact information elements that are known to create spam or are deceptive. The site must also have valid About and/or Contact pages with options to contact the company using an e-mail address or form. The site must consist of RCS (real company s**t) which is structure, content, and behavior of a company site. One element of RCS is marketing content and activities optionally including articles, print advertisement, social media, etc. These things and more are required before a card will be created.\\nThis topic is far too huge to get into all the details. Any one of us could write a book on the topic! However, this answer explains what influences the knowledge graph in enough terms to likely create a card.\\nFeatured Snippet\\nA featured snippet will requires some of the above in that the site must have a reasonably significant trust score. This will include NAP as described above, About and/or Contact page, proper registration data, a reasonable link profile, etc. All of the standard trust metrics you would see of a quality site should be in order.\\nA featured snippet comes from the answer engine which is part of the query engine. There are two primary things required for a featured snippet to appear. One is that the search query appear to be asking a question or looking for a fact.\\nOne common example is Italian restaurant in Atlanta Ga. where \\nexamining this query, restaurant in Atlanta Ga is a full semantic representation of the semantic elements required for a fact link; subject, predicate, and object. Restaurant is the object, in is the predicate, and Atlanta Ga is the object. Italian is a modifier of restaurant. It defines further what the user is looking for. The answer engine first solves restaurant in Atlanta Ga, with a fact link, then further solves Italian restaurant in Atlanta Ga by applying the modifier.\\nIn order for a featured snippet to appear, the content must match the semantic query in the knowledge graph. This does not have to be as linear as you may think, though often it is fairly linear. I have seen two forms of featured links, one is a URL and snippet from a site that ranks well in the knowledge graph and would likely be used as a card. The other is a URL and snippet from a site that presents answers or facts. This is far more common.\\nIn the case of a site that ranks well for a card, you can often also see a knowledge graph card for the site or another higher ranking site. Using my example, this would be a restaurant with good reviews within the Google+ profile which may be required.\\nIn the case of a site that provides facts or answers, you will often find sites that are sources for answers or factual information and not a business site as described above. This could be a review site such as yelp.com, an informational site such as a Wikipedia, an answer site such as answers.com, etc.\\nIf you follow the featured snippet link to the page, you will see a content block that represents semantic information that matches the query. Most of the time this is rather linear. It can be as simple as a header tag with Italian restaurant in Atlanta Ga. What is key in this example is in. You will notice that it is the same predicate as the search query and represents a fact link.\\nSome results are less linear. For example, a restaurant name and a location may be enough. For example, Luigi's Italian Restaurant as a header with schema.org mark-up for NAP should work.\\nAs for asking a question in a query and the results, there is a different set of semantic clues that make this happen. The list is long, however, a simple example could be who is george washington. Like the previous example, the query represents a subject, predicate, and object, however, semantically, the use of the term who changes the result of the answer engine from a fact to an answer. Here is an example.\\n\\nYou will see that the result answers the question.\\nThere are other forms of questions such as how to. What the answer engine looks for and the requirements changes again based upon the semantics.\\nIt is all about sending semantic signals to search engines and being a trusted source of information.\\nSo what is the difference between a knowledge graph card and a featured snippet?\\nThe answer is relatively simple. A knowledge graph card offers information from the knowledge graph directly while a featured snippet offers content from the target site itself.\\nAgain, this is a rather large topic, and again, any of us could write a book. However, paying attention to a sites trust metrics and sending the proper semantic signals can make any site appear as a featured snippet. For what it is worth, the bar is set lower for a featured snippet than for a knowledge graph card so it should be easier to get a featured snippet than a knowledge graph card.\",\n",
       " \"Rather simple.... you can't. \\nShared hosting even with SSH enabled does not give the necessary permissions to install new server-side processes because your actions could affect their security or the performance of the server that will ultimate affect everyone on it. The SSH provided to you as apart of cPanel for non-administrators is for administrating of files and folders, e.g:\\n\\nvi access.log viewing logs\\nchmod: changing file and directory permissions\\nwget: downloading remote files\\ntar, unzip decompressing files\\nmkdir creating directories\\nrm removing files or directories\\n\\nIt does not support su root, sudo, yum or install.\\nIf you want to install mod_pagespeed or any other 3rd party module for Apache then you need to upgrade to a dedicated or virtual environment that allows such freedom of control ~ shared hosting is designed for simple hosting and not advanced server-side modification.\",\n",
       " \"What you choose is really up to you, but if you're avoiding Virtual Machines, Azure provides Web Apps (part of the App Service PaaS offering). I have no idea what your subscription supports, but Web Apps allow you to push your code up without concern for underlying OS (thought it supports both Windows and Linux, in case your app has OS-specific dependencies). Simple to scale up to higher/more capable tiers, and out to multiple instances (the features, and scaling, have differences for the various performance tiers). Note that the Linux variant is in preview, though both variants support PHP. You'll be able to push your code through an SCM provider (e.g. github) or via ftp.\\nOutside of Web Apps, you'd have:\\n\\nCloud Services (web/worker stateless Windows Server VMs)\\nVirtual Machines (Windows/Linux with many supported variants / distros)\\nService Fabric (a managed microservice offering)\",\n",
       " 'Working from this: It seems there\\'s a limitation that when you add a link, it must be the same domain as the page.\\nNo. This is not true. You can create a canonical link to any page that is the original regardless of what domain it is on.\\nSo why are you seeing so many blog posts stating something else?\\nThe answers are rather simple.\\n1] SEOs tend to parrot the same thing they read somewhere else. Most SEOs are not technical people and only slicing out a piece of the pie for themselves for fame or revenue. Most fail. Do not believe all that you read even from MOZ which is not on my short list of SEO sites to reference. Sorry MOZ. There are enough factual errors on MOZ.\\n2] Old information which may have been true or thought to be true at one point remains online forever and results in a huge misinformation campaign retained purely for traffic and revenue. MOZ has been called out for providing false information on this site before and to their credit, they have responded positively.\\n3] The original intent of the canonical tag was to suggest that pages on a single site were in effect duplicates or near duplicates. While that is not how the RFC was written, many SEO bloggers parroted the original motion that the canonical tag would primarily refer to the same page with parameters that offer different results. While this is good advice, bloggers rarely stepped out of the rapid blog post development mindset to add value to their posts. It is a me too(!) proposition.\\nIt would help to read the original RFC6596 which makes it clear that a canonical link can indeed point to another domain.\\n\\nThe Canonical Link Relation\\n\\n..The target (canonical) IRI MAY:\\n...Exist on a different hostname or domain.\\n\\nThis is supported here:\\nhttps://webmasters.googleblog.com/2009/02/specify-your-canonical.html\\n\\nCan this link tag be used to suggest a canonical URL on a completely\\n  different domain?\\nUpdate on 12/17/2009: The answer is yes! We now support a cross-domain rel=\"canonical\" link element.\\n\\nThe original adoption of the canonical tag may not have originally considered the fact that duplicate content could exist in different domain names.\\nAs good as Rand Fishkin is, he is completely wrong on this point. At least in today\\'s terms. Here is what he says.\\nhttps://moz.com/blog/canonical-url-tag-the-most-important-advancement-in-seo-practices-since-sitemaps\\n\\nThis is NOT THE CASE with the Canonical URL tag, which operates\\n  exclusively on a single root domain (it will carry over across\\n  subfolders and subdomains).\\n\\nHowever, to properly defend Rand, he was probably following what Google has said and not what the RFC said. I will not dig up a link for this. Rand is good so I will make this an assumption.\\nNow here is the most important advice I can give you.\\nBe extremely careful when seeking SEO advice. Even with Rand, who is one of the best of the best, what is common, still exists on MOZ. Old posts can often be completely wrong based upon current times or even at the time the post was written. You will notice that the dates of the links I provided are all from 2009. This is important! This is far too old for current information. I would suggest reading nothing older than 2013 short of RFCs, research papers, and patents.',\n",
       " 'This is actually outdated. It is now stated that there is no link equity lost through 301 redirects. However, there are still risks linked with changing the URL structure and redirecting. For example, all pages which are redirected to must be relevant (i.e. is it just a URL change redirecting to the previous version of the page or are you redirecting to a new page). Also clearly the URL structure must be SEO friendly and adhere to SEO guidelines. For more info, I recommend this Moz article.',\n",
       " \"You should not redirect your user/Googlebot to your main subdomain when your static assets is not found. Instead you should return 404 error. But because you're redirecting them, hence your subdomain is index by Google, so\\nThere is three solution for you.\\n\\nRedirect only your main static.domain.com page to domain.com, for example facebook redirect their static content cdn domain (fbcdn.net) to facebook.com, so Google will follow the redirection and will drop your subdomain link from search result. \\nUse below robots.txt \\n\\nUser-Agent: Googlebot\\n Allow: .js\\n Allow: .css\\n Allow: .png\\n Allow: .jpg\\n Disallow: /\\n\\nUse meta noindex tag on that webpage. \\n\\nMake sure the noindex tag only placed on those webpages that you don't want to see on search result, I am telling that, because may be you're using some kind of parent template technique which applied to all child templates. \\nAnother thing is that noindex tag don't stop from crawling, it just used to noindex specific webpage, so that page will not going to appear on Google search result, so if you're using any links on that webpage then Google will still follow all the links and pass it's value just like any normal webpage.\",\n",
       " \"Open Search Console and go to Crawl -> 'robots.txt Tester'\\nAt the dropdown for the latest version seen, it will list any times at which it had issues getting the robots.txt or times it was recently updated. Double check that Google isn't having problems reading it or has pulled an old copy. Then enter URL for the mobile homepage into the test area and hit 'TEST'.\\nIt's also worth double checking that the page HTML doesn't contain a 'noindex' meta tag.\\nIf all of that is fine, it's likely a bug you'll want to raise with Google. Homepages tend to be high priority and re-scraped often. Use the 'Help' dropdown top right to check.\",\n",
       " \"Now you've correctly told Google the purpose of the URL parameter in Google Search Console, it will take a while for Google to remove the URL with the parameter from search results. After a couple of weeks, the result should disappear from search.\\nYou can still try Google Search Console's URL removal tool. This may encourage Google to drop the result faster. The tool provides temporary removal of URLs but upon re-crawling the URL parameters settings will take effect.\",\n",
       " 'It is not \"readable\" from the Web\\n\\nWhat you are seeing is the output after the PHP file has been processed. Since you are probably only setting some variables then there is no output, but it is still processed. If PHP should fail, or the file should get an error/corrupted then this could expose the PHP contents.\\nIdeally, you would simply put this file outside/above the document root. That way you don\\'t have to do anything to block it from the public and will still be blocked should anything untoward happen (such as your .htaccess being accidentally deleted!).\\nTo block this with mod_rewrite in .htaccess:\\nRewriteEngine On\\nRewriteRule ^file-to-block\\\\.php$ - [F]\\n\\nThe above should be placed near the top of your .htaccess file.\\nNote, however, that if you include another .htaccess file in a subdirectory, that also uses mod_rewrite, then this directive could be overridden.\\n\\nHowever, it would be preferable to block (403) these files with mod_authz_host (Apache 2.2) or mod_authz_core (Apache 2.4). For Apache 2.2 see Simon\\'s answer.\\nOn Apache 2.4, using mod_authz_core:\\n<Files \"file-to-block.php\">\\n    Require all denied\\n</Files>\\n\\nAlternatively, to send a 404 Not Found instead of a 403 Forbidden, then you can modify the above mod_rewrite directive:\\nRewriteRule ^file-to-block\\\\.php$ - [R=404,L]',\n",
       " \"After attempting what lunametrics did to combine their URIs with & without ending slashes, and not getting it to work, I started to think about the problem in a different way. Instead of appending slashes when they are missing, why not remove them if they are there? i.e. example.com/path/ would then become example.com/path in my reports.\\nNote: This is probably only a good idea if you also have the other URI rewriting filter I use, that adds the hostname to your reports. Otherwise views to your index page, which would normally be reported as / would probably show as a blank line. I'm not actually sure what would happen... But this is a solution for me that answers my problem: how to combine related rows in my reports.\\nHere is my filter that I have working ($ denotes the end of the string and the preceding / is a single character match which then gets 'replaced' with nothing):\",\n",
       " 'As @Tgr mentioned in the comments, the problem was that I was missing <?php at the top of LocalSettings.php.',\n",
       " 'I\\'m using this query inurl:\"m=\" \"site:mydomain.com\" to detect those posts with m=0 and m=1.\\n\\nIt would seem that what are seeing is simply the results of a site: search. Using the site: operator is not a \"normal\" Google search and has been shown to return non-canonical (including redirected) URLs in the SERPs. These are URLs that don\\'t ordinarily get returned in a \"normal\" organic search (when no search operators are used). Even URLs that are the source of 301 redirects have been shown to be returned for a site: search, when they are not returned normally. These non-canonical URLs are still crawled (and processed) by Google and they are often acknowledged in a site: search.\\nReference:\\n\\nHow to submit shortened URLs to Google so that they are included in the index\\nRelated question: Google indexing duplicate content despite a canonical tag pointing to an extarnal URL. Am I risking a penalty from Google?\\n\\nNormally, a rel=\"canonical\" (which you have already done) is sufficient to resolve such conflicts with query parameters and duplicate content. But note that it doesn\\'t necessarily prevent the non-canonical pages from being indexed (which you see when doing a site: search), but from being returned in a \"normal\" Google search.\\n\\nblocked m=0 and m=1 on robots.txt ....\\n\\nYou probably don\\'t want to block these URLs from being crawled as it could damage your ranking on mobile search.\\n\\nBTW what about Disallow: /.html, Allow: /.html$\\n\\nAside: This looks \"dangerous\". Google doesn\\'t process the robots.txt directives in top-down order. They are processed in order of specificity (length of URL), but when it comes to the use of wildcards, the order is officially \"undefined\" (which also means it could even change). The Allow: directive is also an extension to the \"standard\" and might not be supported by all search engines. It would be better to be more explicit. eg. Disallow: /*?m=. But, as mentioned, you probably should not be blocking these URLs in robots.txt anyway.\\nSee also my answer to this question for more info about robots.txt and how it is processed:\\n\\nRobots.txt with only Disallow and Allow directives is not preventing crawling of disallowed resources',\n",
       " 'In your robots.txt add\\n#Baiduspider\\nUser-agent: Baiduspider\\nDisallow: /\\n\\n#Yandex\\nUser-agent: Yandex\\nDisallow: /',\n",
       " \"First of don't block anything(http website) via anything(robots.txt or meta tags).\\nUse 301 redirection from http://domain.com to https://domain.com (Feel free to use www subdomain if you want). So both your user and googlebot will redirect to your https version, and Google will index the https version of webpage and will drop http version of that page from search result, the ranking will not change a more(Slietly it does initially, but you will get back in few months). \\nUse canonical link tag on http webpages and make sure it point to https version of website. \\n\\nMigration from http to https is really easy, just don't block anything. Google already trying to access your https version of website when it is available, so if you provide same content on both protocol(http and https), then Google will pick only https version and will handle duplicate content it self, but setting proper 301 redirecion and canonical link tag really help to get your ranking position back quickly. \\nIf you're site is really big then do 301 redirection in few section of website, and see your search console to get analytics data, but if it is small than full migration will be good choice. \\nFore more reference read this epic guide from John Muller.\",\n",
       " \"Despite being (as of today) way too short for a subject like this, the Wikipedia article about Microdata (HTML5) still puts the relationship between the three common semantic markup approaches nicely:\\n\\nMicrodata can be viewed as an extension of the existing microformat idea which attempts to address the deficiencies of microformats without the complexity of systems such as RDFa.\\n\\nSo from a distance (i.e. ignoring technical differences) I'd say this pretty much is all the story indeed (for Microdata vs. Microformats - RDFa is another subject matter), consequently your main concern should be the target 'audience', i.e. whether your format of choice is supported by the clients you expect to consume your semantically enriched content - if Google is the usual benchmark here, it appears your are free to go with the upcoming HTML5 standard.\",\n",
       " \"It's called cloaking, where the spammer shows one URL to Facebook advertising team and another to the real user. Generally, those spammers use third party service for cloaking which may charges more then we believe, also it is invite based so Facebook/Google ads team don't know how they really do. They use many secret things to do cloaking but here is few of list that I know so far.\\n\\nWhen the referral are from dev.facebook.com or ads.facebook.com or similar directory (On which FB ads team review ad) they show different URL.\\nThey know IP address of facebook advertising team from which they will review ad.\\nThey check GEO IP from which the ad will be reviewed, for example facebook advertising team review specific ad from particular geo/city, but if they review same URL from other geo then they will get different landing page or destination webpage.\\n\\nThe solution is, simply report these kind of ads, and they will shut it down, in short time.\",\n",
       " \"The best practice for CSS optimization and, specially, for removing render-blocking CSS from the head area of the HTML document is not done with one step.\\nWell, you are indeed able to remove the whole CSS to the bottom of the page and defer it (load it after the onLoad event). If it would works anywhere - it would be the whole job.\\nBut under some circumstances (in some browsers / with some user agents) user will get FOUC - flash of unstyled content.\\nTo avoid fouc, you should:\\n\\nget to know, which style rules are needed for displaying of your above-the-fold area,\\ninclude these style rules into the head of your HTML document, using the <style>-tag.\\n\\nTo get these style rules to know there are many different approaches - all of them aren't trivial. Many of those tools are listed on a github page of a googler. A simple one is here to find.\\nOn the similar way should be inlined into the head not only style rules, but images and fonts too - like base64 data URIs.\\nFinally your HTML document will grow - but if browser gets really all needed information for displaying the above-the-fold area through only reading of the single one HTML document (no additional file requests, TCP connections, handshakes, all that jazz), your optimization job is to 98,5% done.\",\n",
       " \"Use OpenSSL's speed command to benchmark the two types and compare results. Here's an example command to run on the server to compare only the key types and sizes you mention:\\nopenssl speed rsa2048 rsa4096\\nFor reference, here are some benchmark results from a modest VPS:\\n                  sign    verify    sign/s verify/s\\nrsa 2048 bits 0.000685s 0.000032s   1459.1  31629.7\\nrsa 4096 bits 0.007574s 0.000113s    132.0   8851.0\\n\\nAs you can see, doubling the certificate key size places an enormous additional burden on the server's CPU and is many times slower. Avoid 4096 bit keys unless you have a specific threat model which requires their use.\",\n",
       " \"That report shows how much time Googlebot spends downloading each page on your site, not including linked resources such as JS, CSS, iframes, and images. \\nIt is less important to optimize your website for Googlebot than it is to optimize it for users.   Instead of looking at this report, start by looking at how long it takes to request, download all resources, and render your pages.  Developer tools in Firefox and Chrome can show detailed breakdowns of each stage like this:\\n\\nAs long as the time to download the page is under 3 second (3000 ms) you are fine from an SEO perspective.  Google starts penalizing sites that take 7 or more seconds.   Improvements under 3 seconds can be great for users, but don't typically get you better search engine rankings.  \\nThe techniques for improving total performance for users can include putting JS and CSS inline in the page.    That can slow your initial page load time down for Googlebot, but it is important to optimize for users first.  For a better list of how to optimize your site for users see: Ideas to improve website loading speed?\\nBack to your question about the time that Googlebot spends downloading the initial page.   When making optimizations, it is possible to make optimizations that are better for Googlebot but bad for users.   For example, you could try moving CSS and JS into files rather than inline.   Then Googlebot wouldn't have to download them every page.   Resist the temptation to optimize for Googlebot at the expense of users.\\nThe initial page download that Googlebot sees is also what users start with.   Most optimizations done for Googlebot in this area will also improve user experience.  700ms is not great.   If it takes almost a second for the initial page load, you can easily see how the full page with images could take many seconds to render.  I would try to aim for no more than 300ms.   Your browser tools will give you some insight into where to look.   If you see that it is 400ms between when the request is made and when the server responds, that usually indicates that the server is sitting and thinking for a long time.  You can remedy that by:\\n\\nGetting a faster server\\nMoving to static pages rather than dynamic\\nImplementing caching\\nImproving database performance\\nOptimizing database queries\\nReducing the number of database queries\\n\\nIf you are using a CMS such as WordPress, caching plugins can really help.\\nWhen you improve the performance for Googlebot, you will find that Googlebot is able to crawl your site more frequently and more deeply.   The speed at which your servers can feed pages to Googlebot is often the limiting factor in how much Googlebot is willing to crawl your site.\\nA really fast site will take 50ms or so per page.   I have been able to optimize one of my sites to close to that:\",\n",
       " \"The error, This url is not allowed for a Sitemap at this location. is actually rather self explanatory. It has nothing to do with underscores (_) which are perfectly valid.\\nSimply put, you cannot refer to another domain name within your sitemap. A sitemap must refer to the domain name on which it resides. The purpose for a sitemap is to inform a crawler of your site's URLs. In your example, it appears you are referring to BuzzFeed.com.\\nFrom: https://en.wikipedia.org/wiki/Sitemaps\\n\\nThe Sitemaps protocol allows a webmaster to inform search engines\\n  about URLs on a website that are available for crawling. A Sitemap\\n  is an XML file that lists the URLs for a site. It allows\\n  webmasters to include additional information about each URL: when it\\n  was last updated, how often it changes, and how important it is in\\n  relation to other URLs in the site. This allows search engines to\\n  crawl the site more intelligently. Sitemaps are a URL inclusion\\n  protocol and complement robots.txt, a URL exclusion protocol.\\n\\nFrom: https://www.sitemaps.org/index.html\\n\\nSitemaps are an easy way for webmasters to inform search engines\\n  about pages on their sites that are available for crawling. In its\\n  simplest form, a Sitemap is an XML file that lists URLs for a site\\n  along with additional metadata about each URL (when it was last\\n  updated, how often it usually changes, and how important it is,\\n  relative to other URLs in the site) so that search engines can more\\n  intelligently crawl the site.\\n\\nFrom: https://stackoverflow.com/questions/1702004/why-wont-google-accept-my-sitemap-xml-url-not-allowed-this-url-is-not-allowed\\n\\nThat error usually means that you have an URL pointing to a different\\n  Domain from yours.\\n\\nAnother typical reason for the error is when a relative URL is offered instead of an absolute URL. The URL must include the protocol HTTP or HTTPS, the domain name of your site, and the URI for the page. The URL must be fully valid including a trailing slash where the web server does not provide one. It is likely better to always provide a trailing slash.\\nWith sitemaps.org, Google, Wikipedia, there seems to be no reference to the requirement that the URL location refer to the site on which the sitemap resides. However, the purpose of the sitemap makes it clear that the sitemap is to aide a crawler to access pages within the site for which the sitemap is created for and that the sitemap for the domain must reside on the domain preferably/traditionally within the root though not required.\",\n",
       " 'The links shown in the \"Links to your site\" report in Google Search Console (formerly Google Webmaster Tools) shows both dofollow and nofollow links (but it\\'s not possible to distinguish between them).\\nFrom the following Google+ thread by John Mueller (Google employee and user on this site):\\n\\nYehoshua Coren (30 Jul 2012)\\n  +John Mueller I\\'m curious why the links that GWT shows include nofollow links.  Your thoughts?  Does it make sense to note that somehow?\\nJohn Mueller\\n  +Yehoshua Coren we show them there because to users, those links can be quite useful regardless. For example, if you have an advertisement on a website, that link - even with a rel=nofollow - can be an important source of users for your website. \\n\\nAlso, in this Google+ thread regarding Disavowed links:\\n\\nJohn Mueller (3 Jul 2014)\\n  Disavowed links are generally treated similarly to nofollow links, and we show those in Webmaster Tools as well (since nofollow links can still be useful to bring traffic to your website).',\n",
       " 'Google’s SDTT is intended for checking if the structured data meets the recommendations/requirements for Google’s search features (like rich results). These search features make use of the vocabulary Schema.org.\\nThe type http://xmlns.com/foaf/0.1/Image is from the vocabulary FOAF. This vocabulary is not used by Google for their search features.\\nSo as far as the SDTT is concerned, you are using a property from a known vocabulary (schema:image) with a type from an unknown vocabulary (foaf:Image). Note that it’s perfectly fine and useful to mix vocabularies, and it should be no problem to keep the unrecognized types and properties in your RDFa (although the SDTT has several related bugs, as least when displaying the output).\\nIf you want one of Google’s search features, you have to use the value which Google expects. For the schema:image property, that’s likely the schema:ImageObject type.',\n",
       " \"Create the remote connection\\nTo create a remote connection:\\n\\nOn your database server, as a user with root privileges, open your MySQL configuration file.\\nTo locate it, enter the following command:\\nmysql --help\\nThe location displays similar to the following:\\nDefault options are read from the following files in the given order:\\n/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf\\nSearch the configuration file for bind-address.\\nIf it exists, change the value as follows.\\nIf it doesn’t exist, add it anywhere except the [mysqld] section.\\nbind-address = 192.168.19.33\\nSave your changes to the configuration file and exit the text editor.\\nRestart the MySQL service: service mysql restart\\n\\nGrant access to a database user\\nTo enable your web node to connect to the database server, you must grant a web node database user access to the database on the remote server.\\nThis example grants the root database user full access to the database on the remote host.\\nTo grant access to a database user:\\n\\nLog in to the database server.\\nConnect to the MySQL database as the root user.\\nEnter the following command:\\nGRANT ALL ON website.* TO username@192.168.19.33 IDENTIFIED BY 'password';\\n\\nVerify database access\\nOn your web node host, enter the following command to verify the connection works:\\nmysql -u username -h 192.168.19.41 -p\\nIf the MySQL monitor displays as follows, the database is ready for use by the website:\\nWelcome to the MySQL monitor.  Commands end with ; or \\\\g.\\nYour MySQL connection id is 213\\nServer version: 5.6.26 MySQL Community Server (GPL)\\n\\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\\n\\nOracle is a registered trademark of Oracle Corporation and/or its\\naffiliates. Other names may be trademarks of their respective\\nowners.\\n\\nType 'help;' or '\\\\h' for help. Type '\\\\c' to clear the current input statement.\\n\\nSource: http://devdocs.magento.com/guides/v2.0/install-gde/prereq/mysql_remote.html\",\n",
       " \"Search Engine Watch recently posted a good article which suggested that engagement metrics (such as CTR and, yes, avg time on page) may have been correlated to rankings. (I believe that Rand at Moz has also suggested this for a few years.) \\nhttps://searchenginewatch.com/2016/12/13/6-seo-experiments-that-will-blow-your-mind/\\n\\n4. Do Website Engagement Rates Impact Organic Search Rankings?\\nIt’s super important to create clickable headlines, but the goal isn’t just to create clickbait. You also must have great engagement metrics. If people feel cheated and go right back to the SERP, Google can detect that.\\nDwell time is really the thing that matters. And time on site is a much better proxy for dwell time than bounce rate.\\nMy theory is that Google uses dwell time (which we can’t measure, but is proportional to time on site) to validate click-through rates. These metrics help Google figure out whether users ultimately got what they were looking for (i.e., a successful search).\\n\\nWhile I can't validate this from my own in depth studies, this does make sense to me from the perspective that Google favors content that is useful. SEW's theory about this correlation seems to be at least reasonable. \\nAs Stephen pointed out before more, Google says they don't use Analytics data for ranking factors. However, I don't think that necessarily excludes metric data that is relevant and similar.\\nWith that said, I would not expect that writing longer posts would be in your benefit. If Site A answered a query with excellent content in 500 words, and Site B offered a decent content with 1000 words, it would not make sense to favor Site B in the SERP.\",\n",
       " \"As always, it depends. You give your users pretty much permissions - this causes responsibility in form of constant surveillance.\\nI see some causes to dispense with user generated content in the form you describe. The causes are i. e.:\\n\\nUser could upload images containing malware,\\nUser could upload child porn,\\nUser could upload images causing DMCA to your site,\\nUser could add tags not matching content,\\nUser could add redundant tags, as duplicate of existing tags, just in different spelling,\\n\\nEven if everything goes OK, you'll be forced to set all of your tag pages as noindex, because such pages would display same images in slightly different sorting and generate nearly duplicated content.\\nThe optimal SEO-minded way would be imho:\\n\\ninstead of tags use categories, where images would be displayed in unique order,\\n!!! don't publish images uploaded by users automatically !!!. Firstly look at images, check, whether images and image names given by users are OK. Then apply meaningful alt tags, sort images into correct category and publish.\",\n",
       " 'Anyone can register .uk domain names on a first-come, first-served basis, but an address for service in the United Kingdom will be\\n  required for registrants who live overseas.     This address must be\\n  listed as the admin contact for your domain name.     This\\n  requirement does not apply for .co.uk and .org.uk domain names. Source\\n\\nSo you only need a UK address for the admin contact for .UK domains.  I have been working with domains for nearly 20 years and I have yet to have anyone attempt to contact me via the Admin, Billing or Tech contacts.  \\nGiven that forwarding services charge you on a usage basis, if you set up an address with one of them, it is unlikely to ever be used and if it is, then it must be really important and worth paying a few (insert currency unit here) for.',\n",
       " 'You can not use change of address tool to migrate http to https version of website.\\nHere is official reference webpage, says.\\n\\nThe tool does not currently support the following kinds of site moves:\\n  subdomain name changes, protocol changes (from HTTP to HTTPS), or\\n  path-only changes.\\n\\nThis post from john mueller might help you in future.',\n",
       " 'Using the user\\'s email address as the \\'from\\' address in a form notification email is not a good idea because it will trigger many spam filters due to a \"spoofed\" sender.  There are two systems which help detect spoofing:  SPF and DKIM.  SPF allows the email domain owner to specify which IP addresses and/or domains are valid senders for that domain. If the email message originates from an IP address not listed in the SPF record, email systems will (should) flag it as possible spam.\\nAlso, DKIM is regarded as important by many email servers (Gmail et al) and is used to detect spoofed email addresses by adding a signature header which can be validated against a DNS record owned by the originating domain.\\nRecommendation: only send mail \"from\" email addresses that you control, and for which you have properly configured SPF and DKIM records.  This will help ensure recipient servers don\\'t classify the messages as spam.\\nGood luck!',\n",
       " \"Referring to the @CP3O's Answer, Here's the sequence you should follow:\\n\\nMake the HTTPs website live to the domain.\\nRedirect HTTP robots.txt to HTTPS robots.txt.\\nPlace a new HTTPS sitemap in the robots.txt, and remove the HTTP sitemap URL from robots.txt.\\nMake sure that the old HTTP sitemap was intact, and made sure it was still existing in our HTTP property in the Search Console.\\nChange all canonical URLs to HTTPS.\\nMake sure all 301 redirects were in order and a one to one mapping was intact for all HTTP -> HTTPS.\\nSubmit a New HTTPS Property in Search Console\\nSubmit the new HTTPS sitemap in the HTTPS property in the Search Console.\\nMake sure the HTTPS sitemap was the only sitemap being updated with new URLs. (Stopped adding new URLs to the HTTP sitemap after the redirects.)\",\n",
       " 'Nope - what you\\'re seeing is the result of an out of date index.\\nThings to notice:\\n\\nThe dates in the results. Notice that they\\'re from September.\\nOne of the results clearly displays the barcode in the description. In other words, at some point the website did provide it to Google\\'s spider.\\nA direct search for \"Faolan O\\'Connor Drawing Dead\" brings up 2k+ results. A search for the barcode only gives those 4. If Google had related them, you probably would\\'ve seen at least some portion of those 2k results.\\n\\nCan I see what Google did?\\nYes - click the little green arrow then click on \"Cached\":\\n\\nThen click the \\'view source\\' button and you\\'ll see that in both instances, the barcode is discoverable somewhere on the page itself:\\n\\nIn one result the barcode is visible directly on the page; in the [PDF] result, the barcode can only be seen in the source.',\n",
       " 'As noted in comments, try with \\nscp -v -r -i node.pem /Users/path/to/file/gametest ubuntu@35.164.71.227:\\n\\nso the gametest directory is created in the server.\\n\\nthe folder called gametest is highlighted in green. What does that mean?\\n\\nThe green color depends on your environment configuration, it can mean that it is a directory, or specify its permissions, like if it writable or readable.',\n",
       " 'Assuming your \"other websites\" are using different domains/hosts then you can check the HTTP_HOST before redirecting. The same as you would do for a canonical www redirect.\\nHowever, this does require the use of mod_rewrite. Assuming example.com is your main domain, then you can do something like the following:\\nRewriteEngine On\\nRewriteCond %{HTTP_HOST} ^(www\\\\.)?example\\\\.com [NC]\\nRewriteRule ^index\\\\.html$ / [R=301,L]\\n\\nWhat this does is... for all requests that match /index.html and the host matches example.com (or www.example.com) then 301 redirect to example.com/ (or www.example.com/).\\nNote that this simply redirects to / and allows mod_dir to fetch the required DirectoryIndex (ie. index.php). If you really want to be explicit and redirect to index.php (not recommended, unless that is what you are currently doing) then include the full substitution /index.php.\\nThe (www\\\\.)? (making www optional) would be unnecessary if you have a canonical redirect in place.\\nIf you are currently using Redirect (or RedirectMatch), part of mod_alias, then you would ideally convert these to mod_rewrite as well. Otherwise, be careful of any conflicts. mod_alias will run after mod_rewrite, despite the apparent order of the directives in .htaccess.\\n\\nMy obvious first thought was to use two absolute URLs, but apparently that\\'s a no-no?\\n\\nIt\\'s only a \"no-no\" in so far as the relevant Apache directives don\\'t allow you to specify an absolute URL as the source. They expect a URL-path only. The host part of the source URL is separated out. However, an absolute target URL (or substitution in mod_rewrite terminology) is perfectly OK.\\n\\n...other websites (separate URLs) which are running from subfolders\\n\\nRunning separate websites in subdirectories off the main domain is not ideal, unless they are closely related. Because .htaccess files are inherited along the filesystem path you can have conflicts.',\n",
       " \"Yes, avoid Schema for now and use Data Vocabulary, for exactly the reason you cite. I've used the latter, and it works.\\nJohn Mueller from Google has said that he expects Schema will have to change, and the discussion around it seems to suggest that Schema needs to be more like the current Data Vocabulary syntax, so any future adaptations you need to make may be less onerous than they currently appear.\",\n",
       " 'The closest thing I know of is creating a \"shortcut\" link which saves the segment that was applied.\\n\\nShortcuts\\nUsing shortcuts provides fast access to your most commonly viewed reports straight from the Home tab. When you save your report as a shortcut, all of the report configurations will be stored as well. This can include applied filters, advanced segments, dimensions, and more.\\nTo begin using shortcuts, click the \"Shortcut\" button on the toolbar of the report you wish to save. Learn more\\n\\nWhen I create a shortcut with a segment applied, I get a quick link to that report with that segment at the top of the left hand navigation.   It doesn\\'t seem to apply that segment to all reports from that point, however.\\nIt does not appear that adding a shortcut changes anything in the URL. Because of that, creating a bookmark with a segment does not appear to be a possibility.',\n",
       " 'You\\'ll need to submit the XML sitemap in Google Search Console (formerly Google Webmaster Tools) in order to verify this.\\nIt\\'s quite possible that Google is already picking up your sitemap, having seen the reference in robots.txt. (Since Google does support the Sitemap directive in robots.txt) You should be able to check your access logs to determine whether Googlebot has requested it. However, that doesn\\'t tell you whether Google found any problems with it.\\nI tried this some time ago myself. With the XML sitemap referenced only in robots.txt, even with the sitemap in its \"default\" location ie. example.com/sitemap.xml, there is no acknowledgement from Google unless you explicitly add it to GSC.\\nThe Sitemap: directive in robots.txt is supported by all major search engines, not just Google.\\nIt\\'s not necessary to submit the sitemap in GSC for Google to pick it up, but it is necessary if you want to report on and diagnose it.\\n\\nThe reason for this is, that I use robots.txt to reference it, since the the sitemap is hosted on S3.\\n\\nHowever, whilst the Sitemap URL can be on a different host to the robots.txt file, if the URLs in the sitemap are referencing a different URL-path to where the sitemap is located then this could be a problem. From Sitemaps.org:\\n\\n...all URLs listed in the Sitemap must use the same protocol ... and reside on the same host as the Sitemap. For instance, if the Sitemap is located at http://www.example.com/sitemap.xml, it can\\'t include URLs from http://subdomain.example.com.\\n\\nYou might be able to get around this with Google by registering all properties in GSC, but can you verify S3? And this won\\'t work with other search engines.',\n",
       " \"Is it bad practice to force html to be read as php? Are there possible SEO repercussions for doing so?\\n\\nSearch engines don't know nor care about how your pages are generated. They only see the output the request URL provides (in other words the output of your PHP file). \\n\\nShould I forgo file extensions altogether and move into a folder structure? ( .com/players/ instead of .com/players.html)\\n\\nCool URIs don't change. It doesn't matter if you have a file extension or not, and if you do use one, it doesn't change when you change technologies. So choose whatever you think will be easiest for you to manage and stick with it.\",\n",
       " 'Google and other search engines treat sub domains exactly the same as a stand alone domain, therefore you will not benefit from the move. If you want to benefit more from the blog then hosting it within a sub folder will be more beneficial assuming that the content is high quality and on topic with the rest of the site.\\nThis is because the blog will rank higher due to the authority of the main domain and the links generated from the blog will benefit the entire site pushing both indirect and direct leads.',\n",
       " \"How long ago did you do this? DNS names can take up to 24 hours to update, maybe more. \\n\\nMy second best guess would be that excel.net is purposely doing this, you'd be surprised by what ISP's are allowed to do in terms of editing traffic.\\nThe third option would be that there's an error on excel.net's side. They could be improperly updating DNS, or simply have a bug.\\n-\\nIn any case, if it's been more than 48 hours I would contact excel and see if they know anything.\",\n",
       " \"Use noindex to keep pages out of Google’s index\\nThe only correct way to keep results out of Google’s index is to use noindex.\\nAt the risk of being pendantic, Google’s (or any search engine’s) search results are composed of items that have been indexed. Googlebot honors a couple of ways to instruct it to omit a page from its index. If you don’t use these methods, don’t be surpised if your page ends up in the search results.\\nSo the short answer is yes, use noindex to keep things out of the index. Or better yet, use the X-Robots-Tag HTTP header (see below).\\nDon’t use robots.txt for this\\nrobots.txt prevents pages from being spidered which is a related, but distinct, concept to indexing. Many non-spidered pages that have strong backlinks can and do rank well in the Google search results. \\nYou may have seen some, they look like the example at the bottom of this Moz.com article.\\nGoogle explains:\\n\\nrobots.txt Disallow does not guarantee that a page will not appear in results: Google may still decide, based on external information such as incoming links, that it is relevant. If you wish to explicitly block a page from being indexed, you should instead use the noindex robots meta tag or X-Robots-Tag HTTP header. In this case, you should not disallow the page in robots.txt, because the page must be crawled in order for the tag to be seen and obeyed.\\n\\nCanonical URL’s don’t exclude anything from Google’s index\\nCanonical URL’s tell Google that the referring and referred pages represent the same content, for “consolidating link signals for the duplicate or similar content” — that is, they help with SEO. \\nBut to really drive traffic from one particular page, Google suggests:\\n\\nIt's a good idea to pick one of those URLs as your preferred (canonical) destination, and use 301 redirects to send traffic from the other URLs to your preferred URL. A server-side 301 redirect is the best way to ensure that users and search engines are directed to the correct page. The 301 status code means that a page has permanently moved to a new location.\\n\\nBut this 301 solution won’t help you, because you need users to be able to see the dev. site.\\nA note on canonical and alternative URLs\\nNote, it is perfectly reasonable for Google to send traffic to non-canonical URLs — different presentations of the same content can be appropriate in different contexts. Consider content you share at both at your regular “www.” site and a mobile “m.” site that is highly optimized for phones. Another example, Google might present a non-canonical PDF version if the user included “PDF” in their search phrase.\\nBut why does Google like your “dev.” site anyway?\\nGoogle’s algorythm doesn’t care that your dev site might have unapproved content, and your users probably don’t either. (It also doesn’t much care how you or your bosses feel about this.) \\nHere are a few things Google does care about:\\n\\nGoogle rewards freshness of content. If you dev site changes\\nmuch more often (it does, doesn’t it?) that may be a positive SEO\\nsignal. \\nPeople on the web might have discovered your dev site and be linking\\n    to it for one reason or another.\\nIf your dev site has significant technical upgrades, or gets less\\ntraffic than your production site, it might be faster — and Google  rewards speed.\\n\\nWhy an HTTP header solution would be better for you than a meta tag\\nIf you use the X-Robots HTTP tag to return the noindex instruction, that can be configured on the web server, not on your HTML files or other content files. So you won’t need to change anything when you promote the files to your production site.\",\n",
       " 'You should use the php method. PHP is processed on the server, so the full page layout is put together before it is sent to the browser. JavaScript is run in the browser once the rest of the page has loaded.\\nSo with your JavaScript method, there is a chance that the page will be shown to the user before the header has been inserted.',\n",
       " \"It's not common for a payment to be 'pending' for an extended period of time. I've not heard of the 'foreign' payment issue - but it doesn't matter what country a persons PayPal account is from, transactions should complete immediately unless there is an issue with the funding on their end.\\nCommon issues with 'funding' can be:\\n\\nCustomer uses a suspicious card.\\nPayPal is receiving slow payment form like a bank transaction (5-7 days to clear)\\nThere are details missing from the transaction (unconfirmed email, missing address, etc.)\\n\\nWait until the payment is confirmed. If it's not, the customer is almost certainly at fault. Make sure you have a system set up to handle some of the common issues such as missing details or suspicious cards.\\nYour main problem with subscriber content and PayPal will be charge backs. PayPal enforces these aggressively from the customer end. We keep logs of all actions and have still lost the majority of charge back requests due to the nature of our business - leaving us eating the transaction charge and having to cancel their order. I suspect you'll have similar issues fighting charge backs, take that into consideration and accept that you're going to get scammed a certain amount.\",\n",
       " 'Redirecting with UTM parameters is a good way of doing it.   See Can I track referral traffic in Google Analytics from a domain that is redirecting to my site?.\\nHere is a rewrite rule that should do the redirection and add the needed UTM parameters:\\nRewriteCond \"%{HTTP_HOST}\"   \"example\\\\.cc$\" [NC,OR]\\nRewriteCond \"%{HTTP_HOST}\"   \"example\\\\.biz$\" [NC,OR]\\nRewriteCond \"%{HTTP_HOST}\"   \"do-main\\\\.example\\\\.com$\" [NC]\\nRewriteRule \"^/?(.*)\"        \"http://www.example.com/$1?utm_campaign=domainnames&utm_medium=domain&utm_source=%{HTTP_HOST}\" [L,R=301,QSA]\\n\\nIf you are on IIS, we had somebody ask about how to implement it there: Track Google Analytics from a redirected domain using IIS\\nIf you want to have Apache log them you can do so as well.  I recommend turning on logging of the host name in Apache logs.  I use the following log directives to append it to the end of the log file:\\n# Log format: combined logs with virtual host\\nLogFormat \"%h %l %u %t \\\\\"%r\\\\\" %>s %b \\\\\"%{Referer}i\\\\\" \\\\\"%{User-Agent}i\\\\\" %v\" combinedserver\\nCustomLog /var/log/apache2/access_log combinedserver',\n",
       " 'What your SEO agency is doing won\\'t make a lick of difference for SEO.  Adding heading tags was an SEO strategy that maybe worked five years ago.   Today Google uses signals other than heading markup to know what text is important an the page.  Googlebot actually renders pages.   I\\'ve seen SEO testing that indicates that Googlebot now views any text that is big, bold, and prominent as important.  It doesn\\'t matter if your site even has heading tags.   Styling a bit of text with CSS will have the same effect.\\nOn the other hand, added heading tags won\\'t hurt in any way.   Google has never paid any attention to \"proper use of headings\" as a ranking signal.   As far as Google is concerned, there is no requirement that headings have content.  There is no requirement that headings be in the proper order.   There is no requirement that headings convey any sort of semantic meaning.    Your rankings will not change in the slightest having these tags added.\\nIt also turns out that users don\\'t care about the tags you use.   They only care about what they see on the page.   It just isn\\'t worth getting upset about this.   Maybe other than that the SEO agency should be doing something more productive and not wasting time.',\n",
       " 'The right way to handle this situation is to have two index.html files, one for each language. \\nThen you will need to fill the title html tag and meta description tag in English and in the other page the French version.\\nAfter that you should specify that each page has the same content but in a different language using the special hreflang in both pages something like:\\n<link rel=\"alternate\" hreflang=\"en\" href=\"http://mywebsite.com/\" />\\n<link rel=\"alternate\" hreflang=\"fr\" href=\"http://mywebsitr.com/fr\" />\\n\\nThen when people google Céline, Google will decide the best page version to show up regarding user context, its language or google version used, etc.',\n",
       " 'Issues with the publish button normally occur when there is an issue with a plugin or theme you are using.\\nYou will need to:\\n\\nDisable all plugins and see if the publish button appears. If so, reactive one by one until you have found the one causing the issue.\\nDisable any custom theme and choose one that comes with WordPress. If so, you need to find out why the theme is causing you headaches.\\n\\nMore possible solutions:\\n\\nAdd a new administrator and login to that account then change ownership of the article and see if the publish button appears. If so then there is a problem with the user profile being damaged within the SQL.\\nTry a different browser and clear the session and cookies.\\n\\nWordPress Stack:\\nIf the problem is related to theme development, plugin development or anything else to do with WP programming codex then you will need to use the WordPress stack as programming is considered off-topic on Pro Webmasters. For example: Posts stuck as Drafts.',\n",
       " 'Note that if you have a dedicated server with it\\'s own IP address then it\\'s possible that the other domain is simply pointing to your IP address, in which case, see the additional note at the end of this answer.\\n\\nI can see numerous requests by the clone site for images in the apache2/access.log file.\\n\\nIt sounds like this other site is acting as a reverse proxy and serving your content directly. They haven\\'t necessarily \"copied\" anything, in terms of literally finding/saving your images/content to another server.\\n\\nincluding links to images on the webserver that are not linked to at all anywhere on the site.\\n\\nThis is indicative of the site proxying the request. You make a request to this fake website and they literally just forward the request to your site, receive the response and send this back to the client/user.\\n\\nit will update live everytime that I update my site...\\n\\nYes, again indicates a proxy. (As you suggest in comments, \"simply forwarding the page through another server\").\\n\\nis this a security concern?\\n\\nWell, it\\'s unlikely that your site has been compromised. The main purpose of such an \"attack\" would seem to be to damage your SEO / search ranking. Or to build up \"credibility\" for the domain responsible, to ultimately sell it.\\n\\nI\\'m thinking I should block the cloners IP address?\\n\\nYes. Unfortunately, this is probably the only thing you can do. It is rather difficult to safeguard against such \"attacks\". The request coming from the proxy might look like a \"normal user\". As such, most sites, are probably vulnerable to such \"attacks\".\\nUnfortunately, the hacker might have multiple IP addresses available at their disposal, so simply blocking a single IP address might not be enough.\\nYou could also examine the HTTP request headers that are coming from these \"proxied\" requests. A \"good\" proxy will set X-Forwarded-For and Proto- request headers, etc. If so, you can perhaps block the request based on these headers. However, these are unlikely to be set if the hacker knows what they are doing and doing this maliciously\\nYou could try issuing a server-side redirect back to your domain, in the hope this will redirect the client. However, the proxy server would probably intercept this so it might not do anything. You also have to be careful of redirect loops, since from your server you can\\'t see the (fake) domain through which the site is being accessed at the client end. You could perhaps redirect to ?redirect=1 (or something) to ensure you don\\'t \"loop\".\\nYou could also try redirecting from client-side JavaScript. JavaScript can obviously see the (fake) domain through which the site is being accessed, so can potentially \"redirect\" back to your domain. However, many proxy servers will manipulate the client-side HTML/JavaScript/CSS so can easily manipulate the domain you redirect to - unless perhaps you somehow obfuscate this in your client-side code? Or perhaps redirect through an intermediary domain??\\nJust to add, blocking hotlinking by checking the HTTP Referer header is unlikely to help here (apart from carrying a certain amount of risk anyway). The proxy server will fake the HTTP Referer, either remove it completely (like a direct request), or make it look like an internal request.\\n\\nAside: Dedicated server with own IP Address\\nJust to add, if you have a dedicated server/VPS with it\\'s own IP address then it\\'s possible that this other domain is simply pointing to your IP address, thus effectively creating a \"cloned\" site.\\nThis is easily \"blocked\" in the server config by creating a virtual host for this domain and simply denying the request. Or, ensure you have a default virtual host (usually the first one defined) that catches all non-canonical host requests and denies access (preferable).\\nSee this question on StackOverflow for more information:\\n\\nhttps://stackoverflow.com/questions/25236810/how-do-i-block-someone-elses-domain-pointing-to-my-apache-hosted-website\\n\\nThis can also be blocked/redirected with mod_rewrite (in the server config or .htaccess), in fact, depending on how you\\'ve implemented your canonical www redirect you might already be redirecting such requests. However, since this is your own server (with it\\'s own IP address) then the virtual host solution is preferable. For example, a canonical redirect such as the following, will redirect all requests that are not for the canonical domain to the canonical domain (ie. http://example.com):\\nRewriteCond %{HTTP_HOST} !=example.com\\nRewriteRule (.*) http://example.com/$1 [R=301,L]',\n",
       " \"As you have access to ssh, just copy your file replacing the server's index.html file scp <localfile.html> <remoteuser>@<remoteserver>:<remote-path>/index.html for example:\\nscp 20170108.html webuser@myserver.com:/var/www/index.html\\n\\nYou can place that in a shell script take a file as the first parameter, for example script.sh:\\n#!/bin/bash\\nscp $1 webuser@myserver.com:/var/www/index.html\\n\\nThen you can make a script to take  file with the current day as the first parameter to the server\\n$ ./script.sh `date +%Y%m%d`.html \\n\\nwould execute\\n$ scp 20170108.html webuser@myserver.com:/var/www/index.html\\n\\nPutting that in a cronjob should be trivial.\\n\\nYou can use Tramp mode to copy the files from Emacs itself.\",\n",
       " \"MP3\\nMP3 is the most popular format and it's supported by most of phones, including Android, Windows Phone, Symbian... \\niPhone? m4r\\niPhone apparently uses another format, M4R, as I understand it's just a regular iTunes AAC with a different extension.\\nIt's very easy to convert an MP3 to iPhone ringtone. see this\\nWhat to do\\nYou can either add two different formats, one for iPhone and one for all other phones, or only provide an MP3 with instructions for iPhone users.\\nIt can be downloaded to a desktop first, also if no converting is needed, users can download it directly to their phone.\\nOfficial sources?!\\n\\nWindows Phone: Add or create ringtones\\nAndroid Supported Media Formats (Android OS has native support for using MP3 audio files as ringtones)\\niPhone Ringtone: What is iPhone 5 Ringtone Format? (not official!)\",\n",
       " \"No. A DNS is a simple mechanism, you give it a domain, it returns an A-record (with the targetserver's IP address). There isn't really anything you can do.\\nEven if you manage to create a check in the DNS record, or add a meta tag with robots info, or create a robots.txt, they don't have to listen to it. They're simply being polite and respect the robots.txt, but there is no technical obligation for them to do so. \\nIf they change the HTTP_USER_AGENT or a X-ROBOTS request-header info your check will no longer work. Or when you have an IP check and they switch to a new/unknown IP addrress, your check no longer works.\\nThis is why I prefer to see these are guides for crawlers, not as rules.\\nAlways assume that everything a visitor can see, can been seen by a crawler, one way or another. From a security perspective, this is safer.\",\n",
       " \"Check that:\\n\\nThe mail.example.com address points to your mailserver IP address. (You can look up the IP at https://www.whatsmydns.net/ by searching for the A record for that domain.)\\nIf you see no IP address for that domain, you'll have to add an A record for your mail.example.com address that points to your mailserver's IP address, or a CNAME that points to Bluehost's email domain. Bluehost should provide the mailserver IP or domain in your email setup instructions.\\nYour CNAME and MX records have propagated. A search at https://www.whatsmydns.net/ for the MX record of example.com should return your mail.example.com address worldwide. You should also see the mailserver IPs when you search for the A record for mail.example.com. \\nConfirm that Bluehost have completed setup of your mail account.\",\n",
       " 'As covered here, GitHub Pages is served with Nginx and automatically gzip\\'s content.\\nYou can confirm gzip compression for your site by checking the HTTP headers with online tools like this one. Enter the URL to a webpage or resource, and type in gzip under \"Accept-Encoding\" to indicate that the HTTP client (i.e., the online testing tool in this case) accepts gzip compression, as most browsers do.\\nYou might review the other information and suggestions here to optimize serving as well. ',\n",
       " 'There are specific elements of HTML to handle this situation. Using canonical to indicate that your page contains much content from another web would literally make your page undiscoverable through Google.\\nBlockquote + cite\\n\\nThe blockquote element represents content that is quoted from another\\n  source, optionally with a citation which must be within a footer or\\n  cite element, and optionally with in-line changes such as annotations\\n  and abbreviations.\\nThe cite element represents a reference to a creative work. It must\\n  include the title of the work or the name of the author (person,\\n  people or organization) or an URL reference, or a reference in\\n  abbreviated form as per the conventions used for the addition of\\n  citation metadata.\\n\\nUse the cite element to give proper attribution of the text to an author and a link to the author or source.\\n<blockquote>\\n  The people recognize themselves in their commodities; they find their\\n  soul in their automobile, hi-fi set, split-level home, kitchen equipment.\\n  — <cite><a href=\"https://en.wikipedia.org/wiki/Herbert_Marcuse\">Herbert Marcuse</a></cite>\\n</blockquote>\\n\\nq + cite\\n\\nThe q element represents some phrasing content quoted from another\\n  source.\\n\\nExplicit citation link in and outside the q element:\\n<p>The W3C page <cite>About W3C</cite> says the W3C’s\\nmission is <q cite=\"https://www.w3.org/Consortium/\">To lead the\\nWorld Wide Web to its full potential by developing protocols and\\nguidelines that ensure long-term growth for the Web</q>. I\\ndisagree with this mission.</p>\\n\\nExamples and citations are from HTML5 specification: blockquote, q.',\n",
       " 'There is no way at this point to do that with Linode. Having used them a while back as a provider I believe they sent emails at the time to alert when usage approached 100% of the plan allowance so you could make the appropriate changes to reduce your costings but there is no way in advance to prevent the usage from going over without actually monitoring it.',\n",
       " 'RewriteEngine on\\nRewriteCond \"%{REQUEST_URI}\" \"!^(/index\\\\.php|/robots\\\\.txt|/favicon\\\\.ico)$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/images/(.*)\\\\.(jpg|png|jpeg|gif)$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/css/(.*)\\\\.css$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/js/(.*)\\\\.js$\"\\nRewriteRule ^(.*)$ /index.php/$1 [L]\\n\\nThe format of your initial directives are OK for .htaccess as well. However, in .htaccess you will need an additional condition to prevent a rewrite loop after rewriting to index.php.\\n\\nRewriteCond \"%{REQUEST_URI}\" \"!^index\\\\.php\"\\n\\nI can see you\\'ve tried to do this in your second code block, however, the value of REQUEST_URI is the same whether you are coding directly in the server config or in .htaccess - it still begins with a slash. So, the above negated condition would always be true (like all the other conditions in the second block) and so the RewriteRule is always processed.\\nHowever, you could just add another RewriteRule to the start of your first code block:\\nRewriteRule ^index\\\\.php - [L]\\n\\nSo, put all together, this becomes:\\nRewriteEngine on\\n\\nRewriteRule ^index\\\\.php - [L]\\n\\nRewriteCond \"%{REQUEST_URI}\" \"!^(/index\\\\.php|/robots\\\\.txt|/favicon\\\\.ico)$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/images/(.*)\\\\.(jpg|png|jpeg|gif)$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/css/(.*)\\\\.css$\"\\nRewriteCond \"%{REQUEST_URI}\" \"!^/js/(.*)\\\\.js$\"\\nRewriteRule ^(.*)$ /index.php/$1 [L]\\n\\n# AccepthPathInfo on\\n\\nAnd AcceptPathInfo will need to be enabled if it isn\\'t already.',\n",
       " 'It\\'s quite typical of GoDaddy to have a low introductory offer for the first year. But the \"renewal\" costs are hard to find. This is, unfortunately, quite common practise, but not necessarily \"standard\". It relies on the fact that most users probably don\\'t change the registrar. But this domain might have been heavily discounted in the first year. Personally, I avoid registrars/hosts that are not upfront with their costs and prefer registrars have the same registration/renewal costs.\\n\\nif so, is there anyway I can change my registrar while keeping the same domain name for cheaper rates?\\n\\nYes, you can change registrar at \"any time\". Providing you\\'re not too close to renewal and they\\'ve already taken payment! It can also take several days to transfer the domain - but that doesn\\'t necessarily mean any down time.\\nThere might also be restrictions on transferring a domain too soon after registration. eg. in the first 3 months.\\nAlso, some registrars might charge an additional \"admin\" fee for transferring away. Need to check their T&Cs. Again, I would avoid any registrar that imposes additional fees like this, most do not. (I don\\'t think GoDaddy does.)\\n\\nAnd, as mentioned in comments, be wary of any scam emails (that have plucked your details from the WhoIs database) trying to get you to \"renew\" with them, rather than your real registrar.',\n",
       " 'Since this does appear to be the real Googlebot, the recommended way to block access/crawling is to use /robots.txt:\\nUser-agent: googlebot\\nDisallow: /blocked.php\\n\\nHowever, if you still want to block this IP using .htaccess then you can do something like the following, near the top of your root .htaccess file:\\nRewriteEngine On\\nRewriteCond %{REMOTE_ADDR} =66.249.79.70\\nRewriteRule ^blocked\\\\.php - [F]\\n\\nThis will return a \"403 Forbidden\" when a user with the above IP address attempts to access /blocked.php. Note that the \"user/bot\" still tries to access the URL by making a request, whereas with robots.txt they shouldn\\'t even make the request (assuming they obey the robots.txt \"standard\"; which Googlebot does).\\nHowever, bear in mind that Googlebot can crawl from different IP addresses, so this may need to be updated in the future.\\n\\nIf you did want to block the range of IP addresses denoted by 66.249.79.xxx, then you could change the above condition to:\\nRewriteCond %{REMOTE_ADDR} ^66\\\\.249\\\\.79\\\\.\\n\\nHowever, that could block more than just Googlebot and may not be any more successful at blocking Googlebot than checking the specific IP address above (Googlebot does not necessarily crawl on a continuous IP block).',\n",
       " '1) Remove all links to the ad. If someone manages to come across the ad, then raise an error 404. When the ad resumes, the link becomes available again.\\n\\nI think you need to get feedback on what users expect to happen when an advert is \"paused\". This might be what they expect? Although this is essentially \"unpublishing\" the ad, albeit temporarily. The 404 could be a custom 404, explaining that this particular advert is not currently available....?\\n\\n2) Remove the link to the ad from the site, but still have the link in sitemaps.xml (not sure what Google thinks of that)\\n\\nSounds OK. Why would Google have a problem with that? After all, the page still exists.\\n\\n2) But if many ads are on hold and have the same text, that\\'s not a good thing either. Could be seen as duplicate content?\\n\\nHaving a single line of text like \"this ad is on hold\" on multiple pages is not duplicate content. \"Duplicate content\" is when a significant portion of the page is the same, enough to make two or more pages \"essentially the same\".\\n\\n3) Upon resuming the ad, the ad could be deleted behind the scenes, and a new one created - seamlessly, e.g. new url is example.com/advert/501.\\n\\nNot sure why you would only do this when the ad was \"resumed\" and not when \"paused\"? But either way, deleting and creating a new ad URL is not really \"pausing\" IMO.\\n\\n3) If this is done too quickly, it might also look like duplicate content, since the old advert\\'s details may still be indexed.\\n\\nWell, not really \"duplicate content\". The old advert URL presumably won\\'t exist and return a 404 so will quickly drop behind the new URL.\\nBut if the \"resumed\" advert has a new URL then not only will search engines have to find and index the new URL. Users who have bookmarked the old URL will also be a bit lost. Or is \"resuming\" an advert intended to be \"starting afresh\"?\\nYou seem to be focusing heavily on sitemaps.xml? Removing a URL from your XML sitemap does not even begin to de-index that URL if that is what you are thinking? In fact, simply removing an existing URL from your XML sitemap will probably do nothing at all.',\n",
       " 'Google is very wary of any data or element found that can artificially influence it\\'s results. Throughout the years, Google has made a few mistakes here and there in it\\'s algorithms that allow people to manipulate the ranking of a site, however, that has always been within very narrow margins and not a general rule only effecting a site in small ways. The reason for this is simple. Google is made up of human capital that is often separated into stove piped efforts (groups) where communication between them has allowed a \"hole\" in it\\'s algorithm. These are often found and fairly quickly fixed but not always as fast as we would like. One example was the exact match domain name. While it is still true that term matches in a domain name can positively influence how a page is found, keeping in mind that the content still must be relevant to the search query, for a while, search term matches found in domain names were over-optimized and would bring a particular site to the top of the SERP list. This is no longer true except in smaller ways and only if the search query match is a strong one.\\nGoogle has a general rule. If a site owner can do something to influence search results other than through the ordinary means of content and site creation, then Google will not put influence on the data or content element. Why? Because then anyone can artificially influence results. SEOs, for a long time, would look for these holes in the Google algorithm. By and large, these so-called holes were nothing more than a small or narrow effect. The game of manipulation has been whittled down over the years and largely a thing of the past. Google wants honest results from honest sites.\\nBe that as it may, reviews can be a powerful thing, however, they do not influence a sites rank or influence a SERP link placement. The exception to this rule is for Google+. Why? Because users are fully vetted and Google can rely upon the results as being genuine. Otherwise, reviews can be manipulated artificially. Reviews will sometimes be used in SERP snippets. However, Google+ reviews can move a SERP link up one or two positions assuming that the search query and results are relevant. Reviews of any kind found within a SERP snippet can influence CTR (click-through rates) and over time influence where a SERP link is placed. This along with bounce rates, time spent on page, time spent on site, and other metrics are the required metrics to make this happen. This means that while reviews have a positive effect, reviews are balanced with actual site performance metrics to ensure that a site is a relevant search query match. Both machine and human influences are used.\\nIf the question is to use reviews, I would say Yes! If the question is to manipulate reviews, I would say do not do it. Why? Two reasons. Google can compare reviews from different places and use algorithms to know where reviews come from. This allows Google to know if a set of reviews have been cherry-picked and therefore an accurate representation. As well, please consider that algorithms can be used to recognize natural curves within a set of reviews to know if reviews are likely influenced or manipulated. Both are done with remarkable results.\\nLastly, separate the use of the term \"rank\" into two camps. One is site rank and page rank where metrics are stored within the index. The other is SERP rank where metrics are applied dynamically during the search query process and orders the SERP list. Reviews never influence a site or page rank. However, Google+ reviews can influence if a SERP link is raised one or two positions as a result of a fully vetted and trusted set of reviews.',\n",
       " 'The initial request for http://www.esdc.gc.ca/en/cpp/post_retirement/eligibility.page returns a page that contains the following...\\n<script>\\nwindow.location.replace(\"/cgi-bin/op-so/err/404-fra.asp?servertype=apache&p=404;http://www.esdc.gc.ca/en/cpp/post_retirement/eligibility.page\");\\n</script>\\n\\n... which is how you get from step 1 to step 2. If you disable javascript, you\\'ll get stuck on step 1 and can see the source.',\n",
       " 'This is a bit of a subjective question and grey area, but that said, you should ultimately consult your privacy lawyers (if you have them) on what is deemed as \"personally identifiable information\" (PII) or not. For what it\\'s worth, here are my thoughts:\\n\\nCity - Not PII. You should have no problems collecting this information. It cannot be used to identify a specific individual.\\nHalf Postcode - Not PII. I couldn\\'t perceive this as being able to identify an individual, so I think you\\'d be safe here.\\nFull Postcode - here\\'s where it might be tricky. Normally I would say this is not PII, and should be OK, but there could be the case where a single dwelling is assigned it\\'s own postal code, and hence it becomes personally identifiable. How likely is that though, really? Probably not likely, but you never know. Here is where you should contact a better authority on privacy issues. Best would be to just avoid collecting this altogether.',\n",
       " \"IP address is not guaranteed to remain the same in long term because it depends on your service provider's network. A domain name will not be changed as long as you keep paying for it.\\nSo just point your host name e.g. service.yourdomain.com to your IP address and tell your users to make CNAME records in their host file.\\nSo that\\nwww.clientdomain.com -> service.yourdomain.com -> [ 1.2.3.4 ] \\nFinally, the application on your server should observe the host in the http request and handle accordingly.\",\n",
       " 'Your client, who obviously entered the wrong email address, should contact the registry directly and generally provide all sorts of ID.\\nIf you are the reseller, best thing you can do to look after your customer is contact your wholesaler as well and check what the process is.  I doubt it will be easy.\\nBut I reckon you have left your run a bit late.  Your client had 15 days to do this, there are 5 left :(',\n",
       " \"An Edge node is the server that delivers the content to the user. A CDN is typically spread throughout the globe having edge nodes at multiple continents and even multiple Internet Backbones.\\nThe edge node might be an application server, a caching server, or reverse proxy. When you query the CDN, it checks the Edge location which can get you the data with the minimum number of hops. While it optimizes for performance, it then tries to deploy popular data near the client who asked for it.\\nJust because you're using a CDN will not mean that your data is replicated across each and every node. There are different caching mechanisms that are employed by different CDN networks\\nYou can read more about it here: https://support.rackspace.com/how-to/what-is-a-cdn/\",\n",
       " \"Yes, it is possible, however it really depends on the ad network itself. For a good starting point, please see Google's Rich Media site. It has examples of ad formats, info where a given format can run, building blocks for creating ads, real life examples, and dev docs.\",\n",
       " 'Spam filters cannot definitively tell that the second email is not from the legitimate user unless an SPF record exists to inform them of the only authorised servers for sending messages.\\nThe legitimate user could get their domain blacklisted as a result of the actions of malicious individuals and sometimes this is the end-goal for them. This will depend on how well the emails are spoofed and the quantity of messages. Blacklists may require you to implement SPF prior to removing you from their naughty list and you may also have to pay a charge for this.\\nIf this is happening to you then ensure you have setup certain DNS records (TXT records) for your domain name that help to combat spam. An SPF record for your domain describes which servers are authorised to send mail from your domain and instructing recipient mail servers to treat all others as spam and discard the messages. For example:\\nlegitimate.com IN TXT v=spf1 mx ptr ip4:1.2.3.4 -all\\n\\nThis would authorise mail to be sent from your mailservers as defined in your mx records, your webserver (using the reverse IP of legitmate.com if you include ptr), and any other ip address you specify. -all instructs mail transfer agents to reject any exceptions as spam. If you are uncertain you could use ~all to say accept but tag as spam. Read up on the SPF Record Syntax.\\nSecondly you could setup DKIM message authentication on your mailserver, which automatically applies a cryptographic signature to your message. If the receiving mail transfer agent cannot verify that the message came from your mail server using this signature then it may delete the message or at least tag it as spam (depending on the policies they have configured). A DKIM record might look like this:\\nlegitimate.com IN TXT k=rsa; t=s; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDGMjj8MV\\naESl30KSPYdLaEreSYzvOVh15u9YKAmTLgk1ecr4BCRq3Vkg3Xa2QrEQWbIvQj9FNqBYOr3XIczzU8gkK5\\nKh42P4C3DgNiBvlNNk2BlA5ITN/EvVAn/ImjoGq5IrcO+hAj2iSAozYTEpJAKe0NTrj49CIkj5JI6ibyJw\\nIDAQAB\\n\\nThirdly, you could implement DMARC which is designed to help reduce phishing and spoofing attacks, essentially by reporting back to you all delivery reports of messages claiming to be from your domain name so that you can ensure your SPF and DKIM setup is effective. A DMARC record might look like this:\\nlegitimate.com IN TXT v=DMARC1; p=none; rua=mailto:dmarc_reports@legitimate.com',\n",
       " \"No, the name does not matter.  In the Google Search Console (and the Bing version of that, FWIW), you tell it which sitemaps to look at. Other search engines might look for sitemap.xml by default but this is not guaranteed nor standard behavior and doesn't really need to be worried about.\",\n",
       " '#<Files ~ \"\\\\.js$\">\\n#   order allow,deny\\n#  deny from all\\n#</Files>\\n\\nYes, you will get a 403 Forbidden with the above code when trying to access any .js file, regardless of the way you access it, either from a direct link or when linked in your HTML source (unless it is pulled from your browser cache).\\nAnd that\\'s the problem, you can\\'t really protect client-side JavaScript files. They need to be downloaded to the client/browser in order to run. Once they are downloaded, they can be viewed.\\nHowever, sendmail.php is a different matter. This runs entirely on the server. But it depends on how it is being called whether it can be \"protected\". For example, if it\\'s being called directly from an HTML form\\'s action attribute then it can\\'t simply be \"hidden\", as it will be hidden from your HTML form as well. In this case you need to make sure the script is secure, validating all inputs, spam protection, etc.',\n",
       " \".google actually is a valid top level domain (or top level zone), as is '.youtube'. Google applied for those TLDs a long time ago...\\xa0successfully as we can now see. Google can now further delegate authority within that zone and com.google and other subdomains (or delegated zones) can become valid and be operated. \\nHere's a news article on zdnet about this from 2012 and you can find out some .google background info about the WHY at The Register. \\nThe mirror effect 'on page' on the other hand is probably done with a little css3 (as e.g. described here) – I now only quickly checked with firebug and saw that on rollover the elements are actually highlighted on the opposite side.\",\n",
       " \"Go to Admin in Google Analytics, in the third column click on Filters.\\nAdd a filter that only includes the hostname you do want to track, that way anything else, include localhost, will be excluded from your data going forward.\\nIt's not possible to correct data that has already been gathered.\",\n",
       " \"Sorry, forgot to post the resolution to this!\\nSo it was a forwarding thing as suspected but my SPF rules in DNS were overly strict and didn't allow for forwarding - hence SPF failed. Changing from \\n    -all \\nto \\n    ~all \\nsorted it.\",\n",
       " 'Since you have clearly mentioned that you saw the history after you deleted and added the property again, nothing can be done and you will have to just live with it.',\n",
       " 'Point #1:\\nYou must make sure that the hostname or IP address you are trying to redirect is already pointing to the web-host & location where you have the .htaccess file. If they are being pointing somewhere else, then no matter what you put in .htaccess, there will be no result.\\nPoint #2:\\nRedirects are generally cached by browsers. So while testing with different redirect rules, you must clear browser cache every time you try a different rule.\\nPoint #3:\\nThe CODE you\\'ve posted should work if you\\'ve followed points #1 and #2 correctly. So most likely you are doing something wrong there.\\nPoint #4: (bonus)\\nThe easiest trick is to use the not (!) operator in RewriteCond.\\nThe best thing about the not operator is that it\\'ll redirect any accessible IP or hostname to your target/canonical hostname. So if you have multiple domain / IP address / sub-domain pointing to your desired host, all of them will be redirected to the desired host. The .htaccess CODE will be something like:\\nRewriteCond %{HTTP_HOST} !^example\\\\.com$\\nRewriteRule ^(.*)$ \"http://example.com/$1\" [R=301,L]\\n\\nThis way, not only www.example.com, but also anything-else.example.com will be redirected to example.com (if both were pointing to the same host configuration).\\nSimilarly, any IP address pointing to that same host+location will be redirected to example.com.',\n",
       " 'As far as I know, there is no way to transfer specific keyword from one site to another site, because keywords are indexed by Google according to the content of your site.\\nHowever, you can create a new site & then redirect all the links that are being indexed for \"loan software\" from the old site to the new site. Of course, you\\'ll have to transfer the contents of those links to the new site as well (or at least have very similar contents to the redirected links).\\nThat way Google will give some of the ranking points to your new site and index your desired \"loan software\" keyword for the new site.\\nHowever, keep in mind that you may not have the same ranking points in the new site. Since Google SERP ranking may depend on other metrics (e.g. backlinks, Page Rank, authority etc.) that may not be immediately transferable to the new site.',\n",
       " \"We've run split-URL tests like this ourselves, and haven't seen any fall in SERPs. Some customers had lingering doubts, so here's what we did after scouring the web:\\n\\nChoose two pages; the control and the variation \\nAdd canonical tag to both pages... pointing to the control page \\nRun split test for 17 days\\n\\nResult: The control page did not fall out of Google rankings.\\n(I work for VWO.)\",\n",
       " \"You have to first open your analytics and analyze all the landing pages. Go to Behavior then landing pages.|\\nWhen you do that check the pages that received the most hits and collect all the keywords used to find your content organically.\\nWith that data you will need to search trends, events, terms and anything that the organic world could have done to send users to your website.\\nI recently experienced the same exact thing on one of my websites.\\nOne page spiked to over 300% organic traffic, doubling the traffic of the entire site to nearly 100% increase:\\n\\nI then opened up analytics, collected the keywords and then pages that received the majority of the hits. Started researching and quickly found out that it was a Christmas recipe for making some type of cultural food that sent the users to my website because I was ranking first page and third position for that term.\\nIn January the same thing happened just last week on this same website:\\n\\nIt started on the 7th and dropped back to normal on the 11th.\\nThis time I haven't done the research because I already know that sometimes new trends, events, and things just happen to trigger the web to our pages. So I haven not done any research and you surely don't have to worry about. But feel free to dig deep and you will find the reason as to why that happened.\",\n",
       " 'Yes you have to add it manually in search console, because ping tool is available for anyone, it means me and you can also submit stackexchange sitemap to Google ping tool, and they will not going to check weather you own the domain or not. \\nSo that ping tool does not gives you any index data, neither they automatically add your sitemap into search console. You have to verify your property in search console and add the sitemap manually.\\nBut if you just want to index your website without checking the index data and errors as you said in comment, then feel free to use ping tool, Google support it very well, here is official statements.\\n\\nYou can also resubmit a sitemap by sending an HTTP GET request to the\\n  following URL, specifying your own sitemap URL:\\n  http://google.com/ping?sitemap=http://www.example.com/my_sitemap.xml',\n",
       " 'I worked for a company once who went down this road. It was manageable with a handful of domains, but at one point we held several hundred and it just got to be silly. Sure we had a spreadsheet to tell us what was where, but when it came to ongoing management like renewals (and remembering to charge end-clients for them) it led to more frustration than it was worth.\\nUltimately we decided to find one registrar who gave a good balance of price & ease of management. This also meant that whenever we needed to transfer a domain in/out for a client we had the process down pat without having to think about which registrar requires what information &c.\\nThere were a few cases where a separate registrar was required for specific TLDs (like Switzerland & Hong Kong) but at least that was easier to remember than randomly spreading them apart.',\n",
       " 'As a European (Dutch) and a web builder:\\nYes, this is still required (if you have tracking/3rd party cookies). But now the cookie storm is over, and the dust has settled, most sites only show a small banner \"we use cookies\" and stick to that. Unless you\\'re in the big league, there\\'s not much to worry about, with just that notification you\\'re already doing better than most sites. I have yet to encounter an actual court case about this.\\nDutch law requires opt-in, but that rarely happens. European law says opt-out should be possible, but most websites just tell the user they use cookies and keep it to that.\\nThis applies to EVERY website targeting europeans, no matter where you host or where the company originates.  \\n• This website has plenty of info about EU legislation on cookies\\nMight be nice to know, you no longer need to place the notification if you only use Google Analytics (you had to because GA uses a cookie to check for returning visitors) and cookies specific for the website. Because of this, most small common websites don\\'t need a notification to the user.\\nThe reason GA is allowed, is because they don\\'t track you from site to site, only if you come back. This is considered acceptable because it is basic information which is useful for a webmaster and not privacy invasive for visitors. These cookies are available for the visited domain only and therefor seen as first party.\\nFYI, it\\'s called cookie law, but this doesn\\'t only apply to cookies. Session.storage and similar functionalities fall under the same rules. Everything that tracks users for the purpose of tracking users.',\n",
       " 'First, forget sitemap and robots.txt. Those are fun, but not magic, they just help on a technical level for hard to reach content.\\nSecond: Google doesnt work in \"hours\", Google works in \"a few days\". If you change something, it can take up to a few days, they\\'re not waiting until you update something. For bigger/more popular sites this will be more often, but a \\'normal\\' websites take a few days (Why do SO questions appear so quickly).\\nThen, I don\\'t want to be rude, but if you appear in SERP page 2 or 3, you probaly don\\'t have good content, because if you would, you would be on page one.\\nI recommend you use a few SEO score websites, there are free ones. Do a few of those. It might be worth your money to invest in a paid report as those give you much much more info.',\n",
       " \"After a lot more research and experimentation we've found the following. Cross-domain referral exclusion works only when you are using the same UA code to push to. In other words for one UA that tracks domain.co.uk and one UA code that tracks domain.com.au and then a third UA code that we have come to call Rollup which we push traffic from both domains to.\\nThe GA tags in GTM for the Rollup property have the auto-link domains set up for each other and it also has both domains in its own referral exclusion list. This way we can actually see the google / organic as the source/medium when somebody comes from Google, goes to domain.co.uk and then to domain.com.au. That visitor will be listed as google / organic on URL domain.com.au/.....\",\n",
       " \"Database sites like IMDB are not a problem in and of themselves.     Content can be far more than editorial prose.   Content can be:\\n\\nPictures\\nLists\\nFunctionality (such as a tool that users can use)\\nSpecial effects\\nSounds\\nVideo\\nMaps\\nLinks\\nData\\n\\nA great page doesn't have to have paragraphs of text to provide a good user experience.   In fact, in many cases too much text can drive users away.  If your site is providing what users want when they click from the Google search results, Google is generally going to be happy.\\nWith database driven sites you do have to worry about creating thin pages.   You may have a great page about Tom Cruise, but not very much information at all about Michel Cera.   In that case, your Michel Cera page isn't going to satisfy the users that it attracts.  When you have a substantial number of thin pages, it can reflect badly on your site as a whole.   Google can penalize sites with great pages when they have other pages that are thin.  You can avoid problems by not generating pages with little content, or restricting Google from indexing them.\\nYou also say that some of your site focuses on aggregated data.   Google usually only indexes one copy of duplicated text and data.   If your site consists mostly of aggregated data, you will have trouble getting many of your pages indexed. A page needs a substantial amount of original content along side the duplicate in order to get indexed.   As a whole, I'd say that 50% of the text and data on your site shouldn't appear any where else on the internet verbatim if you want to get well indexed.\",\n",
       " \"RewriteRule ^(.*)$ http://example.com/%1/$1 [L,NC,QSA]\\n\\nWhen you specify an absolute URL (scheme + hostname) in the RewriteRule substitution it will implicitly trigger an external redirect - which is what you are seeing.\\nHowever, if the subdomain(s) and main domain are all on the same shared hosting account (ie. the same filesystem) then you shouldn't have to explicitly specify the full domain in the susbstitution, just specify the file system path.\\nFor example:\\nRewriteRule (.*) /%1/$1 [L]\\n\\nThe NC and QSA flags would seem to be superfluous here. The query string is appended by default, unless you explicitly include a query string in the substitution. And the pattern anchors (^ and $) are not required either when using a match everything .* pattern.\\n\\nUPDATE#1: The first RewriteRule would seem to be superfluous. This is a wildcard subdomain, so I assume the subdomain points to the same document root as the main domain.\\nWhat you are really trying to do is internally rewrite http://anything.example.com/par1/par2 to /anything/index.php?a=par1&b=par2 on the same subdomain.\\nTry the following instead:\\nRewriteEngine on\\nRewriteCond %{ENV:REDIRECT_STATUS} ^$\\nRewriteCond %{HTTP_HOST} ^(.*)\\\\.example\\\\.com [NC]\\nRewriteRule ^([^/.]+)/([^/.]+)/?$ %1/index.php?section=$1&page=$2 [L,QSA]\\n\\nThe QSA flag is required here, if the original request (pretty URL) could have a query string, which needs to be passed through.\\nNo need to escape the dot in a character class in order to match a literal dot.\\nThe RewriteCond directive checking against the REDIRECT_STATUS environment variable prevents a potential rewrite loop (which would result in a 500 error in the browser).\\nUPDATE#2: To avoid the explicit use of the domain (eg. example.com) then you could just match the first domain segment, ie. upto the first dot. For example:\\nRewriteCond %{HTTP_HOST} ^([^.])+\\n\\nHowever, this would also match the main apex domain (or www), unless you already have a directive to block that. I assume accessing the main domain like this would strictly be invalid? However, this might be OK if your script handles such an invalid request with a 404?\\nOr, to match only subdomains of the main domain, except for www then try:\\nRewriteCond %{HTTP_HOST} !^www\\\\. [NC]\\nRewriteCond %{HTTP_HOST} ^([^.])+\\\\.[a-z-]\\\\.(com|co\\\\.uk|net) [NC]\\n\\nYou would need to add the TLDs as required (maybe that could be generalised further?).\",\n",
       " 'Presumably you are already linking to the directory URLs without the trailing slash? That is the first step.\\nSince these are physical directories you need to tell mod_dir to not \"fix\" these URLs by appending a trailing slash. You can do this with the DirectorySlash directive near the top of your file:\\nDirectorySlash Off\\n\\nThis means that you now need to manually append the trailing slash with an internal rewrite. Note that mod_dir ordinarily 301 redirects to the slashed URL, so this will have been cached by the browser. Ensure your browser cache is clear before testing.\\n\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteCond %{DOCUMENT_ROOT}/$1/index\\\\.html -f [NC]\\nRewriteRule ^(.+?)/?$ /$1/index.html [L]\\n\\nThe main problem with this is that the first RewriteCond directive prevents directories from being rewritten, which would seem to be the opposite of what you are trying to achieve. Try the following instead:\\nRewriteCond %{REQUEST_FILENAME} -d\\nRewriteCond %{DOCUMENT_ROOT}/$1/index.html -f\\nRewriteRule ^(.+?)/?$ /$1/index.html [L]\\n\\nErrorDocument 404 http://www.example.com/error404.html\\n\\nAside: You should specify a root-relative URL to your error document, like:\\nErrorDocument 404 /error404.html\\n\\nIf you use an absolute URL Apache will trigger an external redirect to your error document (instead of an internal subrequest) which is generally undesirable.',\n",
       " 'RedirectMatch 301 ^(.*)$ http://www.example.com\\n\\nA potential problem with this is that you are redirecting everything to your homepage. Redirecting everything to the homepage is likely to be seen as a soft-404 anyway in the eyes of Google. So, no benefit.\\nThat includes all the \"backlinks\" on that 4th domain as well. But unless you have related content for all those old backlinks that you specifically redirect to then you are unlikely to benefit.\\n\\nObviously redirecting the .com, .org and .net to my main site will neither help nor harm my SEO efforts.\\n\\nYes, SEO doesn\\'t really come into it.\\n\\nThe 4th domain (containing great backlinks) will help my SEO efforts if I have the redirect set up right.\\n\\nNot really, they\\'d have to be some pretty amazing backlinks. As mentioned above, unless you have the content that the backlink is expecting then any benefit is going to fall pretty flat. A redirect tells the search engines (and users) that the content has moved. If you point the redirect at something unrelated then Google is going to wise-up.\\n\\nI have redirected each of these 4 sites to my main site using an .htaccess file for each domain.\\n\\nIf you are not intending to put any content on these domains then you could just point them all to the one account (DNS and VirtualHost config) and then have a single redirect in a single .htaccess file (or preferably server config). In fact, your canonical www / non-www redirect could do it already.\\n\\nI have not added the www or non www version directive. Being that I have redirected the domains I didn\\'t think I needed to do this. Do I need to do this?\\n\\nYour redirect redirects everything, no matter how the user has accessed the other domain. So if example1.com and www.example1.com are accessible then they will both be redirected to your canonical domain. Which is what you want.\\n\\nWill this domain be indexed in the search engines or will me redirecting it keep it out of the search engines? I do not want duplicate content.\\n\\nThey should not appear in normal search results. It\\'s not \"duplicate content\".\\n\\nDo I need to ALSO use a robots.txt file that disallows the site from being indexed or will the redirect do that on its own?\\n\\nNo. If you block with robots.txt then the search engines won\\'t see the redirects (if they would ever find them). Note that robots.txt prevents crawling, it doesn\\'t necessarily prevent indexing if the URL is being linked to.',\n",
       " \"Parked domains typically just have adverts and this won't impact on your SEO but might frustrate some customers if they guess the .com when typing in your brand name.\\nIf you don't own a domain name then there's always the possibility a competitor or malicious user might use it to try and harm your brand reputation or redirect potential customers away to their own website.\\nPerhaps one day it will be worth paying the price they ask if your app is successful (though the price may go up!). Keep an eye on it in case they choose to let it expire (unlikely). It's also worth remembering premium domain prices are open to some negotiation. The sooner you get your app monetized, the sooner you can buy the .com and stop a competitor getting it!\\nIf you're serious about your business and protecting the name, register the trademark in your brand name so that you go after misusers of the name in a court. Registered trademarks can be an effective deterrent, and while you can't just take ownership of a prior registered domain name you may be able to stop commercial use of it especially by a competitor.\",\n",
       " \"You need to alter your title tag to complete their character limit. If not google will take your website URL or your H1 tag as your extra tag. Title is for user to understand what is your page is all about so that elaborate your title for user understanding. Google things that 60 would be the best limit to understand what your page is all about.\\n   Most of them uses some of the highly searched related keywords for mention theri titles that was also one of the good way to increase the visibility of your website. Utilise all the charecter limit for title and don't exceed that and don't use less character to write title.\",\n",
       " 'The answer to this question is detailed here : https://askubuntu.com/questions/653923/force-https-and-www-with-virtual-host-apache2/653947#653947\\nThe key thing was to  have 3 VirtualHost : \\n\\none for the http to https redirection\\none for the https non-www to www redirection, which includes all the SSL settings \\none for the https www version, which contains all the SSL settings + anything else you want.',\n",
       " 'My first question is is my filtering correct if I want to use some one\\'s else image on my blog.\\n\\nYes (although you might be able to use less strict rules depending on your specific needs, e.g. if you want to use an image unedited or if your blog is non-commercial).\\n\\nSecondly after I have found the image with proper license, as I understand I need to provide reference to the image. How do I do that? Should I just paste the link below the image I picked from Flickr or Google? Or is there some other way?\\n\\nIt depends. You should examine the license that the image is provided under.\\nFor example, I did a search for \"old west\" and turned up these images:\\nOuthouse which says:\\n\\nCredit me as author using the following credit line clearly and legibly next to the image: \"Betty Wills, Wikimedia Commons, License CC-BY-SA 4.0\".\\n\\nand\\nWagon which says:\\n\\nPublic Domain. Free for commercial use. No attribution required',\n",
       " \"GA itself is able to dedupe transactions within the session, but if someone reloads a thank-you page after their initial session has timed out, then GA will count that transaction, resulting in a transaction ID appearing more than once in your reports. You can either set up something in the backend that will prevent the thank-you page from being reloaded (which is probably more robust), or you could follow this guide https://www.simoahava.com/gtm-tips/prevent-repeat-transactions/. However, this guide sounds much like what you're describing already. It's not overly complex, but if you're still not convinced, then perhaps your only solution would be to work out some logic in the backend to prevent the thank-you page from being served again.\",\n",
       " \"Regex is not supported in robots.txt and this was asked awhile back on Pro Webmasters. However the code that you have provided is valid which will match certain paths and files. According to Google's bot it defines:\\nUsing /*.js$ matches:\\n\\n/jquery.js\\n/scripts/jquery.js\\n\\nUsing /*.js$ would NOT match:\\n\\n/jquery.js?ver3.0\\n/jquery.js/\\n/jquery.JS.MIN\\n/jquery.JS\\n\\nIf you are using versioning or any other format that is not supported then you can be less specific by using simply /*.js without the $ on the end. \\nSince your code looks correct the issue you have is likely something else or a rule that is conflicting with your current ones you have. You should be as specific as possible and look at any rules that could be conflicting with them. \\nSee: \\n\\nOrder of precedence for group-member records\\nAt a group-member level, in particular for allow and disallow\\n  directives, the most specific rule based on the length of the [path]\\n  entry will trump the less specific (shorter) rule. The order of\\n  precedence for rules with wildcards is undefined.\",\n",
       " 'I hope I\\'m understanding this correctly. But it sounds likes you want to run three domains (maindomain.com, location1.com, location2.com, and location3.com) on a single web server and have each one point to a particular URL or folder on the web server. If so, you can do this with virtual hosts and the Nginx web server. A virtual host file describes a single domain and where the web server can find the files to be served. You can have multiple virtual host files, each one different from the others. When Nginx receives a request for \"location1.com\" it will load the \"location1.com\" virtual host and then point the requester to the right spot. Same for the other virtual hosts. \\nNginx \\nVirtual host configuration files are stored in the \\'/etc/nginx/sites-available/default /etc/nginx/sites-available\\' directory. You will need to create four hosts files to make this work:\\n/etc/nginx/sites-available/default/etc/nginx/sites-available/maindomain.com\\n/etc/nginx/sites-available/default/etc/nginx/sites-available/location1.com\\n/etc/nginx/sites-available/default/etc/nginx/sites-available/location2.com\\n/etc/nginx/sites-available/default/etc/nginx/sites-available/location3.com\\n\\nVirtual Host File\\nConfiguring a host file is super easy too.\\nserver {\\nlisten 80; ## listen for ipv4; this line is default and implied\\n#listen [::]:80 default ipv6only=on; ## listen for ipv6\\n\\n# Make site accessible from maindomain.com  \\nserver_name maindomain.com; \\n\\n# Location of folder to serve files from  \\nroot /var/www/maindomain.com;  \\nindex index.html index.htm;\\n\\n# Location of log files  \\naccess_log /var/log/nginx/maindomain.com.access_log;  \\nerror_log /var/log/nginx/maindomain.com.error_log;\\n\\nI hope this helps!',\n",
       " \"You do need to be PCI compliant because you do capture and handle credit card information. It doesn't require you sending it to a bank or storing it. Just having it in your system at any time puts you within PCI scope.\\nThe self assessment is good but to be sure you are actually 100% compliant you should have an auditor check your system. They will be unbiased and detail issues you may not be aware of.\\nWhomever is telling you that you need to be PCI compliant because you link to sites that accept credit cards for payment is mistaken. Your site does not have credit card information pass through your system so you are outside of the scope of PCI. But, if you share a database with any of these sites and have access to their payment data you enter a gray area and probably either fall under PCI already or will soon because the rules gets stricter every year.\",\n",
       " 'Google allocates time spent on a website mostly based on your domains authority and how it important your content is considered. Domain authority is complex but considered to be based mostly on age, influence and quality. \\nIf your site has become slower but your authority has remained the same then Google will not increase your allocated crawl time and will be able to crawl less within the given time.\\nTo increase your authority:\\n\\nFocus on publishing even higher quality content.\\nTry and get your content buzzing which will increase your SEO leads that in turn improves authority.\\nBecome the best in your niche.\\n\\nImprove the amount of pages crawled in one session\\n\\nImprove your site optimisation with various methods to decrease render and download time.\\nSpeed test your website using Web Page Speed Test, use multiple locations and passes. \\n\\n1 second = Good\\n2 second = Ok\\n3 second = Poor\\n4 second = Bad',\n",
       " 'this is how to fix this:\\n\\nsudo nano /etc/apache2/apache2.conf\\nPut in this code at the bottom:\\n\\n.\\n<IfModule mod_headers.c>\\n    Header set Access-Control-Allow-Origin: \"http://example.org\"\\n    Header set Access-Control-Allow-Credentials: true\\n    Header set Access-Control-Expose-Headers: \"Content-Length\"\\n    Header set Access-Control-Allow-Methods: \"POST, GET, PUT, OPTIONS, DELETE, HEAD\"\\n    Header set Access-Control-Allow-Headers: \"Range\"\\n</IfModule>\\n\\nEnable headers in config file: sudo a2enmod headers\\nCheck for errors do: sudo apachectl -k graceful\\nsudo service apache2 reload\\nsudo service apache2 restart\\n\\nDone',\n",
       " 'I/O Transfers\\nI/O is the transfer rate between the hard disk and ram. All computers, servers and mobile devices have a maximum I/O the max is determined by the cpu speed, bus speed, hard drives and ram. All shared hosts, clouds and virtual private server use some form soft I/O limit or cap to ensure that you do not effect other users on the same server.\\nUnderstanding Bitrate\\nWhen sharing a video online on a shared server you need to know your the I/O of your hosting and bitrate of video. Using the two you can work out the maxium users that can use your site at one time without getting in trouble with your web host. For example if your video is 1080p x264 at 4000 Kbps then that is 400kb/sec which means... 10 users watching your video would be 4000kb/sec which is your limit of the I/O.\\nShared hosting is a not a platform that is recommended for video hosting due to the fact of the I/O and bandwidth restrictions these shared hosting companies enforce. This is why so many companies use YouTube and other streaming services because its cheaper to do so.\\nShared Hosts\\nMost shared hosting platforms will have thousands to hundreds of thousands of customers on the same server, each will be allocated a % of the max I/O transfer or have terms and conditions not to constantly use too much. In your case you have exceeded your maximum I/O write allocated to you and therefore have landed yourself a suspension for breaking the terms of service.\\nBut... Wait... What About Unlimited Hosting\\nSadly many people believe they have unlimited hosting when in fact they do not. Unlimited hosting is according to their acceptable usage policy. Shared hosting is never unlimited and its kinda false advertising but they have been getting away with it for decades.\\nVirtual Private Servers, Dedicated Servers, Content Delivery Networks and Cloud Servers\\nVPS, Dedicated Servers, CDN, Cloud servers will have a higher I/O limit because they generally use better technologies, less user contention and a higher array of disks. If you can afford better hosting then you will need to use something like this or use YouTube.',\n",
       " \"DNS lookups are not taking six seconds for all users, and you can confirm this by testing DNS speed with and of the third-party DNS speed testing services. Here is one example result from SolveDNS' test showing that DNS responses are actually very fast for the domain: http://www.solvedns.com/dnsspeedtest/evamilbrandt.de\\nLooking at the bigger picture, it appears that your web server does not have Keep Alive enabled, which is very strange. Enabling this is recommended (it is enabled by default) and will likely resolve the DNS issue because established connections will be re-used.\\nIf speed is the goal, you have other optimizations you can make which are beyond the scope of this question, but you can consult a third-party testing site like GTmetrix for feedback on how fast your site is and what can be done to improve page load time. Here's a test I just ran for example: https://gtmetrix.com/reports/evamilbrandt.de/WN0LkH9B\\nNice looking site by the way, now go make it fast and everyone will be happy!\",\n",
       " 'It\\'s very simple consider if you have a user account in any website means your details are stored in that particular website server/host. Each and every time you access your account details by entering some details as username and password. This was a front end purpose only. On the backend, the process was different while you sign up on that website to make connecting between you and that particular website at that time a key(eg:adgot-1247) is generated in the backend and you can\\'t able to see that key.\\nWhen you log in again with you log in metrics same key(eg:adgot-1274) will be generated for you every time and you can access your account.\\n\"In your question, you ask how the items in a cart as a guest change while logged into that account after you logged in your backend key sent to the server and you access your particular saved account. Where you won\\'t get any added cart items as a guest because it never saved with any of the particular accounts in a server it saved only for general purpose reference.\"\\nBy this method, you can see a recommended product in some e-commerce websites while you using it with your own account. If you use your guest account you won\\'t get any recommendation product according to your last viewed product at all...\\nI think this may solve your query if not feel free to ask in comment...!!',\n",
       " 'Based on your question and your clarification comment you would be better served using the URL rather than a header in order to best deal with downstream caching servers. In other words the first time you get the connection check the language header and then redirect them to the appropriate content via URL (something like www.domain.com/en/ or www.domain.com/de). By doing this you will not have to do anything special for caching servers. Additionally some caching servers are not correctly configured to support variants on pages and so can break the header based language changes (the user wants German language but the English version is cached and returned from the caching server).\\nEven the largest sites on the internet such as Microsoft put the language code into the URL to ensure the best support for downstream caching servers.',\n",
       " 'One server needs to be configured to receive the emails for your domain.   It sounds like you are choosing Google for this.   To make this happen you need to change your DNS MX records to the values that Google gives you when you sign up for their system.\\nYou need to then configure it so multiple servers can send mail on behalf of your domain.   Traditionally any server could send email from your domain and no configuration was needed to make this happen.   These days, mail sent from random places will likely get marked as spam and discarded.  You will have to configure DKIM and/or SPF records for each server that you are allowing to send email from your domain.   Both Google and sendGrid should give you instructions for configuring one or both of those.   There is no limit to the number of servers you can configure to send email from your domain.\\nI have this set up for my domain.   I have the mailboxes hosted with RackSpace email and I use Amazon SES to send email from the website.   RackSpace is set up as the MX records, but both places that send have DKIM or SPF set up.',\n",
       " 'The reasons to prevent Google from indexing content are:\\n\\nThe content should be private and not available through search engines\\nNobody would be searching for the information and thus wouldn\\'t be of interest to search users\\nThe content is in a format that is not meant for human consumption\\nThe content is thin or low quality  (if it isn\\'t going to satisfy the searcher, such that they leave unhappy, it will ultimately reflect badly on your site as a whole)\\n\\nI\\'ve never seen Google index CSS or JavaScript files, so there shouldn\\'t be a reason to add a noindex to those.   Noindex is typically done with a meta tag in the HTML source code, I\\'m not sure if your plugin could actually noindex those or not.   If it did, it would have to use a X-Robots-Tag header.\\nIt sounds like \"orders\", \"messages\", and \"refunds\" should be private.   You also might not want \"coupons\" indexed.\\n\"webhooks\" shouldn\\'t have human readable content, so you probably want to noindex those. \\nI\\'m not sure exactly how you use the other categories, but you could apply my rules above based on your usage.',\n",
       " \"It sounds like you are on shared hosting, which means you are not administering the system you use. This severely limits your options and it means that you will have to open a support ticket and ask your hosting company to set everything up for you.\\nThere are two HTTPS connections at issue here: client browsers <--> web host's reverse proxy, and web host's reverse proxy <--> web server hosting your site. The first connection is the one causing you trouble but best practice would be to use HTTPS for both connections. You'll have to ask your hosting company if this is possible.\\nIt may be worth switching web hosts to a different company that transparently proxies, so you do not have this problem. Another option may be to edit your CMS templates to include the JavaScript resources from a third-party HTTPS host. Common resources like JQuery are widely available on HTTPS CDNs and are easy to find, but custom JS for your templates will be a problem. You might even consider in-lining those. That isn't best practice but it may work.\\nPersonally I think changing hosting companies is the way to go, especially if your current host wants to charge you extra to work around their proxy configuration, but there are a few things to try first.\",\n",
       " 'Yes.\\nIn fact, you probably already have multiple CNAME records. (eg. you\\'ll probably already have one for www.) However, you can\\'t have the same name pointing to different canonical names (one to many) - if that is what you mean.\\nhubspot and unbounce will need to have different source names.\\nCNAME records are associated with the domain\\'s DNS. Not the \"website\".',\n",
       " 'According to your current setup:\\n\\nYou have dedicated page for every event with the details of that event.\\nEvery event may have multiple locations / dates and for those locations / dates you have event listing pages.\\nThe listing pages for locations / dates contain short description of the original events. Other than that they don\\'t provide any additional unique information.\\n\\nFor this sort of structure, abide by the following two rules for better Search Engine Optimization (SEO):\\n\\nMake sure each dedicated event page have rel=\"canonical\" properly set to it\\'s own URL. You\\'ll find details on canonical here.\\nSince your event listing pages for locations & dates do not contain any additional unique information, there is no reason to index them for search engines. You may use noindex meta tag to prevent Search Engines from indexing them for search results. Search Engines will still find your events, but they\\'ll simply show the dedicated event pages in the Search Result.\\n\\nYou\\'ll find more information on noindex here and here.\\nThis way Search Engines will index all your unique event information without any duplicates.',\n",
       " 'Googlebot does not use cookies and therefore does not use sessions.   As a result, Google never sees the URLs for each town.\\nYou are on the right path creating a sitemap.   However, putting URLs into a sitemap is not sufficient to get them crawled and indexed.   See The Sitemap Paradox.\\nYou will need to find some way of linking to the town URLs from other pages on your site that Googlebot already accesses.   You should then also place links on each town page to some other town pages.',\n",
       " 'You are misunderstanding Yoast.   It is partly due to their terminology.   Instead of \"focus keyword\" think of it as \"focus phrase\".   You are targeting three different keywords that you think may be searched separately, but you are putting them into Yoast together.    Instead put them each in individually.   Yoast treats the words put in together as a phrase that users search for verbatim.\\nYou have the right basic approach for Google.   You are using keywords in the title and body in grammatically correct ways.   Hopefully in ways that sound natural to your readers and hopefully without repeating the words too often.\\nAll you need to do is use your SEO tools differently.',\n",
       " \"Other copies of your work can exist and you can still have great SEO.   As long as you are seen as the authoritative original source, your site will still rank and the content on the other sites won't show up in search results at all.\\nThere is a chance that when somebody copies your content that Google gets the sources all wrong and indexes the other site instead of yours.   One way to prevent that is by requiring all copies link back to the original.\\nThis site operates under that model.   All posts are licensed here under cc by-sa 3.0 with attribution required.  There is a blog post that says what attribution means.  It includes a link back to the original: http://blog.stackoverflow.com/2009/06/attribution-required/\",\n",
       " \"There are occasions where a large number of search engines will try to index your site at once. As has been mentioned in the comments this can be from one or more of the large number of ping tools on the web which tell all search engines out there that there is new content to be indexed.\\nYou have done the right thing in verifying the IP's as resolving to Bing and Google as that is the most important thing to ensure that the crawling is coming from a legitimate location.\\nIt isn't necessarily something to worry about, sometimes by pure chance a large number of search engines may try and crawl your site at the same time, especially if someone adds a new link to your site for a page on your site that the search engines don't know about yet.\",\n",
       " \"I think your question is open to too much opinion and interpretation. There is no definite answer for the simple fact that the domain itself expired. So, how could it have been hijacked? Well, if he registered your best option and advice would be to contact this person as directly as you can and I would avoid any email contact. Try to get their phone number. I would then call him and behave as nice as possible and explain the situation in a way that it is showing that you would really like their cooperation. Done. If he agrees it's all good, if not, then you would have to seek other alternatives based on your client`s interest.\",\n",
       " 'These are usually hits from bad bots.  Unfortunately, it is very common for bots to attempt to lowercase the entire URL.   I have a website with mixed case URLs.   I get thousands of hits per day for URLs that have been incorrectly lowercased.    Here are the top user agents that did so yesterday:\\n  20494 Mozilla/5.0 (compatible; Gluten Free Crawler/1.0; +http://glutenfreepleasure.com/)\\n    312 Mozilla/5.0 (compatible; GrapeshotCrawler/2.0; +http://www.grapeshot.co.uk/crawler.php)\\n    281 Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; .NET CLR 1.1.4322)\\n    252 Mozilla/5.0 (compatible; proximic; +http://www.proximic.com/info/spider.php)\\n     77 Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\\n     55 Mozilla/5.0 (iPhone; CPU iPhone OS 10_2 like Mac OS X) AppleWebKit/602.3.12 (KHTML, like Gecko) Version/10.0 Mobile/14C92 Safari/602.1\\n     20 YisouSpider\\n     15 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20120101 Firefox/29.0\\n     14 ADmantX Platform Semantic Analyzer US - Turn - ADmantX Inc. - www.admantx.com - support@admantx.com\\n     13 Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\\n\\nAs you can see, there are usually a couple big offenders, but I had 120 distinct user agents that hit all lower case URLs on my site yesterday.\\nEven Googlebot has gotten some requests in.   That isn\\'t because Googlebot itself has this problem, but because it finds all lowercase links somewhere on the web.   It isn\\'t exclusively a bot problem.   Some people lowercase the whole URL before they link.   Some scraper sites post lower case URLs.   There is even an occasional content management software package that won\\'t allow posting of mixed case URLs.\\nIn short, while mixed case URLs are allowed by the spec, in practice it takes extra work to support them.   Because it is such a common problem, you  should be put \"301 permanent\" redirects from the all lowercase versions to the mixed case version.',\n",
       " \"When Googlebot visits your site it sees both the source and the end results, markup plays little to no role in rankings because why should it; your visitors don't care about your markup and neither do search engines.\\nIt's important to note that older html documents you needed to use such HTML codes because the UNICODE didn't support the symbol. Now with HTML5 and onwards we don't need silly little codes.\\n\\n&#38; is exactly the same as &\\n&amp; is exactly the same as &\\n& shares the same meaning with and\",\n",
       " \"There are a lot of factors that went into it including:\\n\\nBrowser and server technology for security with virtual hosts.   You used to need a dedicated IP address per secure site, but that is not the case anymore using SNI.\\nLower cost and free security certificates.   Let's Encrypt now issues about half of all certificates for free.  Ten years ago I was looking at $300/year for a wildcard domain, but now even paid wildcard certificates can be as little as $70/year.\\nOverhead of HTTPS dropped significantly.   It used to require extra server resources, but now the overhead is negligible.  It is even often built into load balancers that can talk HTTP to backend servers.\\nAd networks such as AdSense started supporting HTTPS. A few years ago, it was not possible to monetize an HTTPS website with most ad networks.\\nGoogle announcing HTTPS as a ranking factor.\\nBig companies like Facebook and Google that moved to HTTPS for everything normalized the practice.\\nBrowsers are starting to warn about HTTP being insecure.\\n\\nFor big companies like Google that could always afford to move to HTTPS I think there were a couple of things that pushed them to implement it:\\n\\nLeaking of competitive intelligence data over HTTP.  I believe that Google moved to HTTPS in large part because so many ISPs and competitors were looking at what users were searching over HTTP.  Keeping search engine queries under wraps was a big motivation for Google.\\nRise of malware targeting sites like Google and Facebook.   HTTPS makes it harder for malware to intercept browser requests and inject ads or redirect users.\\n\\nThere are also some reasons that you are seeing HTTPS more often in cases where both work:\\n\\nGoogle is preferring to index the HTTPS version when the HTTP version also works\\nMany folks have the HTTPS Everywhere browser plugin that automatically has them use HTTPS sites when available.   That means that those users also create new links to HTTPS sites\\nMore sites are redirecting to HTTPS because of security and privacy concerns.\",\n",
       " \"no-cache is not as strong as no-store.    Chrome's documentation indicates that no-cache indicates a re-usable document, while no-store indicates that it should not be re-used.\\n\\nBased on this information, your Cache-Control header should simply be:\\nCache-Control: no-store\\n\\nThe extra values in it may be letting Chrome pick and choose which it would like to honor.\",\n",
       " \"Actually, the answer above is incorrect. You would need to read the question carefully. In this case, the title tag is too long and Google decided to use what it determines as a brand which is simply the domain name without the TLD.\\nI detailed the process in this answer: \\nMy title tag doesn't appear to be getting crawled by Google properly\\nBut I will paraphrase it for you.\\nFor a few years now, Google has been experimenting with the SERPs most notably since March 2014. As a part of this, the title tag under certain circumstances may be changed. One factor is the title length. As of this writing, if the title tag length is greater than 512 pixels in length Google may chose to use:\\n\\nWhat appears to be a brand name which can be the domain name with or\\nwithout the TLD. (most likely)\\nWhat it knows to be a brand name. (fairly likely)\\nThe first header h1 tag. (very likely)\\nA portion of the content that closely matches the search query.\\n(fairly likely)\\nInformation from a publicly available source such as ODP DMOZ (not\\nlikely).\\nInformation from rich snippets mark-up. (less likely)\\nText from anchors text. (less likely)\\n\\nIn the case of the OP's scenario, Google has chosen to use the domain name (IGN) as a brand name and return it as the SERP link. Managing the title tag length will solve this issue.\\nAlso, this answer may help too: Title in Google does not match <title> of document\",\n",
       " \"In addition to my update above, my experiment is now tracking correctly. The data took around 5 hours to come through so if you're setting one up, I recommend being patient!\",\n",
       " 'If the user-agent is Googlebot, then you can serve a different content, by hiding whatever you want. \\nif(strstr(strtolower($_SERVER[\\'HTTP_USER_AGENT\\']), \"googlebot\"))\\n{\\n    // remove the words\\n}\\nelse\\n{\\n   //normal text\\n}\\n\\nBut Google will not like serving different content, but if the difference is minor, there shouldn\\'t be any problem.',\n",
       " 'Ideally a 404 status would be used.  However, it is not uncommon for a 200 status in such cases. \\nIf the 9th page is the last page, ensure that it doesn\\'t link to page 10.   If there are no links beyond the last page, search engine bots are unlikely to a significant amount of crawling to out of range pages.\\nThere are some cases where bots will find out of range pages.  For example when the number of results decreases and a bot re-crawls a high numbered page that used to have results but no longer does.\\nIn such cases, some bots will be confused about the 200 status.  They do have to deal with this situation frequently though.  Many websites return a 200 status in a situation like this.   Google calls \"not found\" pages with a 200 status \"soft 404\".   Soft 404 detection algorithms rely on things like small page size and presence of the text \"not found\".    Using a single sentence like \"No results found\" as your content is likely to have the page viewed as a soft 404 by most search engine bots, even with a 200 status.\\nIn short, modify the response code to a 404 if you can do so without too much effort.  But if not, just print out \"No results found\" and search bots will mostly do the right thing.',\n",
       " \"In 2010 Google announced that page loading speed was a ranking factor.  As far as I am aware, Google has never put an official number on how  many seconds your site needs to load in.   Rather they say:\\n\\nCurrently, fewer than 1% of search queries are affected by the site speed signal in our implementation \\n\\nAfter this announcement, many web site owners tried to optimize for speed and reported how their rankings changed.  We found that:\\n\\nGoogle only penalizes very slow sites.   Their slow page penalty only applies to the initial page load (before images, and scripts).   If the initial page load is greater than seven seconds, Google will downgrade rankings.\\nOptimizing the site speed for users can have positive SEO implications.   This is probably due to usability ranking factors rather than to direct measurements of site speed by Google.  Getting the page to load above the fold and be ready to use in under 3 seconds can improve search engine rankings.   Further improvements can have positive user experience results (such as better conversions) but don't tend to lead to better search engine rankings.\\n\\nThe 2.8 seconds that you state is very similar to the 3 seconds that I have seen confirmed experimentally.    However, this has never been a statement directly from Google as far as I am aware.  Google will certainly crawl and index sites that take longer than 2.8 seconds to load.\",\n",
       " \"Prestashop has pretty good online documentation and below is a snippet taken from the documentation for version 1.6 and I'm sure that nothing major has changed since 1.6 > 1.7 regarding the database settings file.\\n\\nSOURCE\\nThere are situations where nothing happens when you request a new\\n  password, and it doesn't work. There can be a number of reasons why\\n  this would happen, but the important thing is to be able to log in\\n  again.\\nThis is will require you to access your database, using phpMyAdmin for\\n  instance.\\nYou will have to follow the following procedure:\\nOpen the /config/settings.inc.php file, from your shop's root\\n  directory\",\n",
       " 'In poring over the config files again, I see that both servers have identical httpd.conf files, but they include different files from the \"extra\" subdirectory to define the vhosts. (I\\'m based on XAMPP for Linux 1.8.2-6, which has httpd.conf in /opt/lampp/etc/ and a directory /opt/lampp/etc/extra/ containing various other conf files, including vhosts.conf).\\nThe common httpd.conf file has DocumentRoot \"/opt/lampp/htdocs\" and it contains roughly this near the top:\\n<Directory />\\n    AllowOverride none\\n    Require all denied\\n</Directory>\\n<Directory \"/opt/lampp/htdocs\">\\n    Options Indexes FollowSymLinks ExecCGI Includes\\n    AllowOverride All\\n    Require all granted\\n</Directory>\\nOn the server that gives a directory listing, the vhosts.conf file contains no Options directives at all. So it makes perfect sense that the Options directive from httpd.conf effectively enables Indexes.\\nOn the server that gives the Access Denied error, the vhosts.conf file contains these directives:\\n<Directory /opt/lampp/htdocs>\\n    AllowOverride None\\n    Options       FollowSymLinks\\n</Directory>\\n<Directory \"/opt/lampp/htdocs/xampp\">\\n    AllowOverride All\\n    Options FollowSymLinks Indexes\\n</Directory>\\n<Directory \"/opt/lampp/htdocs/test\">\\n    AllowOverride All\\n    Options FollowSymLinks Indexes\\n</Directory>\\n<Directory \"/opt/lampp/phpmyadmin\">\\n    AllowOverride All\\n    Options FollowSymLinks Indexes\\n</Directory>\\n\\nI think what\\'s happening is that because none of the Options directives contain \"+\" or \"-\" prefixes on their attributes, the Options FollowSymLinks directive in the first directory section of this file has the effect of cancelling all the other attributes (including Indexes) from the first Options directive in the httpd.conf file!\\nAnd in fact I just \"proved\" it by editing the vhosts.conf file on the server that gave the directory listing, to add:\\n<Directory /opt/lampp/htdocs>\\n    Options       FollowSymLinks\\n</Directory>\\n\\nThat one change (and running lampp reloadapache) caused that server also to give the access denied error, just like the other server.\\nAlthough I could leave things this way, I think I\\'ll change all three configuration files to start out with Options None for \"/\" and Options +FollowSymLinks +Indexes +ExecCGI +Includes for \"/opt/lampp/htdocs\" in the httpd.conf file. And in the vhosts.conf files I\\'ll put:\\n<Directory /opt/lampp/htdocs>\\n    Options       +FollowSymLinks -Indexes -ExecCGI -Includes\\n</Directory>',\n",
       " 'There is no SEO Risk from temporary errors on your site.  Google is very forgiving of small amounts of down time.   Down time lasting less than a day will generally not hurt your site from an SEO perspective. When Googlebot encounters a 404 or 500 error on a page that it has found in the past, it tries again for 24 hours before removing the page from the index.  \\nA few milliseconds of down time is not going to matter in the least.',\n",
       " \"RedirectMatch 301 ^/([^/]+)/$ http://www.example.com/$1\\n\\nThe pattern ^/([^/]+)/$ doesn't match the URL-path /2017/01/30/sample-post/, so these URLs will not be redirected. However, it does match /sample-post/ - which is why you get the redirect loop.\\nHowever, RedirectMatch is also a mod_alias directive, not mod_rewrite. (RewriteEngine does not apply.) You should change this to a mod_rewrite directive to avoid potential conflicts and unexpected results. (Different Apache modules execute at different times, despite their apparent order in the config file.)\\nSo, try the following instead:\\nRewriteRule ^\\\\d{4}/\\\\d\\\\d/\\\\d\\\\d/(.+) /$1 [R=302,L]\\n\\nThis specifically matches a URL of the form /2017/01/30/sample-post/ (the trailing slash is not enforced). Your redirect would have actually stripped the trailing slash.\\nMake sure your browser cache is cleared before testing. Change the 302 to a 301 only when you are sure it's working OK. (302s avoid being cached.)\",\n",
       " 'MX Record will not automatically send email from your server. You need to make some CODE change. Check this answer if you want to do it with PHPMailer.\\n$mail = new PHPMailer;\\n// Tell PHPMailer to use SMTP\\n$mail->isSMTP();\\n// Enable SMTP debugging\\n// 0 = off (for production use)\\n// 1 = client messages\\n// 2 = client and server messages\\n$mail->SMTPDebug = 2;\\n// Ask for HTML-friendly debug output\\n$mail->Debugoutput = \\'html\\';\\n// Set the hostname of the mail server\\n$mail->Host = \\'smtp.gmail.com\\';\\n// use\\n// $mail->Host = gethostbyname(\\'smtp.gmail.com\\');\\n// if your network does not support SMTP over IPv6\\n// Set the SMTP port number - 587 for authenticated TLS, a.k.a. RFC4409 SMTP submission\\n$mail->Port = 587;\\n// Set the encryption system to use - ssl (deprecated) or tls\\n$mail->SMTPSecure = \\'tls\\';\\n// Whether to use SMTP authentication\\n$mail->SMTPAuth = true;\\n// Username to use for SMTP authentication - use full email address for gmail\\n$mail->Username = \"username@gmail.com\";\\n// Password to use for SMTP authentication\\n$mail->Password = \"yourpassword\";\\n\\nHowever, that solution has some problems of its own. The best way is to properly setup your mail server using DKIM and other settings, so that your emails don\\'t go to spam.',\n",
       " 'So you have three types of pages of which you want two to appear in the search engine results.\\nTherefore you can use rel=\"canonical\" link elements in order to tell the search engine which page of two pages it should index and serve as a result and which one it should skip.\\nThe rel=\"canoncal\" attribute has to be placed in a <link > element in a HTML document\\'s <head> section:\\n<html> …\\n  <head> …\\n   <link rel=\"canonical\" http://www.example.com/file/xyz-assembly.pdf\" >\\n   …\\n   </head>\\n   …\\n</html>\\n\\nFor your setup this means:\\n\\nhttp://www.example.com/antique-xyz-game-table-manuals\\n\\nThis page should be indexed and ranked. No need to specify duplicates or alternate versions, as long there are none of them. Basically it is a good practice to mark up these page with a rel=\"canonical\" to themselves to avoid duplicate content issues with URL variations.\\n\\nhttp://www.example.com/file/view/f?=xyz-assembly.pdf\\n\\nThis page is a duplicate of \\n\\nhttp://www.example.com/file/xyz-assembly.pdf\\n\\nAs you only want the PDF file to rank in the SERPs you make use of rel=\"canonical\" to the PDF document (despite the reader\\'s menu the documents are identical).\\nTo speak more generally:\\n\\nEach overview page has a self referential canonical link element in the <head> section of it\\'s source code.\\nEach viewer page has as canonical link element pointing to the real PDF document in the <head> section of it\\'s source code.\\nThe alternate Links are not needed in terms of SEO.\\n\\nFor your specific Questions\\n\\nOn the direct link in Page B, to the actual PDF, Should I be using a\\n  rel=\"canonical\" meta tag, and if so, should it point to the actual\\n  PDF or should it point to the page A? \\n\\nSpecify the rel=\"canonical\" link in the <head> section of the viewer page pointing to to the actual PDF. \\nIf you cannot access the HTML source of the viewer page you may set up a \"canonical\" header for viewer pages pointing to the actual PDF file. (For more detailed information on how to implement \"canonical\" headers see: How To: Advanced rel=\"canonical\" HTTP Headers (Moz.com)\\n\\nOn Page A - should there be any rel attribute on the link itself?\\n\\nNot for SEO reasons.\\n\\nAny other SEO factors I should consider with this type of set-up?\\n\\nMaybe, but it depends on further information to judge this :)\\nAs you have direct links to the PDF files you may think of using optimized anchor texts. F.e.: \"Assembly Guide for Table Type XYZ (PDF)\" instead of \"xyz-assembly.pdf\". Make sure the PDF link is the first to be crawled by the search engine.\\nMake sure you do not mix \"noindex\" and \"canonical\"! maybe you think of marking the viewer pages as \"noindex\" in order to keep them out of the search engine\\'s index. This would hurt the canonical set-up.\\nIn order to save crawling resources you may set up your overview page in a way that it only serves links to the actual PDF files ind leverages Java Script or something alike to enable the viewer mode. This would avoid search engines crawling the viewer pages. But you would stay with the canonicalization of viewer page and PDF file, as users may link to the viewer URL from outside of your page.',\n",
       " \"A canonical is meant to handle duplicates. Differing telephone numbers, price tags or alike make two pages unique, even though they ma not be interpreted as high quality content.\\nAs far as I understand you have a kind of an overview page A describing a service/busines. \\nThis page links to pages b, c, and d. \\nAll of them contain either 80% of page A + 20% unique content OR page A contains 80% content that is assembled by the contents of pages b, c, and d.  \\nLet's have it a bit more visual: the colours represent content. Same color means, same content. \\nIn either cases a canonical is not the weapon of choice as it only indicate a relation like d == e.\\nPage A is unique. Also are pages b, c and d. Only e (built just for clarification) is an exact copy of d and therefore we can use a canonical. The decision in which direction it has to pint ( e > d or d > e is up to you)\\n\\n[…] we think it would be appropriate to have rel=canonical on each child point to the parent.\\n\\nIf you put a canonical on b, c and d pointing to A only A would be interpreted as the unique page that should be indexed and ranked.\\nFurther, the 20% unique content on b, c and d that differs on all pages may lead to the canonical being ignored.\\n\\nI want to make sure the children pages are indexed and searchable via search engines as well because they'll have important info such as each locations address, phone, and unique stats.\\n\\nTherefore you do not need a canonical you just have to make sure they can be crawled and indexed by linking them properly.\\nYou can improve the importance of all pages by adding further individual information. At the end of the day this is the way a lot of category/product pages in shops are structured.\",\n",
       " 'Google Analytics only allows two dimensions on standard reports at any one time.   You are asking for a report with three dimensions (OS, Browser, Browser Version).\\nStarting with the \"Operating System\" report (from \"Audience\", \"Technology\", \"Browser & OS\"), you can add a secondary dimension of either \"Browser\" or \"Browser Version\".   \\n\\nWhile neither of those reports do exactly what you want, the latter is pretty close because there is little overlap between browser versions between browser.  I know that \"39.0.2383\" is a Chrome version, \"11.0\" is IE, and \"34.0\" is Firefox.\\nYou can make it work with a custom report.   Click \"Customization\" and then \"+ New Custom Report\" from where you can build a report with multiple dimensions like this one:\\n\\nThen it ends up looking like this:\\n\\nI really wish that Google Analytics had a \"Browser and major version\" dimension concept so that you could actually see \"Chrome 39\" as one entity and as a single second dimension under operating system.',\n",
       " '...a 301 redirect from the GUI offered by my web hosting company ... I have to keep product_category1.php file in the public_html folder in order for the redirection to happen, otherwise the only redirection that happens is to the 404 page.\\n\\nIt sounds like the redirect has been implemented incorrectly (or rather, in the wrong place in your .htaccess file) by the hosting GUI. You should not need to keep the old .php files from which you are redirecting (although this makes no difference from an SEO point of view - it\\'s being redirected).\\nIt is a common problem with using a 3rd party tool (hosting GUI / cPanel for example) to implement redirects when you have an existing system in place. WordPress itself places directives in your .htaccess file (Apache per-directory config file) and the \"hosting GUI\" is probably doing the same when it implements the redirect. Without knowing exactly what your current directives are doing (which your \"hosting GUI\" probably does not) it is difficult to reliably make any \"automatic\" changes.\\nMy guess is that the redirect directives have been placed after the existing WordPress directives by the \"hosting GUI\". Whereas they would need to be placed before the existing WP directives.\\nIf the redirect directives are placed after the WP directives then they will only be processed if the file actually exists, because WP routes all non-existent files and processing stops there. But if the file doesn\\'t exist then WP routes the URL and you get the 404, because the old URL is not a valid WP URL. This seems to be what is happening here.\\nIf you add the contents of your /.htaccess file to your question then we can verify this.\\n\\nWill they [vanish] if they always get redirected, even though they still exist in the public_html folder?\\n\\nYes. Eventually the old URLs will disappear from the SERPs if they are 301 redirected. From a search engine / end user point of view, it makes no difference whether the original file still exists or not. It is redirected, so they have no way of knowing.\\n\\nIs there a way to do the redirection without keeping the files?\\n\\nYes, as mentioned above. It would only seem to be because of a conflict with existing directives that you are getting this seemingly bizarre behaviour. Taken out of context, you would get a puzzled look if you said to someone that the file needed to exist before you could redirect it!\\n\\nOr should this be dealt in a completely different manner in order to have maximum SEO and visibility for correct URLs in the new website?\\n\\nA 301 redirect is all that\\'s required. However, it should have been implemented in the very beginning, as soon as the URLs changed, in order to have had \"maximum SEO\".',\n",
       " \"(Posting a copy of my answer on Server Fault, per OP's request.)\\nI think what you want is AuthzSendForbiddenOnFailure:\\nAuthzSendForbiddenOnFailure On\\n\\nContext: directory, .htaccess\\nIf authentication succeeds but authorization fails, Apache HTTPD will respond with an HTTP response code of '401 UNAUTHORIZED' by default. This usually causes browsers to display the password dialogue to the user again, which is not wanted in all situations. AuthzSendForbiddenOnFailure allows to change the response code to '403 FORBIDDEN'.\\n\\nNote that it carries a security warning:\\n\\nSecurity Warning\\nModifying the response in case of missing authorization weakens the security of the password, because it reveals to a possible attacker, that his guessed password was right.\",\n",
       " 'You can set these HTTP response headers in your server config (or .htaccess file). For example, to apply the X-Content-Type-Options HTTP response header only to .css and .png files, try the following:\\n<FilesMatch \"\\\\.(css|png)$\">\\n    Header set X-Content-Type-Options nosniff\\n</FilesMatch>',\n",
       " 'Directory sites is not consider as link farm unless you can\\'t moderate it very well. To make your directory site high quality, allowed only trusted website to make their profile, for example dmoz does.\\nIf your directory sites maintained very well, then I will say don\\'t use nofollow links also don\\'t use redirection (Which is blocked under robots.txt, for example domain.com/recom/some-website/ redirect to some-website.com, and your competitor might block /recom/ direcory in robots.txt, so no juicyrank is pass to them, but you will also not get that juicyrank, it will be wasted ).\\nThe directory sites are ranking because of outbound links. There are so many patterns(this and this, also many more which I can\\'t find right now) say that, actually natural outbound links help in ranking. When you use nofollow links into it, Google will simply avoid them while calculating any link metrics value, because it is made for spam prevention. \\nJust search on Google \"thesauri reference\" and you will see dmoz is ranking on top 3 position. It is because the listed website in dmoz article is actually trusted, and high quality. Here dmoz does very fantastic job, because they allowed to add only high quality website, and those website is consider as high quality because many of website also links to them. But if they allowed to add all kind of website on their directory, then Google will consider them as link farm. \\nSo, It\\'s all about whom you link them. If you link to good, you will actually get benefits, if you link to bad, you will get penalty. But if you don\\'t have to time to consider which is good and which is bad, then simply use nofollow link or redirection link which is blocked under robots.txt. Both will help you to prevent link spam. And both will do same job.',\n",
       " \"I have some bad news for you. StartSSL's certificates are no longer trusted by Chrome, Firefox, and soon other browsers, beginning with newly issued certificates first. StartSSL won't tell you this of course and will happily sell you new certs, continuing their extremely shady pattern of behaviour.\\nAt this point all I can recommend is damage control by purchasing another wildcard cert (assuming you won't/can't use Certbot?) from somewhere like cheapsslsecurity.com. No affiliation, just a previous customer and they were cheap and easy to use.\\nYour new certificate is no good any more, and you must replace it.\",\n",
       " 'First Analyze where those 16 users came from.  If they are legitimate SEO traffic you will want to build more article, posts, etc to continue to feed that niche.  You will need to ensure analytics is setup and analyze the data coming in.  You will also want to make sure you are focusing on internal linking and backlinks to your website from reputable sources.  There are many thin websites that have ranked very well with great authoritative links. \\nSomethings to consider:\\n\\nFocus on the load speed \\nInstall an SSL Certificate If you can.\\nEnsure the IP address of your website is unique or not saturated with other websites.\\nEnsure your sitemaps are setup correctly so search engines know about the pages that exists on your site.\\nEnsure your anchor text is unique and connects to keywords of pages you want to rank for.\\nMake sure your robots.txt file is setup correctly.  Since you built the theme yourself you maybe missing some of the core files that search engines bots needs to index your site properly.\\n\\nIf you have gotten traffic ultimately you will want to duplicate the process.  Data is your best friend.  The more you dive into your analytics data the better you will get at spotting trends and attacking your seo from there.',\n",
       " 'You can force a reindex of your website by using Google Webmaster tools > fetch as google.  You can submit up to 500 Individual pages manually to be crawled and 10 full crawls from the main index page which is preferable.  This will speed up the process of updating all of your information.  301 Redirects will take a while to find, or you can demote the pages that no longer exist in google webmaster tools.  (Not suggested)  Never remove old pages if they still exists.  You only remove pages that are being redirected.',\n",
       " \"Generally Schema can take several months to appear in the search results, once its been first discovered. If you have 700 profiles then you need to do the math and see if Google has discovered them all in Web Master Tools, if it hasn't then it will take much longer for Google to display those in the results. \\nThin content is considered less important and therefore given less priority\\nProfile pages are generally considered low-quality to Google and other search engines unless they have unique content, are freshened once in a while and have around 200-300 words on the page. If the they are low quality then it can take months to have Google revisit those pages to discover the markup, Google may never return unless you force it hands. While Google has an insane amount of resources its sadly not unlimited, sites are prioritised on importance and quality.\\nSchema markup is sometimes is disabled in search results\\nIt's NOT UNCOMMON for Google not to be display SCHEMA in all serps, its pretty common and just because you have the markup doesn't mean Google will display it. Google can select pages it believes is more important and others, and enable SCHEMA in the results, lower quality pages it may not, also page content and niche plays a huge role in what gets returned or not.\\nSchema is automated and the only way to get it to appear quicker is to have those pages discovered, if they have been the its out of your hands. They will show in time and if they don't then its because Google has decided not to return SCHEMA in the search on that practical page which could be for many unknown reasons.\",\n",
       " 'The specifics of Rel next and Rel Prev can be found on the google blog here: https://webmasters.googleblog.com/2011/09/pagination-with-relnext-and-relprev.html \\nThe Rel next and rel prev go in the header.\\nEdit:  Php is fully supported on any aspect of an dynamic page.  Simply doing something like \\n<?php echo \\'https://yourdomain.com/\\'.$yourdynamicinfo; ?>\\n\\nwould work perfectly fine in your html head.\\nIt also states that the code must use the\\n<link rel=\"next\" must be used in the header.  \\n\\nIt is not like a \\nrel=\"no follow\".\\n\\nAs for indexing properly, if google says it is looking for a specific tag, it is best to follow the guidelines they have in place for a tag.  As it won\\'t hurt you SEO wise, it will also not benefit you either.  Also please be aware that post was from 2011 so it is quite old.  \\nUltimately this is to help bots understand the layout of your website and should not affect your rankings overall.',\n",
       " \"You point out that a small but significant proportion of your users are using out-dated browsers. You are never going to be able to get all users to upgrade to the latest version but you can establish a phase out period during which you can detect the security mechanisms supported by the browser. If the browser doesn't support modern technologies you can output a banner message to the browser above the site telling the end user that on such a date support for their browser will end and they need to upgrade if they wish to continue to access the site. After that date just switch off SSL3 and TLS1. After that point if a user connects with an unsupported browser they will get a server generated error message.\\nThere is not much you can do to filter those connections before the fact as SSL is done prior to htaccess or application level rules. If you need this done at the server layer then there will be the trade off of blocking some users.\",\n",
       " 'In order to minimize the reduction in ranking you will ideally keep the changes to the original site minimal. So the way that you would do this is copy certain pages from the original site to the new dogfood or catfood site and add a 301 redirect to the original page address in your htaccess file. By doing this when users connect to the original page they will be directed to the appropriate dogfood or catfood site. This has the advantage of ensuring old links will still map to the appropriate pages on the new sites. You will see a drop in SERP for the original site for the keywords that have moved to the new product specific sites and this is to be expected. The trick here is to forward the old connections but maintain enough content on the original site in order to maintain a reasonable SERP ranking while building up organic links to the new pages on the dogfood and catfood sites.',\n",
       " \"Firstly do away with the purchased public IP's. Unless you are wanting people from outside your network to access the servers there is no need to do anything like that.\\nWhat you need to do is define a static IP on each of your VM's. Choose one to be your DNS server and configure your router DHCP so that it sets the DNS server to the DNS VM's ip address. Then in that DNS you can define whatever domain addressing you want (best practice is to keep away from the public routed domains like .com, .net, etc.\\nIf you do want these VM's connected to from outside your own network then what you need is a router equipped with what's known as Network Address Translation (NAT) and what this does is convert the public IP address to a local IP address and route the connection that comes in on a particular public IP to the appropriate server based on private IP. This can be exceedingly difficult and is beyond the scope of webmastering and goes more into systems administration.\",\n",
       " 'Your meta description and meta title are written wrong. In order to be valid opengraph you need to use the property attribute not the name attribute. So in order to fix this all you need to do is change header2.php to...\\n<?php \\necho \"<meta property=\\'og:description\\' content=\\'$description\\' />\"; \\necho \"<meta property=\\'og:title\\' content=\\'$title\\' />\"; \\n?>\\n\\nUpdate #1\\nA quick check of your site shows that the og:title, twitter:title, og:description, and twitter:description tags are empty. These must contain the appropriate text and can\\'t be blank. You are also missing the twitter:site and twitter:creator tags which will cause a validator problem next and will need to be added as they are required.\\nWhat you effectively need in your head section is a block that looks like this...\\n<meta property=\\'og:description\\' content=\\'<?php echo($description) ?>\\' />\\n<meta property=\\'og:title\\' content=\\'<?php echo($title) ?>\\' />\\n<meta name=\"twitter:card\" content=\"summary_large_image\">\\n<meta name=\"twitter:title\" value=\"<?php echo($title) ?>\" >\\n<meta name=\"twitter:description\" value=\"<?php echo($description) ?>\" >\\n<meta name=\"twitter:site\" value=\"Site Name\" />\\n<meta name=\"twitter:creator\" value=\"<?php echo($author) ?>\" />\\n\\nThe twitter:site meta tag should be manually defined as it remains the same across all pages (It is just the name of your site). The creator meta tag can be either manually defined as the site name as well if you want all content to show as being authored by the site and not a specific user or you can dynamically echo it out by specifying the $author PHP variable to allow you to specify the actual author of the articles.\\nNB: The code you supplied was missing the twitter:site and twitter:creator meta tags which are required by Twitter.',\n",
       " 'There is no real way to get around these laws. The purpose of these laws is so that there is a physical address at which you can have snail mail sent if someone chooses to send a letter saying they want to be removed from your mailing list. Other than getting a PO Box or providing your personal address the only other way to do this is to work with a legal firm or a mail clearing house company (such as a virtual office company) that provides you with a physical postal address and can relay any deliveries to that address to you electronically.',\n",
       " 'Dumping Data is a big no no just in general.  Sure a couple hundred lines of plain html text will not be a problem, but imagine scaling it to thousands and thousands of html data. A computers web browser was not designed for data dumping.  It was designed to display content in spurts. Take the example of JavaScript in the browser.  Sure animations and advanced renderings look great on a decently powered computer, but ever try to run it on a subpar computer like a google chrome laptop.  The javascript runs super slow or crashes the browser completely.  Go for a balance of Performance and User Interface design.\\nAs for the seo side of it, search engines like google look at relevance and quality more then ever.  That is why you no longer see results for pages with page numbers in google search results aka pagination pages.  Since the pagination page does not provide much relevance to answer a query, search engines will usually just use these pages to drive to the pages that add value to search which is your content.  So Your pagination results page will probably never rank the way you expect, unless you highly optimize the pagination pages as if they were content pages, which takes a huge effort on your side to pull off, but is possible, but probably not worth the time.\\nAs for Google Crawlers and content, the crawlers have set limits on how long they will spend on a page so you may actually be hurting yourself because the crawler may never get to the bottom of the results page with the data dump of links.  So you may find not all your pages are indexed correctly.\\nSo the advice for you would be to focus on the User Interface and the actual content that will actually drive results.  Also images and other interface elements being left out from your link dump may actually hurt your chances to help build relevance to the articles you intend to rank for.\\nSo in short, follow the pagination of popular websites, unless you feel you have a method that will truly help your site SEO Wise.',\n",
       " \"There are two possibilities that I am taking from your question so I will cover them both.\\nMake http://111.111.111.111:8080/website Appear as http://www.domain.com:8080/website\\nIn this instance all you need to do is setup a standard DNS record pointing www.domain.com to your server's IP address. The important thing to note here is that DNS just translates a domain name to an IP address, the port number and file path must be in the address bar as well.\\nMake http://111.111.111.111:8080/website Appear as http://www.domain.com\\nIn order to make this work you need to setup a proxy server. A proxy server works by translating the URL you are requesting into something else. Commonly used to provide external access into internal company sites (such as allowing you to access intranet.local externally of the local network with the address intranet.domain.com). This would allow you to hide the IP address, port number, and even the site specific path (so that www.domain.com/about/something.html would allow access to http://111.111.111.111:8080/website/about/something.html through the proxy). The difficulty here is securing the proxy environment. A poorly secured proxy server can be a huge security hole into your local network, and can also be used as a launching point to attack or spam other websites and appear as though to be coming from your network. This can get extremely complicated to do and is not for someone without experience in network security and at least an awareness of network protocols. The vastly better solution if possible would be to change the port that serves the HTTP requests from 8080 to port 80 which is the standard HTTP port and so can be ommitted from a HTTP request as no specified port in HTTP is taken to mean port 80.\",\n",
       " \"Given some inherent security issues with SSL it tends to be best to disable SSLv3 and lower and stick to TLS 1.0+ however if there is a high chance of substantially old browsers connecting that only support SSL3 then the only real choice is to enable SSL3 as well. This really is something better checked with your government's IT services department to check what the regulatory and organisational requirements are within your government.\",\n",
       " \"Can I just send pre-rendered HTML, using phantomjs or other headless\\n  browser, to the google bot?\\n\\nIn short... no you shouldn't. Google's stance on differing versions for visitors and search bots is clear and regardless of your intentions it is classed as a cloak. Google also goes to great lengths to activity look for sites that specifically treat the Googlebot differently to other website visitors. \\nAll sites should render the same with both Googlebot and Users\\nEvery site regardless of the JavaScript should be render the same by both users and search engines, if it does not then there is something wrong with your site. \\nEmulating what Googlebot sees...\\nYou can emulate and render what Googlebot sees when visiting your site by using the Google's Fetch Test with Google search console, with the results provided you can debug your site and ensure that its read and renders as expected  for both search bots and users.\",\n",
       " 'This decision has nothing to do whether it is a YouTube channel, and nobody can answer this without knowing the contents of those two assets.\\nYou can think of a Google Analytics property to be just that: a property, a discrete thing you own. \\nLet’s imagine somebody offered you money for either your website or your YouTube channel. Would it really only make sense to sell the two things as a package? If so, put them in the same Analytics property.\\nThis is why options like “Industry Category” are tracked at the property level in analytics: you might have a side business selling widgets, and other one about collecting sprockets. These would be separate properties, and each could contain whatever sites you’d created for them.',\n",
       " 'You can enter up to 5 different query parameters for the site search:\\n\\nIn the Query Parameter field, enter the word or words that designate an internal query parameter, such as \"term,search,query\". Sometimes the word is just a letter, such as \"s\" or \"q\". Enter up to five parameters, separated by commas. \\n\\nCf. https://support.google.com/analytics/answer/1012264?hl=en',\n",
       " 'May this lead to Google penalizing my 2nd level domain for content duplication?\\n\\nJust to clarify, there is no \"penalty\" as such for duplicate content, it\\'s just that your ranking will be divided between the two domains if both are returned in the SERPs. Not only could this reduce the ranking for your primary domain, the 3rd level domain could even rank higher and result in confusion for your users.\\nHowever, Google is pretty good at deciding for itself which is the canonical domain so the above scenario may not cause you much of a problem. But you can bet that Google will find the 3rd level domain and this will be searchable.\\nSo, steps should be taken to avoid the duplicate content and to avoid the 3rd level domain being indexed:\\n\\nIf you can, you should set up a 301 redirect from the 3rd level domain to your primary domain. eg. Using .htaccess on Apache. However, this might not be possible on your \"free\" host. As a fallback, you could perhaps implement a JavaScript redirect instead.\\nUse a rel=\"canonical\" element in the head section of your HTML document that points to the primary domain.\\nUse absolute URLs/links throughout your site. Or, root-relative URLs and a base element in the head section that points to the primary domain.\\n\\nAlso note that there can be other issues with free hosts/subdomains. For example:\\n\\nFree subdomain not indexed',\n",
       " \"You could actually remove .html from the files on your server and set settings in htaccess so that they get served up as html files, but that's probably not what you're looking for.\\nDo this :\\nRewriteEngine on\\n# Check this condition to ensure that it's not a directory.\\nRewriteCond %{REQUEST_FILENAME} !-d\\n# Check this condition to ensure that there's a file there\\nRewriteCond %{REQUEST_FILENAME}\\\\.html -f\\n# Replace html with your file extension, eg: php, htm, asp\\nRewriteRule ^(.*)$ $1.html\",\n",
       " 'Here\\'s the setup you want:\\n\\nAn \\'A Record\\' pointing to your VPS server IP. In this case, example.com. Or: \"A Record | @ |  VPS IP address\"\\nA Cname record stating the subdomain (www) is the same as the root version (@). Or: \"Cname Record | @ | www.example.com\". This means when someone looks up \\'www.example.com\\' it redirects them to the A record for \\'example.com\\'\\nRemove other A or Cname Records with @ or www in them as a \\'Host\\'. These are conflicting with the first two directives.\\n\\nNow make sure your host is configured to respond to both www.example.com and example.com in it\\'s config. Redirect the example.com requests to www.example.com using a 301 redirect in Apache/Nginx/IIS. An example Apache setup using the rewrite module would be:\\nRewriteEngine On\\nRewriteBase /\\nRewriteCond %{HTTP_HOST} !^www\\\\. [NC]\\nRewriteRule ^(.*)$ http://www.%{HTTP_HOST}/$1 [R=301,L]\\n\\nIt\\'s generally a bad idea to cname your root (@) domain if you\\'re using subdomains. This could save you a headache in the future. You can also set up two A records (one for www, one for @) but then when you change the IP it means changing both. A Cname makes it look up another record - so you only need to change one. This is just good practice but your method of setting two A records works fine.',\n",
       " \"[I'll only speak to the event handling aspect of GTM, but you can find many more resources about GTM in general]\\nHow come you are not doing this with the built in click listener (and possibly setting your trigger to fire when the Click Classes variable equals sign-up-action)? That may be easier than having to push to the dataLayer. \\nBut anyway, if you absolutely must use a dataLayer push, then read on. Part of how GTM works is by listening for certain things to happen via the dataLayer such as clicks, link clicks, page loads, form submissions, and custom events. When any of these happen, an event gets pushed into the dataLayer, eg. gtm.dom for when the DOM has loaded, or gtm.click when the mouse is clicked on the page, or in your case when the signup-btn-click event gets pushed. When any event happens via the ever-magical dataLayer, GTM can react and fire off  tags. In your case, you have defined the custom event, but you haven't defined the tag to fire yet. So what you'll need to do is to create a new event tag, populate it with appropriate event parameters, and, here's the key part, enable it to fire based on a trigger of signup-btn-click. This last part is important. And that's all there is to it. Once you've done that, you can add other fancy parameters to the tag, like custom dimensions or metrics and such.\",\n",
       " \"As you noted, be straight-forward.\\nOutreach to relevant sites, visit the sites, follow any instructions, don't use a subject line that tricks the user into opening, use your real email, use your first and last name.\\nHaving an email address through the domain you are doing outreach for goes a long way, too.\\nCan't hurt to include links to a couple of your social profiles (or your @username).\\nI've gotten good results from being helpful by pointing out broken image links or misspellings or anything else that shows you spent some time on the site.\\nMeaningful comments on their blog is a great way to warm them up to you.\\nOh and proofread, the stilted language and misspellings in that email are a dead giveaway for outsourced outreach.\",\n",
       " 'By looking at your data, it appears to fall inline with the console update posted in your screen shot.  It looks like the data dipped at the same time google did a console update.  May want to investigate and see if that data anomaly is associated to your dip as it seems very coincidental that the graph was effected around the same time the console had an update which shows your dip.',\n",
       " \"Spammers will do anything to see if they can exploit the system to make money. They could be exploring your site looking for weaknesses. They could be looking to sell their accounts if they discover they have lots of value. They may be following those links blindly because the bot doesn't know any better.\\nSince they can automate the whole process spammers and other sinister parties aren't concerned whether their actions make a whole lot of sense. It costs very little for their bots to run and as long as the net result of their work is profitable they're not going to optimize their bots actions to avoid sites like yours.\",\n",
       " \"I believe your problem is you're rewriting to a file that doesn't exist.\\nYou'll need to find the non-rewritten URL.\\nFor example:\\nRewriteRule ^floor-plans$ index.php?option=com_content&view=article&id=54\\nTo find this you could try turning off Search Engine Friendly (SEF) URL's, open the page and check the URL.\\nYou can also read more about SEF URL's here if you wish.\\nhttps://docs.joomla.org/Search_Engine_Friendly_URLs\",\n",
       " 'I now want to make my old urls of the same format so that the older urls use the custom post types \"items\".\\n\\nAssuming you have already changed the URL structure in WordPress then in order to redirect the old URLs (for the benefit of search engines and user\\'s bookmarks) you may be able to get away with just modifying your existing redirect (since it\\'s \"only\" been 10 days since that was implemented), for example:\\nRewriteRule ^\\\\d{4}/\\\\d\\\\d/\\\\d\\\\d/(.+) /items/$1 [R=301,L]\\n\\nHowever, ideally you will need to redirect both /2017/01/30/sample-post/ (the \"very old\" URL format) and /sample-post/ (the now \"old\" URL format), particularly if your site gets significant traffic.\\nThis is made a little more complex because /sample-post/ could be anything (we can\\'t check whether \"sample-post\" is a valid WordPress page from .htaccess), but we can check if the URL starts with /items/ or not. So, for example, try:\\nRewriteCond %{REQUEST_URI} !^/items/\\nRewriteRule ^([^/]+)/$ /items/$1/ [R=301,L]\\n\\nThe RewriteRule pattern only matches a single path segment and the condition makes sure this is not /items/. (The trailing slash is currently enforced.)\\nThis can be used together with the above redirect. The order should not matter.\\nYou will need to make sure your browser cache is clear before testing, since the earlier 301s will have been cached by the browser. (They will also have been cached on your user\\'s machine - but you can\\'t do much about that unfortunately, since these are \"permanent\" redirects after all; they are not meant to change.)\\nAside: You only need the RewriteEngine directive once in the file. It can occur multiple times, but the last instance wins and controls the entire file. Ideally, if you are hand coding your directives, you would just have one RewriteEngine On directive at the top of the file - logical, easier to read, less prone to error, etc. However, when you have different \"plugins\" writing to the .htaccess file, often the case with WordPress, it is not always practical/possible.',\n",
       " \"Let's Encrypt is a free SSL made by Linux Foundation\\nThere are already some panels that automatically install it in your site, like VestaCP, which also is a free panel\\nYou can also install it manually using certbot as long as your hosting provides you SSH access.\\nConcrete steps (added by baszero)\\nThis is what I have done (Web Application running on Tomcat, running on Ubuntu 14). The solution is free of charge and the resulting HTTPS access is fully trusted and secure (marked as green HTTPS in Google Chrome):\\n\\nLogin to Ubuntu Server via ssh\\nwget https://dl.eff.org/certbot-auto\\nchmod a+x certbot-auto\\n./certbot-auto\\n./certbot-auto certonly (choose option 2, install manually, not via webroot)\\ncd /etc/letsencrypt/live/YOURDOMAIN/\\nopenssl pkcs12 -export -in fullchain.pem -inkey privkey.pem -out le-pkcs.p12 -name CERT_ALIAS_NAME (it asks for password, remember it!)\\nkeytool -importkeystore -destkeystore le-keystore.jks -srckeystore le-pkcs.p12 -srcstoretype PKCS12 -srcstorepass PASSWORD -alias CERT_ALIAS_NAME\\n\\nAs a result you get the le-keystore.jks file which you can use from tomcat as usual. The HTTPS is now fully trusted and secure.\\nSources: \\n\\nhttps://certbot.eff.org/#ubuntutrusty-other\\nhttps://maximilian-boehm.com/hp2121/Create-a-Java-Keystore-JKS-from-Let-s-Encrypt-Certificates.htm\",\n",
       " \"I found the answer to my question by using the AdSense policy violation appeal troubleshooter.   It says:\\n\\nIf a site or URL displaying your ad code is not in the Allowed sites list in your account, no further action is needed and this issue won't negatively affect your account in any way.\\n\\nI'm still not sure why Google would send out a worrying notice at all in this case, but it is good to know that I don't need to do anythng and that it won't hurt my account.\",\n",
       " \"I found that this is against Google Guidelines and shouldn't be done.\\nhttps://support.google.com/webmasters/answer/66353\\n\\nHiding text or links in your content to manipulate Google’s search\\nrankings can be seen as deceptive and is a violation of Google’s\\nWebmaster Guidelines. Text (such as excessive keywords) can be hidden\\nin several ways, including:\\n\\nUsing white text on a white background\\nLocating text behind an image\\nUsing CSS to position text off-screen\\nSetting the font size to 0\\nHiding a link by only linking one small character—for example, a\\nhyphen in the middle of a paragraph\\n\\nWhen evaluating your site to see\\nif it includes hidden text or links, look for anything that's not\\neasily viewable by visitors of your site. Are any text or links there\\nsolely for search engines rather than visitors?\",\n",
       " 'The answer has to be seen in business context rather than technical. Technically it is possible to have subdomain almost like an independent domain with its own hosting, cms, tracking etc.\\nFor me, advantages of using subdomain are\\n\\nprovides better value from already existing branding \\nlooks more legitimate eg money.asda.com is much more legitimate to me\\nthan asda-money.com as it could have been anybody other than ASDA\\nas well\\npossibility or at least lesser work in integrating back end, database\\n(depends on use case though)\\n\\nYou may also want to consider other factors like management, consistency, brand messaging etc.',\n",
       " \"Site is based on WordPress and turns out it had an old plugin installed that was using the former version of Google Analytics to send events itself; as soon as I deactivated that plugin events started showing up.\\nThe aforementioned plugin is: Analytics Reduce Bounce Rate\\nI also commented out this line (as suggested by @nyuen):\\nvalue = typeof value !== 'undefined' ? value : '';\\n\\nNot sure if it had any effect, but eventValue is supposed to be of integer type so setting it to blank probably wasn't helping either.\",\n",
       " 'As the first, your example page https://www.fantasticservices.com/professional-oven-and-bbq-cleaning/ includes two different, not nested JSON-LD snippets. This is a massive cause for bad interpretation of structured data on the page: think, how Google should decide, what is the main thing on the page, your localBusiness markup or Product?\\nAs second: you don\\'t offer products, but your markup does. It is a service, what you offer. This could be the second cause of misinterpretation of your structured data by Google.\\nAs third, your review/rating markup belongs to product, that is wrong, because you offer no products.\\nIt is true: google\\'s raters aren\\'t the best writers, their emails mislead often.\\nTo make your markup guidelines-proven and streamlined you should firstly decide, what do you want should be rated. In general there is only one possibility to rate/review your page, which fulfills Google\\'s guidelines: it should be rating of your localBusiness, which provides this or that service. Google writes here: refer clearly to specific service. But not to service itself! Note: it is a rating of business, how good or bad it provides a given service. \\nCorrect markup for example page should look like:\\n<script type=\"application/ld+json\">\\n{\\n\"@context\" : \"http://schema.org\",\\n\"@type\" : \"ProfessionalService\",\\n\"aggregateRating\": {\\n\"@type\": \"AggregateRating\",\\n\"ratingValue\": \"8.3\",\\n\"bestRating\": \"10\",\\n\"worstRating\": \"1\",\\n\"reviewCount\": \"1292\"\\n},\\n\"makesOffer\":\\n{\"@type\": \"Offer\",\\n\"itemOffered\": {\\n\"@type\": \"Service\",\\n\"name\": \"Oven %26 BBQ Cleaning\",\\n\"sameAs\": \"https://en.wikipedia.org/wiki/Commercial_cleaning\",\\n\"url\": \"https://www.fantasticservices.com/professional-oven-and-bbq-cleaning/\"\\n}},\\n\"url\" : \"https://www.fantasticservices.com/\",\\n\"image\" : \"https://www.fantasticservices.com/wp-content/uploads/2016/10/fantastic-pros-1-1.png\",\\n\"name\" : \"Fantastic Services\",\\n\"priceRange\": \"££\",\\n\"telephone\" : \"020 3404 3444\",\\n\"contactPoint\" : [{\\n\"@type\" : \"ContactPoint\",\\n\"telephone\" : \"+44 20 3404 3444\",\\n\"contactType\" : \"Customer Service\"\\n}] ,\\n\"openingHours\" : \"Mo-Su 08:00-22:00\",\\n\"paymentAccepted\" : \"Cash, Credit Cards\",\\n\"address\": {\\n\"@type\": \"PostalAddress\",\\n\"addressLocality\": \"London\",\\n\"postalCode\": \"SE1 2TH\",\\n\"streetAddress\": \"98 Tooley Street\" \\n},\\n\"geo\": {\\n\"@type\": \"GeoCoordinates\",\\n\"latitude\": \"51.5043105\",\\n\"longitude\": \"-0.0827881\"\\n},\\n\"hasMap\": \"https://www.google.com/maps/place/Fantastic+Services/@51.4991109,-0.087624,15z/data=!4m5!3m4!1s0x0:0x41a17cd8609f141c!8m2!3d51.4991109!4d-0.087624\",\\n\"areaServed\": {\\n\"@type\": \"AdministrativeArea\",\\n\"name\": \"London\",\\n\"address\": \"London, United Kingdom\"\\n},\\n\"logo\" : \"https://www.fantasticservices.com/wp-content/themes/fantasticservices-master/static/css/images/logos/logo.svg\",\\n\"sameAs\" : [ \\n\"https://www.facebook.com/FantasticServicesUK\",\\n\"https://twitter.com/Fantastic\",\\n\"https://plus.google.com/+FantasticServices/posts\",\\n\"https://www.pinterest.com/GoFantastic/\",\\n\"https://www.youtube.com/channel/UCPr-OYIcHpv0Xr6q5PUSzPQ\",\\n\"https://www.trustpilot.com/review/fantasticservices.com\",\\n\"https://www.ipo.gov.uk/tmcase/Results/1/UK00003134615\"\\n]\\n}\\n</script>\\n\\nPS: you don\\'t need to escape slashes in url. But special characters, as & should be percent-encoded, as i\\'ve done with %26',\n",
       " \"When it comes to search engines duplicate content is not based on the GUID rather it is based on the content itself. How the search engine companies do this is a closely guarded proprietary secret but in very broad strokes is based on comparing the content of the page to other pages already in the index. Where page content is the same but in different languages then duplicate content penalties don't come into play as it is clearly demonstrated that the content is targeted at two different markets based on language.\\nAs for the GUID aspect of the question search engines don't really consider the GUID of a wordpress article at all. The GUID is used in wordpress to uniquely identify the article across the internet, however search engines use the URL of the page in question to identify the page. If two pages have the same GUID search engines won't have any issue with it unless the content of the page is the same with the same language.\\nSo based on your question given that the two sites which will have pages with the same GUID will be in different languages then you won't have any issues with search engines or feed readers.\",\n",
       " \"Technically speaking the DNS provider is allowed to change your zone file whenever they want as you don't own the zone file you are licensing services from the provider. Having said that most providers wont arbitrarily make changes to your zone file without your permission unless you have not paid the bill or they have been directed to by court order as it is not a good business strategy to alienate your customers and make them want to leave your services behind.\\nIn your comments you mention that the provider identified this as the result of human error. Given the situation this is most likely true as it is fairly easy for an admin to accidentally change the wrong clients DNS settings if they just glance at the records.\",\n",
       " \"What is happening here is mainly due to the differences in garbage collection between different flavours of Linux. As mentioned above by @Tim Fountain garbage collection in debian-based flavours of linux perform garbage collection via a cron job. A check of PHP's documentation on sessions observes that the time set in gc_maxlifetime is the number of seconds after which data will be seen as garbage and potentially cleaned up. If garbage collection is not running on your server regularly then this won't be triggered and the data will remain.\\nThe most reliable method to log a user out from a session after a certain amount of idle time is to add a session variable called something like lastAction which should be a unix timestamp using the time() function. Then each time a request is made you are first checking to see if the lastAction timestamp is older than a certain number of seconds (based on the max idle time you want), if it is then kill the session, run the logout code, and redirect the user to the login page, if it is less than the max number of seconds then reset the value of the lastAction session variable with the current unit timestamp and proceed.\",\n",
       " 'This is one of the absolute worst case scenarios when inhereting a web project that is old, poorly designed, badly documented, and can\\'t easily be debugged. Any code at all let alone mission critical autonomous code like this should be well documented and have thorough error reporting. Assuming this has been done in a language like PHP you can increase the error logging level to informational which will show every last error message that may be generated by the server. This will report on any errors that are being handled by the language and not the code. The next step would be to go through the code and identify anywhere that the code outputs any data to the connection (assuming a http style connection any data that is effectively \"printed\" to the buffer for outputting) and add logging so all the data is logged to file. If you add each instance of data output to an array with details on the line it was generated at you can push all data to to a file one time during each request to the web service to minimise file writes causing delays and then read through what is being sent back to the bank.\\nIt is going to be extremely difficult and complicated but there is not much more that can be done in the short term to identify the problem. In the long term you should absolutely be going through the code and implementing sanity checks, data validation, and error logging and reporting. Basically anything that will prevent the code from running should be logged in a manner that will let you go through and find out not just what the error was but what could have caused the error, the data being worked on, and what line the error occurred on.',\n",
       " 'You wouldn\\'t want to use CSS based media queries in any case because browsers will download all those images regardless if they are being shown or hidden. You can achieve what you want using either HTML5 markup or Javascript.\\nHere is just a few methods that should get you started:\\n\\nImg Srcset (HTML5 Markup)\\n\\n<img sizes=\"100vw\" srcset=\"sm.jpg 400w, md.jpg 800w, lg.jpg 1600w\" src=\"sm.jpg\">\\n\\nPicture (HTML5 Markup)\\n\\n<picture>\\n   <source media=\"(min-width: 36em)\" srcset=\"lg.jpg 1024w, md.jpg 640w, sm.jpg 320w\" sizes=\"33.3vw\">\\n   <source srcset=\"cropped-lg.jpg 2x, cropped-sm.jpg 1x\" />\\n   <img src=\"sm.jpg\" />\\n</picture>\\n\\nZurb Foundation Interchange (JavaScript)\\n\\n<img data-interchange=\"[sm.jpg, small], [md.jpg, medium], [lg.jpg, large]\">\\n\\nKoen Vendrik\\'s Responsive Images (JavaScript)\\n\\n<img data-src-base=\\'/\\' data-src=\\'<480:xs.jpg,<768:sm.jpg,<960:md.jpg,>960:lg.jpg\\'>\\n\\nResponsiveJS (JavaScript)\\n\\n<img src=\"sm.png\" data-min-device-width-641=\"md.png\">\\n\\nResponsive Img (JavaScript)\\n\\n<img id=\"img1\" src=\"example.jpg\" style=\"max-width:100%;\" />\\n\\nlazySizes (JavaScript)\\n\\n<img sizes=\"(min-width:1000px) 930px, 90vw\" data-srcset=\"sm.jpg 500w, md.jpg 640w, lg.jpg 1024w\" data-src=\"md.jpg\" class=\"lazyload\">\\n\\nImagerJS (JavaScript) Kinda old but still works\\n\\n<div class=\"delayed-image-load\" data-src=\"example{width}\" data-alt=\"text\"></div>',\n",
       " \"There is the potential for site wide impact but more often than not you will find a page specific impact occurring first. With any website with user generated content you need to invest your time in moderation or establish some community driven moderation (like Stack Exchange) in order to quickly catch out content that is of low quality and could affect your ranking. As a basic rule of thumb if poor quality content is identified and removed within a few hours it is unlikely to cause a substantial hit to your site as Google does encourage moderation of user generated content.\\nIf we use Stack Exchange as an example there are a large number of poor quality posts that come up on any given day and yet Stack Exchange sites still rank extremely highly within Google, and the reason for this is proactive moderation, in effect every member of the site is a moderator as anyone (starting from a very low rep level) can flag content for moderator attention and beyond a certain reputation users are deemed to have been part of the community long enough to know what belongs and what doesn't belong and so they are given access to a range of moderation tools.\",\n",
       " 'Google has a feature in webmaster tools where you can add login information if you want Google to crawl content behind a user login form. If you have provided Google this information in the past and have not changed the login information since then Google will have access to the content accessible to the login information you provided to it. Google does not however try to \"guess\" login information even if this can be parsed from the URL of the page.\\nGoogle can also try to crawl a page that is protected by a user login if the page exists in your sitemap file or has been linked to from another website or a link on your site that Google has access to. In this case Google will try to crawl the page and will detect that it is protected by HTTP basic authentication and so won\\'t list it in the index but it a crawl attempt will still have been attempted.',\n",
       " 'Short Answer: You should not add the nofollow attribute to (any) internal links.\\nLong Answer: What you\\'re describing sounds suspicious: masking; nofollow; user agent redirects.\\nI don\\'t know if they\\'ll do any damage but I don\\'t think they\\'ll help so why risk it?\\nnofollow was invented to prevent comment spam from passing PageRank, in effect indicating the publishing site \"doesn\\'t trust\" the link.\\nWhen a link has the nofollow attribute Google no longer passes that PageRank to other links on the page, it simply evaporates.\\nBlogger is built and hosted by Google, I\\'m pretty sure it\\'s search engine friendly right out of the box.',\n",
       " 'With HTTP/1, the answer is yes.\\nAs mentioned by Michael Hampton, combining files (which prefer I call \"concatenation\") is a typical web site performance optimization. This is due to overhead related to requesting new resources. Every time the browser wants a new resource (a new file), it has to open a new connection to the server.\\nHTTP/2, however, changes how browsers interact with servers. Instead of one resource per connection, we can have multiple resources on a single connection, so a single file is no longer faster than multiple files.\\nOf course, HTTP/2 has its own browser support and server support issues. Not all hosting providers offer it yet.\\nBut in the short run, you can also optimize the JavaScript loading. Do you really need all 7KB of JavaScript on every page? Do you really need all 7KB immediately? It could be better to load the most important bit and then use requestAnimationFrame to wait for an idle moment before loading the less-important parts unobtrusively.',\n",
       " 'As stated in the question, the best option for this type of issue would be to allow GoogleBot to index the site, but restrict the viewing of regular users. \\nTo do this, you need to be able to differentiate between three types of viewers:\\n\\nGoogleBot\\nA visitor being referred by Google search (as stated in the First Click Policy)\\nA direct visitor either being referred by a site other than Google or with no referrer\\n\\nIn order to differentiate between these three types, you need to do a series of two checks, one through HTTP_USER_AGENT and another through HTTP_REFERER.\\n\\nThrough the use of $_SERVER[\\'HTTP_USER_AGENT\\'], detect if GoogleBot is accessing the website. If true, allow GoogleBot to access it without requiring a login.\\nSimilarly, with $_SERVER[\\'HTTP_REFERER\\'] detect where the user came from. If the user was referred by another site, this variable will contain a string containing the URL of that site. If this variable contains https://www.google.com then you should allow the viewer to access the site.\\nIf the viewer isn\\'t GoogleBot and wasn\\'t referred by Google, you should force them to log in. This ensures that even if they were brought to your site by Google, their second click will require them to log in. If they just simply are from a site other than Google, they will also be forced to log in.\\n\\nHere is a piece of code that will do these basic checks\\n    //check if the viewer is googlebot, if so, allow\\n    if(strstr(strtolower($_SERVER[\\'HTTP_USER_AGENT\\']), \"googlebot\"))\\n    {\\n       //allow access\\n    }\\n\\n    //check if the viewer was referred by google, if so, allow\\n    elseif(strstr(strtolower($_SERVER[\\'HTTP_REFERER\\']), \"https://www.google.com\"))\\n    {\\n      //allow access\\n    }\\n\\n    //if the viewer isn\\'t from google, block vistor viewing\\n    else{\\n      //redirect to login\\n    }\\n\\nFor news sites, it is important to note that Google now requires 3 free articles per day if complying with the First Click Policy.\\n\\nIt is possible to limit the number of free articles that a Google News reader can access via First Click Free. A user coming from a host matching [www.google.] or [news.google.] must be able to see a minimum of 3 articles per day. This practice is described as \"metering\" the user: when the user has clicked on too many of a publisher’s articles from Google News, the meter for freely accessible articles on that site is exhausted.\\n\\nIt is also extrememly important to note that it is very easy to \"spoof\" a HTTP_USER_AGENT. There are numerous browser plugins and applications which would allow a viewer to have a user agent pretending to be GoogleBot. This would allow a spoofer to view every crawlable and indexable page on the site, regardless of their membership status. Secondary steps to ensure that it really is the real GoogleBot may be necessary, but a basic check like this one should work in many cases.',\n",
       " 'If you want search engines to take notice that your site  moved, you need to implement 301 redirects. In general, bots do not follow redirects via meta tags.\\nAnother option would be to use canonical tag, however keep in mind that 301 is better.\\nAs for the ranking aspect in general - just stick to normal website development, give it some time, add some social signals from Twitter, G+, LinkedIn, GitHub (for example, your public LinkedIn profile shows the correct link, but GitHub still links to the edu domain).\\nOn your personal site, make one about me page and implement person schema. Link back to your social footprint. Sample code:\\n<script type=\"application/ld+json\">\\n{\\n  \"@context\": \"http://www.schema.org\",\\n  \"@type\": \"person\",\\n  \"name\": \"Something Something\",\\n  \"url\": \"https://example.com/\",\\n  \"image\": \"https://example.com/images/me.jpg\",\\n  \"sameAs\" : [\\n    \"https://www.twitter.com/someid\",\\n    \"https://www.twitter.com/anotherid\",\\n    \"https://plus.google.com/+someID/posts\",\\n    \"https://www.linkedin.com/in/someid\",\\n    \"https://github.com/someid\"\\n  ],\\n  \"jobTitle\": \"Something\",\\n  \"telephone\": \"+1234567890\",\\n  \"worksFor\": \"Some Corp, Inc.\",\\n  \"description\": \"Some description\"\\n}\\n </script>\\n\\nGetting a rank for [first name last name] with exact match domain should be rather easy.',\n",
       " \"Early 2018 update to Google Search Console will now tell you if it's overridden a page with a different canonical:\\n\\nGoogle chose different canonical than user: This URL is marked as canonical for a set of pages, but Google thinks another URL makes a better canonical. Because we consider this page a duplicate, we did not index it; only the canonical page is indexed. We recommend that you explicitly mark this page as a duplicate of the canonical URL. To learn which page is the canonical, click the table row to run an info: query for this URL, which should list its canonical page.\\n\\nhttps://support.google.com/webmasters/answer/7440203\\n\\nIt is definitely possible. Here is a source from Google Webmaster Central blog:\\n\\nthe canonical designation might be disregarded by search engines\\n\\nhttps://webmasters.googleblog.com/2013/04/5-common-mistakes-with-relcanonical.html\\n(Also note that Google has stated categorically that GA does not factor into any Google rankings.)\",\n",
       " 'You need breadcrumb snippet for that. Click on see markup data for example.\\nBelow is breadcrumb I use it, in my own website.\\n<div class=\"breadcrumb\">\\n<span itemscope=\"itemscope\" itemtype=\"http://data-vocabulary.org/Breadcrumb\"><a href=\"/\" itemprop=\"url\"><span title=\"Goyllo\" itemprop=\"title\">Goyllo</span></a></span>\\n<span itemscope=\"itemscope\" itemtype=\"http://data-vocabulary.org/Breadcrumb\">&#187; <a href=\"/seo/\" itemprop=\"url\"><span title=\"Search Engine Optimization\" itemprop=\"title\">Search Engine Optimization</span></a></span>\\n<span itemscope=\"itemscope\" itemtype=\"http://data-vocabulary.org/Breadcrumb\">&#187; <a href=\"/seo/basics/\" itemprop=\"url\"><span title=\"SEO Basics\" itemprop=\"title\">SEO Basics</span></a></span>\\n</div>\\n\\nThis will show snippet like this in serp\\nwww.yoursite.com » Search Engine Optimization » SEO Basics\\nData vocabulary is still supported by Google, I am still using it in my website from 2015 to 2017. Here is SS',\n",
       " 'Here\\'s how you would do it with php. \\n<html>\\n<body>\\n\\n<h1>Welcome to my home page!</h1>\\n<p>Some text.</p>\\n<p>Some more text.</p>\\n<?php include \\'footer.php\\';?>\\n\\n</body>\\n</html>\\n\\nThis is the content of footer.php \\n<?php\\necho \"<p>Copyright &copy; 1999-\" . date(\"Y\") . \" W3Schools.com</p>\";\\n?>\\n\\nMore info here:\\nhttps://www.w3schools.com/php/php_includes.asp\\nUnlike html the files in php end with .php just change your html files to php by changing the file extension. Later on you can use rewrite to change how the extension appears on browsers if that bothers you. But that\\'s another topic on its own. \\nI\\'d avoid using the html javascript way because it\\'s not supported by all browsers and the user can turn off javascript on their browser. That\\'ll render it broken and ineffective.',\n",
       " \"To your questions:\\n\\nThe top paginated item isn't equivalent to the other pages, it wouldn't be considered the canonical page for the whole set. A view-all page would work though. \\nIf fetching page 3 returns page 1, that's broken :). In that case, pages 2+ would never get indexed. If there's no missing content on, or linked from, pages 2+, then maybe that's not such a big deal for you though. \\nThat sounds like it would be wrong... I'd check out the blog post on faceted navigation for other ways to handle this. There's also one on infinite scroll, if you're curious.\",\n",
       " 'I\\'m experiencing this same problem, but I got my email a week ago.  For that entire week, my account has said, \"Reviewing your site will take up to 3 days. We\\'ll email you when we\\'re done.\"  I\\'ve already got the email saying the review is successful.\\nAny additional advice...?  It doesn\\'t seem that waiting a little longer is doing the trick.',\n",
       " 'Working with your hypothetical, stevensonphotocreation.com and weddingphotovermont.photography, much depends upon the site, however, I can tell you this.\\n1] The forwarding in of itself is not helping. It may not hurt, but it is not helping.\\n2] The specialty gTLD .photography is not helping. It appears that the developer thinks that the gTLD adds semantic value, it may or may not, however, what is true in search is what was true before a sea change. In otherwords, the TLD had no semantic value before all the new gTLDs, I would bet they have no real value today.\\n3] The domain name weddingphotovermont.photography is not helping to build a brand.\\n4] If you break down the semantic meaning of both domain names, the weddingphotovermont.photography does have more semantic value with wedding photo vermont, however, the notion of keyword loading domain names is a misnomer.\\nThere is a greater weight given to search terms found in a domain name, however, this does not work in a vacuum and is not as great as people think. For a while, people thought that keyword loaded domain names performed better. And they were not completely wrong in thinking this. There was plenty of anecdotal evidence without context. Google for about a year and a half, did over-weigh semantic values within the domain giving the domain name a ridiculous amount of significance in some searches. But there was a catch.\\nAgain, this did not work in a vacuum. For this to have worked, all other search query filters would have to have failed. And that is the key.\\nGoogle did properly correct the error.\\nFor the record, Google does not make keyword matches, never has, never will. In fact, the original research paper written by Page and Brin stated that their research is based upon the notion that keyword matches were at least limited in potential and poor in results. Page and Brin argues that semantics and not direct keyword matches was what made Google better. And they were right.\\nSo given the scenario today, yes there is semantic value in terms found in a domain name, however, that requires that the site and content match the semantic value found. Otherwise, any semantic value used within the domain name fails as an outlier. This means that the site has to be created to support the domain name. This is bass ackwards. It is like putting the cart before the horse.\\nAlsoconsider that any site needs to build a brand. Weddingphotovermont.photography is not brand worthy whereas stevensonphotocreation.com would be.\\nWhat I would advise is something more in between,  stevensonweddingphotography.com. I give this as an example. It has both supportable semantic value and the opportunity to build a brand. However, the domain names have already been chosen. Of the two hypotheticals, only the first, stevensonphotocreation.com, makes sense. Dump the forwarding. Dump the extra domain name and focus on one properly developed site. Do not look for tricks and do honest work. Then be done with it.',\n",
       " 'It appears that for cPanel, and perhaps others, certainly to be expected with web hosts these days, while creating a second or third web site or even a sub-domain, cPanel in order to save on setting file permissions, creates any additional site within the web space of the first site created. This may be a result of the old Apache schema using httpd.conf and compatibility. Who knows? The result is that there is confusion and potential problems that can arise.\\n/public_html/ is the original sites web space for example.com\\n/public_html/example_2/ is the second site for example2.com\\nFor example.com, the directory directive within the configuration file points to /public_html/ and for example2.com, the directory directive within the configuration file points to /public_html/example_2/.\\nThese are local directories on your file system and not how things are seen from the web.\\nWhat is put into /public_html/ such as index.html will be seen as example.com/index.html and what is put into /public_html/example_2/ such as index.html will be seen as example2.com/index.html.\\nThe same will be true for robots.txt and any .htaccess file you choose to create. Example.com robots.txt would go into /public_html/ and example2.com robots.txt would go into /public_html/example_2/.\\nThe drawback to this configuration is that example.com/example_2/index.html will show the sites home page for example2.com/index.html. This is generally not a problem. Just something to remember.',\n",
       " \"In order to match the query string (ie. page_id=123 part) you need a RewriteCond directive and the QUERY_STRING server variable (from mod_rewrite). Try something like the following in the /abc/.htaccess file (ie. in the subdirectory you want to redirect from):\\nRewriteEngine On\\nRewriteCond %{QUERY_STRING} ^page_id=123\\nRewriteRule ^$ /xyz/newindividualpage/ [R=302,L]\\n\\n(Change the 302 to 301 when you are sure it's working OK. 302 (temporary) redirects aren't cached by the browser, so makes testing easier.)\\nThis redirects any URL within the /abc subdirectory that has a query string that starts page_id=123 (as per your example).\\n\\nAlternatively, if you prefer to only use the .htaccess file in the document root, then add the following to the top of the .htaccess file (at least before any other mod_rewrite directives) in the root directory:\\nRewriteEngine On\\nRewriteCond %{QUERY_STRING} ^page_id=123\\nRewriteRule ^abc/?$ /xyz/newindividualpage/ [R=301,L]\\n\\nThe RewriteEngine directive only needs to appear once in the file. It can appear anywhere in the file, although it is more logical if it appears at the top. (It doesn't matter if it occurs more than once, it's just unnecessary.)\",\n",
       " \"I have managed to get this working by using a snippet provided by anubhava on Stack Overflow. If anyone spots a potential issue with this code or has a better and cleaner redirect then please don't hesitate to plonk your code as an answer.\\n\\nSOURCE\\nRewriteEngine On\\nRewriteBase /\\n\\nRewriteCond %{HTTP_HOST} ^domain\\\\.com$ [OR]\\nRewriteCond %{HTTPS} off\\nRewriteRule ^/?$ https://www.domain.com [R=301,L]\\n\\nRewriteCond %{HTTP_HOST} ^domain\\\\.com$ [OR]\\nRewriteCond %{HTTPS} off [OR]\\nRewriteCond %{REQUEST_URI} !(/$|\\\\.)\\nRewriteRule ^(.+?)/?$ https://www.domain.com/$1/ [R=301,L]\\n\\nSo my .htaccess file now looks like this:\\nRewriteEngine On\\nRewriteBase /\\n\\nRewriteCond %{HTTP_HOST} ^example\\\\.com$ [OR]\\nRewriteCond %{HTTPS} off\\nRewriteRule ^/?$ https://www.example.com/ [R=301,L]\\n\\nRewriteCond %{HTTP_HOST} ^example\\\\.com$ [OR]\\nRewriteCond %{HTTPS} off [OR]\\nRewriteCond %{REQUEST_URI} !(/$|\\\\.)\\nRewriteRule ^(.+?)/?$ https://www.example.com/$1/ [R=301,L]\\n\\n<IfModule mod_rewrite.c>\\nRewriteEngine On\\nRewriteBase /\\nRewriteRule ^index\\\\.php$ - [L]\\nRewriteCond %{REQUEST_FILENAME} !-f\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteRule . /index.php [L]\\n</IfModule>\",\n",
       " \"You cannot do that, directly, from the GoDaddy control panel. You can use the control panel to obtain similar results, if you are willing to set slightly different operations. You can make multiple, uniquely named, subdomains for the different apps. You can add those domains to the DNS Manager under the Domains menu, then use the Redirects under the Domains menu to do a premenant redirect to that URI. I.e.: Add the subdomain project1.demo pointing to the main IP, then redirect project1.demo.example.com to http://demo.example.com/project1. The redirect requires a protocol, so you'll need to choose whether or not to use TLS for each one before hand.\\nNot sure of you exact requirements, so you may not need to go through all this. If your apps can be set to point to demo.example.com/project1 as their root, then creating the directory project1 under the location set as the root of demo.example.com and creating the subdomain demo.example.com in the normal method: Subdomains on the Domains menu, could be enough.\\nThe way their system does things, if demo.example.com is set to have a root of public_html/my_demo, then demo.example.com/project1 will load from public_html/my_demo/project1.\",\n",
       " 'The masking is done with frameset\\n\\nUsing a frame, often called \"framed forwarding\" for masking the real URL is generally bad for SEO and delivers a bad user experience.\\nThe URL inside the frame is not hidden from search engines (or users). There are essentially two separate pages/URLs and both can be indexed. The only page with content is the inner/framed page, so this will likely be the page that will be returned in search results (if it is indexed).\\n\\nI have removed the original page \"www.domain.com/p/page.html\" from google index to avoid duplicate.\\n\\nYou can\\'t remove the inner/framed page from the index without removing the actual content from the index. Without the content being indexed then you are obviously not going to rank for anything on that page. The outer \"frameset\" is simply a container, by itself it has no content.\\nThere is no \"duplicate\". They are two different URLs. One simply contains the other in a frame.\\nWhen the user navigates your site the URL in the address bar does not change (as you suggest) and they are unable to bookmark individual pages. This can be confusing and frustrating for users.\\n\\n<frame src=\"www.domain.com/p/page.html\"\\n\\nIncidentally, without the scheme (eg. http://) on the URL this is likely to break in most browsers (historically this would have only worked in IE).',\n",
       " \"There's a small but important difference between the concurrent connections (Entry Process) and the number of visitors allowed to view your website.\\nEntry Process only counts the users who are actively downloading content from your website. Once user finishes the loading of a page content, its connection (Entry process) is released for other users. \\nHaving said that, if your web page takes 2 seconds to fully load, it counts as one entry process in use for 2 seconds only, unless visitor clicks on any other page that establishes another connection. \\nThat means, a good number of visitors can view your site concurrently as long as they don't make all requests simultaneously. \\nYou can calculate how many visitors per minute you can serve if you know your site's average load time and Entry Processes.\\nSay for example, you have hosting account with 20 Entry Processes and a website with average page load of 2 seconds.\\nEntry Processes per second = (Total No. of Entry Processes / seconds) \\n=> 20/2 \\n=> 10 \\nIn this example , you'll have 10 Entry Processes per second to serve visitors. Roughly, your hosting account can serve 8 to 10 concurrent visitors per second purely based on their browsing pattern.\\nEntry Processes per minute = (Total No. of Entry Processes / seconds * 60 seconds) \\n=> 10 * 60 \\n=> 600\\nYou'll have 600 Entry Processes per minute to serve visitors. Roughly, your hosting account can serve 500 to 600 concurrent visitors per minute purely based on their browsing pattern.\",\n",
       " \"Take a look at: https://www.w3schools.com/tags/ref_language_codes.asp\\nYou should be using the lang attributes on your html tag that comply to ISO 639-1.\\nIn WordPress you can achieve this by modifying your theme to add this automatically to the html tag in your templates, defaulting to the option set in your WordPress dashboard (in your case, I'd guess German). You could then override this using some kind of post-meta for posts which are not the global language.\\nAs to the impact on SEO, I can't answer that, but by telling Google what language each of the pages are written in should provide more benefits than penalties.\",\n",
       " \"I run a web hosting business.  Hundreds of clients are (or aren't if they are lazy) switching to https via free LetsEncrypt certificates.\\nTo be honest, I think you are overcomplicating things.  Install the SSL for the domain, then without changing anything, try using https and see how you go.  \\nYou might find a bunch of mixed-content warnings when viewing the developer tools console.  This is one of the most important steps as if there is any mixed content i.e. files included using http, you will not see a secure padlock.\\nThere is no need to put a site into maintenance mode.  Even if you have the certificate installed, people can still use the legacy http while you are testing.\\nThen when your are satisfied it is working, add the code to your .htaccess file.\\nHere's how you test:  go to htps://example.com and see if there is a padlock in the address bar.  If there is, it is working.  If there is still an insecure notification, back to developer tools to look for the problems.\",\n",
       " \"It's frustrating to wait, but for that many pages it's probably the best advice.  You could use the Google Remove URLs tool to achieve what you want, but be aware that Google says in the instructions they intend for the tool to be used for temporary URL removal from search results. They would prefer you let Googlebot get a 404 return on permanent page deletions.  You can read more about it here: https://support.google.com/webmasters/answer/1663419?hl=en \\nIf the deleted pages are demo pages, the chances that a human who is not a spammer/hacker will see them in Google AND click on them is very, very small. Just because Google Console sees the pages doesn't mean they rank at all in search results. Still, this might be a good opportunity edit your 404 page for human visitors and add a link to your homepage.\",\n",
       " 'Yes, it is possible. But that doesn’t necessarily mean that search engines follow these references (they don’t document it).\\nHow?\\n\\nGive each item an URI with the itemid attribute.\\nUse this URI as value for relevants properties.\\n\\nYou might want to use URIs that represent the thing instead of the page (see the section Why are identifiers useful? in my answer).\\nExample\\nSo on the product page, you could have:\\n<div itemscope itemtype=\"http://schema.org/Product\" itemid=\"/products/42#this\">\\n  <link itemprop=\"offers\" href=\"/offers/42#this\" />\\n</div>\\n\\nAnd on the offer page, you could have:\\n<div itemscope itemtype=\"http://schema.org/Offer\" itemid=\"/offers/42#this\">\\n  <link itemprop=\"itemOffered\" href=\"/products/42#this\" />\\n</div>',\n",
       " \"Under Manual Configuration, you can specify Google Cloud Platform nameservers and click on Set name servers. Checkout this Google KB to determine your Google Cloud Platform nameservers...,https://cloud.google.com/dns/update-name-servers\\nNowadays, web hosting services are available on much cheaper rates, approximately 3 USD per month. IMHO, it isn't a good idea to host a application on home computer, though this thread would help you get started..., https://stackoverflow.com/questions/24066148/can-i-host-a-web-site-on-my-home-computer\",\n",
       " 'How to reference/link with Microdata and Schema.org\\nIf possible, I would use both ways (url property from Schema.org & itemid attribute from Microdata).\\nWhile Schema.org isn’t that strict about it, I think it makes sense to use different URIs for them, if you want to differentiate between the page and the thing:\\n\\nSchema.org’s url property gives the URI of the page about the thing.\\nMicrodata’s itemid attribute gives the URI of the thing.\\n\\nFor example, the URI https://example.org/employees/joe-blow.html points to a page about Joe Blow, and the URI https://example.org/employees/joe-blow.html#i is the URI for Joe Blow himself. Then you could specify:\\n<div itemscope itemtype=\"http://schema.org/Person\" itemid=\"/employees/joe-blow.html#i\">\\n  <link itemprop=\"url\" href=\"/employees/joe-blow.html\" />\\n</div>\\n\\n<div itemscope itemtype=\"http://schema.org/AboutPage\" itemid=\"/employees/joe-blow.html\">\\n  <link itemprop=\"about mainEntity\" href=\"/employees/joe-blow.html#i\" />\\n  <!-- just for the sake of the example; you would typically include the \\'Person\\' item here -->\\n</div>\\n\\nSearch engine support\\nGoogle’s SDTT doesn’t follow links. Copying my comment to a related question:\\n\\nThe SDTT doesn’t support it, but of course that doesn’t necessarily mean that Google doesn’t support it. I guess it makes sense that the tool doesn’t follow references, because there could be many, possibly endless, even to external documents. -- My guess (I have no evidence, just a guess) is that the Googlebot will follow those references (these are, after all, normal a/area/link hyperlinks), but without \"adding\" the structured data from the linked page to the linking page.\\n\\nAs far as I know, no search engine documents that they would follow references in structured data to do something with it.',\n",
       " \"After browsing the schema.org docs I could not find a suitable data type. If you can do without schema markup I would suggest using the definition list HTML element to markup your content. This will convey the context of the information on the page to search engines as a term with a definition. For example(from 2):\\n<dl>\\n  <dt>Firefox</dt>\\n  <dd>A free, open source, cross-platform,\\n      graphical web browser developed by the\\n      Mozilla Corporation and hundreds of\\n      volunteers.</dd>\\n\\n  <!-- other terms and descriptions -->\\n</dl> \\n\\nIf you are set on using the Book item, which I don't believe to be valid, you may find using JSON-LD Schema formatting to be more efficient than inline schema format. An overview of the JSON-LD format can be found on the Google Schemas site. It is much more flexible since it is not directly tied to the HTML.\",\n",
       " \"Create a forwarder for you@example.com to you@gmail.com\\ngmail will only send through an external SMTP server.  I create an email account outgoing@example.com on my hosting mail server and then provide those details to gmail.\\nIn gmail go to gear icon > settings > accounts and import > send mail as\\nStep through the settings.  A verification email will be sent to your email address, it will be forwarded to gmail.  Grab the code and verify the email address.  Don't forget to set your newly verified email address as the default.\\nYou can create multiple email addresses in one gmail account using a single SMTP account using this method.  But be sure to forward them to gmail before you do the verification step or you won't receive the code.\",\n",
       " 'As mentioned, there is no evidence that title attributes on HTML elements are used by Google for ranking purposes - this is correct.\\nThe previous answer cites an irrelevant Tweet but does provide some value because the Googler explicitly advises testing. I\\'d urge you to discard the source citation and the statement by Simon which states that \"It\\'s unlikely that Google or Bing actively monitors the DOM for Title Changes\"\\nThis is beyond easy to test especially since you\\'re developing in jScript. Google does in fact wait for DOM mutations - and multiple tests suggest that they wait up to 5 seconds - here\\'s a simple test where the DOM was manipulated and Google\\'s fectch and render service in Google search console did in fact wait for 5 seconds. \\n\\nThere\\'s also a full post of JS tests that also confirm this : http://searchengineland.com/tested-googlebot-crawls-javascript-heres-learned-220157. If we can inject links, metadata, change HTML and Google accounts for DOM changes, there\\'s no reason why they wouldn\\'t be able to pick up title attribute changes. Bottom line, test it - this test should take 5 minutes and you\\'ll get confirmation.',\n",
       " 'Having a 200 page returning content that looks like a 404 page is wrong and could be flagged as being deceptive.\\nIn this case you can either create an archive with all the old content on there and 301 redirect to there or use a 410 code which means gone, this will stop Google from going back to the page to check if its live again like they do with a 404.\\nThe problem with using the 410 code is, if you have any external backlinks linked to those pages, you lose that influence.',\n",
       " \"To be honest, I was surprised it's only 105 euro's. Yes, it's more that the €3 is cost them, or the €10 it would cost you to register it.\\n\\nYou could get a lawyer, but that would be more expensive  \\nYou got get some professional to look into it, but that would be more expensive  \\nYou could try contacting/persuading them, but that'll cost you more time/frustration than it's worth.\\n\\nI don't think €105 is that much at all (especially for a bussiness). They've spend time on appointments and making an offer, but didn't get the deal. This is a way to regain a some loss, not that uncommon.  \\nCall it a learning experience and proceed. You could try the following, but be prepared for a 'no thanks':\\n\\nHi. We think €105 is a bit much for just a domainname, but we understand that you've spend some time to make an offer for [clientname]. We're prepared to pay €75,- and call it a deal.\\n\\nThat might save you €30 euro. If you're lucky they're not in the mood to hussle and just go for it. \\nRemember: They've allready spend money on your client which won't  see again and you want something they have. Its's not very polite, but nevertheless bussiness. Respond accordingly.\",\n",
       " 'Since my last answer using the \"if\" statement is not recommended in most cases, I came up with another solution which seems to be working perfectly. \\nWhat I did was create another server block with www in the server_name and had my other server blocks redirect to it.\\nserver {\\n    listen 80;\\n    listen [::]:80 default_server ipv6only=on;\\n    return 301 https://www.example.com$request_uri;\\n}\\n\\n# HTTPS — proxy all requests to the Node app\\nserver {\\n    # Enable HTTP/2\\n    listen 443 ssl http2;\\n    listen [::]:443 ssl http2;\\n    server_name example.com;\\n    return 301 https://www.example.com$request_uri;\\n\\n    # Use the Let’s Encrypt certificates\\n    ...\\n}\\n\\nserver {\\n    # Enable HTTP/2\\n    listen 443 ssl http2;\\n    listen [::]:443 ssl http2;\\n    server_name www.example.com;\\n\\n    # Use the Let’s Encrypt certificates\\n    ...\\n}',\n",
       " \"It might but will depend on the host. Some plug-ins are just a huge drag no matter what you do. When you think about it adding extra features via plug-ins might be great for users but slow site will have a reverse effect for UX.\\nMy recommendation is something like WP Engine. They tailor their hosting just for WordPress. Using them also eliminates the need for caching plug-ins. They have all of that running for you on their end. I use them fro about 20 clients and never had a performance issue. I would check the Disallowed Plug-ins page first to make sure that none of yours are on the list.\\nI would stay away from GoDaddy WP hosting because it doesn't really show much improvement in performance when comparing to their standard shared hosting. Basically, stay away from any of the usual suspects like DreamHost and etc. VPS are also not a good option due to cost especially if you can only afford the non managed ones. \\nAlso, I would look for alternatives for the plug-ins that are giving you trouble. I know that there are a ton of Event Calendar plug-ins out there.\",\n",
       " 'You could add a \"Log Data\" clause in your Privacy Policy to inform users that your web server will collect this data automatically.\\nAn example of this kind of clause:\\n\\nThere are certain examples of Privacy Policies where the website does not collect personal data, but these examples are very limited as most websites will collect at least some pieces of data (for example, automated web servers logs) even if the data isn\\'t used by the business.',\n",
       " 'I could not solve above problem for using extensions to embedding PDF in MediaWiki. So finally I used $wgRawHtml and uesed this code in my Wiki page:\\n<html>\\n<embed src=\"http://localhost/<wiki-name>/images/5/59/GraphicsandAnimations-Devoxx2010.pdf#page=14\" width=\"1200px\" height=\"500px\" />\\n</html>',\n",
       " \"Well, after so many failed attempts, I found out that during the transfer of the domain, the nameserver from the previous registrar was still being used in the new registrar.\\nSo, what I did was to update the nameserver of the domain to point to the new registrar's default nameservers, and it worked perfectly.\",\n",
       " 'Google, at least, will consider a name with a hyphen in it as part of the whole. In other words, the name of a product with a URI of \"1111-product\" will become \"1111 product\". So, it would make more sense to use a URI with the actual product name followed by any variation of the name or product: \"/product/1111\".\\nThat can then be thought of product ID 1111. If you come out with a new variation or model of that product, it can have an URI of /product/2222.\\nThis is actually how we do things in a REST methodology.\\nI would not use underscores. Use hyphens for the reason I gave above. Google considers hyphens to be spaces in listings while underscores are not (if I recall correctly). \\nIn addition, underscores are often missed in the address bar cause they\\'re harder to see. \\nThere are other reasons for not using underscores that I just don\\'t recall right now but I abandoned any thoughts of using them 10 years ago.',\n",
       " \"404s do not directly affect your rankings.\\n404s are bad when the user is expecting there to be a page and there is not as this can badly affect user experience which can indirectly affect rankings.\\nIf you just want to solve them to make your search console data look neat then you would have to stop the search robots from crawling  the pages.\\nIf there is a clear pattern to the URL then you could use the robots.txt file them remove this when the page exists.\\nIf there isn't then I don't see there being any safe way of stopping these 404s from being reported without changing the functionality of the site itself (To not create 404 pages)\",\n",
       " 'These are pages that do not need to be indexed and give no real value to the site.\\nThe quickest way to solve this is to block them in the robots.txt file.',\n",
       " \"Looking at pages that work and those that don't has led me to believe the functionality is based on the type of mark-up in a block. \\nFor that to work it seems to work what the largest block is which has the largest ratio of text elements (<p>, <br/>, <ul>, img etc) and text. Anything in a block type element, such a 'div', in that section is then ignored (presumably assuming it is out of context).\\nThere are probably other subtleties, but removing an unnecessary block element from my content section and ensuring it only contained text type markup helped it be selected for the reader.\",\n",
       " 'Under construction pages are generally no longer used. There is a reason for this. It is an option, however, likely not a good one since you indicated that they have good traffic today. It is far better to retain this traffic flow and not disrupt it.\\nMost site owners go ahead and develop the new site and leave to the old site until done. From there, when you are deploying the new site, you can use a 301 redirect from valuable links and for high value pages to the replacement pages seamlessly. This retains search value as much as possible without disruption.\\nOtherwise, while the under construction page exists, even with a 302, you will very likely see a drop in site performance - enough that it may not be easily recovered if at all. Unless the site is totally useless and you simply want to start over and discard old links and search performance, I highly advise not doing what your customer suggested.',\n",
       " \"I think it makes sense to include it on non-HTML content. Not all content can create referrals, but HTML content is not the only thing that can. Flash, for example, often does. CSS can include URL info for fonts and background images, if not other content. In addition, unless I didn't understand the policy correctly, the policy applies to any request, not just to page navigation. Therefore, when the CSS uses a remote resource as a background image, the retrieval of that image is supposed to follow the policy in effect at the time of that fetch. By extrapolation, the same should apply to anything else that requests a resource; Flash, PDF files, JavaScript, etc. The flip side is that browser support, according to the link you provided, is spotty at best. Mostly Firefox, possibly other Mozilla products as well. IE is totally non-supportive, and even Chrome and Safari only provide basic support.\\n\\nSource: MDN Referrer Policy\\nSo, if you need the security from setting that header, you should probably look for other methods as well.\",\n",
       " 'This is a known \"scam\".\\nThis is a legitimate, though questionable approach. Big chance there isn\\'t actually anyone trying to register it, they just tell you that to add time pressure. They do nothing wrong with offering it to (and if they\\'re not offering it now, they will be in their follow up mail).\\nThey often simply do [your_domain].[some less common extention] via an automated process.\\nThey\\'re trying to scare people into buying the domains. We\\'ve encountered this a few times ourselves, that a client of ours will call us to say that someone has offered them the same as you\\'ve been offered. They\\'re often slightly panicked that someone might \\'steal their domain\\'. We then explain that they\\'re hoping you panic and buy the domain(s). Sometimes acting all \\'friendly\\' that they wanted to inform you first.\\nThe question you must ask yourself: \\nIs this a domain I\\'d like to have?\\n- If so, register it yourself (or via your host) because they haven\\'t actually registered it for you\\n- If not -> ignore them.',\n",
       " \"As mentioned in comments, when some redirects work and others don't then it suggests a conflict of some kind. The first thing to try (since it's quick) is to disable MultiViews (it's disabled by default on Apache, but it often gets enabled in many shared server environments):\\nOptions -Indexes +Includes -MultiViews\\n\\nWithout knowing your filesystem structure, it's not clear whether this will have any benefit or not. But if you have directories and file basenames that share the same name then MultiViews (part of mod_negotiation) can cause problems. For instance, a request for /PCHardware/HardwareFaults could result in the file HardwareFaults.html being served, rather than the directory index inside the physical directory /HardwareFaults (if that was how your filesystem was structured).\",\n",
       " 'Basically, the steps are:\\n\\nEnsure that both the subdomain and main domain are verified properties in Google Search Console.\\nRedirect (301 permanent) all pages from the subdomain to the main domain. (This is the most important step.)\\nUse Google\\'s change of address tool to specifically tell Google that the \"site\" has moved from the subdomain to the main domain. (You can only do this if you have completed steps #1 and #2 above.)\\n\\nTake down these pages and send URL removal request in search console.\\n\\nDo not \"send URL removal request\". This will simply remove the URLs from the SERPs. You want Google to recrawl these pages and see the redirect.\\n\\nno-index them only.\\n\\nNo. The pages have simply moved and you want them indexed at the new location (at the main domain).',\n",
       " 'I\\'d say you should never skip such letters unless that\\'s the actual name of the website/company/product/etc.\\nGoing by your example I would understand that papereview.com would be something like paper-e-view - something related to digital viewing of papers. Or Pape-review - a website that reviews Papes (whatever that might be). But you want it to be about reviewing papers so it probably could be misleading if you removed that \"repeating\" letter. Also this might bring more confusion to visitors as instead of writing two full words, they must explicitly remember that one letter must be removed. The same would be when you create a domain with extra letters like Arrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrg.com. So who will actually do the counting of those letters and remember it each time they want to visit the site? \\nWhat you could do is prevent such \"repeats\" (if the situation allows) like changing paperreview.com to something like ireviewpapers.com (depending on what your website is actually about).\\nAlso, SEO might be actually in your favor if you use separate dictionary words instead of making some abomination. Blogging-platform.com would perform better than tumblr.com as it has a searchable phrase in it\\'s domain name by default (though my SEO knowledge is rusty).',\n",
       " \"The term you're looking for is URL fragment, i.e. anything after the # symbol.\\nGoogle Analytics does not track it by default, from what I understand, although I can't find actual GA documentation confirming this.\\n\\nIn Google Analytics, fragment changes are not tracked by default, and the URL paths that are passed to GA with your Pageview hits are stripped of these fragments. With Google Tag Manager, this can be remedied with a History Change Trigger and some Variable magic.\\n\\nThus, www.mycompany.com/accounts#profile is tracked as www.mycompany.com/accounts and hits aggregated with the same URLs with different fragments.\\nThe link above has some useful tips as to how to get fragment tracking working.\",\n",
       " \"Since you have no control over your client sites, and the URL campaign parameters are not preserved, I'd say it is not possible if you link to the client sites first.\\nOn the other hand, if you are able to track on example.com, send the visitor there first and redirect to the client site. You'd need to give the javascript a chance to trigger and run on the browser before redirecting -- alternatively, set up Google Measurement Protocol. Once they navigate back to example.com from the client site, tracking will resume, and since they have a cookie set from visiting example.com first, tracking will resume using the same session.\",\n",
       " 'Since all the domains point to the same filesystem, you don\\'t need to think about the main_domain.com. Although if you did need this (for some back-end reason?) it complicates matters, as you can\\'t simply rewrite to another domain, you would need to proxy the request.\\nUsing mod_rewrite in .htaccess...\\nVerbose method:\\nRewriteEngine On\\n\\n# domain1.com\\nRewriteCond %{HTTP_HOST} domain1\\\\.com [NC]\\nRewriteRule ^home$ index.php?loc=1&sect=1 [L]\\nRewriteCond %{HTTP_HOST} domain1\\\\.com [NC]\\nRewriteRule ^about$ index.php?loc=1&sect=2 [L]\\nRewriteCond %{HTTP_HOST} domain1\\\\.com [NC]\\nRewriteRule ^contact$ index.php?loc=1&sect=3 [L]\\n\\n# Repeat for domain2.com\\n# :\\n\\n# Repeat for domain3.com\\n# :\\n\\nMore concise Method:\\nThis avoids the repetitive checks for the host before each rule.\\nRewriteEngine On\\n\\n# Set an environment variable (LOC) according to which domain is accessed\\nRewriteCond %{HTTP_HOST}_1 domain1\\\\.com_(\\\\d+)$ [NC,OR]\\nRewriteCond %{HTTP_HOST}_2 domain2\\\\.com_(\\\\d+)$ [NC,OR]\\nRewriteCond %{HTTP_HOST}_3 domain3\\\\.com_(\\\\d+)$ [NC]\\nRewriteRule ^ - [E=LOC:%1]\\n\\n# (Optional) If it\\'s not one of the recognised domains then abort (403)\\nRewriteCond %{ENV:LOC} ^$\\nRewriteRule ^ - [F]\\n\\n# Route the request for each domain\\nRewriteRule ^home$ index.php?loc=%{ENV:LOC}&sect=1 [L]\\nRewriteRule ^about$ index.php?loc=%{ENV:LOC}&sect=2 [L]\\nRewriteRule ^contact$ index.php?loc=%{ENV:LOC}&sect=3 [L]\\n\\nThe %1 in the first block is a backreference to whatever matched the captured group (ie. (\\\\d+)) in the last matched RewriteCond directive (if any). This is your location code, which is assumed to be numeric (as in your example). This is then assigned to the LOC environment variable (with [E=LOC:%1]).\\n\\nRewriteCond %{HTTP_HOST}_1 domain1\\\\.com_(\\\\d+)$ [NC,OR]\\n\\nFor example, the 1 in %{HTTP_HOST}_1 is the location code for domain1.com. If %{HTTP_HOST}_ matches the regex domain1\\\\.com_ then the 1 is captured by (\\\\d+) and later assigned to the LOC environment variable.\\nIf you could simply pass the domain name as the \"location code\" then you wouldn\\'t need the first block. You could just pass %{HTTP_HOST} directly and your script would then need to handle it.\\n\\nQUESTION: I used to be able to access the site via server IP (something like 203.0.113.111/~theultr5/index.php...) but it doesn\\'t work now. What rule do I have to add to get this functionality back?\\n\\nNormally you want to block access by the IP address (prevent duplicate content, etc.). However, one way is to make an exception for the IP address. Immediately after the RewriteEngine directive, try the following:\\nRewriteCond %{HTTP_HOST} =203.0.113.111\\nRewriteRule ^ - [L]\\n\\nThis will prevent any further processing if you access the site by the IP address. So, the /home, /about and /contact URLs will not function - you will need to specify the resulting URL yourself.',\n",
       " 'You\\'re looking for the rel next, rel previous HTML attributes: https://support.google.com/webmasters/answer/1663744?hl=en\\nFrom the source link above, here is the generic example:\\nhttp://www.example.com/article-part1.html\\nhttp://www.example.com/article-part2.html\\nhttp://www.example.com/article-part3.html\\nhttp://www.example.com/article-part4.html\\n\\nIn the  section of the first page (http://www.example.com/article-part1.html), add a link tag pointing to the next page in the sequence, like this:\\n<link rel=\"next\" href=\"http://www.example.com/article-part2.html\">\\n\\nBecause this is the first URL in the sequence, there’s no need to add markup for rel=\"prev\".\\nOn the second and third pages, add links pointing to the previous and next URLs in the sequence. For example, you could add the following to the second page of the sequence:\\n<link rel=\"prev\" href=\"http://www.example.com/article-part1.html\">\\n<link rel=\"next\" href=\"http://www.example.com/article-part3.html\">\\n\\nWhen developing, one of the most common development mistakes is to not end the rel next attribute on the final item in the cluster. For example, if your 5 part article ends at /article-part5, ensure you do not include a rel next to /article-part6. Seems obvious, but it\\'s a common oversight - the real problem occurs when the server /article-part6 returns \"something\\' with a 200 response code, which also contains a rel next directive. That means these rel next links to \"fake\" pages get crawled, indexed and bloat your site, add overhead for crawlers and create unnecessary duplication.',\n",
       " \"Most commonly unknown 404's come from third party linking either mis-spelling a link, or as appears in this case, randomly creating a whole heap of links in the hope that something links. Basically it is safe to ignore these 404 errors. The fact that GWT shows them as 404 errors means in all likelihood your 404's are correctly returning hard 404's that Google can detect.\",\n",
       " \"Firstly using port 587 isn't enough on its own. This particular error message is common when trying to send email through an SMTP server that you haven't authenticated with. Trying to authenticate with the web link is useless as you need to authenticate the SMTP session. In your SMTP settings you need to specify the server, username (which is your full outlook.com email address), your outlook.com password, and specify TLS/SSL required as yes. Once you have done this your session is authenticated with outlook.com.\\nNow as you don't mention if you are using a free outlook.com email address or a business paid outlook.com email address I will just add a caveat I have encountered in the past where outlook.com has chucked a fuss at sending what its filters deem to be automated emails when the account is a free account. Couldn't find any documentation on it or even any specific support from Microsoft but it seems like there may be a restriction on free email addresses on outlook.com sending automated emails which should be sent either from a paid account or another SMTP server.\",\n",
       " 'Have worked this out...\\nA check of DeepCrawl shows that they did some testing some time in the past and found that when crawling sitemap files Google does not have a huge issue with sitemaps taking a while to generate however sitemaps which took over a minute where hit or miss on if Google would accept them or not and nothing over 200 seconds was ever indexed. DeepCrawl has suggested out of their testing that sitemaps can be generated in real time but that webmasters should aim for a sitemap generation time of no more than 60 seconds to ensure that Google will accept the sitemap.\\nA further check has shown that through testing the code even if every single sitemap file was requested at the same time and generated in real time at the same time and was pulling a million rows from the database the generation time would only be around 30 seconds, and I believe this could be further optimised in time, so live generation would seem to be a good fit here.\\nI should note that this may not be the right way for everyone and whether you use scheduled generation or live generation of your sitemaps depends on your specific use case, the underpinning technology, and the criticality of having the most up to date sitemap possible.',\n",
       " \"You see this on a number of sites and generally it is related to the underlying software technology powering the site. A number of libraries here in Australia have a similar feature on their online catalogues and according to them it relates to the software that powers the catalogues.\\nBasically what is happening is that when you connect to the online catalogue a new session is being created as a guest session (similar to if you entered a username and password attached to a guest account). This allows you to browse the catalogue without needing to log in, however it uses the same code as what supervises authenticated sessions for actual users. The timeout is a security feature that is inserted into the catalogue as generally the online catalogue is also used on shared computers in the library itself. This feature means that after 2 minutes of not being used if someone else where to go onto the catalogue computer it would have reset the session so that the new user couldn't access the previous users search record or patron profile.\\nAs a side note a quick check of the html for the LoC online catalogue shows a common directory name of /vwebv. A check online shows that this exists in the HTML and/or link structure for a large number of library catalogues. Further searches indicate that this software was created by ExLibris which was acquired by ProQuest in 2015, and is used in some of the worlds largest libraries with the same feature enabled based on my testing.\\nDisclaimer: I am not in any way affiliated with LoC, ExLibris, or ProQuest and have come across all of this through my research.\",\n",
       " 'Edit based on Joao\\'s comment*\\nThe domains in question are cctld, i.e .es, .it, .pt. Geotargeted subfolders on country specific domains don\\'t work well, if the top level domain is generic (i.e. .com) this solution works. Otherwise, from a pure \"best for ranking\" perspective, keeping cctlds that only serve one country is best, but, like every solution, each has it\\'s drawbacks such as additional technical overhead and having to market/optimize each cctld independently rather than consolidating signals to one domain (subfolder approach).\\nEdit end\\nGreat question, I don\\'t fully understand your setup but I think this should describe it and how to deal with it:\\nWhere...\\nexemple.com/index_en.html === exemplo.com/index_en.html &&\\nexemple.com/index_es.html === exemplo.com/index_es.html\\nSetup:\\nYour 3rd option is best, use 301 redirects to redirect all pages from exemplo.com to exemple.com on a 1:1 basis, i.e. redirect each page to its equivalent. You should determine which domain is the preferred domain by checking which one ranks better/gains more traffic (the preferred domain should also have higher external link count and links from decent websites)\\nThe preferred domain in this hypothetical scenario will be exemple.com. When a request originating from a UK IP to exemple.com, you will 302 redirect them to exemple.com/index_en.html. \\nA request originating from Spain with a Spanish IP to exemple.com should 302 to exemple.com/index_es.html.\\nThat would be it - don\\'t redirect any other requests on the domain, use hreflang to annotate the spanish and english versions accordingly.\\nI did the Redbull.com setup and a friend of mine did theguardian.com.  We followed Google engineer\\'s advice despite doubting the 302 redirect. The hreflang lead, Christopher Semturs confirmed in a comment on a post I wrote here.\\nI would highly advise you take precaution when consolidating your domains (vigoriously check google search console, run crawls, etc..) and test the international configuration (be ready to roll back immediately) as well.',\n",
       " 'Morgan Lewis is a law firm with offices in the UK and they have published information (as of 2012 which is the most recent authoritative publication I can find on the subject) on the ICO\\'s guidance on deleting personal data under the DPA 1998.\\nAccording to their legal assessment of the guidance the ICO recognized the difficulty in deleting electronic data under the act as it can still exist in the organisations systems in one form or another (backup records would seem to apply here) and as such they have adopted what they refer to as a \"realistic approach\" towards the deletion of electronic data on the basis that it is possible to put the data \"beyond use\" without actually deleting every last trace of the data. The article states that the key findings of the ICO are...\\n\\nWhere information has been deleted, but where it still exists in the \"electronic ether\", such data will not be \"live data\", and therefore data protection compliance issues will not apply to the data, as long as the data controller does not intend to use or access the data again. The ICO draws an analogy with a bag of shredded paper files-it would be possible to reconstitute the information from the shredded paper, but it would be extremely difficult, and it is unlikely that the organisation would have any intention of doing so.\\n\\nIt is possible for a data controller to put undeleted data \"beyond use\" if the data controller is not able, or will not attempt, to use the personal data to inform any decision in respect of any individual or in a manner that affects the individual in any way, does not give any other organisation access to the personal data, puts appropriate security measures in place in relation to the data, and commits to permanent deletion of the information if and when it becomes possible.\\n\\nBased on the above one option that would not require the extraction, decompression, then recompression and storage of a large number of backup archives would be to add some form of data source separate to the existing backup and restoration systems where an index of people who have opted to have their data deleted are recorded. Then if a restoration of backed up data needs to be done after the restoration has been completed the index can be loaded and the records gone through to see if anyone listed in the index has had their personal information restored through backups, and then have it deleted as needed. Given the fact that you state that you have not had to deal with this to date an easier option (given the low chance of restoring data where someone has opted to be deleted) would be to maintain hardcopy records of deletion requests with the minimal amount of data needed to identify the record needing to be deleted and establish a business policy where one of the steps taken after data restoration is to compare these records to the restored database and see if a record which is meant to have been deleted has been restored.\\nThis would work based on my reading of the Act and my reading of the supporting article by the Morgan Lewis law firm and would be unlikely to cause a major hassle being even a manual process as the number of records that would be requested to be deleted prior to the recommended 6 year auto destruction of data timeline would be low to begin with, and when taken with the rarity of needing to restore complete copies of databases from backup archives would wind up reducing it to an exceedingly small level whereby the process could be achieved in a very small amount of time by a pre-defined user going through and searching for the requested data based on the deletion index list to ensure that if it has been restored it is then deleted manually again. Based on what you have stated this would mean that only data deleted in the last 93 days would likely need to be manually deleted again (even less chance as it is a higher chance of restoring a more recent backup) which would present minimal manpower requirements and ensure compliance with the act.',\n",
       " \"You can literality use any font you choose regardless whether or not they have the font installed on their OS. This is done by using CSS @font-face which informs the users browser to download the font to their device. However the font needs to be served as multiple compatible web font formats because different operating systems use different formats. This can be easily done by using any decent online webfont generator. \\nWeb Font Formats:\\n\\nTrueType Fonts (TTF)\\nOpenType Fonts (OTF)\\nThe Web Open Font Format (WOFF)\\nThe Web Open Font Format 2.0 (WOFF)\\nSVG Fonts (SVG)\\nEmbedded OpenType Fonts (EOT)\\n\\nLicensing Problems\\nThe main issue with serving fonts isn't the technology but more so because the licensing of the font does not allow for free web use even if you have received the font free with your operating system or application. You can however purchase a web use license for most fonts.\\nHow do people overcome the licensing issue?\\nNowadays most websites use online fonts provided by Google Fonts because it is free for both personal and commercial use. You can find many other open font providers using Google search or you could opt to use a paid service from Adobe called TypeKit.\\nHelvetica Alternative\\nThere are many Helvetica alternative fonts that are similar such as Google's Open San. Using CSS you could actually serve the font Arial to Windows machines and Helvetica to MacOS machines by putting Helvetica first in the font-family.\\nList of System Fonts on Mac / Windows\\nYou should note that mobile devices often use different operating systems from desktop and laptop computers, for example Android, BlackBerry and so on, this makes services like TypeKit or Google Fonts more appealing because it solves this issue. You can find a list below of the system fonts installed on both Windows and MacOS.\\n\\nStatistics from CSS Font Stack\\n\\nArial - Win: 99.84% | Mac: 98.74%\\nArial Black - Win: 98.08% | Mac: 96.22%\\nArial Narrow - Win: 88.39% | Mac: 94.77%\\nArial Rounded MT Bold - Win: 59.53% | Mac: 95.14%\\nAvant Garde - Win: 0% | Mac: 1.08%\\nCalibri - Win: 83.41% | Mac: 38.74%\\nCandara - Win: 83.08% | Mac: 34.41%\\nCentury Gothic - Win: 87.62% | Mac: 53.15%\\nFranklin Gothic Medium - Win: 99.18% | Mac: 2.1%\\nFutura - Win: 1.26% | Mac: 94.41%\\nGeneva - Win: 2.08% | Mac: 99.64%\\nGill Sans - Win: 58.54% | Mac: 95.5%\\nHelvetica - Win: 7.34% | Mac: 100%\\nImpact - Win: 0% | Mac: 95.14%\\nLucida Grande - Win: 0% | Mac: 100%\\nOptima - Win: 2.52% | Mac: 93.69%\\nSegoe UI - Win: 75.36% | Mac: 0%\\nTahoma - Win: 99.95% | Mac: 91.71%\\nTrebuchet MS - Win: 99.67% | Mac: 97.12%\\nVerdana - Win: 99.84% | Mac: 99.1%\\nSerif - Big Caslon - Win: 0% | Mac: 92.61% \\nBodoni MT - Win: 55.81% | Mac: 0% \\nBook Antiqua - Win: 86.09% | Mac: 49.01%\\nCalisto MT - Win: 58.43% | Mac: 46.31%\\nCambria - Win: 83.35% | Mac: 35.32%\\nDidot - Win: 0% | Mac: 93.51%\\nGaramond - Win: 86.47% | Mac: 49.91%\\nGeorgia - Win: 99.4% | Mac: 97.48% \\nGoudy Old Style - Win: 58.11% | Mac: 47.57%\\nHoefler Text - Win: 0.99% | Mac: 92.61%\\nLucida Bright - Win: 76.12% | Mac: 99.64%\\nPalatino - Win: 99.29% | Mac: 86.13%\\nPerpetua - Win: 66.54% | Mac: 0%\\nRockwell - Win: 65.94% | Mac: 0%\\nRockwell Extra Bold - Win: 66.1% | Mac: 0%\\nBaskerville - Win: 60.35% | Mac: 93.33%\\nTimes New Roman - Win: 99.67% | Mac: 97.48%\\nMonospaced Consolas - Win: 82.97% | Mac: 34.77%\\nCourier New - Win: 99.73% | Mac: 95.68%\\nLucida Console - Win: 99.18% | Mac: 0%\\nLucida Sans Typewriter - Win: 74.81% | Mac: 99.64% \\nMonaco - Win: 2.74% | Mac: 99.82%\\nAndale Mono - Win: 4.16% | Mac: 94.59%\\nFantasy Copperplate - Win: 66.87% | Mac: 92.61%\\nPapyrus - Win: 70.37% | Mac: 92.43%\\nBrush Script MT - Win: 59.64% | Mac: 90.99%\",\n",
       " \"If you check out this page https://creativecommons.org/licenses/by/3.0/ that I found by searching for CC3 reuse of work you will see that:\\n\\nYou are free to:\\nShare — copy and redistribute the material in any medium or format\\n  Adapt — remix, transform, and build upon the material for any purpose,\\n  even commercially. The licensor cannot revoke these freedoms as long\\n  as you follow the license terms.\\n\\nHowever, as it is stated further down the page, you must give appropriate credit\\n[EDIT] On re-reading your question I am not sure I am answering it.  You want to know if you can share your work commercially with no CC3 license if it contains others' CC3 works\\nAs you have no doubt noticed, reading and understanding CC3 licensing is not for the faint of heart.  \\nThis likely does answer the question though \\n\\nWhen you receive material under a Creative Commons license, you may\\n  not place additional terms and conditions on the reuse of the work .\\n\\nBut it is more complex than that, check out other answers on that FAQ page.\",\n",
       " \"There's a great roundup of the factors, and how important they are thought to be, at SEOMoz\\nhttp://www.seomoz.org/article/search-ranking-factors#ranking-factors\\nOn-Page (Keyword-Specific) Ranking Factors\\n\\nKeyword Use Anywhere in the Title Tag\\nKeyword Use as the First Word(s) of the Title Tag\\nKeyword Use in the Root Domain Name (e.g. keyword.com)\\nKeyword Use Anywhere in the H1 Headline Tag\\nKeyword Use in Internal Link Anchor Text on the Page\\nKeyword Use in External Link Anchor Text on the Page\\nKeyword Use as the First Word(s) in the H1 Tag\\nKeyword Use in the First 50-100 Words in HTML on the Page\\nKeyword Use in the Subdomain Name (e.g. keyword.seomoz.org)\\nKeyword Use in the Page Name URL (e.g. seomoz.org/folder/keyword.html)\\nKeyword Use in the Page Folder URL (e.g. seomoz.org/keyword/page.html)\\nKeyword Use in other Headline Tags ( – )\\nKeyword Use in Image Alt Text\\nKeyword Use / Number of Repetitions in the HTML Text on the Page\\nKeyword Use in Image Names Included on the Page (e.g. keyword.jpg)\\nKeyword Use in  or  Tags\\nKeyword Density Formula (# of Keyword Uses ÷ Total # of Terms on the Page)\\nKeyword Use in List Items  on the Page\\nKeyword Use in the Page’s Query Parameters (e.g. seomoz.org/page.html?keyword)\\nKeyword Use in  or  Tags\\nKeyword Use in the Meta Description Tag\\nKeyword Use in the Page’s File Extension (e.g. seomoz.org/page.keyword)\\nKeyword Use in Comment Tags in the HTML\\nKeyword Use in the Meta Keywords Tag\\n\\nOn-Page (Non-Keyword) Ranking Factors\\n\\nExistence of Substantive, Unique Content on the Page\\nRecency (freshness) of Page Creation\\nUse of Links on the Page that Point to Other URLs on this Domain\\nHistorical Content Changes (how often the page content has been updated)\\nUse of External-Pointing Links on the Page\\nQuery Parameters in the URL vs. Static URL Format\\nRatio of Code to Text in HTML\\nExistence of a Meta Description Tag\\nHTML Validation to W3C Standards\\nUse of Flash Elements (or other plug-in content)\\nUse of Advertising on the Page\\nUse of Google AdSense (specifically) on the Page\\n\\nPage-Specific Link Popularity Ranking Factors\\n\\nKeyword-Focused Anchor Text from External Links\\nExternal Link Popularity (quantity/quality of external links)\\nDiversity of Link Sources (links from many unique root domains)\\nPage-Specific TrustRank (whether the individual page has earned links from trusted sources)\\nIterative Algorithm-Based, Global Link Popularity (PageRank)\\nTopic-Specificity/Focus of External Link Sources (whether external links to this page come from topically relevant pages/sites)\\nKeyword-Focused Anchor Text from Internal Links\\nLocation in Information Architecture of the Site (where the page sits in relation to the site’s structural hierarchy)\\nInternal Link Popularity (counting only links from other pages on the root domain)\\nQuantity & Quality of Nofollowed Links to the Page\\nPercent of Followed vs. Nofollowed Links that Point to the Page\",\n",
       " \"As with any other page detection in Google it is done through linking. Zillow doesn't host every single address as a page rather it is being done using a single page template and a database of addresses. In order to make Google detect these there are two methods which are used by the various sites that do something similar. One is to create an index of all the addresses through a series of links so that search engines can detect it using standard crawling practices (this is what Zillow appears to do).\\nThe other method (which is done by a few similar sites here in Australia) is to use Sitemap files, grouped by geographic region, and link them all through a sitemap index file which is all re-generated regularly (anywhere from every 3 hours to every 24 hours) and then submitted to Google or even just left in place until Google re-indexes the site and detects the updated sitemaps.\\nI will add a side note that some sites use both techniques for added reliability in being detected, and you should always make sure that if the link exists or the sitemap entry exists that the address (the page) can be accessed at that link and doesn't wind up failing with a 404 error.\",\n",
       " \"It's not exactly that your ranking is dropping. What of the huge number of signals used by Google in indexing and ranking sites and pages is the frequency of updates and how recently the site has been updated. Now not updating your site for a week or two isn't going to reduce your ranking, rather the other sites rankings are increasing as they may have new content.\\nAs @Simon Hayter points out there is no hard and fast rule or magic number on how many articles per week or month should be added. It is actually to do with the number of interactions made with the site. A good balance that I have found is one new blog article a week or fortnight (the time doesn't seem to matter so much as long as it is reasonably regular) and as this attracts more readership to the site and in turn the potential for more linking and sharing this can help increase ranking as well.\",\n",
       " 'You will run into duplicate content issues sooner or later. It is just a matter of time.\\nYou have two choices, three actually, but really just two.\\n1] Redirect HTTP to HTTPS or HTTPS to HTTP. I recommend HTTP to HTTPS since you seem to have what you need, however I understand not taking the leap. HTTPS adds an additional Trust factor. However, not wanting to take the leap today, which is perfectly understandable, you can just make sure that there is a redirect from HTTPS to HTTP.\\n2] Add a canonical tag in each page that points to the same page using either HTTP or HTTPS. Since you seem to prefer HTTP for now I suggest pointing to HTTP.\\n3] Do nothing. I do not recommend option 3. [insert cheesy grin]\\nOf the options, option 1 is the easiest. Redirecting HTTPS to HTTP for now will solve the problem quickly.\\nHere is a Q&A on just your scenario: https://stackoverflow.com/questions/12999910/https-to-http-redirect-using-htaccess',\n",
       " 'As long as the only thing that you are changing is moving to https, then all that you would need to do is add the following code to your .htaccess file, and make sure that is is above everything else (if you already have code on the .htaccess): \\nRewriteEngine On \\nRewriteCond %{SERVER_PORT} 80 \\nRewriteRule ^(.*)$ https://www.example.com/$1 [R,L]\\n\\nBe sure to replace www.example.com with your actual domain name.\\nNext, be sure that all versions of your website are verified in Google Console. This includes the www version, non-www version, as well as the ssl-www version, and the ssl-non-www version. Be sure to tell Google which version of the 4 is the preferred version.',\n",
       " 'I assume you want to remove the trailing slash (if any) with an external redirect in order to canonicalise the URL and then append the .php extension with an internal rewrite (ie. hidden from the user) in order to correctly route the URL. This is a two-stage process.\\nNB: The trailing slash should remain if accessing a physical directory.\\nUsing mod_rewrite in .htaccess:\\nRewriteEngine On\\n\\n# Remove the trailing slash (if any) for non-directories\\n# This is essentially unconditional in order to canonicalise the URL\\nRewriteCond %{REQUEST_FILENAME} !-d\\nRewriteRule (.*)/$ /$1 [R=302,L]\\n\\n# Append \".php\" if that file would exist (internal rewrite / hidden from user)\\nRewriteCond %{DOCUMENT_ROOT}/$1.php -f\\nRewriteRule (.*) $1.php [L]\\n\\nChange the R=302 (ie. temporary redirect) to R=301 to make it permanent. 302s are easier to test with as they aren\\'t cached by the browser. (Make sure the browser cache is clear before testing.)\\nIt is \"wildcard\" in the sense that it works for any file depth. .* simply grabs everything.\\nURL parameters will be appended onto the substitution by default - nothing special you need to do in this respect.\\n\\nUPDATE: Since the canonical URL would seem to be the URL with the trailing slash (to which requests are being POST\\'d) then the above should be changed to the following:\\nRewriteEngine On\\n\\n# Append .php if that file exists (internal rewrite)\\nRewriteCond %{DOCUMENT_ROOT}/$1.php -f\\nRewriteRule (.*?)/?$ $1.php [L]\\n\\nThis will accept requests for both /path/to/file and /path/to/file/ and internally rewrite the request to /path/to/file.php. You will need to handle the URL canonicalisation in your script. If all your URLs already point to /path/to/file/ (ie. with a trailing slash) then you can modify the RewriteRule pattern and remove both ? ie. (.*)/$ - this makes the trailing slash mandatory on the request.',\n",
       " \"You'd have to modify where the existing (first) tracking code is tracking events. Instead of \\nga('send', 'event', 'Contact', 'contact-form', 'Goals');\\ntry:\\nga('second.send', 'event', 'Contact', 'contact-form', 'Goals');\\nThe second tracking script has been given a different name in order to differentiate it from the first tracking script, so simply swapping names in the event tracking calls should be enough. Alternatively, remove the first tracker and remove the second tracker's name, thus:\\nga('create', 'UA-XXXXXXXX-1', {'name':'second'});\\nga('create', 'UA-XXXXXXXX-1', auto);\\nMore info on creating trackers: https://developers.google.com/analytics/devguides/collection/analyticsjs/creating-trackers\",\n",
       " \"It is good practice to update the sitemap when new content is added to the site, or existing content is updated, or deleted. This however does not assure it will be taken into consideration. While Google and in fact most web crawlers use the sitemap file for helping direct the crawling efforts it doesn't control crawling. If you update the page content most crawlers will detect this when they re-crawl the page, however simply updating the sitemap with the new lastMod date and time won't compel them to rather it will be taken as a signal to indicate that the content may have been updated and the crawler may crawl it sooner, or may wait until the next scheduled crawl.\",\n",
       " 'Course\\nCourse seems quite promising since for an online training course, it does describe your case pretty precisely.\\n\\n\"A description of an educational course which may be offered as distinct instances at different times and places, or through different media or modes of study.\"\\n\\nUnfortunately it is pending so won\\'t be in general use yet. If you\\'re confident in being an early adopter pick this one but you are unlikely to see benefits until search engines or other entities start processing it and it leaves pending.\\nProduct\\nFor Product, you have the advantage of being the most common and generic type that is used by search engines for snippets (I note you added the SEO tag) so I would select this one to leverage the existing infrastructure, validations, and stability of this type.\\nEvent\\nFor Event, it is something that happens at specific times and locations so I believe this makes it not suitable for something that is continuously available.',\n",
       " 'About your HTML+Microdata\\nYou can’t use the mainEntity property like that. It has to take an item (with itemscope) as value, but you are giving it the textContent of the main element as value. The main entity on your page seems to be a list of article teasers, so you could use the ItemList type:\\n<main itemprop=\"mainEntity\" itemscope itemtype=\"http://schema.org/ItemList\">\\n\\nTo add each item (i.e., each article teaser) to this list, you need to provide additional properties (see the properties on the ItemList type page).\\nYou could of course also omit the mainEntity property.\\n\\nInstead of providing a link element for the mainEntityOfPage property, you can use the existing a element and give it both properties:\\n<a href=\"/title_1.html\" itemprop=\"mainEntityOfPage url\">Title #1</a>\\n\\nYour questions\\n\\nThe thing is, that if I got it right, I shouldn\\'t apply detailed schema to the titles, images, excerpts but I should simply indicate the url to where the full article located, right?\\n\\nNo. The best practice is to mark up the content you provide. So if the teaser contains an image, you can use the suitable property, etc.\\nProviding only the url property can make sense if you care about file size, or if you have to edit the markup manually and you prefer to have to update it in one place only (i.e., the actual article page).\\n\\nThe Google Structure Tool pop ups with a bunch of errors\\n\\nThese aren’t actual errors. These are just the required properties for one of Google’s rich result. Nothing bad happens if you don’t provide them, except that you won’t get a rich result for this page, of course.\\nNote that Google doesn’t show their article rich result for teaser pages anyway, even if you provide all necessary properties.\\nIf you want to provide the author for the teaser, but you don’t want to show the author’s name (not a good practice, but possible), the most simple way is to use span and meta elements (visually hidden by default):\\n<span itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">\\n  <meta itemprop=\"name\" content=\"\" />\\n</span>',\n",
       " '... meta description tag in google\\n\\nTo be clear, the description that appears in the SERPs in just that, a \"description\", it\\'s not \"the meta description tag\". Google will show the contents of the meta description in the SERPs if it closely matches what the user is searching for, or there is no other text on the page. However, Google will often dynamically generate the description that appears in the SERPs from whatever the user is searching for and the textual content on the page.\\nIn fact, pages will often omit the meta description entirely these days since Google does a good (if not better) job of dynamically generating a description. And sometimes it\\'s not easy for the author of the page to create a suitable meta description anyway. (It also doesn\\'t affect ranking, just takes up a few extra bytes and takes time to maintain.)\\n\\nIs there anything that I can do to properly show the tag?\\n\\nYou may just be unlucky. Google will certainly show text like <html-tag> in the SERPs. You just need to do a search for iframe to see this. However, none of these pages appear to use the text &lt;iframe&gt; in the meta description. In fact, some of these pages don\\'t have a meta description tag (see above). Google is generating this description from the page text.\\n\\ngoogle also adds my domain name at the end of the title\\n\\nIt has often been suggested that this happens when your title tag is too short (eg. below the 55 char \"optimum\"), as closetnoc suggests in comments. This might well be part of it, however, this is certainly not the complete answer by any means, as there are plenty of short titles returned in the SERPs that don\\'t have the domain appended to the end.\\nFor example, currently, if I search for hello on Google I get the following page titles returned:\\n\\nHello Magazine (14 chars)\\nAdele - Hello - YouTube (23 chars)\\nHELLO! | Facebook (17 chars)\\nhello network (13 chars)\\nHello (5 chars)\\nAbout - Hello (13 chars)\\n\\nFrom that example it\\'s difficult to see how the length of the title has anything to do with it?',\n",
       " 'The example is invalid HTML+Microdata. It is not allowed to have the itemprop attribute on meta[name] or link[rel] elements.\\nThe solution for HTML+Microdata would be to duplicate the elements:\\n<head itemscope itemtype=\"http://schema.org/WebSite\">\\n  <title itemprop=\"name\">Example.com - Best Website in the World</title>\\n  <meta name=\"description\" content=\"Blah Blah Blah\">\\n  <meta itemprop=\"description\" content=\"Blah Blah Blah\">\\n  <link rel=\"canonical\" href=\"https://example.com/\">\\n  <link itemprop=\"url\" href=\"https://example.com/\">\\n</head>\\n\\nWith HTML+RDFa, it’s possible to mix:\\n<head typeof=\"schema:WebSite\">\\n  <title property=\"schema:name\">Example.com - Best Website in the World</title>\\n  <meta name=\"description\" property=\"schema:description\" content=\"Blah Blah Blah\">\\n  <link rel=\"canonical\" property=\"schema:url\" href=\"https://example.com/\">\\n</head>',\n",
       " 'To handle category posts of the form /name-of-category/2008/10/20/sample-post then include something like the following directive after the existing redirect.\\nRewriteRule ^([a-z-]+)/\\\\d{4}/\\\\d\\\\d/\\\\d\\\\d/(.+) /$1/$2 [R=301,L]\\n\\nThis would redirect /name-of-category/2008/10/20/sample-post to /name-of-category/sample-post. Note that the category consists of just the lowercase a-z and the - (hyphen), as in your example.\\nIf you needed to allow more characters in the category then changing [a-z-] to [\\\\w-] in the above RewriteRule pattern would allow a-z, A-Z, 0-9, _ and -. But it is better to be as restrictive as possible.\\nSo, in summary:\\nRewriteEngine On\\nRewriteRule ^\\\\d{4}/\\\\d\\\\d/\\\\d\\\\d/(.+) /$1 [R=301,L]\\nRewriteRule ^([a-z-]+)/\\\\d{4}/\\\\d\\\\d/\\\\d\\\\d/(.+) /$1/$2 [R=301,L]\\n\\n:',\n",
       " 'I think you have done the correct thing by serving up a blank page. It looks like Google is just being slow to update. If those scraper websites have little traffic then Google won\\'t be visiting them often and it could take months before every page is removed.\\nHowever, I would suggest removing the \\'nofollow\\' on the page. With nofollow the link is not counting towards your site - if you remove it, your PageRank should be boosted a little more and help to keep you above future scrapers you haven\\'t blocked yet.\\nYou also said the scraper sites are removing all JS code but if you\\'re able to put other HTML on the page you could consider either a meta refresh tag:\\n<meta http-equiv=\"refresh\" content=\"0;URL=\\'YOURURL\\'\" />\\n\\nOr a JavaScript onload attribute:\\n<body onload=\"window.location.href=\\'YOURURL\\'\">\\n\\nAccording to this meta refresh is observed by search engines so again may speed up removal of the scraper sites.',\n",
       " 'Unrelated or off-topic niche backlinks can damage your sites reputation unless they can be associated with one another through a brand or trading name.\\nFor example:\\n\\nStack Exchange\\n\\nStack Overflow\\nPro Webmasters\\n\\nDixons Retail\\n\\nPC World\\nCurrys\\nKnowhow\\nDixons travel\\nDSGi Business \\n\\nOne or two off topic links isn\\'t going to do a great deal in terms of being punished by Google or Bing but they are unlikely going to help either. You should only want to link in and from such websites if they are directly associated with one another because it helps promote the brand.\\nIf ever in doubt... always use rel=\"nofollow\" on your links.',\n",
       " \"all of them have redirected\\n  http://sub.example.com To https://www.sub.example.com\\n\\nThat would seem to suggest that you have a rule that simply checks for the absence of www (which is correct for the first rule, but would conflict with the second) and then prefixes this to the requested host.\\nI assume you also want to redirect http://example.com/<something> to https://www.example.com/<something> etc?\\nTry the following at the top of your .htaccess file:\\nRewriteEngine On\\n\\n# www.example.com\\nRewriteCond %{HTTPS} !on [OR]\\nRewriteCond %{HTTP_HOST} ^example\\\\.com [NC]\\nRewriteCond %{HTTP_HOST} !(www\\\\.)?sub\\\\.example\\\\.com\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\nThis basically states... for all requests that are HTTP (ie. not HTTPS) OR are for example.com AND is not for sub.example.com (or www.sub.example.com) then redirect to https://www.example.com/....\\nThen add a similar rule block for the subdomain canonicalisation:\\n# sub.example.com\\nRewriteCond %{HTTPS} !on [OR]\\nRewriteCond %{HTTP_HOST} ^www\\\\.sub\\\\.example\\\\.com [NC]\\nRewriteCond %{HTTP_HOST} !(www\\\\.)?example\\\\.com\\nRewriteRule (.*) https://sub.example.com/$1 [R=301,L]\\n\\nMake sure you've cleared the browser cache before testing, since your earlier (erroneous) 301 redirects will have been cached. (It can often be easier to test with 302s for this reason.)\\n\\nUPDATE: can I do a wildcard on the subdomain?\\n\\nA subdomain of 1 or more characters...\\n# www.example.com (wildcard subdomain)\\nRewriteCond %{HTTPS} !on [OR]\\nRewriteCond %{HTTP_HOST} ^example\\\\.com [NC]\\nRewriteCond %{HTTP_HOST} !(www\\\\.)?[a-z0-9-]+\\\\.example\\\\.com\\nRewriteRule (.*) https://www.example.com/$1 [R=301,L]\\n\\n# <any-subdomain>.example.com\\nRewriteCond %{HTTPS} !on [OR]\\nRewriteCond %{HTTP_HOST} ^www\\\\.[a-z0-9-]+\\\\.example\\\\.com [NC]\\nRewriteCond %{HTTP_HOST} !(www\\\\.)?example\\\\.com\\nRewriteCond %{HTTP_HOST} ^(?:www\\\\.)?([a-z0-9-]+)\\\\.example\\\\.com [NC]\\nRewriteRule (.*) https://%1.example.com/$1 [R=301,L]\\n\\nThe additional RewriteCond directive for the subdomain block is required in order to capture the subdomain so we can use this in the RewriteRule substitution (%1 backreference). The (?:www\\\\.)? part of the CondPattern is a non-capturing group that makes the www sub-subdomain optional, since we need to be able to redirect sub.example.com (HTTP) or www.sub.example.com (HTTP or HTTPS).\\nThe %1 backreference in the RewriteRule substitution matches the first captured group in the last matched CondPattern.\",\n",
       " \"If you don't want to allowed to crawl your website on Google search result, then use this robots.txt\\nUser-agent: *\\nDisallow: /\\n\\nThese will block all directories.\\nBut if many website links to your website, then Google will start displaying this snippet in future.\\n\\nNow if you don't want to index that website competely, then use noindex meta-tags/HTTP headers. That meta tags simply no index your all pages, but it is allowed to crawl. So if your main concern is about to, not visible in search result, then I highly recommended to use noindex tags.\\nBut don't use it both, because when the site is blocked by robots.txt then Google will not going to see your meta-tags/http headers.\",\n",
       " \"I think only an editor can give you a true answer but, from my experience, DMOZ has been awful quiet since Google dropped them in July 2011.\\nLet's see what the public thinks about DMOZ though...\\n\\nOuch.\\n\\nUpdate: As of Mar 17, 2017, dmoz.org is no longer available. The editors have set up a static mirror here. \",\n",
       " 'As Simon has already stated in comments, Googlebot probably does not pay much attention to the <changefreq> property in the XML sitemap when deciding how often to crawl a URL.\\n\\nPages which are set to never update 92 errors\\n\\nBut it is not strictly an error to set this to \"never\". However, it may not be the best option in this case.\\nAs sitemaps.org notes:\\n\\nThe value \"never\" should be used to describe archived URLs.\\n\\nAn archived URL should literally never change. A page that is \"sometimes\" updated, even though it may not change much, is not really an archived document.\\nA value of \"yearly\" (or even \"monthly\") might be more appropriate. Note that these values are relative. \"yearly\" doesn\\'t necessarily mean once a year, but rather less frequently than once a month. It\\'s a very approximate time frame.\\nOr, omit the <changefreq> entirely.',\n",
       " 'Your old registrar cannot not transfer a domain that has expired. They are just following the rules of the Internet road. You will very likely have to renew with the old company before transferring to the new company.\\nThe good news is that most registrars understand and will transfer your domain without an additional charge knowing the situation. Explain it to them of course. This is because they want your business. For example, GoDaddy did this for me on several remaining domain names out of 30+ but I had to renew them first. After that, they transferred the domain names for me and charged me only when the domain name expired the next year.\\nOn one domain name where I had a similar problem as you are having, GoDaddy called the old/crappy registrar and got it done for me. I had to call GoDaddy tech support, explain things, and explain it again to a supervisor, but it worked a peach! You may have to do something similar.\\nYou may have to get tough with your old registrar. Companies do not like attorneys calling. I often get attorneys to write a letter or make a phone call to resolve simple issues like this. Sometimes the attorney does not charge anything or charges very little. I think the most I have ever paid is $100. Generally, it is just the price of a paralegal ($30 or so) or even free. Why free? They want your business! It is a reoccurring theme.',\n",
       " \"The domain has finally been moved. It runs out that there was/is a problem with the name.com mail servers. The losing registrar received 25 emails in the space of 2 minutes with the transfer request, all of which were out of date. I've now moved the domain to 123-reg which worked first time.\",\n",
       " \"All URLs will change but I've old URLs with a lot of backlinks...\\n\\nYou would need to 301 (permanent) redirect the old URLs to the new. But exactly how you do that could depend on how many old URLs there are, whether there is a pattern that helps the old URLs map to the new, whether you have access to the server config or just .htaccess (per-directory config file on Apache) or even if there is an equivalent new URL for the old one.\\n\\nFor those old URLs with a lot of juice, wouldn't be smarter to reuse them on new similar products and keep the SEO juice instead of a 301 that loses some\\n\\nIf the old URLs are a suitable structure and fit with the new site then go ahead and keep the old URLs, that is certainly the best option.\\nHowever, that option is rarely available (at least for all URLs) when developing a completely new site. The old URL simply may not make sense. A 301 redirect from the old to the new is often the only option. But you can only redirect if there is an appropriate page on the new site. For instance, redirecting to the homepage (for a page that has essentially been removed) is likely to be seen as a soft-404.\\nBut if the content is completely changing then you can still suffer an SEO hit initially, even if the URLs don't change.\",\n",
       " \"No. Once a name is resolved to an IP - that IP is used for that domain. For the HTML on the site Cloudflare is protecting there's no way to redirect a user.\\nHowever, you can host your content (images, videos, js, css, etc.) on a different CDN with a new domain. This would trigger a new DNS request.\\nThe problem with this is that people can still attack your servers. Anything that exposes the IP address of one of your servers will mean they can attack it. This defeats the point of Cloudflare.\\nThe other point I have to raise is that if you're using Cloudflare - it's already acting as a CDN. Depending on your purpose, you could be building a redundant system.\",\n",
       " \"Yes, you can create virtual pageviews in Google Analytics. When the user goes over a certain point in the page, a pageview is recorded. \\nYou can find information on how to set up virtual pageviews here:\\nhttps://developers.google.com/analytics/devguides/collection/gajs/asyncMigrationExamples#VirtualPageviews\\nThis should be done with JavaScript. jQuery Waypoints is a good library to help:\\nhttp://imakewebthings.com/jquery-waypoints/\\nYou should select the virtual URL to put in the code carefully. For example you can set them as subpages of the main page.\\n_gaq.push(['_trackPageview', '/example-page/point-1']);\\n\\n_gaq.push(['_trackPageview', '/example-page/point-2']);\\n\\n_gaq.push(['_trackPageview', '/example-page/point-3']);\",\n",
       " \"RewriteCond %{HTTP:Accept-Language} ^([a-zA-Z]{2}) [NC]\\nRewriteRule ^$ /$1/ [L,R=301]\\n\\nAlmost, you just need to use %1 instead of $1, as a backreference to the last matched CondPattern (RewriteCond directive pattern), as opposed to the RewriteRule pattern (which is what $1 refers to).\\nYou also don't need the NC (nocase - case insensitive match) flag if you are specifying lower and uppercase in the pattern, or remove uppercase from the pattern. In other words:\\nRewriteCond %{HTTP:Accept-Language} ^([a-z]{2}) [NC]\\nRewriteRule ^$ /%1/ [L,R=301]\\n\\nClear your browser cache before testing.\\n\\nI would like also know that maybe I can do this via PHP?\\n\\nWell, you could, but it's more efficient to do this in the Apache config. However, if you needed to do something more complex then maybe PHP would be better. For instance, by testing ^([a-z]{2}) you are only checking the first language (which you assume is the preferred language). However, the Accept-Language header is quite complex as it can contain multiple languages with different weights (eg. Accept-Language: fr-CH, fr;q=0.9, en;q=0.8, de;q=0.7, *;q=0.5). If you needed to parse this header then PHP would be preferable.\\n\\nOr for SEO is better use htaccess?\\n\\nFor SEO it makes no difference.\",\n",
       " 'There is a simpler solution to set up the dimension \"Date and Time\" instead of \"Date\" which shows more interactions from one unique ID at once. It is not necessary to build another ga script, because GA already collected all data about particular unique id.',\n",
       " 'After experimenting and fixing it, the \"Non-Empty Browsing Instance\" seems to be triggered by JavaScript changing DOM in the prerendered page.\\nThe way to solve it is to block such JS until the document is visible. Hint:\\nvar visibility = {\\n    isHidden: function() {\\n        if (\\'hidden\\' in document) return document.hidden;\\n        return false;\\n    }\\n};\\n\\nif (visibility.isHidden()) document.addEventListener(\\'visibilitychange\\', startWhenVisible);\\nelse startStuff();\\n\\nfunction startWhenVisible(){\\n    if (!visibility.isHidden()) {\\n        document.removeEventListener(\\'visibilitychange\\', startWhenVisible);\\n        startStuff();\\n    }\\n}',\n",
       " 'Note that simply deleting the sitemap from Google Search Console (formerly Webmaster Tools) doesn\\'t necessarily mean that Google will stop reading it. As stated in the Google support docs:\\n\\nDeleting a sitemap from Search Console does not prevent Google from reading it. Google might continue to read your sitemap until you either block access to it using robots.txt or remove the sitemap file from your web host\\n\\nHowever, it is often recommended that you leave the old HTTP sitemap in place (for some weeks at least) to make it easier for Googlebot to recrawl the old URLs in order to discover your new HTTPS site (via an external redirect - presumably you have implemented an HTTP to HTTPS 301 redirect?). However, if your sitewide HTTP to HTTPS redirect, literally redirects everything, with no exception for the sitemap then no bot is going to see your old HTTP sitemap (on the HTTP property) anyway.\\nNote that sitemaps are only advisory. It can help search engines crawling a site and discovering URLs, but it does not guarantee that those URLs will be indexed.\\nPersonally, I feel the reasons to keep the old sitemap a bit vague. When migrating from HTTP to HTTPS you are only really concerned about the old HTTP URLs that might already be indexed, you are not interested in trying to index more HTTP URLs. The HTTP URLs that are indexed will be recrawled anyway, regardless of whether you have a sitemap or not. Maybe having a sitemap expedites the recrawl (although I don\\'t think I\\'ve seen that stated)?! However, keeping the old HTTP sitemap in Search Console can help with reporting as it will help to determine how many of the old HTTP and new HTTPS URLs are indexed.\\nReference:\\n\\nIn which webmaster property should I submit the HTTPS sitemap after moving site from HTTP to HTTPS?\\n\\nWhilst looking up references for this question I did stumble across this Webmaster Central Help forum thread regarding HTTP to HTTPS migration and old sitemaps, with an answer from an \"expert\". In this thread it is suggested that both the old HTTP and new HTTPS sitemaps should be submitted to the HTTPS property in Google Search Console.\\nThis would allow an unconditional HTTP to HTTPS 301 redirect and both sitemaps will still be accessible.\\nNote that it is only possible to submit both HTTP and HTTPS sitemaps to the HTTPS property if both properties are verified in Google Search Console under the same account.',\n",
       " 'rewrite ^/picture(.*) /picture$1specialword permanent;\\n\\nThis would seem to result in a redirect loop since (.*) will match the redirected URL which then gets \"specialword\" appended again and again, etc.\\nYou can avoid this loop by being more specific in your regex and only checking for what you need, ie. a number. Try something like the following:\\nrewrite ^/picture/(\\\\d+)$ /picture/$1-specialword permanent;\\n\\nI assume you want an external redirect (ie. the URL in the address bar should change) and not an internal rewrite?\\nYou will need to clear your browser cache before testing, since permanent (301) redirects are cached by the browser.',\n",
       " 'If you use the \"Fetch as Google\" tool in Google Search Console on a page that returns a \"418 I\\'m a Teapot\" status then it simply reports an \"Error\" and indexing cannot be requested for this page.\\nIn the screenshot below, the circled \"Error\"s are the result of requesting a page that returns a 418 status. No further information is available at this stage.\\n\\nAccording to my access log, both Googlebot and Search Console have visited this page, but it has not yet appeared in the index.\\nJust to clarify, this is a new page, not previously indexed. It is linked from a page that is indexed, which has also been resubmitted (together with \"linked pages\") for indexing - seen in the screenshot above. I have also submitted an XML sitemap that contains this page (although the \"Indexed\" count is not yet being reported - SEE UPDATE BELOW). To be honest, I don\\'t hold much hope - I would be surprised if it did get indexed. Not only because it\\'s a 4xx code, but because it\\'s not a 2xx success code.\\nOrdinarily, you can do a \"Fetch as Google\" test and then request the page be indexed. This is usually very quick (\"instant\") for a single page - but this option is not available on the above page.\\n\\nAccording to this 4 year old blog post, status 418 will be ignored by Google.\\n\\nBy \"ignored\", they mean it is treated as 200 OK status. (Which isn\\'t really the same as being \"ignored\" in my book, unless it was literally ignored and Google did \"nothing\"?) The \"problem\" with that blog post, is that they are testing an already indexed page. Returning a 4xx status wouldn\\'t necessarily make the page drop from the index anyway, at least not for a considerable time (depending on crawl rate), although they did reportedly wait \"a few weeks\". They also make no mention of reported crawl errors in Google Webmaster Tools (since changed to Google Search Console).\\n\\nit is not a \"real\" error\\n\\nOr is it? It may have been implemented as a \"joke\" in the beginning, however, it does arguably indicate an \"error state\". I think it would be more contradictory for a 4xx code to not be treated as an \"error state\". And it\\'s still \"current\". The original RFC 2324 from 1998 that defined this status code was even updated in 2014 with RFC 7168.\\nMost tools will see the 418 status as an error. Or only see 200 as success. \"Apache log viewer\" and \"Screaming Frog SEO Spider\" certainly see the 418 code as an error.\\nSome web servers reportedly implement the 418 status code:\\n\\nhttps://stackoverflow.com/questions/24018008/is-there-a-server-that-implements-http-status-code-418\\n\\nStack Exchange even make use of this HTTP status code when detecting CSRF violations:\\n\\nhttps://meta.stackexchange.com/questions/185426/stack-overflow-returning-http-error-code-418-im-a-teapot\\n\\nUPDATE 2017-03-31 (2+ weeks later): The page that returns a 418 HTTP status code is not indexed by Google. The XML sitemap report in GSC now shows that only one of the two URLs submitted in the sitemap are indexed (one URL returns a 200 and is indexed, the other returns a 418 and is not indexed).\\nIncidentally, it took GSC almost 2 weeks to report on the index status of the URLs in the sitemap, but this does not relate to when the page(s) were actually indexed. For example, one page was already indexed at the time the sitemap was submitted, however, looking at the sitemap report alone it looks like the page was only indexed 13 days after the sitemap was submitted.\\nThe URL that returns a 418 is now reported as a \"Crawl Error\" under Crawl > Crawl Errors and the 418 is stated as the response code. According to the report this was \"detected\" on 2017-03-16 (the next day after submitting the index request above), however, it was sometime before this was reported in GSC.',\n",
       " 'The best way to check weather organic traffic is real or fake, I use mostly Google search console, why? Because it is not normal analytics like Google analytics which track something when someone land in your website (including spam bot) or using http headers(Which is easy to spoof for many spammer).\\nTo make it fake search console analytics report, your competitor or any third party tool have to search that phrase in search and also have to click on search result, which is not easy and required more CPU resource. At this time there are some tool which track keyword position, but they just check the position, and does not click on search result pages. So you will get better assumption with number of impression and number of clicks from search analytic report.\\nTo see your search analytics report go to your search console, click on search traffic and finally click on search analytics. Or simply click here and choose your web properties. Hope you have already added your website on search console.',\n",
       " 'You are probably seeing a difference in line-endings. When transferring a file in ASCII/Text mode (as opposed to \"Binary mode\") then most FTP clients will convert/normalise line-endings to the OS being transferred to.\\nOn Classic Mac OS (9.x and earlier) the line-ending char is simply \\\\r (ASCII 13), on Mac OS X this changed to \\\\n (ASCII 10), on Linux it is \\\\n (ASCII 10). And Windows is \\\\r\\\\n or ASCII 13+10. (Thanks @8bittree for the Mac correction.)\\nSo, when downloading from one OS to another all line-endings are silently converted. The conversion is reversed when uploaded. (However, as noted in @Joshua\\'s answer this can result in corruption, depending on the file\\'s character encoding and specific characters contained in the file.) If there is a mishmash of line-endings then it\\'s possible the FTP software is normalising/fixing the line-endings. This would explain why downloading and then uploading the file results in a \"different\" file to what was originally on the server (ie. it is \"fixed\"). Or it is reverting a previously miss-converted file? However, the EOL-conversion may not be so intelligent and you can just end up with either double spaced lines or missing line breaks altogether (ie. mildly corrupted).\\nBy default, most FTP clients are set to \"Auto\" transfer mode and have a list of known file types to transfer in ASCII/Text mode. Other file types are transferred in \"Binary\" mode. If you are transferring between the same OS, or you wish to transfer with no conversion, then you should use \"Binary\" mode only.\\n\\nOrdinarily, the FTP software will not change the character encoding of the transferred file unless the source/target operating systems use a very different character encoding with which to represent text files. As @KeithDavies noted in comments, one such example is when downloading from a mainframe, that uses EBCDIC, to a local Windows machine. EBCDIC is not supported natively by Windows, so a conversion is required to convert this to ASCII. Again, transferring in \"Binary mode\" avoids any such conversion. (Thanks to @KeithDavies for the note regarding character encoding.)\\n\\nThe answer to the above was that if diff was used with the -w flag it ignores whitespace.\\n\\nYes, line-endings (whitespace) are ignored in the comparison.\\n\\nIf I download one of the files I\\'m comparing, and re-upload (overwrite) it through an FTP client, the output changes.\\n\\nIf there was a mixture of line-endings in the original file then downloading and re-uploading in ASCII mode could well \"fix\" the inconsistent line-endings. So, the files are now \"the same\".',\n",
       " 'Redirect 301 /aluminum ....\\nRedirect 301 /aluminum/aluminum.htm ....\\nRedirect 301 /aluminum/process.htm ....\\n\\nThe mod_alias Redirect directive is prefix matching, so the first directive here will catch both /aluminum/aluminum.htm and /aluminum/process.htm. AND everything after the match is appended onto the end of the target URL.\\nSo, given a request for /aluminum/aluminum.htm. This matches the first directive, and /aluminum.htm (the URL-path after the match) is appended onto the target http://example.com/MineralFinder/Aluminum which results in the URL in your screenshot, ie. http://example.com/MineralFinder/Aluminum/aluminum.htm.\\nThe solution is to reverse these directives so the more specific directives are first.\\nYou will need to clear your browser cache before testing, as the earlier 301 will be cached hard by the browser. (It is often safer to test with a temporary 302 redirect as this avoids the browser cache.)\\n\\nOr, use RedirectMatch which matches against a regex and is not prefix matching. For example:\\nRedirectMatch 301 ^/aluminum$ http://example.com/MineralFinder/Aluminum\\n\\nWith the start and end anchors (^ and $ respectively) this only matches a request for /aluminum and not /aluminum/aluminum.htm.\\n\\nTech support basically said ... to use CPanel redirects.\\n\\nIf you are happy editing .htaccess files then I would avoid using \"cPanel redirects\". If you have other directives in .htaccess then I would advise against using \"cPanel redirects\". And if you need to do anything slightly more complex then... you can\\'t use \"cPanel redirects\". (Btw, \"cPanel\" simply edits the .htaccess file when it implements a redirect, but it doesn\\'t always get it right.)',\n",
       " \"While working with Alexa's support on the issue, it turned out that this was a bug in the Alexa On-Page SEO Checker tool. They have fixed it now.\",\n",
       " \"Unless you get meaningful business from Yandex or Baidu then there's no disadvantage to blocking them.\\nHowever, they're known to ignore robots.txt. I'm unsure if this has changed in the last 4 years but you'll be looking at a server level block. Their user agents are easy to spot, a .htaccess example would be:\\nSetEnvIfNoCase User-agent “Baidu” spammer=yes\\nSetEnvIfNoCase User-agent “Yandex” spammer=yes\\n\\n<Limit GET PUT POST>\\norder deny,allow\\ndeny from env=spammer\\n</Limit>\",\n",
       " \"It seems that this method doesn't really let Safari emulate IE, rather it just tells the site that's being visited that the browser is IE and not Safari: http://www.davidalison.com/2008/05/how-to-let-safari-pretend-its-ie.html\\nIn this case, I think it's safe to assume that Google Analytics would record the sessions as IE sessions.\",\n",
       " 'You have a few options.\\n\\nRemove and allow to all requests to issue a 404 error, however, if this is an index.html within the root of each sub-domain, I would highly recommend not doing this and keeping the file for security reasons.\\nExclude the page in a robots.txt file within each sub-domain web space. Here is example code to exclude the index.html file within each sub-domain web root.\\nUser-agent: *\\nDisallow: /index.html\\n\\nUse noindex within the HTML header. Here is example code to place within the HTML <head> tag.\\n<meta name=\"robots\" content=\"noindex\">\\n\\nRedirect any request for the page to the site home page. Here is example code to do just that. You can put this in your .htaccess file within each sub-domain web root. (Assuming Apache)\\nRedirect permanent /index.html http://www.example.com/',\n",
       " 'Sounds like the easiest method without having to edit Yoast SEO PHP code directly would be to disable canonicals being handled by Yoast SEO all together. \\nDisable it like so: \\n\\nSOURCE\\nFilter to Disable Canonical URLs\\nadd_filter( \\'wpseo_canonical\\', \\'__return_false\\');\\nSteps to Disabling Yoast Canonicals\\nYou can disable canonical links being added to the HEAD by following these simple steps:\\n\\nLogin to WordPress\\nHover over Appearance\\nClick Editor\\nFind Theme Functions (functions.php)\\nAdd the code found above to a new link within your functions.php\\n\\nThen I\\'d opt to use canonicals without having to use a WordPress plugin, like so:\\n\\nSOURCE\\nUniversal PHP Code for Canonical URLS\\nBelow is the code that you need to add within the head of your theme template:\\n<link rel=\"canonical\" href=\"https://www.bybe.net<?php echo $url=strtok($_SERVER[\"REQUEST_URI\"],\\'?\\');?>\">\\nObviously you will need to replace bybe.net with your own domain.\\nAdding canonical Links in WordPress\\nBelow are a few easy steps that will have you up and running with WordPress canonical URLS in no time at all, and without a plugin.\\n\\nLogin to WordPress\\nHover over Appearance\\nClick Editor\\nFind Theme Header (header.php)\\nAdd the Universal PHP Code for Canonical URLS found above within the HEAD section of your theme header file.\\n\\nThe PHP code $url=strtok($_SERVER[\"REQUEST_URI\"],\\'?\\'); will get the relative path so you only need to replace the domain before it, then you are set to go. \\nShould you or anyone else stubble across this post and want to have a few pages internal canonicals and a few externals then this can be be done using:\\n\\nis_page()\\nis_single(), not to be confused with a Facebook relationship status, it means post. You should also make good use of IF and ELSE, should you have multiple pages or posts, ideally you should use esc_url() whenever possible.',\n",
       " 'I\\'m kind of wondering if I\\'ve missunderstood your question, but... there is no point putting \"noindex\" pages in your XML sitemap.\\nAn XML sitemap advertises the pages you want indexed. So, including a \"noindex\" page in your XML sitemap is sending a mixed signal to the search engines and doesn\\'t make much sense.\\nA sitemap is only advisory, to help the search engines discover pages that may otherwise be hard to find. So providing the pages you want indexed are linked internally then you may not really need the XML sitemap at all. (Although submitting a sitemap to Google Search Console can provide some additional reporting capability.)',\n",
       " \"Domain names have nothing to do with port numbers at all. All a domain name does is resolve to an IP address, the port is completely independent of that process. In order to achieve what you are wanting to do you will either need to change the server listening port to port 80 (which is the default HTTP port), port 443 (which is the default HTTPS port), or if changing listening port numbers is not an option you will need to implement some form of redirecting logic which can be a proxy server. As you don't mention the network topology I will add here that some business line routers (especially those that support port forwarding) also support changing the port. Either way it is not as simple as a DNS configuration with your domain name and will in fact need to be done at the network level, either in server configuration, gateway configuration, or with an intermediary system like a proxy server.\",\n",
       " \"The webmaster@ convention is simply a device where people might be contactable through a generic email address.  It has nothing to do with using or not using email on a domain.  My experience is that it was more widely used  a few years ago than these days because it was thought to be a method to contact domain owners. \\nI the past I would set up webmaster@, postmaster@ and info@ for every domain, even if I didn't use the address.  Over time I realised that in fact they never receive email and if they do it is spam, so I have stopped doing it.\",\n",
       " 'The Sitemaps.org protocol defines three optional elements that could be useful in your case:\\n\\nlastmod - \"The date of last modification of the file.\"\\n\\nchangefreq - \"How frequently the page is likely to change.\"\\n\\nThe value \"never\" should be used to describe archived URLs.\\n\\npriority - \"The priority of this URL relative to other URLs on your site.\"\\n\\nSearch engines may use this information when selecting between URLs on the same site, so you can use this tag to increase the likelihood that your most important pages are present in a search index.\\n\\nSo you could use:\\n<url>\\n  <loc>http://example.com/2014/registration</loc>\\n  <lastmod>2014</lastmod>\\n  <changefreq>never</changefreq>\\n  <priority>0.1</priority>\\n</url>\\n\\n<url>\\n  <loc>http://example.com/2017/registration</loc>\\n  <lastmod>2017-03-20</lastmod>\\n  <changefreq>weekly</changefreq>\\n  <priority>0.9</priority>\\n</url>\\n\\nHowever, don’t expect consumers (like search engines) to make use of these hints; most consumers ignore these, probably.\\n\\nOther things you could do to give search engines hints which convention URLs to prefer (and to help visitors, too):\\n\\nMake sure that the date of the event is published in an accessible and machine-readable way, e.g., with HTML’s time element and possibly Schema.org’s startDate property.\\n\\nLink from all old event pages to the next (or current) event, e.g., with a banner in the header:\\n\\nThis is an archived page of …. The [next event takes place from …].',\n",
       " 'If you want to prevent crawling, you have to use robots.txt.\\nIt would make sense to go this way if 1) you need to have the pages for these future dates and 2) you want to save your server’s and/or the search engine bot’s resources.\\nYou can decide for which years you want to prevent crawling by specifying the beginning of the corresponding URL paths:\\n\\nPrevent crawling of all years after 2019:\\nDisallow: /202\\n\\nPrevent crawling of all years after 2018:\\nDisallow: /2019\\nDisallow: /202\\n\\nPrevent crawling of all years after 2022:\\nDisallow: /2023\\nDisallow: /2024\\nDisallow: /2025\\nDisallow: /2026\\nDisallow: /2027\\nDisallow: /2028\\nDisallow: /2029\\nDisallow: /203\\n\\netc.\\n\\nMake sure not to forget to remove the previously blocked years when the time comes.',\n",
       " 'FYI, meta tags have no effects on rankings so no meta tags is the same as empty meta tags is the same as full meta tags.\\nHaving said that, empty meta tags are the same as no meta tags. Either way you are not providing search engines or any other crawler any information normally provided in those tags. If I had to choose one or the other I would simply omit them as it means a (slightly) smaller file size and less parsing for crawlers.',\n",
       " \"There are several settings on your laptop that could be preventing you from connecting to your new website.\\nDelete from /etc/hosts\\nThe most obvious choice might be an entry in your /etc/hosts or (hosts.txt on Windows).   You might have created an entry that pointed your laptop to the IP address of your old website for testing purposes.   Remove any line that has your website domain name from the file and try again.\\nUse ping\\nNext I would open a command line and ensure that when you ping your website you get the expected IP address:\\n$ ping example.com\\nPING example.com (93.184.216.34) 56(84) bytes of data.\\n64 bytes from 93.184.216.34 (93.184.216.34): icmp_seq=1 ttl=59 time=19.3ms\\n\\nIs the reported IP address the IP address of your old or your new website?   If it is the old site, check your /etc/hosts file again.   It could also be a local DNS server that you are running.\\nUse dig\\nAnother command line tool I find useful is dig.   It will query your DNS server (bypassing /etc/hosts).   Using it you can get an idea if the problem is local or with your DNS server.  It will also report the IP address of the DNS server that it uses so you can check that it is what you expect as well.\\n$ dig example.com\\n...\\n;; ANSWER SECTION:\\nexample.com.        6419    IN  A   93.184.216.34\\n...\\n;; SERVER: 127.0.1.1#53(127.0.1.1)\\n\\nCheck:\\n\\nYour website IP address (93.184.216.34 for example.com in this case)\\nYour DNS server (127.0.1.1 for me)\\nThe TTL on your domain (6419 seconds in this case)\\n\\nDNS time to live (TTL)\\nIf dig reports that the IP address is incorrect from your DNS server, it could be because DNS has not yet propagated.   Dig will tell you how many seconds you have to wait before it refreshes.\\nYou can test that your DNS server is returning the correct results by querying it directly.   First look up the name server and then use that information to query the nameserver directly\\n$ dig NS example.com\\n...\\n;; ANSWER SECTION:\\nexample.com.        2333    IN  NS  a.iana-servers.net.\\n\\n$ dig @a.iana-servers.net example.com\\n...    \\n;; ANSWER SECTION:\\nexample.com.        86400   IN  A   93.184.216.34\\n\\nAdd to /etc/hosts\\nIf you can't figure out why the IP address on the command line is your old one, you can work around by adding a line to your /etc/hosts file with the new correct IP:\\n93.184.216.34 example.com\\n\\nBrowser Plugins\\nIf everything on the command line is correct, it could be an issue with your browser.  Try using a different browser.   Try starting your web browser in safe mode with all plugins disabled.\\nProxy Settings\\nCheck your web browser setting and ensure that you don't have a proxy set.  Using a proxy server (or an auto proxy configuration (PAC) file) could be causing the problem.\",\n",
       " \"Yes, you can start an EBS snapshot while the website is running.\\nYes, you can close your browser.\\nThat being said, you should realize that while your website is running there might be some data in RAM that's not yet committed to the disk. So the recommended way is to lock the tables, flush all the DB transactions to disk, freeze filesystem (like fsfreeze in linux), start the snapshot, then unfreeze the filesystem and resume DB operations. That should give you more or less consistent snapshot.\",\n",
       " 'Your website would be adding value through:\\n(a) the aggregation of relevant content snippets from many sources;\\n(b) the reformatting of content for easy reading\\nJust be sure to attribute content to their sources and check you are not breaking any terms of use/licenses or infringing copyright.\\nYou could also consider enabling recruiters to post job adverts to your site directly so you add value through the provision of content not available elsewhere.',\n",
       " \"I don't think you'd see any direct SEO impact however provided your primary domain is working correctly, other than the fact that more people landing on error pages means less visitors reaching your website.\\nDepending on your hosting setup you may be able to ensure that the redirect still works with an HTTP 301 response without the SSL protocol - you'll have to test this and see. If you wish to ensure customers don't end up seeing an error message or landing on an error page though it may be worth considering including both variants in the SSL certificate. The cost is not so great if compared to potential customers lost on error pages.\",\n",
       " \"I guess what you are looking for is a robots.txt entry like this:\\nUser-agent: *\\nDisallow: /www.example2.com\\n\\nLet's suggestion you have more than 100 'parked' exampleNR.com URLs, but don't want to write a line for every single one of them...use this:\\nUser-agent: *\\nDisallow: /www.example\\n\\nThe problem is, it actually is not officially supported, but many robots like Googlebot are able to understand those easy wildcards. RegEx are definitely not supported. for additonal information\\nUPDATE\\nDeleted the trailing asterisk since robots.txt uses simple prefix matching anyway. Thanks for your attention, w3dk\",\n",
       " 'You typically need multiple different entities for representing everything in structured data. \\nYou are a Person.\\nThe page where you list your works could be a CollectionPage. You could specify the Person as author (and any other applicable property, like publisher etc.).\\nEvery work which you created would be a CreativeWork, but typically a more specific type, of course (like WebSite). Again, you list the Person item as author or whichever property is applicable. (In case of website development, it seems to be problematic to differentiate between the author of the site (i.e., the development) and the author of the content.)\\nTo connect the CollectionPage and each created CreativeWork, you could either use the hasPart property, or you could add an ItemList via mainEntity.\\n\\nThat said, this structured data doesn’t necessarily lead to any user-visible feature in Google Search. For those user-visible search features, Google lists in their documentation which types and properties are required.',\n",
       " 'Google will alter your content, as they see fit, to provide users with what they want (that is, what Google thinks they want).\\nWhen I googled that query I saw the exact snippet you described but it doesn\\'t match what I see when I \\'view source\\': Official Site &ndash; East Main &amp; Co.\\nI don\\'t mean to be rude but your site/brand doesn\\'t seem like it has enough clout to worry about impostors so \"Official Site\" seems like a waste of valuable title space.\\n\"east main & co\" is a branded query (it\\'s not a search for \"official site\") so why wouldn\\'t Google customize the snippet to match the search intent?\\nIf I were writing a title for your home page (without having done any keyword/competitive research), I\\'d write something like East Main & Co: Men\\'s Clothing, Activewear & Hats',\n",
       " 'The TLDR; answer: Send out a 404 header, don\\'t change the url.\\nThe longer answer:\\nThere are more options then you suggest, which I\\'m not going to list here (because it\\'s not really needed).  \\nThe most basic solution is just to serve a 404 header when the page does not exist. The reason for this is simple; This is what visitors expect. You open a URL and that either works or it doesn\\'t. \\nRedirects to other pages are possible, but this should only be done when there is relevent content on the new page (do not redirect everything to home!). A better solution would be a custom page, with a few suggestions for your visitor, along the lines of:\\n\"Oops! Page not found. Maybe you\\'re looking for X, or perhaps Y? You can also go to Home\"',\n",
       " 'First of all, for the image property, Schema.org expects either an URL value or an ImageObject value. This is typically the case for other properties taking an image value, too.\\nSo for the image property, you could as well use:\\n<div itemscope itemtype=\"http://schema.org/Thing\">\\n  <img itemprop=\"image\" src=\"image.png\" alt=\"\" />\\n</div>\\n\\nSo why should an ImageObject value be used instead?\\nIf you want to provide data about the image, you have to use ImageObject, because you can’t add properties to an URL value. Such data could be the height and width of the image, or its author, or its caption, etc.\\nLike it’s always the case with structured data: some consumers care about this additional data, some don’t (and ignore it).\\nIn case of Google Search\\nFor its image-related search features, Google typically (probably even in all cases) requires an ImageObject value (instead of an URL value). See for example these questions:\\n\\nThe attribute image.itemtype has an invalid value\\n“image” property inside Article with or without ImageObject?\\n\\nSo if you care about Google’s search features and you can provide the required additional data, you’ll probably want to use an ImageObject instead of an URL value.',\n",
       " \"Sub-domain tracking is automatic as long as you have specified your cookieDomain as auto. You can read more about that here: https://www.simoahava.com/analytics/cross-domain-tracking-across-subdomains/\\nHaving done so, you won't require any additional tracking for users between your main site and blog. Make sure you have also included the hostname (via View filter) so that you can also see which subdomains your users are visiting, and also include separate views for each subdomain so that you can see traffic for specific subdomains.\",\n",
       " 'I think you are asking about a conflict between HTTP headers versus page meta elements, correct? If yes, you should use HTTP headers and remove the page meta elements (at least the ones for security) as they are unnecessary.\\nHTTP headers take priority over page meta elements, and HTTP headers are cached by intermediate proxies but page meta elements may not be.\\nIf you are using a scanning tool that expects to see a certain page meta element in the <head> that you properly configured as a HTTP header, you can consider that a bug with the scanner and keep using the HTTP header.\\nHere is a good resource for testing your HTTP headers: securityheaders.io\\nAlso, here is a good resource for configuring your Content Security Policy: content-security-policy.com\\nIf you have a specific error you need help with then please edit your question with the error message, otherwise refer to the general guidance above.',\n",
       " 'The point of the requirement from AdSense on a Privacy Policy is not analytics, but cookies (the DoubleClick cookie) and opt-out information.\\nMore information on what AdSense requires you to have in the Privacy Policy can be found in the terms agreement, at Section 8:\\n\\nAs we mentioned in our blog post here, according to Google your Privacy Policy needs to reflect your use of AdSense:\\n\\nThat Google uses cookies to serve ads to visitors of your website\\nThat the DoubleClick cookie used by Google allows Google to show ads to visitors based on their visit to your site and/or other sites (interest-based advertising)\\nHow users can opt-out from the use of the DoubleClick cookie for interest-based advertising',\n",
       " 'Just to clarify, there is no full proof way to restrict access in this instance (unless this is all behind some kind of authentication) since it needs to be accessible to your form. All you can do is prevent casual browsing. Anyone that really wants to submit requests to this script can do.\\n\\n<Files \"_mf.php\">\\nOrder Allow,Deny\\nDeny from all\\n</Files>\\n\\nThe problem with this, as you probably found out, is that it blocks all requests, including POST requests from your form submission. What you could do is wrap this in a <LimitExcept> wrapper, so that it applies only to non-POST requests:\\n<limitExcept POST>\\n  <Files \"_mf.php\">\\n    Order Allow,Deny\\n    Deny from all\\n  </Files>\\n</LimitExcept>\\n\\nAny GET requests (eg. \"from the address bar of the browser\") will be blocked (403 Forbidden). As well as any PUT, DELETE, CONNECT, OPTIONS, etc. etc. requests. But POST requests will be allowed through unhindered.\\nIf you want to check the HTTP referer as well, then you could resort to mod_rewrite. For example:\\nRewriteEngine On\\nRewriteCond %{REQUEST_METHOD} !POST [OR]\\nRewriteCond %{HTTP_REFERER} !^http://example.com/\\nRewriteRule ^_mf\\\\.php - [F]\\n\\nWhere example.com is your canonical hostname. You could instead specify the exact URL of the referring page (the page that contains your form) if you wish:\\nRewriteCond %{HTTP_REFERER} !^http://example.com/path/to/file-that-contains-form$\\n\\nThis would need to go near the top of your .htaccess file.\\nWhat this says is... for all requests to /_mf.php that are either not POST requests or the HTTP Referer is not your domain then serve a 403 Forbidden.\\nNote that this comes with caveats. It\\'s possible that the user\\'s browser is not set to send HTTP Referers - your script will not be accessible to those users. (This is rare, but some \"power\" users might choose to do this.)\\nAny user with CURL know-how can still fake the request and submit requests to your script without accessing your form. Since they can submit a POST request and fake the HTTP referer.',\n",
       " \"No, this will not affect the non-AMP traffic of your website. It's still your content and your traffic. If you have Google Analytics installed on AMP pages you will see the AMP traffic on your Google Analytics dashboard.\\nSimon Hayter, I beg your pardon but I must disagree with you. If your website is in the news publishing or blog segment, I dare to say that AMP pages are essential nowadays. Our clients on those segments who invested on AMP pages have seen a 30% increase in average traffic in the last 6 months.\",\n",
       " \"To answer my own question, setting up subdomains is practically the same as setting up the naked domain. I was thrown because of the CNAME record; but this is just an alias, and if you are only using the subdomain itself, it's not needed.\\nVerification of the subdomain works through Google App Engine as long as the naked domain is verified first via a domain TXT record, through Google Webmaster Tools.\\nWhat I ended up doing was delegating the subdomain using Google's registrar NS records, pointing to a new DNS zone for the subdomain on Google Cloud DNS. This meant I was able to add AAAA records, and also to make use of record sets, something I was not able to do at the original DNS server UI.\",\n",
       " \"Nofollow is what you are looking for. It doesn't pass any link juice and google wont open or save them. \\n\\nIn general, we don't follow them. This means that Google does not\\n  transfer PageRank or anchor text across these links. Essentially,\\n  using nofollow causes us to drop the target links from our overall\\n  graph of the web. However, the target pages may still appear in our\\n  index if other sites link to them without using nofollow, or if the\\n  URLs are submitted to Google in a Sitemap.\\n\\nSource\\nGoogles crawlers runs javascript and renders the page as close to what a web browser as it can. It will even post form submits.\\nI guess you could hide it behind a captcha since the whole idea of a captcha is that a robot should be able to access it.\",\n",
       " 'There are examples in the draft of the Report-To directive:\\nReport-To: { \"url\": \"https://example.com/reports\",\\n                    \"group\": \"endpoint-1\",\\n                    \"max-age\": 10886400 };\\n\\nYour rule is correctly created, as in it returns a valid JSON object, but since the Reporting API is still a draft, most UA haven\\'t got around implementing it. You could simply have both, report-uri and Report-To header meanwhile if you want to, as one would be ignored either way:\\n\\nNote: The report-uri directive is deprecated. Please use the report-to directive instead. If the latter directive is present, this directive will be ignored. To ensure backwards compatibility, we suggest specifying both, like this:\\nContent-Security-Policy: ...; report-uri https://endpoint.com; report-to groupname',\n",
       " 'Go to the Admin page (rightmost link on the top of the main page)\\nUnder View (rightmost panel of 3) select View Settings.\\nUnder Time zone country or territory, select your desired country and time zone settings.',\n",
       " \"First thing that you need to consider is make up a proper plan that what do you want to archive. what will be the outcome of your work and how much work you need to do.\\nI have been working on the similar kind of website which is on travel and when I started working its on the same condition you mentioned here. But, now the site is having another story to tell and results are speaking at their own. I have been ranking for top spot in search engine for 4-5 keywords and in top 20 with most of the keywords. Here is the outlined what I did to improve it.\\n1. First you need to check out the Analytics and Webmasters to see where the site is standing right now, what are the posts which receives traffic and what are the errors on the site. Make a note of that.\\n2. Now, You need to scan the site with proper on page error that what are the areas you need to improve like titles, meta and all.\\n3. Make a plan for the content that what is your strategy to get the relevant content, is the old content is enough or you need to add the new content.\\n4. you have to make a plan that how you are going to promote the site via SEO and what will be the tasks you will perform for this.\\n5. When you complete with all the things, let's start working on it by removing the errors from site, make the changes on the posts with on page optimization and create your optimized urls. Add the content which need to be added or update the old content.\\n6. Start promoting your new content.\\n7. You can either use redirection plugin to add the redirection to old pages or either use .htaccess for redirection. Both will work fine for you. \\n8. If you are using wordpress add broken links plugins to see which urls you need to fix or you can use webmasters to keep track of it.\\n9. now create your new sitemap with your updated content and submit it webmaster. you can your cache function as well if you want to index pages fast.\\n10. within a couple of days your urls will start index and you are done. :)\",\n",
       " 'At the end of the day it comes down mostly to preference and what will work best for you and be the most intuitive for your customers. \\nYour strategy for procuring domain names seems well thought out although if available the .com gTLD would be preferable. However rather than redirecting these to subdomains of your main website I would recommend hosting each individual region-specific site on its own ccTLD domain. Websites with the ccTLD that is for the country of the end-users when they search in Google/Bing etc will be placed higher in the results as deemed to be more relevant. Redirecting would not get your sites this benefit.\\nAdditionally if you intend to use language codes as subfolders, then to use country codes as subdomains at the same time may confuse users as subdomains are also often used as language specifiers. For English language on the Germany-specific website you could use www.company.de/en/, and for German language on the English-specific website you could use www.company.co.uk/de/ etc. Furthermore if you are on the German language Germany-specific website it would be better to use www.company.de/ rather than www.company.de/de/ or de.company.io/de/ since duplication in the URL is confusing to end-users and unnecessary.\\nYour main website could feature content that applies to all versions of the software, and all versions of your website could feature a menu selector where users can choose their region/country (perhaps with a flag dropdown or selecting a region on a map).\\nIt\\'s more work to maintain separate websites this way, but for SEO you will see the benefit as using the ccTLD\\'s where appropriate will gain the geo targetting bonus. Folders are often used for language variants because they inherit the \"domain authority\", but this does not apply to subdomains, and since you need language and country parameters ccTLD\\'s seems the obvious choice so as to avoid www.company.io/de/de/. Over time your ccTLDs would also benefit from their own domain authority.\\nHelpful reading: Moz.com: Folders vs Subdomains vs ccTLD in International SEO',\n",
       " \"If these pages don't exist and Google tries to crawl them, you should find them in the the Google Search console report for pages that are not found. If you click on the link in that report, Google should show you where they found the links.\\nYou should check your analytics tool to see if you get a lot of traffic to the pages. If you do, fix the error on your site it that is the source.  If the source is another site, create 301 redirects. If there's not much traffic, you can ignore them. \\nGoogle also provide recommendations for handling 404 errors\",\n",
       " 'Thank you guys, after two days of research and try and errors.\\nI have finally found the solution, which might be easy for other people but I did stress out to get the solution.\\n<?php\\n\\n$json_string = \\'http://0.0.0.0:8080/wm/statistics/bandwidth/00:00:00:00:00:00:00:01/1/json\\';\\n$jsondata = file_get_contents($json_string);\\n$obj = json_decode($jsondata,true);\\necho \"<pre>\";\\nprint_r($obj);\\necho \"<br>---------------------------------<br>\";\\necho $obj[2][\\'bits-per-second-rx\\']; \\necho \"<br>---------------------------------<br>\";\\necho $obj[1][\\'bits-per-second-rx\\']; \\necho \"<br>---------------------------------<br>\";\\necho $obj[0][\\'bits-per-second-rx\\']; \\n?>',\n",
       " 'ErrorDocument 400 https://errors.example.com?type=error&error=400\\n\\nIn order to prevent Apache triggering an external redirect to the error document you need to remove the absolute URL (scheme + hostname) and specify a root-relative filesystem path to your error document instead. By including an absolute URL, Apache implicitly triggers an external redirect (not desirable).\\nBasically, you can\\'t get Apache to serve an error document via an internal subrequest that\\'s on a different host.\\nIf your main domain and all subdomains (including the errors subdomain) point to the same area on the filesystem then there shouldn\\'t be a need to change hosts (ie. having a special errors subdomain is going to cause problems). The ErrorDocument URL is hidden from users anyway.\\nAnother \"problem\" with the URL-path you\\'ve given is that you\\'ve not actually specified the error document filename (this becomes more obvious when you strip the scheme + hostname). I assume you are using the DirectoryIndex (eg. index.php)? But if you omit this then mod_dir will trigger another subrequest to the directory index.\\nSo, try something like:\\nErrorDocument 400 /index.php?type=error&error=400\\n\\nAside: If you are using PHP, then there is no real need to explicitly pass the HTTP status code to the error document (eg. &error=400). Providing the error document is triggered by an internal subrequest (as opposed to an external redirect, as mentioned above), then the HTTP status is available in the $_SERVER[\\'REDIRECT_STATUS\\'] superglobal. This can also be used to determine whether the error document has been requested directly (REDIRECT_STATUS is not set in this instance). Although this may require a slight reworking of your PHP script.\\nErrorDocument 400 /index.php?type=error\\nErrorDocument 401 /index.php?type=error\\nErrorDocument 403 /index.php?type=error\\n:\\n\\nUPDATE: With regards to the use of $_SERVER[\\'REDIRECT_STATUS\\'] and your error URL parameter... Note that (as mentioned above) REDIRECT_STATUS is not set on direct access. At other times it is set to the HTTP status code (a numeric string - the variable type is a string). It can also be set to \"200\" (OK) after a successful internal rewrite (this may or may not apply to your system, but it is worth checking for).\\nYou should also assume that your error URL param might not be set. So, something like this in PHP:\\n// Get the HTTP status code...\\n// \"error\" URL param (if set) will override the REDIRECT_STATUS variable\\n$httpStatus = isset($_SERVER[\\'REDIRECT_STATUS\\']) ? $_SERVER[\\'REDIRECT_STATUS\\'] : null;\\n$httpStatus = isset($_GET[\\'error\\']) ? $_GET[\\'error\\'] : $httpStatus;\\n\\nif (empty($httpStatus)) {\\n    // Direct access without the \"error\" parameter - abort?\\n    // Or force a 403 or 404 plus suitable response.\\n    exit();\\n}\\nelseif ($httpStatus == \\'404\\') {\\n    // Process 404 error\\n}\\n\\n// etc.\\n\\nelseif ($httpStatus == \\'200\\') {\\n    // Not an error...\\n}\\nelse {\\n    // Unknown error\\n}\\n\\nThis allows your error URL param to always override the REDIRECT_STATUS superglobal.',\n",
       " 'If you are having problems with the \"Automatic HTTPS Rewrites\" Cloudflare option then it maybe that CF is unable to determine whether your site/resources are HTTPS enabled. From the CF docs on Automatic HTTPS Rewrites:\\n\\nTo determine which URLs do not have HTTPS support, we use data from EFF’s HTTPS Everywhere and Chrome’s HSTS preload list, among others.\\n\\nTo unconditionally redirect all users to HTTPS the preferred method would seem to be to create a page rule. From the CF support doc: How do I redirect all visitors to HTTPS/SSL?\\n\\nThe most effective means of redirecting visitors to HTTPS when using Cloudflare is using a page rule. While you can also redirect to HTTPS using configuration at your origin, page rules are processed at our edge, resulting in a quicker response and reduced requests to your server.\\nThe \"Always use HTTPS” action is the simplest option to redirect HTTP requests to HTTPS.\\n\\nThe following steps describe the process of using page rules (which will behave as a 301 redirect):\\n\\nCloudflare Page Rule 301 Redirect from HTTP to HTTPS\\n Login to Cloudflare Select your\\nsite using the dropdown menu found in the upper left corner\\nClick the Page Rules icon at the top of the screen\\nClick the Create Page Rule button Enter\\nhttp://*example.com/* but obviously changing the domain with\\nyours. Under “Then the settings are:” click + Add a\\nSetting Click the dropdown list, find and click option\\n“Always Use HTTPS” Finally click Save and Deploy\\n\\nHowever, many users still use their own server config (by that I literally mean either the main server config, virtual host or .htaccess file) and mod_rewrite (Apache) to perform the redirect. However, on the CF \"Flexible SSL\" option you need to be careful of a redirect loop, since your site is still serving content over HTTP to CF, so ordinary HTTPS checks cannot be applied. (The CF \"Flexible SSL\" option just protects the connection from the end-user to CF, not the connection from CF to your server.)\\nSo, something like the following near the top of your .htaccess file:\\nRewriteEngine On\\nRewriteCond %{HTTPS} off\\nRewriteCond %{HTTP:X-Forwarded-Proto} !https\\nRewriteRule (.*) https://example.com/$1 [R=301,L]\\n\\nWith \"Flexible SSL\" the HTTPS server variable is always off (since your site is serving content over HTTP), but Cloudflare should be setting the X-Forwarded-Proto HTTP request header as the request passes through Cloudflare\\'s servers. Keeping the first condition ensures it should work regardless of whether you are using CF or not. If you are always going to be behind a proxy (CF) then you can remove the first condition.\\n\\n...so seems like HTST is not really a great idea...\\n\\nIf you are committed to HTTPS then HSTS is a great idea. However, it is a one-way ticket. You will not be able to (easily) revert back to HTTP.',\n",
       " 'Copyright applies to content. So copyright would not apply here since the content is uniquely yours (assumption) and that the theme is available for use by many.\\nTrademarks should be registered (generally). Unless you are using a trademark that is not yours, this does not apply.\\nHowever, too much similarity may still be considered trade infringement without a specific trademark infringement. All it has to do is be reasonably confusing to customers. As long as that argument can be made and agreed with, you can be in trouble. Keep in mind that an argument only has to sound reasonable to an unreasonable judge to work against you.\\nMy advice is to evaluate how you distinguish yourself from your competitors in regard to content, look and feel, and usability. Make sure that no reasonable argument that your site is too similar to another can be made.',\n",
       " \"You're likely using a geolocation service to determine the location of the IP address. This may not accurately tell you where the server for that IP is located - Cloudflare owns large IP blocks. These blocks will be registered to them somewhere in the USA and perhaps the servers for these IPs are even located there. \\nHowever, if they move a B block in that range to Europe, it means a 1 digit difference in the IP changes the location completely. 104.16.0.0/12 for example is a huge range of IPs. That's over 1.4 mil IPs split into around 64k B blocks (excuse my napkin math). \\nThe ISPs would be aware of an edge router's location but IP block registration databases wouldn't. Do a ping command and use response time and TTL to measure distance. TTL will tell you how many routers your ping has bounced through - even if response time doesn't waver much you'll be able to see that it's gone a greater distance. For further detail, a tracert command (Windows) will also reveal more about location by attempting to resolve and ping each individual router along the way. Done from different origin IPs, you'll also be able to see if your ISP is doing any redirecting for Cloudflare in order to shorten distance travelled.\\nEdit: Another answer has pointed out you can also use yourdomain.com/cdn-cgi/trace in order to get a debug output with the 'colo=' code indicating the location of the server being used. Example output:\\nfl=21g22\\nh=yourdomain.co.uk\\nip=your.ip.address\\nts=1497653403.144\\nvisit_scheme=https\\nuag=Mozilla Compatible Agent\\ncolo=LHR // Datacentre location\\nspdy=h2\\nhttp=h2\\nloc=GB\",\n",
       " \"Realistically your options really are as follows:\\n(a) Get the second website setup with SSL as you have already mentioned. This is probably the best long-term solution.\\n(b) Use a proxy script on your SSL-enabled website to serve the images from the other server. This method will increase your overall bandwidth usage and make your server running the secure website work harder, but in reality this could work very well for you as a short to medium term solution. I've used this method before a few times and it can be very effective for certain purposes.\\nAn example PHP script hosted on your SSL-enabled website to achieve this might look like this:\\n<?php\\nconst PROXY_DOMAIN = 'http://www.example-without-ssl.com/';\\n\\nclass AssetProxy {\\n  private $aHeaders;\\n  private $sData;\\n\\n  public function __construct($sURL) {\\n    $this->aHeaders = array();\\n    $this->sData = '';\\n\\n    /* HTTP 404 if resource does not exist */      \\n    if ($this->RemoteFileExists($sURL) == false) {\\n      header('HTTP/1.0 404 Not Found');\\n      exit;      \\n    }\\n\\n    /* HTTP 304 if resource same as held in browser cache */\\n    $sData = @file_get_contents($sURL);\\n    $sETag = md5($sData);\\n    $this->aHeaders[] = sprintf('ETag: %s', $sETag);\\n    $sSuppliedETag = isset($_SERVER['HTTP_IF_NONE_MATCH']) ?\\n      $_SERVER['HTTP_IF_NONE_MATCH'] : '';\\n    if ($sSuppliedETag == $sETag) {\\n      header('HTTP/1.1 304 Not Modified');\\n      header('Content-Length: 0');\\n      exit;\\n    }\\n\\n    /* Send asset with gzip compression */\\n    $this->DetectFileType($sURL);\\n    $this->SetBrowserCache(60 * 60 * 24 * 35); // 35 days\\n    if (!ob_start('ob_gzhandler')) ob_start();\\n    foreach ($this->aHeaders as $sHeader) header($sHeader);\\n    echo($sData);\\n  }\\n\\n  private function RemoteFileExists($sURL) {\\n    $hCURL = curl_init($sURL);\\n    curl_setopt($hCURL, CURLOPT_NOBODY, true);\\n    $bResult = curl_exec($hCURL);\\n    if (!$bResult) return false;\\n    $iStatus = (int)curl_getinfo($hCURL, CURLINFO_HTTP_CODE);\\n    return ($iStatus == 200);\\n  }\\n\\n  private function DetectFileType($sURL) {\\n    $sFileExtension = strtolower(substr($sURL, 1 + strripos($sURL, '.')));\\n    switch ($sFileExtension) {\\n      case 'js': $sMimeType = 'text/javascript'; break;\\n      case 'css': $sMimeType = 'text/css'; break;\\n      case 'swf': $sMimeType = 'application/x-shockwave-flash'; break;\\n      case 'png': $sMimeType = 'image/png'; break;\\n      case 'jpg': $sMimeType = 'image/jpeg'; break;\\n      case 'eot': $sMimeType = 'application/vnd.ms-fontobject'; break;\\n      default: $sMimeType = 'text/plain'; break;\\n    }\\n    $this->aHeaders[] = 'Content-Type: ' . $sMimeType . '; charset: UTF-8';\\n  }\\n\\n  private function SetBrowserCache($iSeconds) {\\n    $dtNow = time();\\n    $dtExpires = strtotime(sprintf('+%s seconds', $iSeconds));\\n    $this->aHeaders[] = 'Expires: ' . date('r', $dtExpires);\\n    $this->aHeaders[] = 'Last-Modified: ' . date('r', $dtNow);\\n    $this->aHeaders[] = 'Cache-Control: public, must-revalidate, ' . \\n      sprintf(max-age=%s', $iSeconds);\\n  }\\n\\n}\\n\\n$sAsset = isset($_GET['asset']) ? $_GET['asset'] : '';\\n$oProxy = new AssetProxy(PROXY_DOMAIN . $sAsset);\\nexit;\\n\\nI like to call this script gzip.php rather than anything that gives away its actually a proxy script. Next, in your HTML change all occurrences of URLs:\\nhttp://www.example-without-ssl.com/images/example.jpg\\nwith the new URLs such as \\nhttps://www.example-with-ssl.com/gzip.php?asset=images/example.jpg\\nI have kept this script file-type agnostic so that it can serve a few different types of static asset, including javascripts, stylesheets, fonts aswell as images. To add more file-types just add the MIME types into the DetectFileType() function otherwise they will be served with a text/plain MIME-type by default.\",\n",
       " 'I got the same problem once with a website with an old version of Joomla, the server was safe but the pages were generated by an hack on at PHP level that I was not able to discover fast.\\nIn the end I preferred to delete DB and all files, recover from a valid backup and update everything to latest version. \\nAfter that I removed all bad URLs using Google removal tool in Google Search Console and resubmit the website to Google for a check of hacking.',\n",
       " \"GA does not currently support reporting of individual value of event values. You do have two choices:\\n\\nIf you only want the distribution then use user timings. They conveniently go with your use case of tracking of tracking interaction timing. Don't forget to set the sample rate parameter or only 1% will be collected, unless you have already set _setSiteSpeedSampleRate.\\nRefactor your event so the timing value is a string of the event's action or label parameter. That way you can then use the API or export into a spreadsheet to generate a distribution. The benefit of this over the user timing is that you will have access to each individual unique value.\",\n",
       " \"After your comments and clarifying that you don't need to access hardware.\\nEverything you say like create reports, keep user preferences etc will be achievable however some things like saving user preference can be achieved with cookies as an example so it'll be different in terms of how to achieve some of the things.  \\nIn web apps you'll need to be able to cater for all browsers as that's how you'll be using it most likely. \\nYou'll be able to access it from different screen sized devices such as mobile, tablet and larger screens so you'll want to take into consideration those factors  (not that huge). Look into responsive web design bootstrap does a good job. \\nYou'll need to use javascript to do front end stuff like asynchronous updates/create/sorting etc. Something like jquery has lots of plugins that you can use. \\nWith web apps you'll always need an Internet connection to use. So it'll need a domain name and it'll be public so more susceptible to hacking. \\nYou could overcome some of that by installing it on a local server which every user has access to.\\nYou'll find many classes and even frameworks that are maintained by communities which has bulk of the work and best practices already implemented. You might just need to adapt etc.\",\n",
       " \"First things first, stop thinking in terms of keywords. In the very early days, before Google, search engines matched keywords and used other metrics including density and proximity. The original Google research paper written by Brin and Page in 1997 railed against this idea citing the limited potential to making successful and meaning matches. And they were right. Instead, Google would use semantics to determine what results would match the search query. In the early days, the use of semantics was somewhat limited, however, it was spot on in how to implement a simple and effective semantic search and has not swayed too far from it's original incarnation.\\nSo stop thinking in terms of using keywords and how Google matches keywords. It doesn't.\\nGoogle has never made direct keyword matches. Instead, it uses several linguistic semantic algorithms and methods such as topical analysis and PoS (parts of speech) and others to determine a good match. Case in point, RankBrain is just a larger implementation of semantics applied to the search query as well as further applying AI (artificial intelligence) machine learning between the query and the results allowing Google to create better results. It is simply an advancement of what has always been done.\\nThe HTML code is broken down to a HTML DOM (model) and evaluated. Content is broken down into segments, a semantic concept, which can be as small as a few words and as large as a whole document, however, generally, a segment would be a content block. A content block is as small as a paragraph and as large as the entire content. Typically, it is best to think of a content block is a single paragraph to as large as all the paragraphs between header tags to include the header tag immediately proceeding the paragraphs. Each content block logically and typically contains one topic whether we realize it or not. Humans naturally organize their writing so that a paragraph is different from another and paragraphs within a larger block follow a logic flow and larger topic. Each content block between headers follow a logic flow throughout the content and is strongly signaled by the header tag. Each header tag and paragraph is analyzed individually using semantics as well as the larger content blocks and the entire document. This analysis is contained within virtual tables of metrics and matrices (metrics of metrics). This is how Google knows what the content is about.\\nThis is also done for any link text, target URL, any link within content, and placement meaning content within a content block before the link and after the link. Remember that content blocks can be the entire content. When a link is analyzed, it is scored against content blocks to determine if any metric matches exist. Add to this, these metric scores are compared to the metrics of the target page.\\nWhere the metrics match using all of the analysis is where relevancy is found.\\nNow, please know that the concept of markets is also applied to sites. For example, we know about directory sites, link sites, whois sites, and so on. While the term market is not generally used by Google anymore, applying analysis to determine market is the original concept and how Google originally applied the concept. In this analysis, Google has determined it's own markets and analysis to place sites into numerous simple categories. Any site that is placed within a market is compared to other sites within the same market. In this, all metrics, semantics and search performance, are used to rate a site within it's market. Niche sites cannot escape this. A blog is a blog regardless of the topic. An e-zine is an e-zine regardless of the topic. If, for example, your blog does not compare well within it's market, it very likely will not perform well in search regardless of what your blog is about. How a site can break out of this is to create a higher quality and more appealing and popular site.\\nWhile the concept of markets does not effect link analysis, it does explain further how Google analyzes sites and determine categories (markets) which is part of your question.\",\n",
       " 'Custom Dimensions turned out to be the way to go. Once setup (each configuration variable getting its own dimension), we were able to use them for view filtering / custom report generation / etc. Make sure to give it a day or so before attempting to use the dimensions, since it appears that Google takes some time to process them.',\n",
       " \"Sadly, you can't disable Cloudflare on a per-directory basis with a page rule: By the time a request is matched to your page rules, it has already reached Cloudflare, and will keep causing an error 520 even if you disable Cloudflare cache on the directory.\\nYet, you can disable Cloudflare for a specific subdomain, so you may want to create that page rule to forward the requests to another URL, that one being behind a subdomain not on Cloudflare?\\nStill, errors 520 can be fixed. Their detailed explanation is here, but the short version is that your webserver is either:\\n\\nreturning headers bigger than 8KB (typically the case when there's a lot of cookies)\\nprematurely closing the TCP connection coming from Cloudflare\",\n",
       " 'Sounds like your site is using HTTP caching which can be adjusted using some form of Leverage Browser Caching or using version strings on your files. \\nBelow you can 3 methods: Apache mod_expires, Apache FilesMatch and versioning strings using PHP.\\nApache mod_expires.c\\n<IfModule mod_expires.c> \\n  ExpiresActive On\\n  ExpiresDefault \"access plus 60 seconds\"\\n  ExpiresByType text/html \"access plus 60 seconds\"\\n  ExpiresByType image/x-icon \"access plus 60 seconds\"\\n  ExpiresByType image/gif \"access plus 60 seconds\"\\n  ExpiresByType image/jpeg \"access plus 60 seconds\"\\n  ExpiresByType image/png \"access plus 60 seconds\"\\n  ExpiresByType text/css \"access plus 60 seconds\"\\n  ExpiresByType text/javascript \"access plus 60 seconds\"\\n  ExpiresByType application/x-javascript \"access plus 60 seconds\"\\n</IfModule>\\n\\nApache FilesMatch\\n<FilesMatch \"\\\\.(?i:gif|jpe?g|png|ico|css|js|swf)$\">    \\n  <IfModule mod_headers.c>\\n    Header set Cache-Control \"max-age=60, public, must-revalidate\"\\n  </IfModule>    \\n</FilesMatch>\\n\\nVersioning\\nAdding a query string to the end of the file will make the browser believe that the file is not the same and it\\'ll re-download it. \\nEach time you make a edit to the a resource you need to update the query string, for example:\\n\\nOriginal version: <link rel=\"stylesheet\" href=\"pro-webmasters.css\">\\nPrevious version: <link rel=\"stylesheet\" href=\"pro-webmasters.css?v=0.1\">\\nLatest version: <link rel=\"stylesheet\" href=\"pro-webmasters.css?v=0.2\">\\n\\nObviously this becomes time consuming when you have multiple css files, JavaScript files and other resources, this can be made easier by using PHP variables, for example:\\n<?php $VerNumber= \"0.0.2\"; ?>\\n\\n<link rel=\"stylesheet\" href=\"style.css?v=<?php echo $VerNumber; ?>\">\\n<script src=\"jquery.js?v=<?php echo $VerNumber; ?>\"></script>\\n<img src=\"image.jpg?v=<?php echo $VerNumber; ?>\">',\n",
       " 'I\\'m assuming the .htaccess file you are referring to is in the document root? ie. example.com/.htaccess?\\n\\nIt redirects from http://example.com/blog/ (no-HTTPS) to https://example.com/blog/ (HTTPS)\\n\\nMaybe this is just a \"typo\", but... it doesn\\'t. Your RewriteRule directive explicitly includes the www subdomain.\\n\\nI achieved the desired result in 2 redirects.\\n\\nWell, the code you posted is just one redirect, not two. So, where is the other redirect coming from? Ordinarily, mod_dir will trigger an implicit redirect from http://example.com/blog to http://example.com/blog/ if blog is a physical directory (which you say it is). However, the directive(s) you posted should override this behaviour, unless...\\n... You have another .htaccess file in the /blog subdirectory that uses mod_rewrite? If you have a WordPress blog in the /blog subdirectory then this is quite probable. If this is the case then your mod_rewrite directives in the parent .htaccess are being completely ignored - they aren\\'t doing anything! (Which, incidentally, would explain why you have www.example.com in your RewriteRule directive but are seeing a redirect to example.com - although maybe that is just a \"typo\"? Please clarify.)\\nAs mentioned, mod_dir will implicitly trigger a 301 redirect from /blog to /blog/ and then maybe WordPress itself is triggering the redirect from HTTP to HTTPS? That\\'s your two redirects. Admittedly that\\'s a bit of a guess, but without more information, that\\'s all we can do. What does the network traffic report? Specifically, what redirects are you seeing?\\nSolution\\nIf the above hypothesis is correct that it\\'s a bit of a tough one! You could get around the double redirect by moving everything from /blog/.htaccess into the parent .htaccess file (and making the necessary adjustments). However, this might break WordPress to some extent - as WP will no longer be able to maintain the .htaccess file.\\nAlternatively, if you are on Apache 2.4+ then you can mess with DirectorySlash Off and RewriteOptions AllowNoSlash and move your directives into the /blog/.htaccess file. Although I kinda wonder whether this is really \"worth it\"?\\nUPDATE: For example, as mentioned above, you can move your redirect into the /blog/.htaccess file (near the top) and turn DirectorySlash Off and set RewriteOptions AllowNoSlash - this requires Apache 2.4+:\\n# Prevent mod_dir implicitly appending a slash on directories (via redirect)\\nDirectorySlash Off\\n\\n# Allow mod_rewrite to function when there is no trailing slash\\nRewriteOptions AllowNoSlash\\n\\n# Redirect http://example.com/blog to HTTPS\\nRewriteCond %{HTTPS} off [OR]\\nRewriteCond %{REQUEST_URI} ^/blog$\\nRewriteRule ^ https://www.example.com/blog/ [R=301,L]\\n\\nThis should do as you require - in a single redirect. However, I\\'ve modified your redirect to include requests for https://www.example.com/blog (ie. HTTPS + /blog no trailing slash), otherwise that will no longer be resolved correctly - presumably this is a requirement?\\nAlso, your existing redirect does not redirect http://www.example.com/blog/<something> - presumably that is intentional?\\nAs with all 301 redirects, your browser cache will need to be cleared before testing.\\nYou should also note that with DirectorySlash Off, mod_dir will not append the trailing slash to any subdirectories - should they be accessed. You will need to manage this as required. This can result in \"unexpected behaviour\", which I think is the biggest concern. Test test test.\\nAFAIK the only security concern is if you have directory indexes enabled. So, to be sure, these should be disabled: Options -Indexes in your .htaccess file. See the DirectorySlash directive in the Apache Docs for more information.',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_batch[\"answer\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceTransformer(\"\").encode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
