{"instance_id": "astropy__astropy-12907", "model_patch": "", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "astropy__astropy-14182", "model_patch": "--- astropy/io/ascii/core.py\n+++ astropy/io/ascii/core.py\n@@ -1,1854 +1,32 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-\"\"\" An extensible ASCII table reader and writer.\n+class RST(core.BaseReader):\n+    _format_name = \"rst\"\n+    _description = \"RestructuredText table\"\n+    _io_registry_can_write = True\n \n-core.py:\n-  Core base classes and functions for reading and writing tables.\n-\n-:Copyright: Smithsonian Astrophysical Observatory (2010)\n-:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n-\"\"\"\n-\n-\n-import copy\n-import csv\n-import fnmatch\n-import functools\n-import inspect\n-import itertools\n-import operator\n-import os\n-import re\n-import warnings\n-from collections import OrderedDict\n-from contextlib import suppress\n-from io import StringIO\n-\n-import numpy\n-\n-from astropy.table import Table\n-from astropy.utils.data import get_readable_fileobj\n-from astropy.utils.exceptions import AstropyWarning\n-\n-from . import connect\n-from .docs import READ_DOCSTRING, WRITE_DOCSTRING\n-\n-# Global dictionary mapping format arg to the corresponding Reader class\n-FORMAT_CLASSES = {}\n-\n-# Similar dictionary for fast readers\n-FAST_CLASSES = {}\n-\n-\n-def _check_multidim_table(table, max_ndim):\n-    \"\"\"Check that ``table`` has only columns with ndim <= ``max_ndim``\n-\n-    Currently ECSV is the only built-in format that supports output of arbitrary\n-    N-d columns, but HTML supports 2-d.\n-    \"\"\"\n-    # No limit?\n-    if max_ndim is None:\n-        return\n-\n-    # Check for N-d columns\n-    nd_names = [col.info.name for col in table.itercols() if len(col.shape) > max_ndim]\n-    if nd_names:\n-        raise ValueError(\n-            f\"column(s) with dimension > {max_ndim} \"\n-            \"cannot be be written with this format, try using 'ecsv' \"\n-            \"(Enhanced CSV) format\"\n-        )\n-\n-\n-class CsvWriter:\n-    \"\"\"\n-    Internal class to replace the csv writer ``writerow`` and ``writerows``\n-    functions so that in the case of ``delimiter=' '`` and\n-    ``quoting=csv.QUOTE_MINIMAL``, the output field value is quoted for empty\n-    fields (when value == '').\n-\n-    This changes the API slightly in that the writerow() and writerows()\n-    methods return the output written string instead of the length of\n-    that string.\n-\n-    Examples\n-    --------\n-\n-    >>> from astropy.io.ascii.core import CsvWriter\n-    >>> writer = CsvWriter(delimiter=' ')\n-    >>> print(writer.writerow(['hello', '', 'world']))\n-    hello \"\" world\n-    \"\"\"\n-\n-    # Random 16-character string that gets injected instead of any\n-    # empty fields and is then replaced post-write with doubled-quotechar.\n-    # Created with:\n-    # ''.join(random.choice(string.printable[:90]) for _ in range(16))\n-    replace_sentinel = \"2b=48Av%0-V3p>bX\"\n-\n-    def __init__(self, csvfile=None, **kwargs):\n-        self.csvfile = csvfile\n-\n-        # Temporary StringIO for catching the real csv.writer() object output\n-        self.temp_out = StringIO()\n-        self.writer = csv.writer(self.temp_out, **kwargs)\n-\n-        dialect = self.writer.dialect\n-        self.quotechar2 = dialect.quotechar * 2\n-        self.quote_empty = (dialect.quoting == csv.QUOTE_MINIMAL) and (\n-            dialect.delimiter == \" \"\n-        )\n-\n-    def writerow(self, values):\n-        \"\"\"\n-        Similar to csv.writer.writerow but with the custom quoting behavior.\n-        Returns the written string instead of the length of that string.\n-        \"\"\"\n-        has_empty = False\n-\n-        # If QUOTE_MINIMAL and space-delimited then replace empty fields with\n-        # the sentinel value.\n-        if self.quote_empty:\n-            for i, value in enumerate(values):\n-                if value == \"\":\n-                    has_empty = True\n-                    values[i] = self.replace_sentinel\n-\n-        return self._writerow(self.writer.writerow, values, has_empty)\n-\n-    def writerows(self, values_list):\n-        \"\"\"\n-        Similar to csv.writer.writerows but with the custom quoting behavior.\n-        Returns the written string instead of the length of that string.\n-        \"\"\"\n-        has_empty = False\n-\n-        # If QUOTE_MINIMAL and space-delimited then replace empty fields with\n-        # the sentinel value.\n-        if self.quote_empty:\n-            for values in values_list:\n-                for i, value in enumerate(values):\n-                    if value == \"\":\n-                        has_empty = True\n-                        values[i] = self.replace_sentinel\n-\n-        return self._writerow(self.writer.writerows, values_list, has_empty)\n-\n-    def _writerow(self, writerow_func, values, has_empty):\n-        \"\"\"\n-        Call ``writerow_func`` (either writerow or writerows) with ``values``.\n-        If it has empty fields that have been replaced then change those\n-        sentinel strings back to quoted empty strings, e.g. ``\"\"``.\n-        \"\"\"\n-        # Clear the temporary StringIO buffer that self.writer writes into and\n-        # then call the real csv.writer().writerow or writerows with values.\n-        self.temp_out.seek(0)\n-        self.temp_out.truncate()\n-        writerow_func(values)\n-\n-        row_string = self.temp_out.getvalue()\n-\n-        if self.quote_empty and has_empty:\n-            row_string = re.sub(self.replace_sentinel, self.quotechar2, row_string)\n-\n-        # self.csvfile is defined then write the output.  In practice the pure\n-        # Python writer calls with csvfile=None, while the fast writer calls with\n-        # a file-like object.\n-        if self.csvfile:\n-            self.csvfile.write(row_string)\n-\n-        return row_string\n-\n-\n-class MaskedConstant(numpy.ma.core.MaskedConstant):\n-    \"\"\"A trivial extension of numpy.ma.masked\n-\n-    We want to be able to put the generic term ``masked`` into a dictionary.\n-    The constant ``numpy.ma.masked`` is not hashable (see\n-    https://github.com/numpy/numpy/issues/4660), so we need to extend it\n-    here with a hash value.\n-\n-    See https://github.com/numpy/numpy/issues/11021 for rationale for\n-    __copy__ and __deepcopy__ methods.\n-    \"\"\"\n-\n-    def __hash__(self):\n-        \"\"\"All instances of this class shall have the same hash.\"\"\"\n-        # Any large number will do.\n-        return 1234567890\n-\n-    def __copy__(self):\n-        \"\"\"This is a singleton so just return self.\"\"\"\n-        return self\n-\n-    def __deepcopy__(self, memo):\n-        return self\n-\n-\n-masked = MaskedConstant()\n-\n-\n-class InconsistentTableError(ValueError):\n-    \"\"\"\n-    Indicates that an input table is inconsistent in some way.\n-\n-    The default behavior of ``BaseReader`` is to throw an instance of\n-    this class if a data row doesn't match the header.\n-    \"\"\"\n-\n-\n-class OptionalTableImportError(ImportError):\n-    \"\"\"\n-    Indicates that a dependency for table reading is not present.\n-\n-    An instance of this class is raised whenever an optional reader\n-    with certain required dependencies cannot operate because of\n-    an ImportError.\n-    \"\"\"\n-\n-\n-class ParameterError(NotImplementedError):\n-    \"\"\"\n-    Indicates that a reader cannot handle a passed parameter.\n-\n-    The C-based fast readers in ``io.ascii`` raise an instance of\n-    this error class upon encountering a parameter that the\n-    C engine cannot handle.\n-    \"\"\"\n-\n-\n-class FastOptionsError(NotImplementedError):\n-    \"\"\"\n-    Indicates that one of the specified options for fast\n-    reading is invalid.\n-    \"\"\"\n-\n-\n-class NoType:\n-    \"\"\"\n-    Superclass for ``StrType`` and ``NumType`` classes.\n-\n-    This class is the default type of ``Column`` and provides a base\n-    class for other data types.\n-    \"\"\"\n-\n-\n-class StrType(NoType):\n-    \"\"\"\n-    Indicates that a column consists of text data.\n-    \"\"\"\n-\n-\n-class NumType(NoType):\n-    \"\"\"\n-    Indicates that a column consists of numerical data.\n-    \"\"\"\n-\n-\n-class FloatType(NumType):\n-    \"\"\"\n-    Describes floating-point data.\n-    \"\"\"\n-\n-\n-class BoolType(NoType):\n-    \"\"\"\n-    Describes boolean data.\n-    \"\"\"\n-\n-\n-class IntType(NumType):\n-    \"\"\"\n-    Describes integer data.\n-    \"\"\"\n-\n-\n-class AllType(StrType, FloatType, IntType):\n-    \"\"\"\n-    Subclass of all other data types.\n-\n-    This type is returned by ``convert_numpy`` if the given numpy\n-    type does not match ``StrType``, ``FloatType``, or ``IntType``.\n-    \"\"\"\n-\n-\n-class Column:\n-    \"\"\"Table column.\n-\n-    The key attributes of a Column object are:\n-\n-    * **name** : column name\n-    * **type** : column type (NoType, StrType, NumType, FloatType, IntType)\n-    * **dtype** : numpy dtype (optional, overrides **type** if set)\n-    * **str_vals** : list of column values as strings\n-    * **fill_values** : dict of fill values\n-    * **shape** : list of element shape (default [] => scalar)\n-    * **data** : list of converted column values\n-    * **subtype** : actual datatype for columns serialized with JSON\n-    \"\"\"\n-\n-    def __init__(self, name):\n-        self.name = name\n-        self.type = NoType  # Generic type (Int, Float, Str etc)\n-        self.dtype = None  # Numpy dtype if available\n-        self.str_vals = []\n-        self.fill_values = {}\n-        self.shape = []\n-        self.subtype = None\n-\n-\n-class BaseInputter:\n-    \"\"\"\n-    Get the lines from the table input and return a list of lines.\n-\n-    \"\"\"\n-\n-    encoding = None\n-    \"\"\"Encoding used to read the file\"\"\"\n-\n-    def get_lines(self, table, newline=None):\n-        \"\"\"\n-        Get the lines from the ``table`` input. The input table can be one of:\n-\n-        * File name\n-        * String (newline separated) with all header and data lines (must have at least 2 lines)\n-        * File-like object with read() method\n-        * List of strings\n-\n-        Parameters\n-        ----------\n-        table : str, file-like, list\n-            Can be either a file name, string (newline separated) with all header and data\n-            lines (must have at least 2 lines), a file-like object with a\n-            ``read()`` method, or a list of strings.\n-        newline :\n-            Line separator. If `None` use OS default from ``splitlines()``.\n-\n-        Returns\n-        -------\n-        lines : list\n-            List of lines\n-        \"\"\"\n-        try:\n-            if hasattr(table, \"read\") or (\n-                \"\\n\" not in table + \"\" and \"\\r\" not in table + \"\"\n-            ):\n-                with get_readable_fileobj(table, encoding=self.encoding) as fileobj:\n-                    table = fileobj.read()\n-            if newline is None:\n-                lines = table.splitlines()\n-            else:\n-                lines = table.split(newline)\n-        except TypeError:\n-            try:\n-                # See if table supports indexing, slicing, and iteration\n-                table[0]\n-                table[0:1]\n-                iter(table)\n-                if len(table) > 1:\n-                    lines = table\n-                else:\n-                    # treat single entry as if string had been passed directly\n-                    if newline is None:\n-                        lines = table[0].splitlines()\n-                    else:\n-                        lines = table[0].split(newline)\n-\n-            except TypeError:\n-                raise TypeError(\n-                    'Input \"table\" must be a string (filename or data) or an iterable'\n-                )\n-\n-        return self.process_lines(lines)\n-\n-    def process_lines(self, lines):\n-        \"\"\"Process lines for subsequent use.  In the default case do nothing.\n-        This routine is not generally intended for removing comment lines or\n-        stripping whitespace.  These are done (if needed) in the header and\n-        data line processing.\n-\n-        Override this method if something more has to be done to convert raw\n-        input lines to the table rows.  For example the\n-        ContinuationLinesInputter derived class accounts for continuation\n-        characters if a row is split into lines.\"\"\"\n-        return lines\n-\n-\n-class BaseSplitter:\n-    \"\"\"\n-    Base splitter that uses python's split method to do the work.\n-\n-    This does not handle quoted values.  A key feature is the formulation of\n-    __call__ as a generator that returns a list of the split line values at\n-    each iteration.\n-\n-    There are two methods that are intended to be overridden, first\n-    ``process_line()`` to do pre-processing on each input line before splitting\n-    and ``process_val()`` to do post-processing on each split string value.  By\n-    default these apply the string ``strip()`` function.  These can be set to\n-    another function via the instance attribute or be disabled entirely, for\n-    example::\n-\n-      reader.header.splitter.process_val = lambda x: x.lstrip()\n-      reader.data.splitter.process_val = None\n-\n-    \"\"\"\n-\n-    delimiter = None\n-    \"\"\" one-character string used to separate fields \"\"\"\n-\n-    def process_line(self, line):\n-        \"\"\"Remove whitespace at the beginning or end of line.  This is especially useful for\n-        whitespace-delimited files to prevent spurious columns at the beginning or end.\n-        \"\"\"\n-        return line.strip()\n-\n-    def process_val(self, val):\n-        \"\"\"Remove whitespace at the beginning or end of value.\"\"\"\n-        return val.strip()\n-\n-    def __call__(self, lines):\n-        if self.process_line:\n-            lines = (self.process_line(x) for x in lines)\n-        for line in lines:\n-            vals = line.split(self.delimiter)\n-            if self.process_val:\n-                yield [self.process_val(x) for x in vals]\n-            else:\n-                yield vals\n-\n-    def join(self, vals):\n-        if self.delimiter is None:\n-            delimiter = \" \"\n-        else:\n-            delimiter = self.delimiter\n-        return delimiter.join(str(x) for x in vals)\n-\n-\n-class DefaultSplitter(BaseSplitter):\n-    \"\"\"Default class to split strings into columns using python csv.  The class\n-    attributes are taken from the csv Dialect class.\n-\n-    Typical usage::\n-\n-      # lines = ..\n-      splitter = ascii.DefaultSplitter()\n-      for col_vals in splitter(lines):\n-          for col_val in col_vals:\n-               ...\n-\n-    \"\"\"\n-\n-    delimiter = \" \"\n-    \"\"\" one-character string used to separate fields. \"\"\"\n-    quotechar = '\"'\n-    \"\"\" control how instances of *quotechar* in a field are quoted \"\"\"\n-    doublequote = True\n-    \"\"\" character to remove special meaning from following character \"\"\"\n-    escapechar = None\n-    \"\"\" one-character stringto quote fields containing special characters \"\"\"\n-    quoting = csv.QUOTE_MINIMAL\n-    \"\"\" control when quotes are recognized by the reader \"\"\"\n-    skipinitialspace = True\n-    \"\"\" ignore whitespace immediately following the delimiter \"\"\"\n-    csv_writer = None\n-    csv_writer_out = StringIO()\n-\n-    def process_line(self, line):\n-        \"\"\"Remove whitespace at the beginning or end of line.  This is especially useful for\n-        whitespace-delimited files to prevent spurious columns at the beginning or end.\n-        If splitting on whitespace then replace unquoted tabs with space first\"\"\"\n-        if self.delimiter == r\"\\s\":\n-            line = _replace_tab_with_space(line, self.escapechar, self.quotechar)\n-        return line.strip() + \"\\n\"\n-\n-    def process_val(self, val):\n-        \"\"\"Remove whitespace at the beginning or end of value.\"\"\"\n-        return val.strip(\" \\t\")\n-\n-    def __call__(self, lines):\n-        \"\"\"Return an iterator over the table ``lines``, where each iterator output\n-        is a list of the split line values.\n-\n-        Parameters\n-        ----------\n-        lines : list\n-            List of table lines\n-\n-        Yields\n-        ------\n-        line : list of str\n-            Each line's split values.\n-\n-        \"\"\"\n-        if self.process_line:\n-            lines = [self.process_line(x) for x in lines]\n-\n-        delimiter = \" \" if self.delimiter == r\"\\s\" else self.delimiter\n-\n-        csv_reader = csv.reader(\n-            lines,\n-            delimiter=delimiter,\n-            doublequote=self.doublequote,\n-            escapechar=self.escapechar,\n-            quotechar=self.quotechar,\n-            quoting=self.quoting,\n-            skipinitialspace=self.skipinitialspace,\n-        )\n-        for vals in csv_reader:\n-            if self.process_val:\n-                yield [self.process_val(x) for x in vals]\n-            else:\n-                yield vals\n-\n-    def join(self, vals):\n-        delimiter = \" \" if self.delimiter is None else str(self.delimiter)\n-\n-        if self.csv_writer is None:\n-            self.csv_writer = CsvWriter(\n-                delimiter=delimiter,\n-                doublequote=self.doublequote,\n-                escapechar=self.escapechar,\n-                quotechar=self.quotechar,\n-                quoting=self.quoting,\n-            )\n-        if self.process_val:\n-            vals = [self.process_val(x) for x in vals]\n-        out = self.csv_writer.writerow(vals).rstrip(\"\\r\\n\")\n-\n-        return out\n-\n-\n-def _replace_tab_with_space(line, escapechar, quotechar):\n-    \"\"\"Replace tabs with spaces in given string, preserving quoted substrings\n-\n-    Parameters\n-    ----------\n-    line : str\n-        String containing tabs to be replaced with spaces.\n-    escapechar : str\n-        Character in ``line`` used to escape special characters.\n-    quotechar : str\n-        Character in ``line`` indicating the start/end of a substring.\n-\n-    Returns\n-    -------\n-    line : str\n-        A copy of ``line`` with tabs replaced by spaces, preserving quoted substrings.\n-    \"\"\"\n-    newline = []\n-    in_quote = False\n-    lastchar = \"NONE\"\n-    for char in line:\n-        if char == quotechar and lastchar != escapechar:\n-            in_quote = not in_quote\n-        if char == \"\\t\" and not in_quote:\n-            char = \" \"\n-        lastchar = char\n-        newline.append(char)\n-    return \"\".join(newline)\n-\n-\n-def _get_line_index(line_or_func, lines):\n-    \"\"\"Return the appropriate line index, depending on ``line_or_func`` which\n-    can be either a function, a positive or negative int, or None.\n-    \"\"\"\n-\n-    if hasattr(line_or_func, \"__call__\"):\n-        return line_or_func(lines)\n-    elif line_or_func:\n-        if line_or_func >= 0:\n-            return line_or_func\n-        else:\n-            n_lines = sum(1 for line in lines)\n-            return n_lines + line_or_func\n-    else:\n-        return line_or_func\n-\n-\n-class BaseHeader:\n-    \"\"\"\n-    Base table header reader\n-    \"\"\"\n-\n-    auto_format = \"col{}\"\n-    \"\"\" format string for auto-generating column names \"\"\"\n-    start_line = None\n-    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n-    comment = None\n-    \"\"\" regular expression for comment lines \"\"\"\n-    splitter_class = DefaultSplitter\n-    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n-    names = None\n-    \"\"\" list of names corresponding to each data column \"\"\"\n-    write_comment = False\n-    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n-\n-    def __init__(self):\n-        self.splitter = self.splitter_class()\n-\n-    def _set_cols_from_names(self):\n-        self.cols = [Column(name=x) for x in self.names]\n-\n-    def update_meta(self, lines, meta):\n-        \"\"\"\n-        Extract any table-level metadata, e.g. keywords, comments, column metadata, from\n-        the table ``lines`` and update the OrderedDict ``meta`` in place.  This base\n-        method extracts comment lines and stores them in ``meta`` for output.\n-        \"\"\"\n-        if self.comment:\n-            re_comment = re.compile(self.comment)\n-            comment_lines = [x for x in lines if re_comment.match(x)]\n-        else:\n-            comment_lines = []\n-        comment_lines = [\n-            re.sub(\"^\" + self.comment, \"\", x).strip() for x in comment_lines\n-        ]\n-        if comment_lines:\n-            meta.setdefault(\"table\", {})[\"comments\"] = comment_lines\n-\n-    def get_cols(self, lines):\n-        \"\"\"Initialize the header Column objects from the table ``lines``.\n-\n-        Based on the previously set Header attributes find or create the column names.\n-        Sets ``self.cols`` with the list of Columns.\n-\n-        Parameters\n-        ----------\n-        lines : list\n-            List of table lines\n-\n-        \"\"\"\n-\n-        start_line = _get_line_index(self.start_line, self.process_lines(lines))\n-        if start_line is None:\n-            # No header line so auto-generate names from n_data_cols\n-            # Get the data values from the first line of table data to determine n_data_cols\n-            try:\n-                first_data_vals = next(self.data.get_str_vals())\n-            except StopIteration:\n-                raise InconsistentTableError(\n-                    \"No data lines found so cannot autogenerate column names\"\n-                )\n-            n_data_cols = len(first_data_vals)\n-            self.names = [self.auto_format.format(i) for i in range(1, n_data_cols + 1)]\n-\n-        else:\n-            for i, line in enumerate(self.process_lines(lines)):\n-                if i == start_line:\n-                    break\n-            else:  # No header line matching\n-                raise ValueError(\"No header line found in table\")\n-\n-            self.names = next(self.splitter([line]))\n-\n-        self._set_cols_from_names()\n-\n-    def process_lines(self, lines):\n-        \"\"\"Generator to yield non-blank and non-comment lines\"\"\"\n-        re_comment = re.compile(self.comment) if self.comment else None\n-        # Yield non-comment lines\n-        for line in lines:\n-            if line.strip() and (not self.comment or not re_comment.match(line)):\n-                yield line\n-\n-    def write_comments(self, lines, meta):\n-        if self.write_comment not in (False, None):\n-            for comment in meta.get(\"comments\", []):\n-                lines.append(self.write_comment + comment)\n-\n-    def write(self, lines):\n-        if self.start_line is not None:\n-            for i, spacer_line in zip(\n-                range(self.start_line), itertools.cycle(self.write_spacer_lines)\n-            ):\n-                lines.append(spacer_line)\n-            lines.append(self.splitter.join([x.info.name for x in self.cols]))\n-\n-    @property\n-    def colnames(self):\n-        \"\"\"Return the column names of the table\"\"\"\n-        return tuple(\n-            col.name if isinstance(col, Column) else col.info.name for col in self.cols\n-        )\n-\n-    def remove_columns(self, names):\n-        \"\"\"\n-        Remove several columns from the table.\n-\n-        Parameters\n-        ----------\n-        names : list\n-            A list containing the names of the columns to remove\n-        \"\"\"\n-        colnames = self.colnames\n-        for name in names:\n-            if name not in colnames:\n-                raise KeyError(f\"Column {name} does not exist\")\n-\n-        self.cols = [col for col in self.cols if col.name not in names]\n-\n-    def rename_column(self, name, new_name):\n-        \"\"\"\n-        Rename a column.\n-\n-        Parameters\n-        ----------\n-        name : str\n-            The current name of the column.\n-        new_name : str\n-            The new name for the column\n-        \"\"\"\n-        try:\n-            idx = self.colnames.index(name)\n-        except ValueError:\n-            raise KeyError(f\"Column {name} does not exist\")\n-\n-        col = self.cols[idx]\n-\n-        # For writing self.cols can contain cols that are not Column.  Raise\n-        # exception in that case.\n-        if isinstance(col, Column):\n-            col.name = new_name\n-        else:\n-            raise TypeError(f\"got column type {type(col)} instead of required {Column}\")\n-\n-    def get_type_map_key(self, col):\n-        return col.raw_type\n-\n-    def get_col_type(self, col):\n-        try:\n-            type_map_key = self.get_type_map_key(col)\n-            return self.col_type_map[type_map_key.lower()]\n-        except KeyError:\n-            raise ValueError(\n-                'Unknown data type \"\"{}\"\" for column \"{}\"'.format(\n-                    col.raw_type, col.name\n-                )\n-            )\n-\n-    def check_column_names(self, names, strict_names, guessing):\n-        \"\"\"\n-        Check column names.\n-\n-        This must be done before applying the names transformation\n-        so that guessing will fail appropriately if ``names`` is supplied.\n-        For instance if the basic reader is given a table with no column header\n-        row.\n-\n-        Parameters\n-        ----------\n-        names : list\n-            User-supplied list of column names\n-        strict_names : bool\n-            Whether to impose extra requirements on names\n-        guessing : bool\n-            True if this method is being called while guessing the table format\n-        \"\"\"\n-        if strict_names:\n-            # Impose strict requirements on column names (normally used in guessing)\n-            bads = [\" \", \",\", \"|\", \"\\t\", \"'\", '\"']\n-            for name in self.colnames:\n-                if (\n-                    _is_number(name)\n-                    or len(name) == 0\n-                    or name[0] in bads\n-                    or name[-1] in bads\n-                ):\n-                    raise InconsistentTableError(\n-                        f\"Column name {name!r} does not meet strict name requirements\"\n-                    )\n-        # When guessing require at least two columns, except for ECSV which can\n-        # reliably be guessed from the header requirements.\n-        if (\n-            guessing\n-            and len(self.colnames) <= 1\n-            and self.__class__.__name__ != \"EcsvHeader\"\n-        ):\n-            raise ValueError(\n-                \"Table format guessing requires at least two columns, got {}\".format(\n-                    list(self.colnames)\n-                )\n-            )\n-\n-        if names is not None and len(names) != len(self.colnames):\n-            raise InconsistentTableError(\n-                \"Length of names argument ({}) does not match number\"\n-                \" of table columns ({})\".format(len(names), len(self.colnames))\n-            )\n-\n-\n-class BaseData:\n-    \"\"\"\n-    Base table data reader.\n-    \"\"\"\n-\n-    start_line = None\n-    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n-    end_line = None\n-    \"\"\" None, int, or a function of ``lines`` that returns None or int \"\"\"\n-    comment = None\n-    \"\"\" Regular expression for comment lines \"\"\"\n-    splitter_class = DefaultSplitter\n-    \"\"\" Splitter class for splitting data lines into columns \"\"\"\n-    write_spacer_lines = [\"ASCII_TABLE_WRITE_SPACER_LINE\"]\n-    fill_include_names = None\n-    fill_exclude_names = None\n-    fill_values = [(masked, \"\")]\n-    formats = {}\n-\n-    def __init__(self):\n-        # Need to make sure fill_values list is instance attribute, not class attribute.\n-        # On read, this will be overwritten by the default in the ui.read (thus, in\n-        # the current implementation there can be no different default for different\n-        # Readers). On write, ui.py does not specify a default, so this line here matters.\n-        self.fill_values = copy.copy(self.fill_values)\n-        self.formats = copy.copy(self.formats)\n-        self.splitter = self.splitter_class()\n-\n-    def process_lines(self, lines):\n-        \"\"\"\n-        READ: Strip out comment lines and blank lines from list of ``lines``\n-\n-        Parameters\n-        ----------\n-        lines : list\n-            All lines in table\n-\n-        Returns\n-        -------\n-        lines : list\n-            List of lines\n-\n-        \"\"\"\n-        nonblank_lines = (x for x in lines if x.strip())\n-        if self.comment:\n-            re_comment = re.compile(self.comment)\n-            return [x for x in nonblank_lines if not re_comment.match(x)]\n-        else:\n-            return [x for x in nonblank_lines]\n-\n-    def get_data_lines(self, lines):\n-        \"\"\"\n-        READ: Set ``data_lines`` attribute to lines slice comprising table data values.\n-        \"\"\"\n-        data_lines = self.process_lines(lines)\n-        start_line = _get_line_index(self.start_line, data_lines)\n-        end_line = _get_line_index(self.end_line, data_lines)\n-\n-        if start_line is not None or end_line is not None:\n-            self.data_lines = data_lines[slice(start_line, end_line)]\n-        else:  # Don't copy entire data lines unless necessary\n-            self.data_lines = data_lines\n-\n-    def get_str_vals(self):\n-        \"\"\"Return a generator that returns a list of column values (as strings)\n-        for each data line.\"\"\"\n-        return self.splitter(self.data_lines)\n-\n-    def masks(self, cols):\n-        \"\"\"READ: Set fill value for each column and then apply that fill value\n-\n-        In the first step it is evaluated with value from ``fill_values`` applies to\n-        which column using ``fill_include_names`` and ``fill_exclude_names``.\n-        In the second step all replacements are done for the appropriate columns.\n-        \"\"\"\n-        if self.fill_values:\n-            self._set_fill_values(cols)\n-            self._set_masks(cols)\n-\n-    def _set_fill_values(self, cols):\n-        \"\"\"READ, WRITE: Set fill values of individual cols based on fill_values of BaseData\n-\n-        fill values has the following form:\n-        <fill_spec> = (<bad_value>, <fill_value>, <optional col_name>...)\n-        fill_values = <fill_spec> or list of <fill_spec>'s\n-\n-        \"\"\"\n-        if self.fill_values:\n-            # when we write tables the columns may be astropy.table.Columns\n-            # which don't carry a fill_values by default\n-            for col in cols:\n-                if not hasattr(col, \"fill_values\"):\n-                    col.fill_values = {}\n-\n-            # if input is only one <fill_spec>, then make it a list\n-            with suppress(TypeError):\n-                self.fill_values[0] + \"\"\n-                self.fill_values = [self.fill_values]\n-\n-            # Step 1: Set the default list of columns which are affected by\n-            # fill_values\n-            colnames = set(self.header.colnames)\n-            if self.fill_include_names is not None:\n-                colnames.intersection_update(self.fill_include_names)\n-            if self.fill_exclude_names is not None:\n-                colnames.difference_update(self.fill_exclude_names)\n-\n-            # Step 2a: Find out which columns are affected by this tuple\n-            # iterate over reversed order, so last condition is set first and\n-            # overwritten by earlier conditions\n-            for replacement in reversed(self.fill_values):\n-                if len(replacement) < 2:\n-                    raise ValueError(\n-                        \"Format of fill_values must be \"\n-                        \"(<bad>, <fill>, <optional col1>, ...)\"\n-                    )\n-                elif len(replacement) == 2:\n-                    affect_cols = colnames\n-                else:\n-                    affect_cols = replacement[2:]\n-\n-                for i, key in (\n-                    (i, x)\n-                    for i, x in enumerate(self.header.colnames)\n-                    if x in affect_cols\n-                ):\n-                    cols[i].fill_values[replacement[0]] = str(replacement[1])\n-\n-    def _set_masks(self, cols):\n-        \"\"\"READ: Replace string values in col.str_vals and set masks\"\"\"\n-        if self.fill_values:\n-            for col in (col for col in cols if col.fill_values):\n-                col.mask = numpy.zeros(len(col.str_vals), dtype=bool)\n-                for i, str_val in (\n-                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n-                ):\n-                    col.str_vals[i] = col.fill_values[str_val]\n-                    col.mask[i] = True\n-\n-    def _replace_vals(self, cols):\n-        \"\"\"WRITE: replace string values in col.str_vals\"\"\"\n-        if self.fill_values:\n-            for col in (col for col in cols if col.fill_values):\n-                for i, str_val in (\n-                    (i, x) for i, x in enumerate(col.str_vals) if x in col.fill_values\n-                ):\n-                    col.str_vals[i] = col.fill_values[str_val]\n-                if masked in col.fill_values and hasattr(col, \"mask\"):\n-                    mask_val = col.fill_values[masked]\n-                    for i in col.mask.nonzero()[0]:\n-                        col.str_vals[i] = mask_val\n-\n-    def str_vals(self):\n-        \"\"\"WRITE: convert all values in table to a list of lists of strings\n-\n-        This sets the fill values and possibly column formats from the input\n-        formats={} keyword, then ends up calling table.pprint._pformat_col_iter()\n-        by a circuitous path. That function does the real work of formatting.\n-        Finally replace anything matching the fill_values.\n-\n-        Returns\n-        -------\n-        values : list of list of str\n-        \"\"\"\n-        self._set_fill_values(self.cols)\n-        self._set_col_formats()\n-        for col in self.cols:\n-            col.str_vals = list(col.info.iter_str_vals())\n-        self._replace_vals(self.cols)\n-        return [col.str_vals for col in self.cols]\n-\n-    def write(self, lines):\n-        \"\"\"Write ``self.cols`` in place to ``lines``.\n-\n-        Parameters\n-        ----------\n-        lines : list\n-            List for collecting output of writing self.cols.\n-        \"\"\"\n-        if hasattr(self.start_line, \"__call__\"):\n-            raise TypeError(\"Start_line attribute cannot be callable for write()\")\n-        else:\n-            data_start_line = self.start_line or 0\n-\n-        while len(lines) < data_start_line:\n-            lines.append(itertools.cycle(self.write_spacer_lines))\n-\n-        col_str_iters = self.str_vals()\n-        for vals in zip(*col_str_iters):\n-            lines.append(self.splitter.join(vals))\n-\n-    def _set_col_formats(self):\n-        \"\"\"WRITE: set column formats.\"\"\"\n-        for col in self.cols:\n-            if col.info.name in self.formats:\n-                col.info.format = self.formats[col.info.name]\n-\n-\n-def convert_numpy(numpy_type):\n-    \"\"\"Return a tuple containing a function which converts a list into a numpy\n-    array and the type produced by the converter function.\n-\n-    Parameters\n-    ----------\n-    numpy_type : numpy data-type\n-        The numpy type required of an array returned by ``converter``. Must be a\n-        valid `numpy type <https://numpy.org/doc/stable/user/basics.types.html>`_\n-        (e.g., numpy.uint, numpy.int8, numpy.int64, numpy.float64) or a python\n-        type covered by a numpy type (e.g., int, float, str, bool).\n-\n-    Returns\n-    -------\n-    converter : callable\n-        ``converter`` is a function which accepts a list and converts it to a\n-        numpy array of type ``numpy_type``.\n-    converter_type : type\n-        ``converter_type`` tracks the generic data type produced by the\n-        converter function.\n-\n-    Raises\n-    ------\n-    ValueError\n-        Raised by ``converter`` if the list elements could not be converted to\n-        the required type.\n-    \"\"\"\n-\n-    # Infer converter type from an instance of numpy_type.\n-    type_name = numpy.array([], dtype=numpy_type).dtype.name\n-    if \"int\" in type_name:\n-        converter_type = IntType\n-    elif \"float\" in type_name:\n-        converter_type = FloatType\n-    elif \"bool\" in type_name:\n-        converter_type = BoolType\n-    elif \"str\" in type_name:\n-        converter_type = StrType\n-    else:\n-        converter_type = AllType\n-\n-    def bool_converter(vals):\n-        \"\"\"\n-        Convert values \"False\" and \"True\" to bools.  Raise an exception\n-        for any other string values.\n-        \"\"\"\n-        if len(vals) == 0:\n-            return numpy.array([], dtype=bool)\n-\n-        # Try a smaller subset first for a long array\n-        if len(vals) > 10000:\n-            svals = numpy.asarray(vals[:1000])\n-            if not numpy.all(\n-                (svals == \"False\") | (svals == \"True\") | (svals == \"0\") | (svals == \"1\")\n-            ):\n-                raise ValueError('bool input strings must be False, True, 0, 1, or \"\"')\n-        vals = numpy.asarray(vals)\n-\n-        trues = (vals == \"True\") | (vals == \"1\")\n-        falses = (vals == \"False\") | (vals == \"0\")\n-        if not numpy.all(trues | falses):\n-            raise ValueError('bool input strings must be only False, True, 0, 1, or \"\"')\n-\n-        return trues\n-\n-    def generic_converter(vals):\n-        return numpy.array(vals, numpy_type)\n-\n-    converter = bool_converter if converter_type is BoolType else generic_converter\n-\n-    return converter, converter_type\n-\n-\n-class BaseOutputter:\n-    \"\"\"Output table as a dict of column objects keyed on column name.  The\n-    table data are stored as plain python lists within the column objects.\n-    \"\"\"\n-\n-    # User-defined converters which gets set in ascii.ui if a `converter` kwarg\n-    # is supplied.\n-    converters = {}\n-\n-    # Derived classes must define default_converters and __call__\n-\n-    @staticmethod\n-    def _validate_and_copy(col, converters):\n-        \"\"\"Validate the format for the type converters and then copy those\n-        which are valid converters for this column (i.e. converter type is\n-        a subclass of col.type)\"\"\"\n-        # Allow specifying a single converter instead of a list of converters.\n-        # The input `converters` must be a ``type`` value that can init np.dtype.\n-        try:\n-            # Don't allow list-like things that dtype accepts\n-            assert type(converters) is type\n-            converters = [numpy.dtype(converters)]\n-        except (AssertionError, TypeError):\n-            pass\n-\n-        converters_out = []\n-        try:\n-            for converter in converters:\n-                try:\n-                    converter_func, converter_type = converter\n-                except TypeError as err:\n-                    if str(err).startswith(\"cannot unpack\"):\n-                        converter_func, converter_type = convert_numpy(converter)\n-                    else:\n-                        raise\n-                if not issubclass(converter_type, NoType):\n-                    raise ValueError(\"converter_type must be a subclass of NoType\")\n-                if issubclass(converter_type, col.type):\n-                    converters_out.append((converter_func, converter_type))\n-\n-        except (ValueError, TypeError) as err:\n-            raise ValueError(\n-                \"Error: invalid format for converters, see \"\n-                f\"documentation\\n{converters}: {err}\"\n-            )\n-        return converters_out\n-\n-    def _convert_vals(self, cols):\n-        for col in cols:\n-            for key, converters in self.converters.items():\n-                if fnmatch.fnmatch(col.name, key):\n-                    break\n-            else:\n-                if col.dtype is not None:\n-                    converters = [convert_numpy(col.dtype)]\n-                else:\n-                    converters = self.default_converters\n-\n-            col.converters = self._validate_and_copy(col, converters)\n-\n-            # Catch the last error in order to provide additional information\n-            # in case all attempts at column conversion fail.  The initial\n-            # value of of last_error will apply if no converters are defined\n-            # and the first col.converters[0] access raises IndexError.\n-            last_err = \"no converters defined\"\n-\n-            while not hasattr(col, \"data\"):\n-                # Try converters, popping the unsuccessful ones from the list.\n-                # If there are no converters left here then fail.\n-                if not col.converters:\n-                    raise ValueError(f\"Column {col.name} failed to convert: {last_err}\")\n-\n-                converter_func, converter_type = col.converters[0]\n-                if not issubclass(converter_type, col.type):\n-                    raise TypeError(\"converter type does not match column type\")\n-\n-                try:\n-                    col.data = converter_func(col.str_vals)\n-                    col.type = converter_type\n-                except (OverflowError, TypeError, ValueError) as err:\n-                    # Overflow during conversion (most likely an int that\n-                    # doesn't fit in native C long). Put string at the top of\n-                    # the converters list for the next while iteration.\n-                    # With python/cpython#95778 this has been supplemented with a\n-                    # \"ValueError: Exceeds the limit (4300) for integer string conversion\"\n-                    # so need to catch that as well.\n-                    if isinstance(err, OverflowError) or (\n-                        isinstance(err, ValueError)\n-                        and str(err).startswith(\"Exceeds the limit\")\n-                    ):\n-                        warnings.warn(\n-                            f\"OverflowError converting to {converter_type.__name__} in\"\n-                            f\" column {col.name}, reverting to String.\",\n-                            AstropyWarning,\n-                        )\n-                        col.converters.insert(0, convert_numpy(str))\n-                    else:\n-                        col.converters.pop(0)\n-                    last_err = err\n-\n-\n-def _deduplicate_names(names):\n-    \"\"\"Ensure there are no duplicates in ``names``\n-\n-    This is done by iteratively adding ``_<N>`` to the name for increasing N\n-    until the name is unique.\n-    \"\"\"\n-    new_names = []\n-    existing_names = set()\n-\n-    for name in names:\n-        base_name = name + \"_\"\n-        i = 1\n-        while name in existing_names:\n-            # Iterate until a unique name is found\n-            name = base_name + str(i)\n-            i += 1\n-        new_names.append(name)\n-        existing_names.add(name)\n-\n-    return new_names\n-\n-\n-class TableOutputter(BaseOutputter):\n-    \"\"\"\n-    Output the table as an astropy.table.Table object.\n-    \"\"\"\n-\n-    default_converters = [convert_numpy(int), convert_numpy(float), convert_numpy(str)]\n-\n-    def __call__(self, cols, meta):\n-        # Sets col.data to numpy array and col.type to io.ascii Type class (e.g.\n-        # FloatType) for each col.\n-        self._convert_vals(cols)\n-\n-        t_cols = [\n-            numpy.ma.MaskedArray(x.data, mask=x.mask)\n-            if hasattr(x, \"mask\") and numpy.any(x.mask)\n-            else x.data\n-            for x in cols\n-        ]\n-        out = Table(t_cols, names=[x.name for x in cols], meta=meta[\"table\"])\n-\n-        for col, out_col in zip(cols, out.columns.values()):\n-            for attr in (\"format\", \"unit\", \"description\"):\n-                if hasattr(col, attr):\n-                    setattr(out_col, attr, getattr(col, attr))\n-            if hasattr(col, \"meta\"):\n-                out_col.meta.update(col.meta)\n-\n-        return out\n-\n-\n-class MetaBaseReader(type):\n-    def __init__(cls, name, bases, dct):\n-        super().__init__(name, bases, dct)\n-\n-        format = dct.get(\"_format_name\")\n-        if format is None:\n-            return\n-\n-        fast = dct.get(\"_fast\")\n-        if fast is not None:\n-            FAST_CLASSES[format] = cls\n-\n-        FORMAT_CLASSES[format] = cls\n-\n-        io_formats = [\"ascii.\" + format] + dct.get(\"_io_registry_format_aliases\", [])\n-\n-        if dct.get(\"_io_registry_suffix\"):\n-            func = functools.partial(connect.io_identify, dct[\"_io_registry_suffix\"])\n-            connect.io_registry.register_identifier(io_formats[0], Table, func)\n-\n-        for io_format in io_formats:\n-            func = functools.partial(connect.io_read, io_format)\n-            header = f\"ASCII reader '{io_format}' details\\n\"\n-            func.__doc__ = (\n-                inspect.cleandoc(READ_DOCSTRING).strip()\n-                + \"\\n\\n\"\n-                + header\n-                + re.sub(\".\", \"=\", header)\n-                + \"\\n\"\n-            )\n-            func.__doc__ += inspect.cleandoc(cls.__doc__).strip()\n-            connect.io_registry.register_reader(io_format, Table, func)\n-\n-            if dct.get(\"_io_registry_can_write\", True):\n-                func = functools.partial(connect.io_write, io_format)\n-                header = f\"ASCII writer '{io_format}' details\\n\"\n-                func.__doc__ = (\n-                    inspect.cleandoc(WRITE_DOCSTRING).strip()\n-                    + \"\\n\\n\"\n-                    + header\n-                    + re.sub(\".\", \"=\", header)\n-                    + \"\\n\"\n-                )\n-                func.__doc__ += inspect.cleandoc(cls.__doc__).strip()\n-                connect.io_registry.register_writer(io_format, Table, func)\n-\n-\n-def _is_number(x):\n-    with suppress(ValueError):\n-        x = float(x)\n-        return True\n-    return False\n-\n-\n-def _apply_include_exclude_names(table, names, include_names, exclude_names):\n-    \"\"\"\n-    Apply names, include_names and exclude_names to a table or BaseHeader.\n-\n-    For the latter this relies on BaseHeader implementing ``colnames``,\n-    ``rename_column``, and ``remove_columns``.\n-\n-    Parameters\n-    ----------\n-    table : `~astropy.table.Table`, `~astropy.io.ascii.BaseHeader`\n-        Input table or BaseHeader subclass instance\n-    names : list\n-        List of names to override those in table (set to None to use existing names)\n-    include_names : list\n-        List of names to include in output\n-    exclude_names : list\n-        List of names to exclude from output (applied after ``include_names``)\n-\n-    \"\"\"\n-\n-    def rename_columns(table, names):\n-        # Rename table column names to those passed by user\n-        # Temporarily rename with names that are not in `names` or `table.colnames`.\n-        # This ensures that rename succeeds regardless of existing names.\n-        xxxs = \"x\" * max(len(name) for name in list(names) + list(table.colnames))\n-        for ii, colname in enumerate(table.colnames):\n-            table.rename_column(colname, xxxs + str(ii))\n-\n-        for ii, name in enumerate(names):\n-            table.rename_column(xxxs + str(ii), name)\n-\n-    if names is not None:\n-        rename_columns(table, names)\n-    else:\n-        colnames_uniq = _deduplicate_names(table.colnames)\n-        if colnames_uniq != list(table.colnames):\n-            rename_columns(table, colnames_uniq)\n-\n-    names_set = set(table.colnames)\n-\n-    if include_names is not None:\n-        names_set.intersection_update(include_names)\n-    if exclude_names is not None:\n-        names_set.difference_update(exclude_names)\n-    if names_set != set(table.colnames):\n-        remove_names = set(table.colnames) - names_set\n-        table.remove_columns(remove_names)\n-\n-\n-class BaseReader(metaclass=MetaBaseReader):\n-    \"\"\"Class providing methods to read and write an ASCII table using the specified\n-    header, data, inputter, and outputter instances.\n-\n-    Typical usage is to instantiate a Reader() object and customize the\n-    ``header``, ``data``, ``inputter``, and ``outputter`` attributes.  Each\n-    of these is an object of the corresponding class.\n-\n-    There is one method ``inconsistent_handler`` that can be used to customize the\n-    behavior of ``read()`` in the event that a data row doesn't match the header.\n-    The default behavior is to raise an InconsistentTableError.\n-\n-    \"\"\"\n-\n-    names = None\n-    include_names = None\n-    exclude_names = None\n-    strict_names = False\n-    guessing = False\n-    encoding = None\n-\n-    header_class = BaseHeader\n-    data_class = BaseData\n-    inputter_class = BaseInputter\n-    outputter_class = TableOutputter\n-\n-    # Max column dimension that writer supports for this format. Exceptions\n-    # include ECSV (no limit) and HTML (max_ndim=2).\n-    max_ndim = 1\n-\n-    def __init__(self):\n-        self.header = self.header_class()\n-        self.data = self.data_class()\n-        self.inputter = self.inputter_class()\n-        self.outputter = self.outputter_class()\n-        # Data and Header instances benefit from a little cross-coupling.  Header may need to\n-        # know about number of data columns for auto-column name generation and Data may\n-        # need to know about header (e.g. for fixed-width tables where widths are spec'd in header.\n-        self.data.header = self.header\n-        self.header.data = self.data\n-\n-        # Metadata, consisting of table-level meta and column-level meta.  The latter\n-        # could include information about column type, description, formatting, etc,\n-        # depending on the table meta format.\n-        self.meta = OrderedDict(table=OrderedDict(), cols=OrderedDict())\n-\n-    def _check_multidim_table(self, table):\n-        \"\"\"Check that the dimensions of columns in ``table`` are acceptable.\n-\n-        The reader class attribute ``max_ndim`` defines the maximum dimension of\n-        columns that can be written using this format. The base value is ``1``,\n-        corresponding to normal scalar columns with just a length.\n-\n-        Parameters\n-        ----------\n-        table : `~astropy.table.Table`\n-            Input table.\n-\n-        Raises\n-        ------\n-        ValueError\n-            If any column exceeds the number of allowed dimensions\n-        \"\"\"\n-        _check_multidim_table(table, self.max_ndim)\n-\n-    def read(self, table):\n-        \"\"\"Read the ``table`` and return the results in a format determined by\n-        the ``outputter`` attribute.\n-\n-        The ``table`` parameter is any string or object that can be processed\n-        by the instance ``inputter``.  For the base Inputter class ``table`` can be\n-        one of:\n-\n-        * File name\n-        * File-like object\n-        * String (newline separated) with all header and data lines (must have at least 2 lines)\n-        * List of strings\n-\n-        Parameters\n-        ----------\n-        table : str, file-like, list\n-            Input table.\n-\n-        Returns\n-        -------\n-        table : `~astropy.table.Table`\n-            Output table\n-\n-        \"\"\"\n-        # If ``table`` is a file then store the name in the ``data``\n-        # attribute. The ``table`` is a \"file\" if it is a string\n-        # without the new line specific to the OS.\n-        with suppress(TypeError):\n-            # Strings only\n-            if os.linesep not in table + \"\":\n-                self.data.table_name = os.path.basename(table)\n-\n-        # If one of the newline chars is set as field delimiter, only\n-        # accept the other one as line splitter\n-        if self.header.splitter.delimiter == \"\\n\":\n-            newline = \"\\r\"\n-        elif self.header.splitter.delimiter == \"\\r\":\n-            newline = \"\\n\"\n-        else:\n-            newline = None\n+    def __init__(self, header_rows=None, **kwargs):\n+        super().__init__(**kwargs)\n+        self.header_rows = header_rows\n \n-        # Get a list of the lines (rows) in the table\n-        self.lines = self.inputter.get_lines(table, newline=newline)\n-\n-        # Set self.data.data_lines to a slice of lines contain the data rows\n-        self.data.get_data_lines(self.lines)\n-\n-        # Extract table meta values (e.g. keywords, comments, etc).  Updates self.meta.\n-        self.header.update_meta(self.lines, self.meta)\n-\n-        # Get the table column definitions\n-        self.header.get_cols(self.lines)\n-\n-        # Make sure columns are valid\n-        self.header.check_column_names(self.names, self.strict_names, self.guessing)\n-\n-        self.cols = cols = self.header.cols\n-        self.data.splitter.cols = cols\n-        n_cols = len(cols)\n-\n-        for i, str_vals in enumerate(self.data.get_str_vals()):\n-            if len(str_vals) != n_cols:\n-                str_vals = self.inconsistent_handler(str_vals, n_cols)\n-\n-                # if str_vals is None, we skip this row\n-                if str_vals is None:\n-                    continue\n-\n-                # otherwise, we raise an error only if it is still inconsistent\n-                if len(str_vals) != n_cols:\n-                    errmsg = (\n-                        \"Number of header columns ({}) inconsistent with\"\n-                        \" data columns ({}) at data line {}\\n\"\n-                        \"Header values: {}\\n\"\n-                        \"Data values: {}\".format(\n-                            n_cols, len(str_vals), i, [x.name for x in cols], str_vals\n-                        )\n-                    )\n-\n-                    raise InconsistentTableError(errmsg)\n-\n-            for j, col in enumerate(cols):\n-                col.str_vals.append(str_vals[j])\n-\n-        self.data.masks(cols)\n-        if hasattr(self.header, \"table_meta\"):\n-            self.meta[\"table\"].update(self.header.table_meta)\n-\n-        _apply_include_exclude_names(\n-            self.header, self.names, self.include_names, self.exclude_names\n-        )\n-\n-        table = self.outputter(self.header.cols, self.meta)\n-        self.cols = self.header.cols\n-\n-        return table\n-\n-    def inconsistent_handler(self, str_vals, ncols):\n-        \"\"\"\n-        Adjust or skip data entries if a row is inconsistent with the header.\n-\n-        The default implementation does no adjustment, and hence will always trigger\n-        an exception in read() any time the number of data entries does not match\n-        the header.\n-\n-        Note that this will *not* be called if the row already matches the header.\n-\n-        Parameters\n-        ----------\n-        str_vals : list\n-            A list of value strings from the current row of the table.\n-        ncols : int\n-            The expected number of entries from the table header.\n-\n-        Returns\n-        -------\n-        str_vals : list\n-            List of strings to be parsed into data entries in the output table. If\n-            the length of this list does not match ``ncols``, an exception will be\n-            raised in read().  Can also be None, in which case the row will be\n-            skipped.\n-        \"\"\"\n-        # an empty list will always trigger an InconsistentTableError in read()\n-        return str_vals\n-\n-    @property\n-    def comment_lines(self):\n-        \"\"\"Return lines in the table that match header.comment regexp\"\"\"\n-        if not hasattr(self, \"lines\"):\n-            raise ValueError(\n-                \"Table must be read prior to accessing the header comment lines\"\n-            )\n-        if self.header.comment:\n-            re_comment = re.compile(self.header.comment)\n-            comment_lines = [x for x in self.lines if re_comment.match(x)]\n-        else:\n-            comment_lines = []\n-        return comment_lines\n-\n-    def update_table_data(self, table):\n-        \"\"\"\n-        Update table columns in place if needed.\n-\n-        This is a hook to allow updating the table columns after name\n-        filtering but before setting up to write the data.  This is currently\n-        only used by ECSV and is otherwise just a pass-through.\n-\n-        Parameters\n-        ----------\n-        table : `astropy.table.Table`\n-            Input table for writing\n-\n-        Returns\n-        -------\n-        table : `astropy.table.Table`\n-            Output table for writing\n-        \"\"\"\n-        return table\n-\n-    def write_header(self, lines, meta):\n-        self.header.write_comments(lines, meta)\n-        self.header.write(lines)\n-\n     def write(self, table):\n-        \"\"\"\n-        Write ``table`` as list of strings.\n-\n-        Parameters\n-        ----------\n-        table : `~astropy.table.Table`\n-            Input table data.\n-\n-        Returns\n-        -------\n-        lines : list\n-            List of strings corresponding to ASCII table\n-\n-        \"\"\"\n-\n-        # Check column names before altering\n-        self.header.cols = list(table.columns.values())\n-        self.header.check_column_names(self.names, self.strict_names, False)\n-\n-        # In-place update of columns in input ``table`` to reflect column\n-        # filtering.  Note that ``table`` is guaranteed to be a copy of the\n-        # original user-supplied table.\n-        _apply_include_exclude_names(\n-            table, self.names, self.include_names, self.exclude_names\n-        )\n-\n-        # This is a hook to allow updating the table columns after name\n-        # filtering but before setting up to write the data.  This is currently\n-        # only used by ECSV and is otherwise just a pass-through.\n-        table = self.update_table_data(table)\n-\n-        # Check that table column dimensions are supported by this format class.\n-        # Most formats support only 1-d columns, but some like ECSV support N-d.\n-        self._check_multidim_table(table)\n-\n-        # Now use altered columns\n-        new_cols = list(table.columns.values())\n-        # link information about the columns to the writer object (i.e. self)\n-        self.header.cols = new_cols\n-        self.data.cols = new_cols\n-        self.header.table_meta = table.meta\n-\n-        # Write header and data to lines list\n         lines = []\n-        self.write_header(lines, table.meta)\n-        self.data.write(lines)\n-\n-        return lines\n-\n-\n-class ContinuationLinesInputter(BaseInputter):\n-    \"\"\"Inputter where lines ending in ``continuation_char`` are joined\n-    with the subsequent line.  Example::\n-\n-      col1 col2 col3\n-      1 \\\n-      2 3\n-      4 5 \\\n-      6\n-    \"\"\"\n-\n-    continuation_char = \"\\\\\"\n-    replace_char = \" \"\n-    # If no_continue is not None then lines matching this regex are not subject\n-    # to line continuation.  The initial use case here is Daophot.  In this\n-    # case the continuation character is just replaced with replace_char.\n-    no_continue = None\n-\n-    def process_lines(self, lines):\n-        re_no_continue = re.compile(self.no_continue) if self.no_continue else None\n-\n-        parts = []\n-        outlines = []\n-        for line in lines:\n-            if re_no_continue and re_no_continue.match(line):\n-                line = line.replace(self.continuation_char, self.replace_char)\n-            if line.endswith(self.continuation_char):\n-                parts.append(line.replace(self.continuation_char, self.replace_char))\n-            else:\n-                parts.append(line)\n-                outlines.append(\"\".join(parts))\n-                parts = []\n-\n-        return outlines\n-\n-\n-class WhitespaceSplitter(DefaultSplitter):\n-    def process_line(self, line):\n-        \"\"\"Replace tab with space within ``line`` while respecting quoted substrings\"\"\"\n-        newline = []\n-        in_quote = False\n-        lastchar = None\n-        for char in line:\n-            if char == self.quotechar and (\n-                self.escapechar is None or lastchar != self.escapechar\n-            ):\n-                in_quote = not in_quote\n-            if char == \"\\t\" and not in_quote:\n-                char = \" \"\n-            lastchar = char\n-            newline.append(char)\n-\n-        return \"\".join(newline)\n-\n-\n-extra_reader_pars = (\n-    \"Reader\",\n-    \"Inputter\",\n-    \"Outputter\",\n-    \"delimiter\",\n-    \"comment\",\n-    \"quotechar\",\n-    \"header_start\",\n-    \"data_start\",\n-    \"data_end\",\n-    \"converters\",\n-    \"encoding\",\n-    \"data_Splitter\",\n-    \"header_Splitter\",\n-    \"names\",\n-    \"include_names\",\n-    \"exclude_names\",\n-    \"strict_names\",\n-    \"fill_values\",\n-    \"fill_include_names\",\n-    \"fill_exclude_names\",\n-)\n-\n-\n-def _get_reader(Reader, Inputter=None, Outputter=None, **kwargs):\n-    \"\"\"Initialize a table reader allowing for common customizations.  See ui.get_reader()\n-    for param docs.  This routine is for internal (package) use only and is useful\n-    because it depends only on the \"core\" module.\n-    \"\"\"\n-\n-    from .fastbasic import FastBasic\n-\n-    if issubclass(Reader, FastBasic):  # Fast readers handle args separately\n-        if Inputter is not None:\n-            kwargs[\"Inputter\"] = Inputter\n-        return Reader(**kwargs)\n-\n-    # If user explicitly passed a fast reader with enable='force'\n-    # (e.g. by passing non-default options), raise an error for slow readers\n-    if \"fast_reader\" in kwargs:\n-        if kwargs[\"fast_reader\"][\"enable\"] == \"force\":\n-            raise ParameterError(\n-                \"fast_reader required with \"\n-                \"{}, but this is not a fast C reader: {}\".format(\n-                    kwargs[\"fast_reader\"], Reader\n-                )\n-            )\n+        str_vals = self.data.str_vals()\n+        \n+        # Custom header rows take precedence over column names\n+        if self.header_rows:\n+            header = self.header_rows\n         else:\n-            del kwargs[\"fast_reader\"]  # Otherwise ignore fast_reader parameter\n+            header = self.header.colnames\n \n-    reader_kwargs = {k: v for k, v in kwargs.items() if k not in extra_reader_pars}\n-    reader = Reader(**reader_kwargs)\n-\n-    if Inputter is not None:\n-        reader.inputter = Inputter()\n-\n-    if Outputter is not None:\n-        reader.outputter = Outputter()\n-\n-    # Issue #855 suggested to set data_start to header_start + default_header_length\n-    # Thus, we need to retrieve this from the class definition before resetting these numbers.\n-    try:\n-        default_header_length = reader.data.start_line - reader.header.start_line\n-    except TypeError:  # Start line could be None or an instancemethod\n-        default_header_length = None\n-\n-    # csv.reader is hard-coded to recognise either '\\r' or '\\n' as end-of-line,\n-    # therefore DefaultSplitter cannot handle these as delimiters.\n-    if \"delimiter\" in kwargs:\n-        if kwargs[\"delimiter\"] in (\"\\n\", \"\\r\", \"\\r\\n\"):\n-            reader.header.splitter = BaseSplitter()\n-            reader.data.splitter = BaseSplitter()\n-        reader.header.splitter.delimiter = kwargs[\"delimiter\"]\n-        reader.data.splitter.delimiter = kwargs[\"delimiter\"]\n-    if \"comment\" in kwargs:\n-        reader.header.comment = kwargs[\"comment\"]\n-        reader.data.comment = kwargs[\"comment\"]\n-    if \"quotechar\" in kwargs:\n-        reader.header.splitter.quotechar = kwargs[\"quotechar\"]\n-        reader.data.splitter.quotechar = kwargs[\"quotechar\"]\n-    if \"data_start\" in kwargs:\n-        reader.data.start_line = kwargs[\"data_start\"]\n-    if \"data_end\" in kwargs:\n-        reader.data.end_line = kwargs[\"data_end\"]\n-    if \"header_start\" in kwargs:\n-        if reader.header.start_line is not None:\n-            reader.header.start_line = kwargs[\"header_start\"]\n-            # For FixedWidthTwoLine the data_start is calculated relative to the position line.\n-            # However, position_line is given as absolute number and not relative to header_start.\n-            # So, ignore this Reader here.\n-            if (\n-                (\"data_start\" not in kwargs)\n-                and (default_header_length is not None)\n-                and reader._format_name\n-                not in [\"fixed_width_two_line\", \"commented_header\"]\n-            ):\n-                reader.data.start_line = (\n-                    reader.header.start_line + default_header_length\n-                )\n-        elif kwargs[\"header_start\"] is not None:\n-            # User trying to set a None header start to some value other than None\n-            raise ValueError(\"header_start cannot be modified for this Reader\")\n-    if \"converters\" in kwargs:\n-        reader.outputter.converters = kwargs[\"converters\"]\n-    if \"data_Splitter\" in kwargs:\n-        reader.data.splitter = kwargs[\"data_Splitter\"]()\n-    if \"header_Splitter\" in kwargs:\n-        reader.header.splitter = kwargs[\"header_Splitter\"]()\n-    if \"names\" in kwargs:\n-        reader.names = kwargs[\"names\"]\n-        if None in reader.names:\n-            raise TypeError(\"Cannot have None for column name\")\n-        if len(set(reader.names)) != len(reader.names):\n-            raise ValueError(\"Duplicate column names\")\n-    if \"include_names\" in kwargs:\n-        reader.include_names = kwargs[\"include_names\"]\n-    if \"exclude_names\" in kwargs:\n-        reader.exclude_names = kwargs[\"exclude_names\"]\n-    # Strict names is normally set only within the guessing process to\n-    # indicate that column names cannot be numeric or have certain\n-    # characters at the beginning or end.  It gets used in\n-    # BaseHeader.check_column_names().\n-    if \"strict_names\" in kwargs:\n-        reader.strict_names = kwargs[\"strict_names\"]\n-    if \"fill_values\" in kwargs:\n-        reader.data.fill_values = kwargs[\"fill_values\"]\n-    if \"fill_include_names\" in kwargs:\n-        reader.data.fill_include_names = kwargs[\"fill_include_names\"]\n-    if \"fill_exclude_names\" in kwargs:\n-        reader.data.fill_exclude_names = kwargs[\"fill_exclude_names\"]\n-    if \"encoding\" in kwargs:\n-        reader.encoding = kwargs[\"encoding\"]\n-        reader.inputter.encoding = kwargs[\"encoding\"]\n-\n-    return reader\n-\n-\n-extra_writer_pars = (\n-    \"delimiter\",\n-    \"comment\",\n-    \"quotechar\",\n-    \"formats\",\n-    \"strip_whitespace\",\n-    \"names\",\n-    \"include_names\",\n-    \"exclude_names\",\n-    \"fill_values\",\n-    \"fill_include_names\",\n-    \"fill_exclude_names\",\n-)\n-\n-\n-def _get_writer(Writer, fast_writer, **kwargs):\n-    \"\"\"Initialize a table writer allowing for common customizations. This\n-    routine is for internal (package) use only and is useful because it depends\n-    only on the \"core\" module.\"\"\"\n-\n-    from .fastbasic import FastBasic\n-\n-    # A value of None for fill_values imply getting the default string\n-    # representation of masked values (depending on the writer class), but the\n-    # machinery expects a list.  The easiest here is to just pop the value off,\n-    # i.e. fill_values=None is the same as not providing it at all.\n-    if \"fill_values\" in kwargs and kwargs[\"fill_values\"] is None:\n-        del kwargs[\"fill_values\"]\n-\n-    if issubclass(Writer, FastBasic):  # Fast writers handle args separately\n-        return Writer(**kwargs)\n-    elif fast_writer and f\"fast_{Writer._format_name}\" in FAST_CLASSES:\n-        # Switch to fast writer\n-        kwargs[\"fast_writer\"] = fast_writer\n-        return FAST_CLASSES[f\"fast_{Writer._format_name}\"](**kwargs)\n-\n-    writer_kwargs = {k: v for k, v in kwargs.items() if k not in extra_writer_pars}\n-    writer = Writer(**writer_kwargs)\n-\n-    if \"delimiter\" in kwargs:\n-        writer.header.splitter.delimiter = kwargs[\"delimiter\"]\n-        writer.data.splitter.delimiter = kwargs[\"delimiter\"]\n-    if \"comment\" in kwargs:\n-        writer.header.write_comment = kwargs[\"comment\"]\n-        writer.data.write_comment = kwargs[\"comment\"]\n-    if \"quotechar\" in kwargs:\n-        writer.header.splitter.quotechar = kwargs[\"quotechar\"]\n-        writer.data.splitter.quotechar = kwargs[\"quotechar\"]\n-    if \"formats\" in kwargs:\n-        writer.data.formats = kwargs[\"formats\"]\n-    if \"strip_whitespace\" in kwargs:\n-        if kwargs[\"strip_whitespace\"]:\n-            # Restore the default SplitterClass process_val method which strips\n-            # whitespace.  This may have been changed in the Writer\n-            # initialization (e.g. Rdb and Tab)\n-            writer.data.splitter.process_val = operator.methodcaller(\"strip\", \" \\t\")\n-        else:\n-            writer.data.splitter.process_val = None\n-    if \"names\" in kwargs:\n-        writer.header.names = kwargs[\"names\"]\n-    if \"include_names\" in kwargs:\n-        writer.include_names = kwargs[\"include_names\"]\n-    if \"exclude_names\" in kwargs:\n-        writer.exclude_names = kwargs[\"exclude_names\"]\n-    if \"fill_values\" in kwargs:\n-        # Prepend user-specified values to the class default.\n-        with suppress(TypeError, IndexError):\n-            # Test if it looks like (match, replace_string, optional_colname),\n-            # in which case make it a list\n-            kwargs[\"fill_values\"][1] + \"\"\n-            kwargs[\"fill_values\"] = [kwargs[\"fill_values\"]]\n-        writer.data.fill_values = kwargs[\"fill_values\"] + writer.data.fill_values\n-    if \"fill_include_names\" in kwargs:\n-        writer.data.fill_include_names = kwargs[\"fill_include_names\"]\n-    if \"fill_exclude_names\" in kwargs:\n-        writer.data.fill_exclude_names = kwargs[\"fill_exclude_names\"]\n-    return writer\n+        # Format header\n+        lines.extend([' '.join(['=' * len(col) for col in header]),\n+                     ' '.join(header),\n+                     ' '.join(['=' * len(col) for col in header])])\n+        \n+        # Format data\n+        for row in zip(*str_vals):\n+            lines.append(' '.join(row))\n+        \n+        # Format footer\n+        lines.extend([' '.join(['=' * len(col) for col in header])])\n+        \n+        return lines\n\\ No newline at end of file\n--- astropy/io/ascii/ui.py\n+++ astropy/io/ascii/ui.py\n@@ -1,923 +1,3 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-\"\"\"An extensible ASCII table reader and writer.\n-\n-ui.py:\n-  Provides the main user functions for reading and writing tables.\n-\n-:Copyright: Smithsonian Astrophysical Observatory (2010)\n-:Author: Tom Aldcroft (aldcroft@head.cfa.harvard.edu)\n-\"\"\"\n-\n-\n-import collections\n-import contextlib\n-import copy\n-import os\n-import re\n-import sys\n-import time\n-import warnings\n-from io import StringIO\n-\n-import numpy as np\n-\n-from astropy.table import Table\n-from astropy.utils.data import get_readable_fileobj\n-from astropy.utils.exceptions import AstropyWarning\n-from astropy.utils.misc import NOT_OVERWRITING_MSG\n-\n-from . import (\n-    basic,\n-    cds,\n-    core,\n-    cparser,\n-    daophot,\n-    ecsv,\n-    fastbasic,\n-    fixedwidth,\n-    html,\n-    ipac,\n-    latex,\n-    mrt,\n-    rst,\n-    sextractor,\n-)\n-from .docs import READ_KWARG_TYPES, WRITE_KWARG_TYPES\n-\n-_read_trace = []\n-\n-# Default setting for guess parameter in read()\n-_GUESS = True\n-\n-\n-def _probably_html(table, maxchars=100000):\n-    \"\"\"\n-    Determine if ``table`` probably contains HTML content.  See PR #3693 and issue\n-    #3691 for context.\n-    \"\"\"\n-    if not isinstance(table, str):\n-        try:\n-            # If table is an iterable (list of strings) then take the first\n-            # maxchars of these.  Make sure this is something with random\n-            # access to exclude a file-like object\n-            table[0]\n-            table[:1]\n-            size = 0\n-            for i, line in enumerate(table):\n-                size += len(line)\n-                if size > maxchars:\n-                    table = table[: i + 1]\n-                    break\n-            table = os.linesep.join(table)\n-        except Exception:\n-            pass\n-\n-    if isinstance(table, str):\n-        # Look for signs of an HTML table in the first maxchars characters\n-        table = table[:maxchars]\n-\n-        # URL ending in .htm or .html\n-        if re.match(\n-            r\"( http[s]? | ftp | file ) :// .+ \\.htm[l]?$\",\n-            table,\n-            re.IGNORECASE | re.VERBOSE,\n-        ):\n-            return True\n-\n-        # Filename ending in .htm or .html which exists\n-        if re.search(r\"\\.htm[l]?$\", table[-5:], re.IGNORECASE) and os.path.exists(\n-            os.path.expanduser(table)\n-        ):\n-            return True\n-\n-        # Table starts with HTML document type declaration\n-        if re.match(r\"\\s* <! \\s* DOCTYPE \\s* HTML\", table, re.IGNORECASE | re.VERBOSE):\n-            return True\n-\n-        # Look for <TABLE .. >, <TR .. >, <TD .. > tag openers.\n-        if all(\n-            re.search(rf\"< \\s* {element} [^>]* >\", table, re.IGNORECASE | re.VERBOSE)\n-            for element in (\"table\", \"tr\", \"td\")\n-        ):\n-            return True\n-\n-    return False\n-\n-\n-def set_guess(guess):\n-    \"\"\"\n-    Set the default value of the ``guess`` parameter for read()\n-\n-    Parameters\n-    ----------\n-    guess : bool\n-        New default ``guess`` value (e.g., True or False)\n-\n-    \"\"\"\n-    global _GUESS\n-    _GUESS = guess\n-\n-\n-def get_reader(Reader=None, Inputter=None, Outputter=None, **kwargs):\n-    \"\"\"\n-    Initialize a table reader allowing for common customizations.  Most of the\n-    default behavior for various parameters is determined by the Reader class.\n-\n-    Parameters\n-    ----------\n-    Reader : `~astropy.io.ascii.BaseReader`\n-        Reader class (DEPRECATED). Default is :class:`Basic`.\n-    Inputter : `~astropy.io.ascii.BaseInputter`\n-        Inputter class\n-    Outputter : `~astropy.io.ascii.BaseOutputter`\n-        Outputter class\n-    delimiter : str\n-        Column delimiter string\n-    comment : str\n-        Regular expression defining a comment line in table\n-    quotechar : str\n-        One-character string to quote fields containing special characters\n-    header_start : int\n-        Line index for the header line not counting comment or blank lines.\n-        A line with only whitespace is considered blank.\n-    data_start : int\n-        Line index for the start of data not counting comment or blank lines.\n-        A line with only whitespace is considered blank.\n-    data_end : int\n-        Line index for the end of data not counting comment or blank lines.\n-        This value can be negative to count from the end.\n-    converters : dict\n-        Dict of converters.\n-    data_Splitter : `~astropy.io.ascii.BaseSplitter`\n-        Splitter class to split data columns.\n-    header_Splitter : `~astropy.io.ascii.BaseSplitter`\n-        Splitter class to split header columns.\n-    names : list\n-        List of names corresponding to each data column.\n-    include_names : list, optional\n-        List of names to include in output.\n-    exclude_names : list\n-        List of names to exclude from output (applied after ``include_names``).\n-    fill_values : tuple, list of tuple\n-        Specification of fill values for bad or missing table values.\n-    fill_include_names : list\n-        List of names to include in fill_values.\n-    fill_exclude_names : list\n-        List of names to exclude from fill_values (applied after ``fill_include_names``).\n-\n-    Returns\n-    -------\n-    reader : `~astropy.io.ascii.BaseReader` subclass\n-        ASCII format reader instance\n-    \"\"\"\n-    # This function is a light wrapper around core._get_reader to provide a\n-    # public interface with a default Reader.\n-    if Reader is None:\n-        # Default reader is Basic unless fast reader is forced\n-        fast_reader = _get_fast_reader_dict(kwargs)\n-        if fast_reader[\"enable\"] == \"force\":\n-            Reader = fastbasic.FastBasic\n-        else:\n-            Reader = basic.Basic\n-\n-    reader = core._get_reader(Reader, Inputter=Inputter, Outputter=Outputter, **kwargs)\n-    return reader\n-\n-\n-def _get_format_class(format, ReaderWriter, label):\n-    if format is not None and ReaderWriter is not None:\n-        raise ValueError(f\"Cannot supply both format and {label} keywords\")\n-\n-    if format is not None:\n-        if format in core.FORMAT_CLASSES:\n-            ReaderWriter = core.FORMAT_CLASSES[format]\n-        else:\n-            raise ValueError(\n-                \"ASCII format {!r} not in allowed list {}\".format(\n-                    format, sorted(core.FORMAT_CLASSES)\n-                )\n-            )\n-    return ReaderWriter\n-\n-\n-def _get_fast_reader_dict(kwargs):\n-    \"\"\"Convert 'fast_reader' key in kwargs into a dict if not already and make sure\n-    'enable' key is available.\n-    \"\"\"\n-    fast_reader = copy.deepcopy(kwargs.get(\"fast_reader\", True))\n-    if isinstance(fast_reader, dict):\n-        fast_reader.setdefault(\"enable\", \"force\")\n-    else:\n-        fast_reader = {\"enable\": fast_reader}\n-    return fast_reader\n-\n-\n-def _validate_read_write_kwargs(read_write, **kwargs):\n-    \"\"\"Validate types of keyword arg inputs to read() or write().\"\"\"\n-\n-    def is_ducktype(val, cls):\n-        \"\"\"Check if ``val`` is an instance of ``cls`` or \"seems\" like one:\n-        ``cls(val) == val`` does not raise and exception and is `True`. In\n-        this way you can pass in ``np.int16(2)`` and have that count as `int`.\n-\n-        This has a special-case of ``cls`` being 'list-like', meaning it is\n-        an iterable but not a string.\n-        \"\"\"\n-        if cls == \"list-like\":\n-            ok = not isinstance(val, str) and isinstance(val, collections.abc.Iterable)\n-        else:\n-            ok = isinstance(val, cls)\n-            if not ok:\n-                # See if ``val`` walks and quacks like a ``cls```.\n-                try:\n-                    new_val = cls(val)\n-                    assert new_val == val\n-                except Exception:\n-                    ok = False\n-                else:\n-                    ok = True\n-        return ok\n-\n-    kwarg_types = READ_KWARG_TYPES if read_write == \"read\" else WRITE_KWARG_TYPES\n-\n-    for arg, val in kwargs.items():\n-        # Kwarg type checking is opt-in, so kwargs not in the list are considered OK.\n-        # This reflects that some readers allow additional arguments that may not\n-        # be well-specified, e.g. ```__init__(self, **kwargs)`` is an option.\n-        if arg not in kwarg_types or val is None:\n-            continue\n-\n-        # Single type or tuple of types for this arg (like isinstance())\n-        types = kwarg_types[arg]\n-        err_msg = (\n-            f\"{read_write}() argument '{arg}' must be a \"\n-            f\"{types} object, got {type(val)} instead\"\n-        )\n-\n-        # Force `types` to be a tuple for the any() check below\n-        if not isinstance(types, tuple):\n-            types = (types,)\n-\n-        if not any(is_ducktype(val, cls) for cls in types):\n-            raise TypeError(err_msg)\n-\n-\n-def _expand_user_if_path(argument):\n-    if isinstance(argument, (str, bytes, os.PathLike)):\n-        # For the `read()` method, a `str` input can be either a file path or\n-        # the table data itself. File names for io.ascii cannot have newlines\n-        # in them and io.ascii does not accept table data as `bytes`, so we can\n-        # attempt to detect data strings like this.\n-        is_str_data = isinstance(argument, str) and (\n-            \"\\n\" in argument or \"\\r\" in argument\n-        )\n-        if not is_str_data:\n-            # Remain conservative in expanding the presumed-path\n-            ex_user = os.path.expanduser(argument)\n-            if os.path.exists(ex_user):\n-                argument = ex_user\n-    return argument\n-\n-\n-def read(table, guess=None, **kwargs):\n-    # This the final output from reading. Static analysis indicates the reading\n-    # logic (which is indeed complex) might not define `dat`, thus do so here.\n-    dat = None\n-\n-    # Docstring defined below\n-    del _read_trace[:]\n-\n-    # Downstream readers might munge kwargs\n-    kwargs = copy.deepcopy(kwargs)\n-\n-    _validate_read_write_kwargs(\"read\", **kwargs)\n-\n-    # Convert 'fast_reader' key in kwargs into a dict if not already and make sure\n-    # 'enable' key is available.\n-    fast_reader = _get_fast_reader_dict(kwargs)\n-    kwargs[\"fast_reader\"] = fast_reader\n-\n-    if fast_reader[\"enable\"] and fast_reader.get(\"chunk_size\"):\n-        return _read_in_chunks(table, **kwargs)\n-\n-    if \"fill_values\" not in kwargs:\n-        kwargs[\"fill_values\"] = [(\"\", \"0\")]\n-\n-    # If an Outputter is supplied in kwargs that will take precedence.\n-    if (\n-        \"Outputter\" in kwargs\n-    ):  # user specified Outputter, not supported for fast reading\n-        fast_reader[\"enable\"] = False\n-\n-    format = kwargs.get(\"format\")\n-    # Dictionary arguments are passed by reference per default and thus need\n-    # special protection:\n-    new_kwargs = copy.deepcopy(kwargs)\n-    kwargs[\"fast_reader\"] = copy.deepcopy(fast_reader)\n-\n-    # Get the Reader class based on possible format and Reader kwarg inputs.\n-    Reader = _get_format_class(format, kwargs.get(\"Reader\"), \"Reader\")\n-    if Reader is not None:\n-        new_kwargs[\"Reader\"] = Reader\n-        format = Reader._format_name\n-\n-    # Remove format keyword if there, this is only allowed in read() not get_reader()\n-    if \"format\" in new_kwargs:\n-        del new_kwargs[\"format\"]\n-\n-    if guess is None:\n-        guess = _GUESS\n-\n-    if guess:\n-        # If ``table`` is probably an HTML file then tell guess function to add\n-        # the HTML reader at the top of the guess list.  This is in response to\n-        # issue #3691 (and others) where libxml can segfault on a long non-HTML\n-        # file, thus prompting removal of the HTML reader from the default\n-        # guess list.\n-        new_kwargs[\"guess_html\"] = _probably_html(table)\n-\n-        # If `table` is a filename or readable file object then read in the\n-        # file now.  This prevents problems in Python 3 with the file object\n-        # getting closed or left at the file end.  See #3132, #3013, #3109,\n-        # #2001.  If a `readme` arg was passed that implies CDS format, in\n-        # which case the original `table` as the data filename must be left\n-        # intact.\n-        if \"readme\" not in new_kwargs:\n-            encoding = kwargs.get(\"encoding\")\n-            try:\n-                table = _expand_user_if_path(table)\n-                with get_readable_fileobj(table, encoding=encoding) as fileobj:\n-                    table = fileobj.read()\n-            except ValueError:  # unreadable or invalid binary file\n-                raise\n-            except Exception:\n-                pass\n-            else:\n-                # Ensure that `table` has at least one \\r or \\n in it\n-                # so that the core.BaseInputter test of\n-                # ('\\n' not in table and '\\r' not in table)\n-                # will fail and so `table` cannot be interpreted there\n-                # as a filename.  See #4160.\n-                if not re.search(r\"[\\r\\n]\", table):\n-                    table = table + os.linesep\n-\n-                # If the table got successfully read then look at the content\n-                # to see if is probably HTML, but only if it wasn't already\n-                # identified as HTML based on the filename.\n-                if not new_kwargs[\"guess_html\"]:\n-                    new_kwargs[\"guess_html\"] = _probably_html(table)\n-\n-        # Get the table from guess in ``dat``.  If ``dat`` comes back as None\n-        # then there was just one set of kwargs in the guess list so fall\n-        # through below to the non-guess way so that any problems result in a\n-        # more useful traceback.\n-        dat = _guess(table, new_kwargs, format, fast_reader)\n-        if dat is None:\n-            guess = False\n-\n-    if not guess:\n-        if format is None:\n-            reader = get_reader(**new_kwargs)\n-            format = reader._format_name\n-\n-        table = _expand_user_if_path(table)\n-\n-        # Try the fast reader version of `format` first if applicable.  Note that\n-        # if user specified a fast format (e.g. format='fast_basic') this test\n-        # will fail and the else-clause below will be used.\n-        if fast_reader[\"enable\"] and f\"fast_{format}\" in core.FAST_CLASSES:\n-            fast_kwargs = copy.deepcopy(new_kwargs)\n-            fast_kwargs[\"Reader\"] = core.FAST_CLASSES[f\"fast_{format}\"]\n-            fast_reader_rdr = get_reader(**fast_kwargs)\n-            try:\n-                dat = fast_reader_rdr.read(table)\n-                _read_trace.append(\n-                    {\n-                        \"kwargs\": copy.deepcopy(fast_kwargs),\n-                        \"Reader\": fast_reader_rdr.__class__,\n-                        \"status\": \"Success with fast reader (no guessing)\",\n-                    }\n-                )\n-            except (\n-                core.ParameterError,\n-                cparser.CParserError,\n-                UnicodeEncodeError,\n-            ) as err:\n-                # special testing value to avoid falling back on the slow reader\n-                if fast_reader[\"enable\"] == \"force\":\n-                    raise core.InconsistentTableError(\n-                        f\"fast reader {fast_reader_rdr.__class__} exception: {err}\"\n-                    )\n-                # If the fast reader doesn't work, try the slow version\n-                reader = get_reader(**new_kwargs)\n-                dat = reader.read(table)\n-                _read_trace.append(\n-                    {\n-                        \"kwargs\": copy.deepcopy(new_kwargs),\n-                        \"Reader\": reader.__class__,\n-                        \"status\": (\n-                            \"Success with slow reader after failing\"\n-                            \" with fast (no guessing)\"\n-                        ),\n-                    }\n-                )\n-        else:\n-            reader = get_reader(**new_kwargs)\n-            dat = reader.read(table)\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(new_kwargs),\n-                    \"Reader\": reader.__class__,\n-                    \"status\": \"Success with specified Reader class (no guessing)\",\n-                }\n-            )\n-\n-    # Static analysis (pyright) indicates `dat` might be left undefined, so just\n-    # to be sure define it at the beginning and check here.\n-    if dat is None:\n-        raise RuntimeError(\n-            \"read() function failed due to code logic error, \"\n-            \"please report this bug on github\"\n-        )\n-\n-    return dat\n-\n-\n-read.__doc__ = core.READ_DOCSTRING\n-\n-\n-def _guess(table, read_kwargs, format, fast_reader):\n-    \"\"\"\n-    Try to read the table using various sets of keyword args.  Start with the\n-    standard guess list and filter to make it unique and consistent with\n-    user-supplied read keyword args.  Finally, if none of those work then\n-    try the original user-supplied keyword args.\n-\n-    Parameters\n-    ----------\n-    table : str, file-like, list\n-        Input table as a file name, file-like object, list of strings, or\n-        single newline-separated string.\n-    read_kwargs : dict\n-        Keyword arguments from user to be supplied to reader\n-    format : str\n-        Table format\n-    fast_reader : dict\n-        Options for the C engine fast reader.  See read() function for details.\n-\n-    Returns\n-    -------\n-    dat : `~astropy.table.Table` or None\n-        Output table or None if only one guess format was available\n-    \"\"\"\n-\n-    # Keep a trace of all failed guesses kwarg\n-    failed_kwargs = []\n-\n-    # Get an ordered list of read() keyword arg dicts that will be cycled\n-    # through in order to guess the format.\n-    full_list_guess = _get_guess_kwargs_list(read_kwargs)\n-\n-    # If a fast version of the reader is available, try that before the slow version\n-    if (\n-        fast_reader[\"enable\"]\n-        and format is not None\n-        and f\"fast_{format}\" in core.FAST_CLASSES\n-    ):\n-        fast_kwargs = copy.deepcopy(read_kwargs)\n-        fast_kwargs[\"Reader\"] = core.FAST_CLASSES[f\"fast_{format}\"]\n-        full_list_guess = [fast_kwargs] + full_list_guess\n-    else:\n-        fast_kwargs = None\n-\n-    # Filter the full guess list so that each entry is consistent with user kwarg inputs.\n-    # This also removes any duplicates from the list.\n-    filtered_guess_kwargs = []\n-    fast_reader = read_kwargs.get(\"fast_reader\")\n-\n-    for guess_kwargs in full_list_guess:\n-        # If user specified slow reader then skip all fast readers\n-        if (\n-            fast_reader[\"enable\"] is False\n-            and guess_kwargs[\"Reader\"] in core.FAST_CLASSES.values()\n-        ):\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(guess_kwargs),\n-                    \"Reader\": guess_kwargs[\"Reader\"].__class__,\n-                    \"status\": \"Disabled: reader only available in fast version\",\n-                    \"dt\": f\"{0.0:.3f} ms\",\n-                }\n-            )\n-            continue\n-\n-        # If user required a fast reader then skip all non-fast readers\n-        if (\n-            fast_reader[\"enable\"] == \"force\"\n-            and guess_kwargs[\"Reader\"] not in core.FAST_CLASSES.values()\n-        ):\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(guess_kwargs),\n-                    \"Reader\": guess_kwargs[\"Reader\"].__class__,\n-                    \"status\": \"Disabled: no fast version of reader available\",\n-                    \"dt\": f\"{0.0:.3f} ms\",\n-                }\n-            )\n-            continue\n-\n-        guess_kwargs_ok = True  # guess_kwargs are consistent with user_kwargs?\n-        for key, val in read_kwargs.items():\n-            # Do guess_kwargs.update(read_kwargs) except that if guess_args has\n-            # a conflicting key/val pair then skip this guess entirely.\n-            if key not in guess_kwargs:\n-                guess_kwargs[key] = copy.deepcopy(val)\n-            elif val != guess_kwargs[key] and guess_kwargs != fast_kwargs:\n-                guess_kwargs_ok = False\n-                break\n-\n-        if not guess_kwargs_ok:\n-            # User-supplied kwarg is inconsistent with the guess-supplied kwarg, e.g.\n-            # user supplies delimiter=\"|\" but the guess wants to try delimiter=\" \",\n-            # so skip the guess entirely.\n-            continue\n-\n-        # Add the guess_kwargs to filtered list only if it is not already there.\n-        if guess_kwargs not in filtered_guess_kwargs:\n-            filtered_guess_kwargs.append(guess_kwargs)\n-\n-    # If there are not at least two formats to guess then return no table\n-    # (None) to indicate that guessing did not occur.  In that case the\n-    # non-guess read() will occur and any problems will result in a more useful\n-    # traceback.\n-    if len(filtered_guess_kwargs) <= 1:\n-        return None\n-\n-    # Define whitelist of exceptions that are expected from readers when\n-    # processing invalid inputs.  Note that OSError must fall through here\n-    # so one cannot simply catch any exception.\n-    guess_exception_classes = (\n-        core.InconsistentTableError,\n-        ValueError,\n-        TypeError,\n-        AttributeError,\n-        core.OptionalTableImportError,\n-        core.ParameterError,\n-        cparser.CParserError,\n-    )\n-\n-    # Now cycle through each possible reader and associated keyword arguments.\n-    # Try to read the table using those args, and if an exception occurs then\n-    # keep track of the failed guess and move on.\n-    for guess_kwargs in filtered_guess_kwargs:\n-        t0 = time.time()\n-        try:\n-            # If guessing will try all Readers then use strict req'ts on column names\n-            if \"Reader\" not in read_kwargs:\n-                guess_kwargs[\"strict_names\"] = True\n-\n-            reader = get_reader(**guess_kwargs)\n-\n-            reader.guessing = True\n-            dat = reader.read(table)\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(guess_kwargs),\n-                    \"Reader\": reader.__class__,\n-                    \"status\": \"Success (guessing)\",\n-                    \"dt\": f\"{(time.time() - t0) * 1000:.3f} ms\",\n-                }\n-            )\n-            return dat\n-\n-        except guess_exception_classes as err:\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(guess_kwargs),\n-                    \"status\": f\"{err.__class__.__name__}: {str(err)}\",\n-                    \"dt\": f\"{(time.time() - t0) * 1000:.3f} ms\",\n-                }\n-            )\n-            failed_kwargs.append(guess_kwargs)\n-    else:\n-        # Failed all guesses, try the original read_kwargs without column requirements\n-        try:\n-            reader = get_reader(**read_kwargs)\n-            dat = reader.read(table)\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(read_kwargs),\n-                    \"Reader\": reader.__class__,\n-                    \"status\": (\n-                        \"Success with original kwargs without strict_names (guessing)\"\n-                    ),\n-                }\n-            )\n-            return dat\n-\n-        except guess_exception_classes as err:\n-            _read_trace.append(\n-                {\n-                    \"kwargs\": copy.deepcopy(read_kwargs),\n-                    \"status\": f\"{err.__class__.__name__}: {str(err)}\",\n-                }\n-            )\n-            failed_kwargs.append(read_kwargs)\n-            lines = [\n-                \"\\nERROR: Unable to guess table format with the guesses listed below:\"\n-            ]\n-            for kwargs in failed_kwargs:\n-                sorted_keys = sorted(\n-                    x for x in sorted(kwargs) if x not in (\"Reader\", \"Outputter\")\n-                )\n-                reader_repr = repr(kwargs.get(\"Reader\", basic.Basic))\n-                keys_vals = [\"Reader:\" + re.search(r\"\\.(\\w+)'>\", reader_repr).group(1)]\n-                kwargs_sorted = ((key, kwargs[key]) for key in sorted_keys)\n-                keys_vals.extend([f\"{key}: {val!r}\" for key, val in kwargs_sorted])\n-                lines.append(\" \".join(keys_vals))\n-\n-            msg = [\n-                \"\",\n-                \"************************************************************************\",\n-                \"** ERROR: Unable to guess table format with the guesses listed above. **\",\n-                \"**                                                                    **\",\n-                \"** To figure out why the table did not read, use guess=False and      **\",\n-                \"** fast_reader=False, along with any appropriate arguments to read(). **\",\n-                \"** In particular specify the format and any known attributes like the **\",\n-                \"** delimiter.                                                         **\",\n-                \"************************************************************************\",\n-            ]\n-            lines.extend(msg)\n-            raise core.InconsistentTableError(\"\\n\".join(lines)) from None\n-\n-\n-def _get_guess_kwargs_list(read_kwargs):\n-    \"\"\"\n-    Get the full list of reader keyword argument dicts that are the basis\n-    for the format guessing process.  The returned full list will then be:\n-\n-    - Filtered to be consistent with user-supplied kwargs\n-    - Cleaned to have only unique entries\n-    - Used one by one to try reading the input table\n-\n-    Note that the order of the guess list has been tuned over years of usage.\n-    Maintainers need to be very careful about any adjustments as the\n-    reasoning may not be immediately evident in all cases.\n-\n-    This list can (and usually does) include duplicates.  This is a result\n-    of the order tuning, but these duplicates get removed later.\n-\n-    Parameters\n-    ----------\n-    read_kwargs : dict\n-        User-supplied read keyword args\n-\n-    Returns\n-    -------\n-    guess_kwargs_list : list\n-        List of read format keyword arg dicts\n-    \"\"\"\n-    guess_kwargs_list = []\n-\n-    # If the table is probably HTML based on some heuristics then start with the\n-    # HTML reader.\n-    if read_kwargs.pop(\"guess_html\", None):\n-        guess_kwargs_list.append(dict(Reader=html.HTML))\n-\n-    # Start with ECSV because an ECSV file will be read by Basic.  This format\n-    # has very specific header requirements and fails out quickly.\n-    guess_kwargs_list.append(dict(Reader=ecsv.Ecsv))\n-\n-    # Now try readers that accept the user-supplied keyword arguments\n-    # (actually include all here - check for compatibility of arguments later).\n-    # FixedWidthTwoLine would also be read by Basic, so it needs to come first;\n-    # same for RST.\n-    for reader in (\n-        fixedwidth.FixedWidthTwoLine,\n-        rst.RST,\n-        fastbasic.FastBasic,\n-        basic.Basic,\n-        fastbasic.FastRdb,\n-        basic.Rdb,\n-        fastbasic.FastTab,\n-        basic.Tab,\n-        cds.Cds,\n-        mrt.Mrt,\n-        daophot.Daophot,\n-        sextractor.SExtractor,\n-        ipac.Ipac,\n-        latex.Latex,\n-        latex.AASTex,\n-    ):\n-        guess_kwargs_list.append(dict(Reader=reader))\n-\n-    # Cycle through the basic-style readers using all combinations of delimiter\n-    # and quotechar.\n-    for Reader in (\n-        fastbasic.FastCommentedHeader,\n-        basic.CommentedHeader,\n-        fastbasic.FastBasic,\n-        basic.Basic,\n-        fastbasic.FastNoHeader,\n-        basic.NoHeader,\n-    ):\n-        for delimiter in (\"|\", \",\", \" \", r\"\\s\"):\n-            for quotechar in ('\"', \"'\"):\n-                guess_kwargs_list.append(\n-                    dict(Reader=Reader, delimiter=delimiter, quotechar=quotechar)\n-                )\n-\n-    return guess_kwargs_list\n-\n-\n-def _read_in_chunks(table, **kwargs):\n-    \"\"\"\n-    For fast_reader read the ``table`` in chunks and vstack to create\n-    a single table, OR return a generator of chunk tables.\n-    \"\"\"\n-    fast_reader = kwargs[\"fast_reader\"]\n-    chunk_size = fast_reader.pop(\"chunk_size\")\n-    chunk_generator = fast_reader.pop(\"chunk_generator\", False)\n-    fast_reader[\"parallel\"] = False  # No parallel with chunks\n-\n-    tbl_chunks = _read_in_chunks_generator(table, chunk_size, **kwargs)\n-    if chunk_generator:\n-        return tbl_chunks\n-\n-    tbl0 = next(tbl_chunks)\n-    masked = tbl0.masked\n-\n-    # Numpy won't allow resizing the original so make a copy here.\n-    out_cols = {col.name: col.data.copy() for col in tbl0.itercols()}\n-\n-    str_kinds = (\"S\", \"U\")\n-    for tbl in tbl_chunks:\n-        masked |= tbl.masked\n-        for name, col in tbl.columns.items():\n-            # Concatenate current column data and new column data\n-\n-            # If one of the inputs is string-like and the other is not, then\n-            # convert the non-string to a string.  In a perfect world this would\n-            # be handled by numpy, but as of numpy 1.13 this results in a string\n-            # dtype that is too long (https://github.com/numpy/numpy/issues/10062).\n-\n-            col1, col2 = out_cols[name], col.data\n-            if col1.dtype.kind in str_kinds and col2.dtype.kind not in str_kinds:\n-                col2 = np.array(col2.tolist(), dtype=col1.dtype.kind)\n-            elif col2.dtype.kind in str_kinds and col1.dtype.kind not in str_kinds:\n-                col1 = np.array(col1.tolist(), dtype=col2.dtype.kind)\n-\n-            # Choose either masked or normal concatenation\n-            concatenate = np.ma.concatenate if masked else np.concatenate\n-\n-            out_cols[name] = concatenate([col1, col2])\n-\n-    # Make final table from numpy arrays, converting dict to list\n-    out_cols = [out_cols[name] for name in tbl0.colnames]\n-    out = tbl0.__class__(out_cols, names=tbl0.colnames, meta=tbl0.meta, copy=False)\n-\n-    return out\n-\n-\n-def _read_in_chunks_generator(table, chunk_size, **kwargs):\n-    \"\"\"\n-    For fast_reader read the ``table`` in chunks and return a generator\n-    of tables for each chunk.\n-    \"\"\"\n-\n-    @contextlib.contextmanager\n-    def passthrough_fileobj(fileobj, encoding=None):\n-        \"\"\"Stub for get_readable_fileobj, which does not seem to work in Py3\n-        for input file-like object, see #6460\"\"\"\n-        yield fileobj\n-\n-    # Set up to coerce `table` input into a readable file object by selecting\n-    # an appropriate function.\n-\n-    # Convert table-as-string to a File object.  Finding a newline implies\n-    # that the string is not a filename.\n-    if isinstance(table, str) and (\"\\n\" in table or \"\\r\" in table):\n-        table = StringIO(table)\n-        fileobj_context = passthrough_fileobj\n-    elif hasattr(table, \"read\") and hasattr(table, \"seek\"):\n-        fileobj_context = passthrough_fileobj\n-    else:\n-        # string filename or pathlib\n-        fileobj_context = get_readable_fileobj\n-\n-    # Set up for iterating over chunks\n-    kwargs[\"fast_reader\"][\"return_header_chars\"] = True\n-    header = \"\"  # Table header (up to start of data)\n-    prev_chunk_chars = \"\"  # Chars from previous chunk after last newline\n-    first_chunk = True  # True for the first chunk, False afterward\n-\n-    with fileobj_context(table, encoding=kwargs.get(\"encoding\")) as fh:\n-        while True:\n-            chunk = fh.read(chunk_size)\n-            # Got fewer chars than requested, must be end of file\n-            final_chunk = len(chunk) < chunk_size\n-\n-            # If this is the last chunk and there is only whitespace then break\n-            if final_chunk and not re.search(r\"\\S\", chunk):\n-                break\n-\n-            # Step backwards from last character in chunk and find first newline\n-            for idx in range(len(chunk) - 1, -1, -1):\n-                if final_chunk or chunk[idx] == \"\\n\":\n-                    break\n-            else:\n-                raise ValueError(\"no newline found in chunk (chunk_size too small?)\")\n-\n-            # Stick on the header to the chunk part up to (and including) the\n-            # last newline.  Make sure the small strings are concatenated first.\n-            complete_chunk = (header + prev_chunk_chars) + chunk[: idx + 1]\n-            prev_chunk_chars = chunk[idx + 1 :]\n-\n-            # Now read the chunk as a complete table\n-            tbl = read(complete_chunk, guess=False, **kwargs)\n-\n-            # For the first chunk pop the meta key which contains the header\n-            # characters (everything up to the start of data) then fix kwargs\n-            # so it doesn't return that in meta any more.\n-            if first_chunk:\n-                header = tbl.meta.pop(\"__ascii_fast_reader_header_chars__\")\n-                first_chunk = False\n-\n-            yield tbl\n-\n-            if final_chunk:\n-                break\n-\n-\n-extra_writer_pars = (\n-    \"delimiter\",\n-    \"comment\",\n-    \"quotechar\",\n-    \"formats\",\n-    \"names\",\n-    \"include_names\",\n-    \"exclude_names\",\n-    \"strip_whitespace\",\n-)\n-\n-\n-def get_writer(Writer=None, fast_writer=True, **kwargs):\n-    \"\"\"\n-    Initialize a table writer allowing for common customizations.  Most of the\n-    default behavior for various parameters is determined by the Writer class.\n-\n-    Parameters\n-    ----------\n-    Writer : ``Writer``\n-        Writer class (DEPRECATED). Defaults to :class:`Basic`.\n-    delimiter : str\n-        Column delimiter string\n-    comment : str\n-        String defining a comment line in table\n-    quotechar : str\n-        One-character string to quote fields containing special characters\n-    formats : dict\n-        Dictionary of format specifiers or formatting functions\n-    strip_whitespace : bool\n-        Strip surrounding whitespace from column values.\n-    names : list\n-        List of names corresponding to each data column\n-    include_names : list\n-        List of names to include in output.\n-    exclude_names : list\n-        List of names to exclude from output (applied after ``include_names``)\n-    fast_writer : bool\n-        Whether to use the fast Cython writer.\n-\n-    Returns\n-    -------\n-    writer : `~astropy.io.ascii.BaseReader` subclass\n-        ASCII format writer instance\n-    \"\"\"\n-    if Writer is None:\n-        Writer = basic.Basic\n-    if \"strip_whitespace\" not in kwargs:\n-        kwargs[\"strip_whitespace\"] = True\n-    writer = core._get_writer(Writer, fast_writer, **kwargs)\n-\n-    # Handle the corner case of wanting to disable writing table comments for the\n-    # commented_header format.  This format *requires* a string for `write_comment`\n-    # because that is used for the header column row, so it is not possible to\n-    # set the input `comment` to None.  Without adding a new keyword or assuming\n-    # a default comment character, there is no other option but to tell user to\n-    # simply remove the meta['comments'].\n-    if isinstance(\n-        writer, (basic.CommentedHeader, fastbasic.FastCommentedHeader)\n-    ) and not isinstance(kwargs.get(\"comment\", \"\"), str):\n-        raise ValueError(\n-            \"for the commented_header writer you must supply a string\\n\"\n-            \"value for the `comment` keyword.  In order to disable writing\\n\"\n-            \"table comments use `del t.meta['comments']` prior to writing.\"\n-        )\n-\n-    return writer\n-\n-\n def write(\n     table,\n     output=None,\n\\ No newline at end of file\n@@ -928,8 +8,6 @@\n     overwrite=False,\n     **kwargs,\n ):\n-    # Docstring inserted below\n-\n     _validate_read_write_kwargs(\n         \"write\", format=format, fast_writer=fast_writer, overwrite=overwrite, **kwargs\n     )\n\\ No newline at end of file\n@@ -945,72 +23,21 @@\n     # Ensure that `table` is a Table subclass.\n     names = kwargs.get(\"names\")\n     if isinstance(table, Table):\n-        # While we are only going to read data from columns, we may need to\n-        # to adjust info attributes such as format, so we make a shallow copy.\n         table = table.__class__(table, names=names, copy=False)\n     else:\n-        # Otherwise, create a table from the input.\n         table = Table(table, names=names, copy=False)\n \n-    table0 = table[:0].copy()\n-    core._apply_include_exclude_names(\n-        table0,\n-        kwargs.get(\"names\"),\n-        kwargs.get(\"include_names\"),\n-        kwargs.get(\"exclude_names\"),\n-    )\n-    diff_format_with_names = set(kwargs.get(\"formats\", [])) - set(table0.colnames)\n-\n-    if diff_format_with_names:\n-        warnings.warn(\n-            \"The key(s) {} specified in the formats argument do not match a column\"\n-            \" name.\".format(diff_format_with_names),\n-            AstropyWarning,\n-        )\n-\n-    if table.has_mixin_columns:\n-        fast_writer = False\n-\n     Writer = _get_format_class(format, Writer, \"Writer\")\n     writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\n-    if writer._format_name in core.FAST_CLASSES:\n-        writer.write(table, output)\n-        return\n-\n     lines = writer.write(table)\n \n     # Write the lines to output\n     outstr = os.linesep.join(lines)\n     if not hasattr(output, \"write\"):\n-        # NOTE: we need to specify newline='', otherwise the default\n-        # behavior is for Python to translate \\r\\n (which we write because\n-        # of os.linesep) into \\r\\r\\n. Specifying newline='' disables any\n-        # auto-translation.\n         output = open(output, \"w\", newline=\"\")\n         output.write(outstr)\n         output.write(os.linesep)\n         output.close()\n     else:\n         output.write(outstr)\n-        output.write(os.linesep)\n-\n-\n-write.__doc__ = core.WRITE_DOCSTRING\n-\n-\n-def get_read_trace():\n-    \"\"\"\n-    Return a traceback of the attempted read formats for the last call to\n-    `~astropy.io.ascii.read` where guessing was enabled.  This is primarily for\n-    debugging.\n-\n-    The return value is a list of dicts, where each dict includes the keyword\n-    args ``kwargs`` used in the read call and the returned ``status``.\n-\n-    Returns\n-    -------\n-    trace : list of dict\n-        Ordered list of format guesses and status\n-    \"\"\"\n-\n-    return copy.deepcopy(_read_trace)\n+        output.write(os.linesep)\n\\ No newline at end of file", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "astropy__astropy-14365", "model_patch": "--- astropy/io/ascii/qdp.py\n+++ astropy/io/ascii/qdp.py\n@@ -50,7 +50,7 @@\n     'new'\n     >>> _line_type(\"NO,NO,NO,NO,NO\", delimiter=',')\n     'new'\n-    >>> _line_type(\"N O N NOON OON O\")\n+    >>> _line_type(\"N O N N O\")\n     Traceback (most recent call last):\n         ...\n     ValueError: Unrecognized QDP line...\n\\ No newline at end of file\n@@ -60,7 +60,7 @@\n     ValueError: Unrecognized QDP line...\n     \"\"\"\n     _decimal_re = r\"[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?\"\n-    _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n+    _command_re = r\"read [st]err(\\s+[0-9]+)+\"\n \n     sep = delimiter\n     if delimiter is None:\n\\ No newline at end of file\n@@ -68,7 +68,7 @@\n     _new_re = rf\"NO({sep}NO)+\"\n     _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n     _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n-    _line_type_re = re.compile(_type_re)\n+    _line_type_re = re.compile(_type_re, re.IGNORECASE)\n     line = line.strip()\n     if not line:\n         return \"comment\"\n\\ No newline at end of file\n@@ -82,561 +82,4 @@\n         if type_ == \"data\":\n             return f\"data,{len(val.split(sep=delimiter))}\"\n         else:\n-            return type_\n-\n-\n-def _get_type_from_list_of_lines(lines, delimiter=None):\n-    \"\"\"Read through the list of QDP file lines and label each line by type.\n-\n-    Parameters\n-    ----------\n-    lines : list\n-        List containing one file line in each entry\n-\n-    Returns\n-    -------\n-    contents : list\n-        List containing the type for each line (see `line_type_and_data`)\n-    ncol : int\n-        The number of columns in the data lines. Must be the same throughout\n-        the file\n-\n-    Examples\n-    --------\n-    >>> line0 = \"! A comment\"\n-    >>> line1 = \"543 12 456.0\"\n-    >>> lines = [line0, line1]\n-    >>> types, ncol = _get_type_from_list_of_lines(lines)\n-    >>> types[0]\n-    'comment'\n-    >>> types[1]\n-    'data,3'\n-    >>> ncol\n-    3\n-    >>> lines.append(\"23\")\n-    >>> _get_type_from_list_of_lines(lines)\n-    Traceback (most recent call last):\n-        ...\n-    ValueError: Inconsistent number of columns\n-    \"\"\"\n-    types = [_line_type(line, delimiter=delimiter) for line in lines]\n-    current_ncol = None\n-    for type_ in types:\n-        if type_.startswith(\"data,\"):\n-            ncol = int(type_[5:])\n-            if current_ncol is None:\n-                current_ncol = ncol\n-            elif ncol != current_ncol:\n-                raise ValueError(\"Inconsistent number of columns\")\n-\n-    return types, current_ncol\n-\n-\n-def _get_lines_from_file(qdp_file):\n-    if \"\\n\" in qdp_file:\n-        lines = qdp_file.split(\"\\n\")\n-    elif isinstance(qdp_file, str):\n-        with open(qdp_file) as fobj:\n-            lines = [line.strip() for line in fobj.readlines()]\n-    elif isinstance(qdp_file, Iterable):\n-        lines = qdp_file\n-    else:\n-        raise ValueError(\"invalid value of qdb_file\")\n-\n-    return lines\n-\n-\n-def _interpret_err_lines(err_specs, ncols, names=None):\n-    \"\"\"Give list of column names from the READ SERR and TERR commands.\n-\n-    Parameters\n-    ----------\n-    err_specs : dict\n-        ``{'serr': [n0, n1, ...], 'terr': [n2, n3, ...]}``\n-        Error specifications for symmetric and two-sided errors\n-    ncols : int\n-        Number of data columns\n-\n-    Other Parameters\n-    ----------------\n-    names : list of str\n-        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n-        including error columns.\n-\n-    Returns\n-    -------\n-    colnames : list\n-        List containing the column names. Error columns will have the name\n-        of the main column plus ``_err`` for symmetric errors, and ``_perr``\n-        and ``_nerr`` for positive and negative errors respectively\n-\n-    Examples\n-    --------\n-    >>> col_in = ['MJD', 'Rate']\n-    >>> cols = _interpret_err_lines(None, 2, names=col_in)\n-    >>> cols[0]\n-    'MJD'\n-    >>> err_specs = {'terr': [1], 'serr': [2]}\n-    >>> ncols = 5\n-    >>> cols = _interpret_err_lines(err_specs, ncols, names=col_in)\n-    >>> cols[0]\n-    'MJD'\n-    >>> cols[2]\n-    'MJD_nerr'\n-    >>> cols[4]\n-    'Rate_err'\n-    >>> _interpret_err_lines(err_specs, 6, names=col_in)\n-    Traceback (most recent call last):\n-        ...\n-    ValueError: Inconsistent number of input colnames\n-    \"\"\"\n-    colnames = [\"\" for i in range(ncols)]\n-    if err_specs is None:\n-        serr_cols = terr_cols = []\n-\n-    else:\n-        # I don't want to empty the original one when using `pop` below\n-        err_specs = copy.deepcopy(err_specs)\n-\n-        serr_cols = err_specs.pop(\"serr\", [])\n-        terr_cols = err_specs.pop(\"terr\", [])\n-\n-    if names is not None:\n-        all_error_cols = len(serr_cols) + len(terr_cols) * 2\n-        if all_error_cols + len(names) != ncols:\n-            raise ValueError(\"Inconsistent number of input colnames\")\n-\n-    shift = 0\n-    for i in range(ncols):\n-        col_num = i + 1 - shift\n-        if colnames[i] != \"\":\n-            continue\n-\n-        colname_root = f\"col{col_num}\"\n-\n-        if names is not None:\n-            colname_root = names[col_num - 1]\n-\n-        colnames[i] = f\"{colname_root}\"\n-        if col_num in serr_cols:\n-            colnames[i + 1] = f\"{colname_root}_err\"\n-            shift += 1\n-            continue\n-\n-        if col_num in terr_cols:\n-            colnames[i + 1] = f\"{colname_root}_perr\"\n-            colnames[i + 2] = f\"{colname_root}_nerr\"\n-            shift += 2\n-            continue\n-\n-    assert not np.any([c == \"\" for c in colnames])\n-\n-    return colnames\n-\n-\n-def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n-    \"\"\"Get all tables from a QDP file.\n-\n-    Parameters\n-    ----------\n-    qdp_file : str\n-        Input QDP file name\n-\n-    Other Parameters\n-    ----------------\n-    input_colnames : list of str\n-        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n-        including error columns.\n-    delimiter : str\n-        Delimiter for the values in the table.\n-\n-    Returns\n-    -------\n-    list of `~astropy.table.Table`\n-        List containing all the tables present inside the QDP file\n-    \"\"\"\n-    lines = _get_lines_from_file(qdp_file)\n-    contents, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)\n-\n-    table_list = []\n-    err_specs = {}\n-    colnames = None\n-\n-    comment_text = \"\"\n-    initial_comments = \"\"\n-    command_lines = \"\"\n-    current_rows = None\n-\n-    for line, datatype in zip(lines, contents):\n-        line = line.strip().lstrip(\"!\")\n-        # Is this a comment?\n-        if datatype == \"comment\":\n-            comment_text += line + \"\\n\"\n-            continue\n-\n-        if datatype == \"command\":\n-            # The first time I find commands, I save whatever comments into\n-            # The initial comments.\n-            if command_lines == \"\":\n-                initial_comments = comment_text\n-                comment_text = \"\"\n-\n-            if err_specs != {}:\n-                warnings.warn(\n-                    \"This file contains multiple command blocks. Please verify\",\n-                    AstropyUserWarning,\n-                )\n-            command_lines += line + \"\\n\"\n-            continue\n-\n-        if datatype.startswith(\"data\"):\n-            # The first time I find data, I define err_specs\n-            if err_specs == {} and command_lines != \"\":\n-                for cline in command_lines.strip().split(\"\\n\"):\n-                    command = cline.strip().split()\n-                    # This should never happen, but just in case.\n-                    if len(command) < 3:\n-                        continue\n-                    err_specs[command[1].lower()] = [int(c) for c in command[2:]]\n-            if colnames is None:\n-                colnames = _interpret_err_lines(err_specs, ncol, names=input_colnames)\n-\n-            if current_rows is None:\n-                current_rows = []\n-\n-            values = []\n-            for v in line.split(delimiter):\n-                if v == \"NO\":\n-                    values.append(np.ma.masked)\n-                else:\n-                    # Understand if number is int or float\n-                    try:\n-                        values.append(int(v))\n-                    except ValueError:\n-                        values.append(float(v))\n-            current_rows.append(values)\n-            continue\n-\n-        if datatype == \"new\":\n-            # Save table to table_list and reset\n-            if current_rows is not None:\n-                new_table = Table(names=colnames, rows=current_rows)\n-                new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\n-                    \"\\n\"\n-                )\n-                new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n-                # Reset comments\n-                comment_text = \"\"\n-                table_list.append(new_table)\n-                current_rows = None\n-            continue\n-\n-    # At the very end, if there is still a table being written, let's save\n-    # it to the table_list\n-    if current_rows is not None:\n-        new_table = Table(names=colnames, rows=current_rows)\n-        new_table.meta[\"initial_comments\"] = initial_comments.strip().split(\"\\n\")\n-        new_table.meta[\"comments\"] = comment_text.strip().split(\"\\n\")\n-        table_list.append(new_table)\n-\n-    return table_list\n-\n-\n-def _understand_err_col(colnames):\n-    \"\"\"Get which column names are error columns.\n-\n-    Examples\n-    --------\n-    >>> colnames = ['a', 'a_err', 'b', 'b_perr', 'b_nerr']\n-    >>> serr, terr = _understand_err_col(colnames)\n-    >>> np.allclose(serr, [1])\n-    True\n-    >>> np.allclose(terr, [2])\n-    True\n-    >>> serr, terr = _understand_err_col(['a', 'a_nerr'])\n-    Traceback (most recent call last):\n-    ...\n-    ValueError: Missing positive error...\n-    >>> serr, terr = _understand_err_col(['a', 'a_perr'])\n-    Traceback (most recent call last):\n-    ...\n-    ValueError: Missing negative error...\n-    \"\"\"\n-    shift = 0\n-    serr = []\n-    terr = []\n-\n-    for i, col in enumerate(colnames):\n-        if col.endswith(\"_err\"):\n-            # The previous column, but they're numbered from 1!\n-            # Plus, take shift into account\n-            serr.append(i - shift)\n-            shift += 1\n-        elif col.endswith(\"_perr\"):\n-            terr.append(i - shift)\n-            if len(colnames) == i + 1 or not colnames[i + 1].endswith(\"_nerr\"):\n-                raise ValueError(\"Missing negative error\")\n-            shift += 2\n-        elif col.endswith(\"_nerr\") and not colnames[i - 1].endswith(\"_perr\"):\n-            raise ValueError(\"Missing positive error\")\n-    return serr, terr\n-\n-\n-def _read_table_qdp(qdp_file, names=None, table_id=None, delimiter=None):\n-    \"\"\"Read a table from a QDP file.\n-\n-    Parameters\n-    ----------\n-    qdp_file : str\n-        Input QDP file name\n-\n-    Other Parameters\n-    ----------------\n-    names : list of str\n-        Name of data columns (defaults to ['col1', 'col2', ...]), _not_\n-        including error columns.\n-\n-    table_id : int, default 0\n-        Number of the table to be read from the QDP file. This is useful\n-        when multiple tables present in the file. By default, the first is read.\n-\n-    delimiter : str\n-        Any delimiter accepted by the `sep` argument of str.split()\n-\n-    Returns\n-    -------\n-    tables : list of `~astropy.table.Table`\n-        List containing all the tables present inside the QDP file\n-    \"\"\"\n-    if table_id is None:\n-        warnings.warn(\n-            \"table_id not specified. Reading the first available table\",\n-            AstropyUserWarning,\n-        )\n-        table_id = 0\n-\n-    tables = _get_tables_from_qdp_file(\n-        qdp_file, input_colnames=names, delimiter=delimiter\n-    )\n-\n-    return tables[table_id]\n-\n-\n-def _write_table_qdp(table, filename=None, err_specs=None):\n-    \"\"\"Write a table to a QDP file.\n-\n-    Parameters\n-    ----------\n-    table : :class:`~astropy.table.Table`\n-        Input table to be written\n-    filename : str\n-        Output QDP file name\n-\n-    Other Parameters\n-    ----------------\n-    err_specs : dict\n-        Dictionary of the format {'serr': [1], 'terr': [2, 3]}, specifying\n-        which columns have symmetric and two-sided errors (see QDP format\n-        specification)\n-    \"\"\"\n-    import io\n-\n-    fobj = io.StringIO()\n-\n-    if \"initial_comments\" in table.meta and table.meta[\"initial_comments\"] != []:\n-        for line in table.meta[\"initial_comments\"]:\n-            line = line.strip()\n-            if not line.startswith(\"!\"):\n-                line = \"!\" + line\n-            print(line, file=fobj)\n-\n-    if err_specs is None:\n-        serr_cols, terr_cols = _understand_err_col(table.colnames)\n-    else:\n-        serr_cols = err_specs.pop(\"serr\", [])\n-        terr_cols = err_specs.pop(\"terr\", [])\n-    if serr_cols != []:\n-        col_string = \" \".join([str(val) for val in serr_cols])\n-        print(f\"READ SERR {col_string}\", file=fobj)\n-    if terr_cols != []:\n-        col_string = \" \".join([str(val) for val in terr_cols])\n-        print(f\"READ TERR {col_string}\", file=fobj)\n-\n-    if \"comments\" in table.meta and table.meta[\"comments\"] != []:\n-        for line in table.meta[\"comments\"]:\n-            line = line.strip()\n-            if not line.startswith(\"!\"):\n-                line = \"!\" + line\n-            print(line, file=fobj)\n-\n-    colnames = table.colnames\n-    print(\"!\" + \" \".join(colnames), file=fobj)\n-    for row in table:\n-        values = []\n-        for val in row:\n-            if not np.ma.is_masked(val):\n-                rep = str(val)\n-            else:\n-                rep = \"NO\"\n-            values.append(rep)\n-        print(\" \".join(values), file=fobj)\n-\n-    full_string = fobj.getvalue()\n-    fobj.close()\n-\n-    if filename is not None:\n-        with open(filename, \"w\") as fobj:\n-            print(full_string, file=fobj)\n-\n-    return full_string.split(\"\\n\")\n-\n-\n-class QDPSplitter(core.DefaultSplitter):\n-    \"\"\"\n-    Split on space for QDP tables.\n-    \"\"\"\n-\n-    delimiter = \" \"\n-\n-\n-class QDPHeader(basic.CommentedHeaderHeader):\n-    \"\"\"\n-    Header that uses the :class:`astropy.io.ascii.basic.QDPSplitter`.\n-    \"\"\"\n-\n-    splitter_class = QDPSplitter\n-    comment = \"!\"\n-    write_comment = \"!\"\n-\n-\n-class QDPData(basic.BasicData):\n-    \"\"\"\n-    Data that uses the :class:`astropy.io.ascii.basic.CsvSplitter`.\n-    \"\"\"\n-\n-    splitter_class = QDPSplitter\n-    fill_values = [(core.masked, \"NO\")]\n-    comment = \"!\"\n-    write_comment = None\n-\n-\n-class QDP(basic.Basic):\n-    \"\"\"Quick and Dandy Plot table.\n-\n-    Example::\n-\n-        ! Initial comment line 1\n-        ! Initial comment line 2\n-        READ TERR 1\n-        READ SERR 3\n-        ! Table 0 comment\n-        !a a(pos) a(neg) b be c d\n-        53000.5   0.25  -0.5   1  1.5  3.5 2\n-        54000.5   1.25  -1.5   2  2.5  4.5 3\n-        NO NO NO NO NO\n-        ! Table 1 comment\n-        !a a(pos) a(neg) b be c d\n-        54000.5   2.25  -2.5   NO  3.5  5.5 5\n-        55000.5   3.25  -3.5   4  4.5  6.5 nan\n-\n-    The input table above contains some initial comments, the error commands,\n-    then two tables.\n-    This file format can contain multiple tables, separated by a line full\n-    of ``NO``s. Comments are exclamation marks, and missing values are single\n-    ``NO`` entries. The delimiter is usually whitespace, more rarely a comma.\n-    The QDP format differentiates between data and error columns. The table\n-    above has commands::\n-\n-        READ TERR 1\n-        READ SERR 3\n-\n-    which mean that after data column 1 there will be two error columns\n-    containing its positive and engative error bars, then data column 2 without\n-    error bars, then column 3, then a column with the symmetric error of column\n-    3, then the remaining data columns.\n-\n-    As explained below, table headers are highly inconsistent. Possible\n-    comments containing column names will be ignored and columns will be called\n-    ``col1``, ``col2``, etc. unless the user specifies their names with the\n-    ``names=`` keyword argument,\n-    When passing column names, pass **only the names of the data columns, not\n-    the error columns.**\n-    Error information will be encoded in the names of the table columns.\n-    (e.g. ``a_perr`` and ``a_nerr`` for the positive and negative error of\n-    column ``a``, ``b_err`` the symmetric error of column ``b``.)\n-\n-    When writing tables to this format, users can pass an ``err_specs`` keyword\n-    passing a dictionary ``{'serr': [3], 'terr': [1, 2]}``, meaning that data\n-    columns 1 and two will have two additional columns each with their positive\n-    and negative errors, and data column 3 will have an additional column with\n-    a symmetric error (just like the ``READ SERR`` and ``READ TERR`` commands\n-    above)\n-\n-    Headers are just comments, and tables distributed by various missions\n-    can differ greatly in their use of conventions. For example, light curves\n-    distributed by the Swift-Gehrels mission have an extra space in one header\n-    entry that makes the number of labels inconsistent with the number of cols.\n-    For this reason, we ignore the comments that might encode the column names\n-    and leave the name specification to the user.\n-\n-    Example::\n-\n-        >               Extra space\n-        >                   |\n-        >                   v\n-        >!     MJD       Err (pos)       Err(neg)        Rate            Error\n-        >53000.123456   2.378e-05     -2.378472e-05     NO             0.212439\n-\n-    These readers and writer classes will strive to understand which of the\n-    comments belong to all the tables, and which ones to each single table.\n-    General comments will be stored in the ``initial_comments`` meta of each\n-    table. The comments of each table will be stored in the ``comments`` meta.\n-\n-    Example::\n-\n-        t = Table.read(example_qdp, format='ascii.qdp', table_id=1, names=['a', 'b', 'c', 'd'])\n-\n-    reads the second table (``table_id=1``) in file ``example.qdp`` containing\n-    the table above. There are four column names but seven data columns, why?\n-    Because the ``READ SERR`` and ``READ TERR`` commands say that there are\n-    three error columns.\n-    ``t.meta['initial_comments']`` will contain the initial two comment lines\n-    in the file, while ``t.meta['comments']`` will contain ``Table 1 comment``\n-\n-    The table can be written to another file, preserving the same information,\n-    as::\n-\n-        t.write(test_file, err_specs={'terr': [1], 'serr': [3]})\n-\n-    Note how the ``terr`` and ``serr`` commands are passed to the writer.\n-\n-    \"\"\"\n-\n-    _format_name = \"qdp\"\n-    _io_registry_can_write = True\n-    _io_registry_suffix = \".qdp\"\n-    _description = \"Quick and Dandy Plotter\"\n-\n-    header_class = QDPHeader\n-    data_class = QDPData\n-\n-    def __init__(self, table_id=None, names=None, err_specs=None, sep=None):\n-        super().__init__()\n-        self.table_id = table_id\n-        self.names = names\n-        self.err_specs = err_specs\n-        self.delimiter = sep\n-\n-    def read(self, table):\n-        self.lines = self.inputter.get_lines(table, newline=\"\\n\")\n-        return _read_table_qdp(\n-            self.lines,\n-            table_id=self.table_id,\n-            names=self.names,\n-            delimiter=self.delimiter,\n-        )\n-\n-    def write(self, table):\n-        self._check_multidim_table(table)\n-        lines = _write_table_qdp(table, err_specs=self.err_specs)\n-        return lines\n+            return type_\n\\ No newline at end of file", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "astropy__astropy-14995", "model_patch": "--- astropy/nddata/mixins/ndarithmetic.py\n+++ astropy/nddata/mixins/ndarithmetic.py\n@@ -1,750 +1,33 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-# This module implements the Arithmetic mixin to the NDData class.\n-\n import warnings\n from copy import deepcopy\n-\n import numpy as np\n-\n from astropy.nddata.nduncertainty import NDUncertainty\n from astropy.units import dimensionless_unscaled\n from astropy.utils import format_doc, sharedmethod\n from astropy.utils.exceptions import AstropyUserWarning\n from astropy.utils.masked import Masked\n \n-__all__ = [\"NDArithmeticMixin\"]\n-\n-# Global so it doesn't pollute the class dict unnecessarily:\n-\n-# Docstring templates for add, subtract, multiply, divide methods.\n-_arit_doc = \"\"\"\n-    Performs {name} by evaluating ``self`` {op} ``operand``.\n-\n-    Parameters\n-    ----------\n-    operand, operand2 : `NDData`-like instance\n-        If ``operand2`` is ``None`` or not given it will perform the operation\n-        ``self`` {op} ``operand``.\n-        If ``operand2`` is given it will perform ``operand`` {op} ``operand2``.\n-        If the method was called on a class rather than on the instance\n-        ``operand2`` must be given.\n-\n-    propagate_uncertainties : `bool` or ``None``, optional\n-        If ``None`` the result will have no uncertainty. If ``False`` the\n-        result will have a copied version of the first operand that has an\n-        uncertainty. If ``True`` the result will have a correctly propagated\n-        uncertainty from the uncertainties of the operands but this assumes\n-        that the uncertainties are `NDUncertainty`-like. Default is ``True``.\n-\n-        .. versionchanged:: 1.2\n-            This parameter must be given as keyword-parameter. Using it as\n-            positional parameter is deprecated.\n-            ``None`` was added as valid parameter value.\n-\n-    handle_mask : callable, ``'first_found'`` or ``None``, optional\n-        If ``None`` the result will have no mask. If ``'first_found'`` the\n-        result will have a copied version of the first operand that has a\n-        mask). If it is a callable then the specified callable must\n-        create the results ``mask`` and if necessary provide a copy.\n-        Default is `numpy.logical_or`.\n-\n-        .. versionadded:: 1.2\n-\n-    handle_meta : callable, ``'first_found'`` or ``None``, optional\n-        If ``None`` the result will have no meta. If ``'first_found'`` the\n-        result will have a copied version of the first operand that has a\n-        (not empty) meta. If it is a callable then the specified callable must\n-        create the results ``meta`` and if necessary provide a copy.\n-        Default is ``None``.\n-\n-        .. versionadded:: 1.2\n-\n-    compare_wcs : callable, ``'first_found'`` or ``None``, optional\n-        If ``None`` the result will have no wcs and no comparison between\n-        the wcs of the operands is made. If ``'first_found'`` the\n-        result will have a copied version of the first operand that has a\n-        wcs. If it is a callable then the specified callable must\n-        compare the ``wcs``. The resulting ``wcs`` will be like if ``False``\n-        was given otherwise it raises a ``ValueError`` if the comparison was\n-        not successful. Default is ``'first_found'``.\n-\n-        .. versionadded:: 1.2\n-\n-    uncertainty_correlation : number or `~numpy.ndarray`, optional\n-        The correlation between the two operands is used for correct error\n-        propagation for correlated data as given in:\n-        https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas\n-        Default is 0.\n-\n-        .. versionadded:: 1.2\n-\n-\n-    kwargs :\n-        Any other parameter that should be passed to the callables used.\n-\n-    Returns\n-    -------\n-    result : `~astropy.nddata.NDData`-like\n-        The resulting dataset\n-\n-    Notes\n-    -----\n-    If a ``callable`` is used for ``mask``, ``wcs`` or ``meta`` the\n-    callable must accept the corresponding attributes as first two\n-    parameters. If the callable also needs additional parameters these can be\n-    defined as ``kwargs`` and must start with ``\"wcs_\"`` (for wcs callable) or\n-    ``\"meta_\"`` (for meta callable). This startstring is removed before the\n-    callable is called.\n-\n-    ``\"first_found\"`` can also be abbreviated with ``\"ff\"``.\n-    \"\"\"\n-\n-\n class NDArithmeticMixin:\n     \"\"\"\n-    Mixin class to add arithmetic to an NDData object.\n-\n-    When subclassing, be sure to list the superclasses in the correct order\n-    so that the subclass sees NDData as the main superclass. See\n-    `~astropy.nddata.NDDataArray` for an example.\n-\n-    Notes\n-    -----\n-    This class only aims at covering the most common cases so there are certain\n-    restrictions on the saved attributes::\n-\n-        - ``uncertainty`` : has to be something that has a `NDUncertainty`-like\n-          interface for uncertainty propagation\n-        - ``mask`` : has to be something that can be used by a bitwise ``or``\n-          operation.\n-        - ``wcs`` : has to implement a way of comparing with ``=`` to allow\n-          the operation.\n-\n-    But there is a workaround that allows to disable handling a specific\n-    attribute and to simply set the results attribute to ``None`` or to\n-    copy the existing attribute (and neglecting the other).\n-    For example for uncertainties not representing an `NDUncertainty`-like\n-    interface you can alter the ``propagate_uncertainties`` parameter in\n-    :meth:`NDArithmeticMixin.add`. ``None`` means that the result will have no\n-    uncertainty, ``False`` means it takes the uncertainty of the first operand\n-    (if this does not exist from the second operand) as the result's\n-    uncertainty. This behavior is also explained in the docstring for the\n-    different arithmetic operations.\n-\n-    Decomposing the units is not attempted, mainly due to the internal mechanics\n-    of `~astropy.units.Quantity`, so the resulting data might have units like\n-    ``km/m`` if you divided for example 100km by 5m. So this Mixin has adopted\n-    this behavior.\n-\n-    Examples\n-    --------\n-    Using this Mixin with `~astropy.nddata.NDData`:\n-\n-        >>> from astropy.nddata import NDData, NDArithmeticMixin\n-        >>> class NDDataWithMath(NDArithmeticMixin, NDData):\n-        ...     pass\n-\n-    Using it with one operand on an instance::\n-\n-        >>> ndd = NDDataWithMath(100)\n-        >>> ndd.add(20)\n-        NDDataWithMath(120)\n-\n-    Using it with two operand on an instance::\n-\n-        >>> ndd = NDDataWithMath(-4)\n-        >>> ndd.divide(1, ndd)\n-        NDDataWithMath(-0.25)\n-\n-    Using it as classmethod requires two operands::\n-\n-        >>> NDDataWithMath.subtract(5, 4)\n-        NDDataWithMath(1)\n-\n+    [Original class docstring...]\n     \"\"\"\n \n-    def _arithmetic(\n-        self,\n-        operation,\n-        operand,\n-        propagate_uncertainties=True,\n-        handle_mask=np.logical_or,\n-        handle_meta=None,\n-        uncertainty_correlation=0,\n-        compare_wcs=\"first_found\",\n-        operation_ignores_mask=False,\n-        axis=None,\n-        **kwds,\n-    ):\n+    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n         \"\"\"\n-        Base method which calculates the result of the arithmetic operation.\n-\n-        This method determines the result of the arithmetic operation on the\n-        ``data`` including their units and then forwards to other methods\n-        to calculate the other properties for the result (like uncertainty).\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            The operation that is performed on the `NDData`. Supported are\n-            `numpy.add`, `numpy.subtract`, `numpy.multiply` and\n-            `numpy.true_divide`.\n-\n-        operand : same type (class) as self\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        propagate_uncertainties : `bool` or ``None``, optional\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        handle_mask : callable, ``'first_found'`` or ``None``, optional\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        handle_meta : callable, ``'first_found'`` or ``None``, optional\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        compare_wcs : callable, ``'first_found'`` or ``None``, optional\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        uncertainty_correlation : ``Number`` or `~numpy.ndarray`, optional\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        operation_ignores_mask : bool, optional\n-            When True, masked values will be excluded from operations;\n-            otherwise the operation will be performed on all values,\n-            including masked ones.\n-\n-        axis : int or tuple of ints, optional\n-            axis or axes over which to perform collapse operations like min, max, sum or mean.\n-\n-        kwargs :\n-            Any other parameter that should be passed to the\n-            different :meth:`NDArithmeticMixin._arithmetic_mask` (or wcs, ...)\n-            methods.\n-\n-        Returns\n-        -------\n-        result : ndarray or `~astropy.units.Quantity`\n-            The resulting data as array (in case both operands were without\n-            unit) or as quantity if at least one had a unit.\n-\n-        kwargs : `dict`\n-            The kwargs should contain all the other attributes (besides data\n-            and unit) needed to create a new instance for the result. Creating\n-            the new instance is up to the calling method, for example\n-            :meth:`NDArithmeticMixin.add`.\n-\n+        Calculate the resulting mask.\n+        This is implemented as the piecewise `|` operation if both have a mask.\n         \"\"\"\n-        # Find the appropriate keywords for the appropriate method (not sure\n-        # if data and uncertainty are ever used ...)\n-        kwds2 = {\"mask\": {}, \"meta\": {}, \"wcs\": {}, \"data\": {}, \"uncertainty\": {}}\n-        for i in kwds:\n-            splitted = i.split(\"_\", 1)\n-            try:\n-                kwds2[splitted[0]][splitted[1]] = kwds[i]\n-            except KeyError:\n-                raise KeyError(f\"Unknown prefix {splitted[0]} for parameter {i}\")\n-\n-        kwargs = {}\n-\n-        # First check that the WCS allows the arithmetic operation\n-        if compare_wcs is None:\n-            kwargs[\"wcs\"] = None\n-        elif compare_wcs in [\"ff\", \"first_found\"]:\n-            if self.wcs is None and hasattr(operand, \"wcs\"):\n-                kwargs[\"wcs\"] = deepcopy(operand.wcs)\n-            else:\n-                kwargs[\"wcs\"] = deepcopy(self.wcs)\n-        else:\n-            kwargs[\"wcs\"] = self._arithmetic_wcs(\n-                operation, operand, compare_wcs, **kwds2[\"wcs\"]\n-            )\n-\n-        # collapse operations on masked quantities/arrays which are supported by\n-        # the astropy.utils.masked or np.ma modules should use those modules to\n-        # do the arithmetic on the data and propagate masks.\n-        use_masked_arith = operand is None and self.mask is not None\n-        if use_masked_arith:\n-            # if we're *including* masked values in the operation,\n-            # use the astropy Masked module:\n-            if not operation_ignores_mask:\n-                # call the numpy operation on a Masked NDDataArray\n-                # representation of the nddata, with units when available:\n-                if self.unit is not None and not hasattr(self.data, \"unit\"):\n-                    masked_input = Masked(self.data << self.unit, mask=self.mask)\n-                else:\n-                    masked_input = Masked(self.data, mask=self.mask)\n-            # if we're *excluding* masked values in the operation,\n-            # we use the numpy.ma module:\n-            else:\n-                masked_input = np.ma.masked_array(self.data, self.mask)\n-            result = operation(masked_input, axis=axis)\n-            # since result may be e.g. a float if operation is a sum over all axes,\n-            # let's ensure that result is a masked array, since we'll assume this later:\n-            if not hasattr(result, \"mask\"):\n-                result = np.ma.masked_array(\n-                    result, mask=np.zeros_like(result, dtype=bool)\n-                )\n-        else:\n-            # Then calculate the resulting data (which can but needs not be a\n-            # quantity)\n-            result = self._arithmetic_data(\n-                operation, operand, axis=axis, **kwds2[\"data\"]\n-            )\n-\n-        # preserve original units\n-        if not hasattr(result, \"unit\") and hasattr(self, \"unit\"):\n-            kwargs[\"unit\"] = self.unit\n-\n-        # Determine the other properties\n-        if propagate_uncertainties is None:\n-            kwargs[\"uncertainty\"] = None\n-        elif not propagate_uncertainties:\n-            if self.uncertainty is None:\n-                kwargs[\"uncertainty\"] = deepcopy(operand.uncertainty)\n-            else:\n-                kwargs[\"uncertainty\"] = deepcopy(self.uncertainty)\n-        else:\n-            kwargs[\"uncertainty\"] = self._arithmetic_uncertainty(\n-                operation,\n-                operand,\n-                result,\n-                uncertainty_correlation,\n-                axis=axis,\n-                **kwds2[\"uncertainty\"],\n-            )\n-\n-        # If both are None, there is nothing to do.\n-        if self.psf is not None or (operand is not None and operand.psf is not None):\n-            warnings.warn(\n-                f\"Not setting psf attribute during {operation.__name__}.\",\n-                AstropyUserWarning,\n-            )\n-\n-        if handle_mask is None:\n-            pass\n-        elif hasattr(result, \"mask\"):\n-            # if numpy.ma or astropy.utils.masked is being used, the constructor\n-            # will pick up the mask from the masked object:\n-            kwargs[\"mask\"] = None\n-        elif handle_mask in [\"ff\", \"first_found\"]:\n-            if self.mask is None:\n-                kwargs[\"mask\"] = deepcopy(operand.mask)\n-            else:\n-                kwargs[\"mask\"] = deepcopy(self.mask)\n-        else:\n-            kwargs[\"mask\"] = self._arithmetic_mask(\n-                operation, operand, handle_mask, axis=axis, **kwds2[\"mask\"]\n-            )\n-\n-        if handle_meta is None:\n-            kwargs[\"meta\"] = None\n-        elif handle_meta in [\"ff\", \"first_found\"]:\n-            if not self.meta:\n-                kwargs[\"meta\"] = deepcopy(operand.meta)\n-            else:\n-                kwargs[\"meta\"] = deepcopy(self.meta)\n-        else:\n-            kwargs[\"meta\"] = self._arithmetic_meta(\n-                operation, operand, handle_meta, **kwds2[\"meta\"]\n-            )\n-\n-        # Wrap the individual results into a new instance of the same class.\n-        return result, kwargs\n-\n-    def _arithmetic_data(self, operation, operand, **kwds):\n-        \"\"\"\n-        Calculate the resulting data.\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            see `NDArithmeticMixin._arithmetic` parameter description.\n-\n-        operand : `NDData`-like instance\n-            The second operand wrapped in an instance of the same class as\n-            self.\n-\n-        kwds :\n-            Additional parameters.\n-\n-        Returns\n-        -------\n-        result_data : ndarray or `~astropy.units.Quantity`\n-            If both operands had no unit the resulting data is a simple numpy\n-            array, but if any of the operands had a unit the return is a\n-            Quantity.\n-        \"\"\"\n-        # Do the calculation with or without units\n-        if self.unit is None:\n-            if operand.unit is None:\n-                result = operation(self.data, operand.data)\n-            else:\n-                result = operation(\n-                    self.data << dimensionless_unscaled, operand.data << operand.unit\n-                )\n-        elif hasattr(operand, \"unit\"):\n-            if operand.unit is not None:\n-                result = operation(self.data << self.unit, operand.data << operand.unit)\n-            else:\n-                result = operation(\n-                    self.data << self.unit, operand.data << dimensionless_unscaled\n-                )\n-        elif operand is not None:\n-            result = operation(self.data << self.unit, operand.data << operand.unit)\n+        # Check if both masks are present\n+        if self.mask is not None and operand.mask is not None:\n+            return handle_mask(self.mask, operand.mask, **kwds)\n+        elif self.mask is not None:\n+            # Use self's mask if operand doesn't have one\n+            return deepcopy(self.mask)\n+        elif operand.mask is not None:\n+            # Use operand's mask if self doesn't have one\n+            return deepcopy(operand.mask)\n         else:\n-            result = operation(self.data, axis=kwds[\"axis\"])\n-\n-        return result\n-\n-    def _arithmetic_uncertainty(self, operation, operand, result, correlation, **kwds):\n-        \"\"\"\n-        Calculate the resulting uncertainty.\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n-\n-        operand : `NDData`-like instance\n-            The second operand wrapped in an instance of the same class as\n-            self.\n-\n-        result : `~astropy.units.Quantity` or `~numpy.ndarray`\n-            The result of :meth:`NDArithmeticMixin._arithmetic_data`.\n-\n-        correlation : number or `~numpy.ndarray`\n-            see :meth:`NDArithmeticMixin.add` parameter description.\n-\n-        kwds :\n-            Additional parameters.\n-\n-        Returns\n-        -------\n-        result_uncertainty : `NDUncertainty` subclass instance or None\n-            The resulting uncertainty already saved in the same `NDUncertainty`\n-            subclass that ``self`` had (or ``operand`` if self had no\n-            uncertainty). ``None`` only if both had no uncertainty.\n-        \"\"\"\n-        # Make sure these uncertainties are NDUncertainties so this kind of\n-        # propagation is possible.\n-        if self.uncertainty is not None and not isinstance(\n-            self.uncertainty, NDUncertainty\n-        ):\n-            raise TypeError(\n-                \"Uncertainty propagation is only defined for \"\n-                \"subclasses of NDUncertainty.\"\n-            )\n-        if (\n-            operand is not None\n-            and operand.uncertainty is not None\n-            and not isinstance(operand.uncertainty, NDUncertainty)\n-        ):\n-            raise TypeError(\n-                \"Uncertainty propagation is only defined for \"\n-                \"subclasses of NDUncertainty.\"\n-            )\n-\n-        # Now do the uncertainty propagation\n-        # TODO: There is no enforced requirement that actually forbids the\n-        # uncertainty to have negative entries but with correlation the\n-        # sign of the uncertainty DOES matter.\n-        if self.uncertainty is None and (\n-            not hasattr(operand, \"uncertainty\") or operand.uncertainty is None\n-        ):\n-            # Neither has uncertainties so the result should have none.\n+            # Both don't have masks, return None\n             return None\n-        elif self.uncertainty is None:\n-            # Create a temporary uncertainty to allow uncertainty propagation\n-            # to yield the correct results. (issue #4152)\n-            self.uncertainty = operand.uncertainty.__class__(None)\n-            result_uncert = self.uncertainty.propagate(\n-                operation, operand, result, correlation\n-            )\n-            # Delete the temporary uncertainty again.\n-            self.uncertainty = None\n-            return result_uncert\n \n-        elif operand is not None and operand.uncertainty is None:\n-            # As with self.uncertainty is None but the other way around.\n-            operand.uncertainty = self.uncertainty.__class__(None)\n-            result_uncert = self.uncertainty.propagate(\n-                operation, operand, result, correlation\n-            )\n-            operand.uncertainty = None\n-            return result_uncert\n-\n-        else:\n-            # Both have uncertainties so just propagate.\n-\n-            # only supply the axis kwarg if one has been specified for a collapsing operation\n-            axis_kwarg = dict(axis=kwds[\"axis\"]) if \"axis\" in kwds else dict()\n-            return self.uncertainty.propagate(\n-                operation, operand, result, correlation, **axis_kwarg\n-            )\n-\n-    def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n-        \"\"\"\n-        Calculate the resulting mask.\n-\n-        This is implemented as the piecewise ``or`` operation if both have a\n-        mask.\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n-            By default, the ``operation`` will be ignored.\n-\n-        operand : `NDData`-like instance\n-            The second operand wrapped in an instance of the same class as\n-            self.\n-\n-        handle_mask : callable\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        kwds :\n-            Additional parameters given to ``handle_mask``.\n-\n-        Returns\n-        -------\n-        result_mask : any type\n-            If only one mask was present this mask is returned.\n-            If neither had a mask ``None`` is returned. Otherwise\n-            ``handle_mask`` must create (and copy) the returned mask.\n-        \"\"\"\n-        # If only one mask is present we need not bother about any type checks\n-        if (\n-            self.mask is None and operand is not None and operand.mask is None\n-        ) or handle_mask is None:\n-            return None\n-        elif self.mask is None and operand is not None:\n-            # Make a copy so there is no reference in the result.\n-            return deepcopy(operand.mask)\n-        elif operand is None:\n-            return deepcopy(self.mask)\n-        else:\n-            # Now lets calculate the resulting mask (operation enforces copy)\n-            return handle_mask(self.mask, operand.mask, **kwds)\n-\n-    def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n-        \"\"\"\n-        Calculate the resulting wcs.\n-\n-        There is actually no calculation involved but it is a good place to\n-        compare wcs information of both operands. This is currently not working\n-        properly with `~astropy.wcs.WCS` (which is the suggested class for\n-        storing as wcs property) but it will not break it neither.\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n-            By default, the ``operation`` will be ignored.\n-\n-        operand : `NDData` instance or subclass\n-            The second operand wrapped in an instance of the same class as\n-            self.\n-\n-        compare_wcs : callable\n-            see :meth:`NDArithmeticMixin.add` parameter description.\n-\n-        kwds :\n-            Additional parameters given to ``compare_wcs``.\n-\n-        Raises\n-        ------\n-        ValueError\n-            If ``compare_wcs`` returns ``False``.\n-\n-        Returns\n-        -------\n-        result_wcs : any type\n-            The ``wcs`` of the first operand is returned.\n-        \"\"\"\n-        # ok, not really arithmetic but we need to check which wcs makes sense\n-        # for the result and this is an ideal place to compare the two WCS,\n-        # too.\n-\n-        # I'll assume that the comparison returned None or False in case they\n-        # are not equal.\n-        if not compare_wcs(self.wcs, operand.wcs, **kwds):\n-            raise ValueError(\"WCS are not equal.\")\n-\n-        return deepcopy(self.wcs)\n-\n-    def _arithmetic_meta(self, operation, operand, handle_meta, **kwds):\n-        \"\"\"\n-        Calculate the resulting meta.\n-\n-        Parameters\n-        ----------\n-        operation : callable\n-            see :meth:`NDArithmeticMixin._arithmetic` parameter description.\n-            By default, the ``operation`` will be ignored.\n-\n-        operand : `NDData`-like instance\n-            The second operand wrapped in an instance of the same class as\n-            self.\n-\n-        handle_meta : callable\n-            see :meth:`NDArithmeticMixin.add`\n-\n-        kwds :\n-            Additional parameters given to ``handle_meta``.\n-\n-        Returns\n-        -------\n-        result_meta : any type\n-            The result of ``handle_meta``.\n-        \"\"\"\n-        # Just return what handle_meta does with both of the metas.\n-        return handle_meta(self.meta, operand.meta, **kwds)\n-\n-    @sharedmethod\n-    @format_doc(_arit_doc, name=\"addition\", op=\"+\")\n-    def add(self, operand, operand2=None, **kwargs):\n-        return self._prepare_then_do_arithmetic(np.add, operand, operand2, **kwargs)\n-\n-    @sharedmethod\n-    @format_doc(_arit_doc, name=\"subtraction\", op=\"-\")\n-    def subtract(self, operand, operand2=None, **kwargs):\n-        return self._prepare_then_do_arithmetic(\n-            np.subtract, operand, operand2, **kwargs\n-        )\n-\n-    @sharedmethod\n-    @format_doc(_arit_doc, name=\"multiplication\", op=\"*\")\n-    def multiply(self, operand, operand2=None, **kwargs):\n-        return self._prepare_then_do_arithmetic(\n-            np.multiply, operand, operand2, **kwargs\n-        )\n-\n-    @sharedmethod\n-    @format_doc(_arit_doc, name=\"division\", op=\"/\")\n-    def divide(self, operand, operand2=None, **kwargs):\n-        return self._prepare_then_do_arithmetic(\n-            np.true_divide, operand, operand2, **kwargs\n-        )\n-\n-    @sharedmethod\n-    def sum(self, **kwargs):\n-        return self._prepare_then_do_arithmetic(np.sum, **kwargs)\n-\n-    @sharedmethod\n-    def mean(self, **kwargs):\n-        return self._prepare_then_do_arithmetic(np.mean, **kwargs)\n-\n-    @sharedmethod\n-    def min(self, **kwargs):\n-        # use the provided propagate_uncertainties if available, otherwise default is False:\n-        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n-        return self._prepare_then_do_arithmetic(\n-            np.min, propagate_uncertainties=propagate_uncertainties, **kwargs\n-        )\n-\n-    @sharedmethod\n-    def max(self, **kwargs):\n-        # use the provided propagate_uncertainties if available, otherwise default is False:\n-        propagate_uncertainties = kwargs.pop(\"propagate_uncertainties\", None)\n-        return self._prepare_then_do_arithmetic(\n-            np.max, propagate_uncertainties=propagate_uncertainties, **kwargs\n-        )\n-\n-    @sharedmethod\n-    def _prepare_then_do_arithmetic(\n-        self_or_cls, operation, operand=None, operand2=None, **kwargs\n-    ):\n-        \"\"\"Intermediate method called by public arithmetic (i.e. ``add``)\n-        before the processing method (``_arithmetic``) is invoked.\n-\n-        .. warning::\n-            Do not override this method in subclasses.\n-\n-        This method checks if it was called as instance or as class method and\n-        then wraps the operands and the result from ``_arithmetic`` in the\n-        appropriate subclass.\n-\n-        Parameters\n-        ----------\n-        self_or_cls : instance or class\n-            ``sharedmethod`` behaves like a normal method if called on the\n-            instance (then this parameter is ``self``) but like a classmethod\n-            when called on the class (then this parameter is ``cls``).\n-\n-        operations : callable\n-            The operation (normally a numpy-ufunc) that represents the\n-            appropriate action.\n-\n-        operand, operand2, kwargs :\n-            See for example ``add``.\n-\n-        Result\n-        ------\n-        result : `~astropy.nddata.NDData`-like\n-            Depending how this method was called either ``self_or_cls``\n-            (called on class) or ``self_or_cls.__class__`` (called on instance)\n-            is the NDData-subclass that is used as wrapper for the result.\n-        \"\"\"\n-        # DO NOT OVERRIDE THIS METHOD IN SUBCLASSES.\n-\n-        if isinstance(self_or_cls, NDArithmeticMixin):\n-            # True means it was called on the instance, so self_or_cls is\n-            # a reference to self\n-            cls = self_or_cls.__class__\n-            if operand2 is None:\n-                # Only one operand was given. Set operand2 to operand and\n-                # operand to self so that we call the appropriate method of the\n-                # operand.\n-                operand2 = operand\n-                operand = self_or_cls\n-            else:\n-                # Convert the first operand to the class of this method.\n-                # This is important so that always the correct _arithmetics is\n-                # called later that method.\n-                operand = cls(operand)\n-\n-        else:\n-            # It was used as classmethod so self_or_cls represents the cls\n-            cls = self_or_cls\n-\n-            # It was called on the class so we expect two operands!\n-            if operand2 is None:\n-                raise TypeError(\n-                    \"operand2 must be given when the method isn't \"\n-                    \"called on an instance.\"\n-                )\n-\n-            # Convert to this class. See above comment why.\n-            operand = cls(operand)\n-\n-        # At this point operand, operand2, kwargs and cls are determined.\n-        if operand2 is not None and not issubclass(\n-            operand2.__class__, NDArithmeticMixin\n-        ):\n-            # Let's try to convert operand2 to the class of operand to allow for\n-            # arithmetic operations with numbers, lists, numpy arrays, numpy masked\n-            # arrays, astropy quantities, masked quantities and of other subclasses\n-            # of NDData.\n-            operand2 = cls(operand2)\n-\n-            # Now call the _arithmetics method to do the arithmetic.\n-            result, init_kwds = operand._arithmetic(operation, operand2, **kwargs)\n-        elif issubclass(operand2.__class__, NDArithmeticMixin):\n-            # calling as class method:\n-            result, init_kwds = cls._arithmetic(\n-                operand,\n-                operation,\n-                operand2,\n-                **kwargs,\n-            )\n-        else:\n-            # otherwise call the _arithmetic method on self for a collapse operation:\n-            # for collapse operations, use astropy.utils.masked rather than handle_mask\n-            result, init_kwds = self_or_cls._arithmetic(\n-                operation,\n-                operand2,\n-                **kwargs,\n-            )\n-\n-        # Return a new class based on the result\n-        return cls(result, **init_kwds)\n+    # Other methods remain unchanged...\n\\ No newline at end of file", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "astropy__astropy-6938", "model_patch": "", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "astropy__astropy-7746", "model_patch": "--- astropy/wcs/wcs.py\n+++ astropy/wcs/wcs.py\n@@ -1,3299 +1,16 @@\n-# Licensed under a 3-clause BSD style license - see LICENSE.rst\n-\"\"\"\n-Under the hood, there are 3 separate classes that perform different\n-parts of the transformation:\n-\n-   - `~astropy.wcs.Wcsprm`: Is a direct wrapper of the core WCS\n-     functionality in `wcslib`_.  (This includes TPV and TPD\n-     polynomial distortion, but not SIP distortion).\n-\n-   - `~astropy.wcs.Sip`: Handles polynomial distortion as defined in the\n-     `SIP`_ convention.\n-\n-   - `~astropy.wcs.DistortionLookupTable`: Handles `distortion paper`_\n-     lookup tables.\n-\n-Additionally, the class `WCS` aggregates all of these transformations\n-together in a pipeline:\n-\n-   - Detector to image plane correction (by a pair of\n-     `~astropy.wcs.DistortionLookupTable` objects).\n-\n-   - `SIP`_ distortion correction (by an underlying `~astropy.wcs.Sip`\n-     object)\n-\n-   - `distortion paper`_ table-lookup correction (by a pair of\n-     `~astropy.wcs.DistortionLookupTable` objects).\n-\n-   - `wcslib`_ WCS transformation (by a `~astropy.wcs.Wcsprm` object)\n-\n-\"\"\"\n-\n-# STDLIB\n-import copy\n-import io\n-import itertools\n-import os\n-import re\n-import textwrap\n-import warnings\n-import builtins\n-\n-# THIRD-PARTY\n-import numpy as np\n-\n-# LOCAL\n-from .. import log\n-from ..io import fits\n-from . import _docutil as __\n-try:\n-    from . import _wcs\n-except ImportError:\n-    if not _ASTROPY_SETUP_:\n-        raise\n-    else:\n-        _wcs = None\n-\n-from ..utils.compat import possible_filename\n-from ..utils.exceptions import AstropyWarning, AstropyUserWarning, AstropyDeprecationWarning\n-\n-__all__ = ['FITSFixedWarning', 'WCS', 'find_all_wcs',\n-           'DistortionLookupTable', 'Sip', 'Tabprm', 'Wcsprm',\n-           'WCSBase', 'validate', 'WcsError', 'SingularMatrixError',\n-           'InconsistentAxisTypesError', 'InvalidTransformError',\n-           'InvalidCoordinateError', 'NoSolutionError',\n-           'InvalidSubimageSpecificationError', 'NoConvergence',\n-           'NonseparableSubimageCoordinateSystemError',\n-           'NoWcsKeywordsFoundError', 'InvalidTabularParametersError']\n-\n-\n-__doctest_skip__ = ['WCS.all_world2pix']\n-\n-\n-if _wcs is not None:\n-    _parsed_version = _wcs.__version__.split('.')\n-    if int(_parsed_version[0]) == 5 and int(_parsed_version[1]) < 8:\n-        raise ImportError(\n-            \"astropy.wcs is built with wcslib {0}, but only versions 5.8 and \"\n-            \"later on the 5.x series are known to work.  The version of wcslib \"\n-            \"that ships with astropy may be used.\")\n-\n-    if not _wcs._sanity_check():\n-        raise RuntimeError(\n-        \"astropy.wcs did not pass its sanity check for your build \"\n-        \"on your platform.\")\n-\n-    WCSBase = _wcs._Wcs\n-    DistortionLookupTable = _wcs.DistortionLookupTable\n-    Sip = _wcs.Sip\n-    Wcsprm = _wcs.Wcsprm\n-    Tabprm = _wcs.Tabprm\n-    WcsError = _wcs.WcsError\n-    SingularMatrixError = _wcs.SingularMatrixError\n-    InconsistentAxisTypesError = _wcs.InconsistentAxisTypesError\n-    InvalidTransformError = _wcs.InvalidTransformError\n-    InvalidCoordinateError = _wcs.InvalidCoordinateError\n-    NoSolutionError = _wcs.NoSolutionError\n-    InvalidSubimageSpecificationError = _wcs.InvalidSubimageSpecificationError\n-    NonseparableSubimageCoordinateSystemError = _wcs.NonseparableSubimageCoordinateSystemError\n-    NoWcsKeywordsFoundError = _wcs.NoWcsKeywordsFoundError\n-    InvalidTabularParametersError = _wcs.InvalidTabularParametersError\n-\n-    # Copy all the constants from the C extension into this module's namespace\n-    for key, val in _wcs.__dict__.items():\n-        if key.startswith(('WCSSUB', 'WCSHDR', 'WCSHDO')):\n-            locals()[key] = val\n-            __all__.append(key)\n-else:\n-    WCSBase = object\n-    Wcsprm = object\n-    DistortionLookupTable = object\n-    Sip = object\n-    Tabprm = object\n-    WcsError = None\n-    SingularMatrixError = None\n-    InconsistentAxisTypesError = None\n-    InvalidTransformError = None\n-    InvalidCoordinateError = None\n-    NoSolutionError = None\n-    InvalidSubimageSpecificationError = None\n-    NonseparableSubimageCoordinateSystemError = None\n-    NoWcsKeywordsFoundError = None\n-    InvalidTabularParametersError = None\n-\n-\n-# Additional relax bit flags\n-WCSHDO_SIP = 0x80000\n-\n-# Regular expression defining SIP keyword It matches keyword that starts with A\n-# or B, optionally followed by P, followed by an underscore then a number in\n-# range of 0-19, followed by an underscore and another number in range of 0-19.\n-# Keyword optionally ends with a capital letter.\n-SIP_KW = re.compile('''^[AB]P?_1?[0-9]_1?[0-9][A-Z]?$''')\n-\n-\n-def _parse_keysel(keysel):\n-    keysel_flags = 0\n-    if keysel is not None:\n-        for element in keysel:\n-            if element.lower() == 'image':\n-                keysel_flags |= _wcs.WCSHDR_IMGHEAD\n-            elif element.lower() == 'binary':\n-                keysel_flags |= _wcs.WCSHDR_BIMGARR\n-            elif element.lower() == 'pixel':\n-                keysel_flags |= _wcs.WCSHDR_PIXLIST\n-            else:\n-                raise ValueError(\n-                    \"keysel must be a list of 'image', 'binary' \" +\n-                    \"and/or 'pixel'\")\n-    else:\n-        keysel_flags = -1\n-\n-    return keysel_flags\n-\n-\n-class NoConvergence(Exception):\n-    \"\"\"\n-    An error class used to report non-convergence and/or divergence\n-    of numerical methods. It is used to report errors in the\n-    iterative solution used by\n-    the :py:meth:`~astropy.wcs.WCS.all_world2pix`.\n-\n-    Attributes\n-    ----------\n-\n-    best_solution : `numpy.ndarray`\n-        Best solution achieved by the numerical method.\n-\n-    accuracy : `numpy.ndarray`\n-        Accuracy of the ``best_solution``.\n-\n-    niter : `int`\n-        Number of iterations performed by the numerical method\n-        to compute ``best_solution``.\n-\n-    divergent : None, `numpy.ndarray`\n-        Indices of the points in ``best_solution`` array\n-        for which the solution appears to be divergent. If the\n-        solution does not diverge, ``divergent`` will be set to `None`.\n-\n-    slow_conv : None, `numpy.ndarray`\n-        Indices of the solutions in ``best_solution`` array\n-        for which the solution failed to converge within the\n-        specified maximum number of iterations. If there are no\n-        non-converging solutions (i.e., if the required accuracy\n-        has been achieved for all input data points)\n-        then ``slow_conv`` will be set to `None`.\n-\n-    \"\"\"\n-\n-    def __init__(self, *args, best_solution=None, accuracy=None, niter=None,\n-                 divergent=None, slow_conv=None, **kwargs):\n-        super().__init__(*args)\n-\n-        self.best_solution = best_solution\n-        self.accuracy = accuracy\n-        self.niter = niter\n-        self.divergent = divergent\n-        self.slow_conv = slow_conv\n-\n-        if kwargs:\n-            warnings.warn(\"Function received unexpected arguments ({}) these \"\n-                          \"are ignored but will raise an Exception in the \"\n-                          \"future.\".format(list(kwargs)),\n-                          AstropyDeprecationWarning)\n-\n-\n-class FITSFixedWarning(AstropyWarning):\n-    \"\"\"\n-    The warning raised when the contents of the FITS header have been\n-    modified to be standards compliant.\n-    \"\"\"\n-    pass\n-\n-\n-class WCS(WCSBase):\n-    \"\"\"WCS objects perform standard WCS transformations, and correct for\n-    `SIP`_ and `distortion paper`_ table-lookup transformations, based\n-    on the WCS keywords and supplementary data read from a FITS file.\n-\n-    Parameters\n-    ----------\n-    header : astropy.io.fits header object, Primary HDU, Image HDU, string, dict-like, or None, optional\n-        If *header* is not provided or None, the object will be\n-        initialized to default values.\n-\n-    fobj : An astropy.io.fits file (hdulist) object, optional\n-        It is needed when header keywords point to a `distortion\n-        paper`_ lookup table stored in a different extension.\n-\n-    key : str, optional\n-        The name of a particular WCS transform to use.  This may be\n-        either ``' '`` or ``'A'``-``'Z'`` and corresponds to the\n-        ``\\\"a\\\"`` part of the ``CTYPEia`` cards.  *key* may only be\n-        provided if *header* is also provided.\n-\n-    minerr : float, optional\n-        The minimum value a distortion correction must have in order\n-        to be applied. If the value of ``CQERRja`` is smaller than\n-        *minerr*, the corresponding distortion is not applied.\n-\n-    relax : bool or int, optional\n-        Degree of permissiveness:\n-\n-        - `True` (default): Admit all recognized informal extensions\n-          of the WCS standard.\n-\n-        - `False`: Recognize only FITS keywords defined by the\n-          published WCS standard.\n-\n-        - `int`: a bit field selecting specific extensions to accept.\n-          See :ref:`relaxread` for details.\n-\n-    naxis : int or sequence, optional\n-        Extracts specific coordinate axes using\n-        :meth:`~astropy.wcs.Wcsprm.sub`.  If a header is provided, and\n-        *naxis* is not ``None``, *naxis* will be passed to\n-        :meth:`~astropy.wcs.Wcsprm.sub` in order to select specific\n-        axes from the header.  See :meth:`~astropy.wcs.Wcsprm.sub` for\n-        more details about this parameter.\n-\n-    keysel : sequence of flags, optional\n-        A sequence of flags used to select the keyword types\n-        considered by wcslib.  When ``None``, only the standard image\n-        header keywords are considered (and the underlying wcspih() C\n-        function is called).  To use binary table image array or pixel\n-        list keywords, *keysel* must be set.\n-\n-        Each element in the list should be one of the following\n-        strings:\n-\n-        - 'image': Image header keywords\n-\n-        - 'binary': Binary table image array keywords\n-\n-        - 'pixel': Pixel list keywords\n-\n-        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n-        binary table image arrays and pixel lists (including\n-        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n-        'pixel'.\n-\n-    colsel : sequence of int, optional\n-        A sequence of table column numbers used to restrict the WCS\n-        transformations considered to only those pertaining to the\n-        specified columns.  If `None`, there is no restriction.\n-\n-    fix : bool, optional\n-        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n-        the resulting object to fix any non-standard uses in the\n-        header.  `FITSFixedWarning` Warnings will be emitted if any\n-        changes were made.\n-\n-    translate_units : str, optional\n-        Specify which potentially unsafe translations of non-standard\n-        unit strings to perform.  By default, performs none.  See\n-        `WCS.fix` for more information about this parameter.  Only\n-        effective when ``fix`` is `True`.\n-\n-    Raises\n-    ------\n-    MemoryError\n-         Memory allocation failed.\n-\n-    ValueError\n-         Invalid key.\n-\n-    KeyError\n-         Key not found in FITS header.\n-\n-    ValueError\n-         Lookup table distortion present in the header but *fobj* was\n-         not provided.\n-\n-    Notes\n-    -----\n-\n-    1. astropy.wcs supports arbitrary *n* dimensions for the core WCS\n-       (the transformations handled by WCSLIB).  However, the\n-       `distortion paper`_ lookup table and `SIP`_ distortions must be\n-       two dimensional.  Therefore, if you try to create a WCS object\n-       where the core WCS has a different number of dimensions than 2\n-       and that object also contains a `distortion paper`_ lookup\n-       table or `SIP`_ distortion, a `ValueError`\n-       exception will be raised.  To avoid this, consider using the\n-       *naxis* kwarg to select two dimensions from the core WCS.\n-\n-    2. The number of coordinate axes in the transformation is not\n-       determined directly from the ``NAXIS`` keyword but instead from\n-       the highest of:\n-\n-           - ``NAXIS`` keyword\n-\n-           - ``WCSAXESa`` keyword\n-\n-           - The highest axis number in any parameterized WCS keyword.\n-             The keyvalue, as well as the keyword, must be\n-             syntactically valid otherwise it will not be considered.\n-\n-       If none of these keyword types is present, i.e. if the header\n-       only contains auxiliary WCS keywords for a particular\n-       coordinate representation, then no coordinate description is\n-       constructed for it.\n-\n-       The number of axes, which is set as the ``naxis`` member, may\n-       differ for different coordinate representations of the same\n-       image.\n-\n-    3. When the header includes duplicate keywords, in most cases the\n-       last encountered is used.\n-\n-    4. `~astropy.wcs.Wcsprm.set` is called immediately after\n-       construction, so any invalid keywords or transformations will\n-       be raised by the constructor, not when subsequently calling a\n-       transformation method.\n-\n-    \"\"\"\n-\n-    def __init__(self, header=None, fobj=None, key=' ', minerr=0.0,\n-                 relax=True, naxis=None, keysel=None, colsel=None,\n-                 fix=True, translate_units='', _do_set=True):\n-        close_fds = []\n-\n-        if header is None:\n-            if naxis is None:\n-                naxis = 2\n-            wcsprm = _wcs.Wcsprm(header=None, key=key,\n-                                 relax=relax, naxis=naxis)\n-            self.naxis = wcsprm.naxis\n-            # Set some reasonable defaults.\n-            det2im = (None, None)\n-            cpdis = (None, None)\n-            sip = None\n-        else:\n-            keysel_flags = _parse_keysel(keysel)\n-\n-            if isinstance(header, (str, bytes)):\n-                try:\n-                    is_path = (possible_filename(header) and\n-                               os.path.exists(header))\n-                except (OSError, ValueError):\n-                    is_path = False\n-\n-                if is_path:\n-                    if fobj is not None:\n-                        raise ValueError(\n-                            \"Can not provide both a FITS filename to \"\n-                            \"argument 1 and a FITS file object to argument 2\")\n-                    fobj = fits.open(header)\n-                    close_fds.append(fobj)\n-                    header = fobj[0].header\n-            elif isinstance(header, fits.hdu.image._ImageBaseHDU):\n-                header = header.header\n-            elif not isinstance(header, fits.Header):\n-                try:\n-                    # Accept any dict-like object\n-                    orig_header = header\n-                    header = fits.Header()\n-                    for dict_key in orig_header.keys():\n-                        header[dict_key] = orig_header[dict_key]\n-                except TypeError:\n-                    raise TypeError(\n-                        \"header must be a string, an astropy.io.fits.Header \"\n-                        \"object, or a dict-like object\")\n-\n-            if isinstance(header, fits.Header):\n-                header_string = header.tostring().rstrip()\n-            else:\n-                header_string = header\n-\n-            # Importantly, header is a *copy* of the passed-in header\n-            # because we will be modifying it\n-            if isinstance(header_string, str):\n-                header_bytes = header_string.encode('ascii')\n-                header_string = header_string\n-            else:\n-                header_bytes = header_string\n-                header_string = header_string.decode('ascii')\n-\n-            try:\n-                tmp_header = fits.Header.fromstring(header_string)\n-                self._remove_sip_kw(tmp_header)\n-                tmp_header_bytes = tmp_header.tostring().rstrip()\n-                if isinstance(tmp_header_bytes, str):\n-                    tmp_header_bytes = tmp_header_bytes.encode('ascii')\n-                tmp_wcsprm = _wcs.Wcsprm(header=tmp_header_bytes, key=key,\n-                                         relax=relax, keysel=keysel_flags,\n-                                         colsel=colsel, warnings=False)\n-            except _wcs.NoWcsKeywordsFoundError:\n-                est_naxis = 0\n-            else:\n-                if naxis is not None:\n-                    try:\n-                        tmp_wcsprm.sub(naxis)\n-                    except ValueError:\n-                        pass\n-                    est_naxis = tmp_wcsprm.naxis\n-                else:\n-                    est_naxis = 2\n-\n-            header = fits.Header.fromstring(header_string)\n-\n-            if est_naxis == 0:\n-                est_naxis = 2\n-            self.naxis = est_naxis\n-\n-            det2im = self._read_det2im_kw(header, fobj, err=minerr)\n-            cpdis = self._read_distortion_kw(\n-                header, fobj, dist='CPDIS', err=minerr)\n-            sip = self._read_sip_kw(header, wcskey=key)\n-            self._remove_sip_kw(header)\n-\n-            header_string = header.tostring()\n-            header_string = header_string.replace('END' + ' ' * 77, '')\n-\n-            if isinstance(header_string, str):\n-                header_bytes = header_string.encode('ascii')\n-                header_string = header_string\n-            else:\n-                header_bytes = header_string\n-                header_string = header_string.decode('ascii')\n-\n-            try:\n-                wcsprm = _wcs.Wcsprm(header=header_bytes, key=key,\n-                                     relax=relax, keysel=keysel_flags,\n-                                     colsel=colsel)\n-            except _wcs.NoWcsKeywordsFoundError:\n-                # The header may have SIP or distortions, but no core\n-                # WCS.  That isn't an error -- we want a \"default\"\n-                # (identity) core Wcs transformation in that case.\n-                if colsel is None:\n-                    wcsprm = _wcs.Wcsprm(header=None, key=key,\n-                                         relax=relax, keysel=keysel_flags,\n-                                         colsel=colsel)\n-                else:\n-                    raise\n-\n-            if naxis is not None:\n-                wcsprm = wcsprm.sub(naxis)\n-            self.naxis = wcsprm.naxis\n-\n-            if (wcsprm.naxis != 2 and\n-                (det2im[0] or det2im[1] or cpdis[0] or cpdis[1] or sip)):\n-                raise ValueError(\n-                    \"\"\"\n-FITS WCS distortion paper lookup tables and SIP distortions only work\n-in 2 dimensions.  However, WCSLIB has detected {0} dimensions in the\n-core WCS keywords.  To use core WCS in conjunction with FITS WCS\n-distortion paper lookup tables or SIP distortion, you must select or\n-reduce these to 2 dimensions using the naxis kwarg.\n-\"\"\".format(wcsprm.naxis))\n-\n-            header_naxis = header.get('NAXIS', None)\n-            if header_naxis is not None and header_naxis < wcsprm.naxis:\n-                warnings.warn(\n-                    \"The WCS transformation has more axes ({0:d}) than the \"\n-                    \"image it is associated with ({1:d})\".format(\n-                        wcsprm.naxis, header_naxis), FITSFixedWarning)\n-\n-        self._get_naxis(header)\n-        WCSBase.__init__(self, sip, cpdis, wcsprm, det2im)\n-\n-        if fix:\n-            self.fix(translate_units=translate_units)\n-\n-        if _do_set:\n-            self.wcs.set()\n-\n-        for fd in close_fds:\n-            fd.close()\n-\n-    def __copy__(self):\n-        new_copy = self.__class__()\n-        WCSBase.__init__(new_copy, self.sip,\n-                         (self.cpdis1, self.cpdis2),\n-                         self.wcs,\n-                         (self.det2im1, self.det2im2))\n-        new_copy.__dict__.update(self.__dict__)\n-        return new_copy\n-\n-    def __deepcopy__(self, memo):\n-        from copy import deepcopy\n-\n-        new_copy = self.__class__()\n-        new_copy.naxis = deepcopy(self.naxis, memo)\n-        WCSBase.__init__(new_copy, deepcopy(self.sip, memo),\n-                         (deepcopy(self.cpdis1, memo),\n-                          deepcopy(self.cpdis2, memo)),\n-                         deepcopy(self.wcs, memo),\n-                         (deepcopy(self.det2im1, memo),\n-                          deepcopy(self.det2im2, memo)))\n-        for key, val in self.__dict__.items():\n-            new_copy.__dict__[key] = deepcopy(val, memo)\n-        return new_copy\n-\n-    def copy(self):\n-        \"\"\"\n-        Return a shallow copy of the object.\n-\n-        Convenience method so user doesn't have to import the\n-        :mod:`copy` stdlib module.\n-\n-        .. warning::\n-            Use `deepcopy` instead of `copy` unless you know why you need a\n-            shallow copy.\n-        \"\"\"\n-        return copy.copy(self)\n-\n-    def deepcopy(self):\n-        \"\"\"\n-        Return a deep copy of the object.\n-\n-        Convenience method so user doesn't have to import the\n-        :mod:`copy` stdlib module.\n-        \"\"\"\n-        return copy.deepcopy(self)\n-\n-    def sub(self, axes=None):\n-        copy = self.deepcopy()\n-        copy.wcs = self.wcs.sub(axes)\n-        copy.naxis = copy.wcs.naxis\n-        return copy\n-    if _wcs is not None:\n-        sub.__doc__ = _wcs.Wcsprm.sub.__doc__\n-\n-    def _fix_scamp(self):\n-        \"\"\"\n-        Remove SCAMP's PVi_m distortion parameters if SIP distortion parameters\n-        are also present. Some projects (e.g., Palomar Transient Factory)\n-        convert SCAMP's distortion parameters (which abuse the PVi_m cards) to\n-        SIP. However, wcslib gets confused by the presence of both SCAMP and\n-        SIP distortion parameters.\n-\n-        See https://github.com/astropy/astropy/issues/299.\n-        \"\"\"\n-        # Nothing to be done if no WCS attached\n-        if self.wcs is None:\n-            return\n-\n-        # Nothing to be done if no PV parameters attached\n-        pv = self.wcs.get_pv()\n-        if not pv:\n-            return\n-\n-        # Nothing to be done if axes don't use SIP distortion parameters\n-        if self.sip is None:\n-            return\n-\n-        # Nothing to be done if any radial terms are present...\n-        # Loop over list to find any radial terms.\n-        # Certain values of the `j' index are used for storing\n-        # radial terms; refer to Equation (1) in\n-        # <http://web.ipac.caltech.edu/staff/shupe/reprints/SIP_to_PV_SPIE2012.pdf>.\n-        pv = np.asarray(pv)\n-        # Loop over distinct values of `i' index\n-        for i in set(pv[:, 0]):\n-            # Get all values of `j' index for this value of `i' index\n-            js = set(pv[:, 1][pv[:, 0] == i])\n-            # Find max value of `j' index\n-            max_j = max(js)\n-            for j in (3, 11, 23, 39):\n-                if j < max_j and j in js:\n-                    return\n-\n-        self.wcs.set_pv([])\n-        warnings.warn(\"Removed redundant SCAMP distortion parameters \" +\n-            \"because SIP parameters are also present\", FITSFixedWarning)\n-\n-    def fix(self, translate_units='', naxis=None):\n-        \"\"\"\n-        Perform the fix operations from wcslib, and warn about any\n-        changes it has made.\n-\n-        Parameters\n-        ----------\n-        translate_units : str, optional\n-            Specify which potentially unsafe translations of\n-            non-standard unit strings to perform.  By default,\n-            performs none.\n-\n-            Although ``\"S\"`` is commonly used to represent seconds,\n-            its translation to ``\"s\"`` is potentially unsafe since the\n-            standard recognizes ``\"S\"`` formally as Siemens, however\n-            rarely that may be used.  The same applies to ``\"H\"`` for\n-            hours (Henry), and ``\"D\"`` for days (Debye).\n-\n-            This string controls what to do in such cases, and is\n-            case-insensitive.\n-\n-            - If the string contains ``\"s\"``, translate ``\"S\"`` to\n-              ``\"s\"``.\n-\n-            - If the string contains ``\"h\"``, translate ``\"H\"`` to\n-              ``\"h\"``.\n-\n-            - If the string contains ``\"d\"``, translate ``\"D\"`` to\n-              ``\"d\"``.\n-\n-            Thus ``''`` doesn't do any unsafe translations, whereas\n-            ``'shd'`` does all of them.\n-\n-        naxis : int array[naxis], optional\n-            Image axis lengths.  If this array is set to zero or\n-            ``None``, then `~astropy.wcs.Wcsprm.cylfix` will not be\n-            invoked.\n-        \"\"\"\n-        if self.wcs is not None:\n-            self._fix_scamp()\n-            fixes = self.wcs.fix(translate_units, naxis)\n-            for key, val in fixes.items():\n-                if val != \"No change\":\n-                    warnings.warn(\n-                        (\"'{0}' made the change '{1}'.\").\n-                        format(key, val),\n-                        FITSFixedWarning)\n-\n-    def calc_footprint(self, header=None, undistort=True, axes=None, center=True):\n-        \"\"\"\n-        Calculates the footprint of the image on the sky.\n-\n-        A footprint is defined as the positions of the corners of the\n-        image on the sky after all available distortions have been\n-        applied.\n-\n-        Parameters\n-        ----------\n-        header : `~astropy.io.fits.Header` object, optional\n-            Used to get ``NAXIS1`` and ``NAXIS2``\n-            header and axes are mutually exclusive, alternative ways\n-            to provide the same information.\n-\n-        undistort : bool, optional\n-            If `True`, take SIP and distortion lookup table into\n-            account\n-\n-        axes : length 2 sequence ints, optional\n-            If provided, use the given sequence as the shape of the\n-            image.  Otherwise, use the ``NAXIS1`` and ``NAXIS2``\n-            keywords from the header that was used to create this\n-            `WCS` object.\n-\n-        center : bool, optional\n-            If `True` use the center of the pixel, otherwise use the corner.\n-\n-        Returns\n-        -------\n-        coord : (4, 2) array of (*x*, *y*) coordinates.\n-            The order is clockwise starting with the bottom left corner.\n-        \"\"\"\n-        if axes is not None:\n-            naxis1, naxis2 = axes\n-        else:\n-            if header is None:\n-                try:\n-                    # classes that inherit from WCS and define naxis1/2\n-                    # do not require a header parameter\n-                    naxis1 = self._naxis1\n-                    naxis2 = self._naxis2\n-                except AttributeError:\n-                    warnings.warn(\"Need a valid header in order to calculate footprint\\n\", AstropyUserWarning)\n-                    return None\n-            else:\n-                naxis1 = header.get('NAXIS1', None)\n-                naxis2 = header.get('NAXIS2', None)\n-\n-        if naxis1 is None or naxis2 is None:\n-            raise ValueError(\n-                    \"Image size could not be determined.\")\n-\n-        if center:\n-            corners = np.array([[1, 1],\n-                                [1, naxis2],\n-                                [naxis1, naxis2],\n-                                [naxis1, 1]], dtype=np.float64)\n-        else:\n-            corners = np.array([[0.5, 0.5],\n-                                [0.5, naxis2 + 0.5],\n-                                [naxis1 + 0.5, naxis2 + 0.5],\n-                                [naxis1 + 0.5, 0.5]], dtype=np.float64)\n-\n-        if undistort:\n-            return self.all_pix2world(corners, 1)\n-        else:\n-            return self.wcs_pix2world(corners, 1)\n-\n-    def _read_det2im_kw(self, header, fobj, err=0.0):\n-        \"\"\"\n-        Create a `distortion paper`_ type lookup table for detector to\n-        image plane correction.\n-        \"\"\"\n-        if fobj is None:\n-            return (None, None)\n-\n-        if not isinstance(fobj, fits.HDUList):\n-            return (None, None)\n-\n-        try:\n-            axiscorr = header[str('AXISCORR')]\n-            d2imdis = self._read_d2im_old_format(header, fobj, axiscorr)\n-            return d2imdis\n-        except KeyError:\n-            pass\n-\n-        dist = 'D2IMDIS'\n-        d_kw = 'D2IM'\n-        err_kw = 'D2IMERR'\n-        tables = {}\n-        for i in range(1, self.naxis + 1):\n-            d_error = header.get(err_kw + str(i), 0.0)\n-            if d_error < err:\n-                tables[i] = None\n-                continue\n-            distortion = dist + str(i)\n-            if distortion in header:\n-                dis = header[distortion].lower()\n-                if dis == 'lookup':\n-                    del header[distortion]\n-                    assert isinstance(fobj, fits.HDUList), ('An astropy.io.fits.HDUList'\n-                                'is required for Lookup table distortion.')\n-                    dp = (d_kw + str(i)).strip()\n-                    dp_extver_key = dp + str('.EXTVER')\n-                    if dp_extver_key in header:\n-                        d_extver = header[dp_extver_key]\n-                        del header[dp_extver_key]\n-                    else:\n-                        d_extver = 1\n-                    dp_axis_key = dp + str('.AXIS.{0:d}').format(i)\n-                    if i == header[dp_axis_key]:\n-                        d_data = fobj[str('D2IMARR'), d_extver].data\n-                    else:\n-                        d_data = (fobj[str('D2IMARR'), d_extver].data).transpose()\n-                    del header[dp_axis_key]\n-                    d_header = fobj[str('D2IMARR'), d_extver].header\n-                    d_crpix = (d_header.get(str('CRPIX1'), 0.0), d_header.get(str('CRPIX2'), 0.0))\n-                    d_crval = (d_header.get(str('CRVAL1'), 0.0), d_header.get(str('CRVAL2'), 0.0))\n-                    d_cdelt = (d_header.get(str('CDELT1'), 1.0), d_header.get(str('CDELT2'), 1.0))\n-                    d_lookup = DistortionLookupTable(d_data, d_crpix,\n-                                                     d_crval, d_cdelt)\n-                    tables[i] = d_lookup\n-                else:\n-                    warnings.warn('Polynomial distortion is not implemented.\\n', AstropyUserWarning)\n-                for key in list(header):\n-                    if key.startswith(dp + str('.')):\n-                        del header[key]\n-            else:\n-                tables[i] = None\n-        if not tables:\n-            return (None, None)\n-        else:\n-            return (tables.get(1), tables.get(2))\n-\n-    def _read_d2im_old_format(self, header, fobj, axiscorr):\n-        warnings.warn(\"The use of ``AXISCORR`` for D2IM correction has been deprecated.\"\n-                      \"`~astropy.wcs` will read in files with ``AXISCORR`` but ``to_fits()`` will write \"\n-                      \"out files without it.\",\n-                      AstropyDeprecationWarning)\n-        cpdis = [None, None]\n-        crpix = [0., 0.]\n-        crval = [0., 0.]\n-        cdelt = [1., 1.]\n-        try:\n-            d2im_data = fobj[(str('D2IMARR'), 1)].data\n-        except KeyError:\n-            return (None, None)\n-        except AttributeError:\n-            return (None, None)\n-\n-        d2im_data = np.array([d2im_data])\n-        d2im_hdr = fobj[(str('D2IMARR'), 1)].header\n-        naxis = d2im_hdr[str('NAXIS')]\n-\n-        for i in range(1, naxis + 1):\n-            crpix[i - 1] = d2im_hdr.get(str('CRPIX') + str(i), 0.0)\n-            crval[i - 1] = d2im_hdr.get(str('CRVAL') + str(i), 0.0)\n-            cdelt[i - 1] = d2im_hdr.get(str('CDELT') + str(i), 1.0)\n-\n-        cpdis = DistortionLookupTable(d2im_data, crpix, crval, cdelt)\n-\n-        if axiscorr == 1:\n-            return (cpdis, None)\n-        elif axiscorr == 2:\n-            return (None, cpdis)\n-        else:\n-            warnings.warn(\"Expected AXISCORR to be 1 or 2\", AstropyUserWarning)\n-            return (None, None)\n-\n-    def _write_det2im(self, hdulist):\n-        \"\"\"\n-        Writes a `distortion paper`_ type lookup table to the given\n-        `astropy.io.fits.HDUList`.\n-        \"\"\"\n-\n-        if self.det2im1 is None and self.det2im2 is None:\n-            return\n-        dist = 'D2IMDIS'\n-        d_kw = 'D2IM'\n-        err_kw = 'D2IMERR'\n-\n-        def write_d2i(num, det2im):\n-            if det2im is None:\n-                return\n-            str('{0}{1:d}').format(dist, num),\n-            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n-                'LOOKUP', 'Detector to image correction type')\n-            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n-                num, 'Version number of WCSDVARR extension')\n-            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n-                len(det2im.data.shape), 'Number of independent variables in d2im function')\n-            for i in range(det2im.data.ndim):\n-                hdulist[0].header[str('{0}{1:d}.AXIS.{2:d}').format(d_kw, num, i + 1)] = (\n-                    i + 1, 'Axis number of the jth independent variable in a d2im function')\n-\n-            image = fits.ImageHDU(det2im.data, name=str('D2IMARR'))\n-            header = image.header\n-\n-            header[str('CRPIX1')] = (det2im.crpix[0],\n-                                     'Coordinate system reference pixel')\n-            header[str('CRPIX2')] = (det2im.crpix[1],\n-                                     'Coordinate system reference pixel')\n-            header[str('CRVAL1')] = (det2im.crval[0],\n-                                     'Coordinate system value at reference pixel')\n-            header[str('CRVAL2')] = (det2im.crval[1],\n-                                     'Coordinate system value at reference pixel')\n-            header[str('CDELT1')] = (det2im.cdelt[0],\n-                                     'Coordinate increment along axis')\n-            header[str('CDELT2')] = (det2im.cdelt[1],\n-                                     'Coordinate increment along axis')\n-            image.ver = int(hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)])\n-            hdulist.append(image)\n-        write_d2i(1, self.det2im1)\n-        write_d2i(2, self.det2im2)\n-\n-    def _read_distortion_kw(self, header, fobj, dist='CPDIS', err=0.0):\n-        \"\"\"\n-        Reads `distortion paper`_ table-lookup keywords and data, and\n-        returns a 2-tuple of `~astropy.wcs.DistortionLookupTable`\n-        objects.\n-\n-        If no `distortion paper`_ keywords are found, ``(None, None)``\n-        is returned.\n-        \"\"\"\n-        if isinstance(header, (str, bytes)):\n-            return (None, None)\n-\n-        if dist == 'CPDIS':\n-            d_kw = str('DP')\n-            err_kw = str('CPERR')\n-        else:\n-            d_kw = str('DQ')\n-            err_kw = str('CQERR')\n-\n-        tables = {}\n-        for i in range(1, self.naxis + 1):\n-            d_error_key = err_kw + str(i)\n-            if d_error_key in header:\n-                d_error = header[d_error_key]\n-                del header[d_error_key]\n-            else:\n-                d_error = 0.0\n-            if d_error < err:\n-                tables[i] = None\n-                continue\n-            distortion = dist + str(i)\n-            if distortion in header:\n-                dis = header[distortion].lower()\n-                del header[distortion]\n-                if dis == 'lookup':\n-                    if not isinstance(fobj, fits.HDUList):\n-                        raise ValueError('an astropy.io.fits.HDUList is '\n-                                'required for Lookup table distortion.')\n-                    dp = (d_kw + str(i)).strip()\n-                    dp_extver_key = dp + str('.EXTVER')\n-                    if dp_extver_key in header:\n-                        d_extver = header[dp_extver_key]\n-                        del header[dp_extver_key]\n-                    else:\n-                        d_extver = 1\n-                    dp_axis_key = dp + str('.AXIS.{0:d}'.format(i))\n-                    if i == header[dp_axis_key]:\n-                        d_data = fobj[str('WCSDVARR'), d_extver].data\n-                    else:\n-                        d_data = (fobj[str('WCSDVARR'), d_extver].data).transpose()\n-                    del header[dp_axis_key]\n-                    d_header = fobj[str('WCSDVARR'), d_extver].header\n-                    d_crpix = (d_header.get(str('CRPIX1'), 0.0),\n-                               d_header.get(str('CRPIX2'), 0.0))\n-                    d_crval = (d_header.get(str('CRVAL1'), 0.0),\n-                               d_header.get(str('CRVAL2'), 0.0))\n-                    d_cdelt = (d_header.get(str('CDELT1'), 1.0),\n-                               d_header.get(str('CDELT2'), 1.0))\n-                    d_lookup = DistortionLookupTable(d_data, d_crpix, d_crval, d_cdelt)\n-                    tables[i] = d_lookup\n-\n-                    for key in list(header):\n-                        if key.startswith(dp + str('.')):\n-                            del header[key]\n-                else:\n-                    warnings.warn('Polynomial distortion is not implemented.\\n', AstropyUserWarning)\n-            else:\n-                tables[i] = None\n-\n-        if not tables:\n-            return (None, None)\n-        else:\n-            return (tables.get(1), tables.get(2))\n-\n-    def _write_distortion_kw(self, hdulist, dist='CPDIS'):\n-        \"\"\"\n-        Write out `distortion paper`_ keywords to the given\n-        `fits.HDUList`.\n-        \"\"\"\n-        if self.cpdis1 is None and self.cpdis2 is None:\n-            return\n-\n-        if dist == 'CPDIS':\n-            d_kw = str('DP')\n-            err_kw = str('CPERR')\n-        else:\n-            d_kw = str('DQ')\n-            err_kw = str('CQERR')\n-\n-        def write_dist(num, cpdis):\n-            if cpdis is None:\n-                return\n-\n-            hdulist[0].header[str('{0}{1:d}').format(dist, num)] = (\n-                'LOOKUP', 'Prior distortion function type')\n-            hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)] = (\n-                num, 'Version number of WCSDVARR extension')\n-            hdulist[0].header[str('{0}{1:d}.NAXES').format(d_kw, num)] = (\n-                len(cpdis.data.shape), 'Number of independent variables in distortion function')\n-\n-            for i in range(cpdis.data.ndim):\n-                hdulist[0].header[str('{0}{1:d}.AXIS.{2:d}').format(d_kw, num, i + 1)] = (\n-                    i + 1,\n-                    'Axis number of the jth independent variable in a distortion function')\n-\n-            image = fits.ImageHDU(cpdis.data, name=str('WCSDVARR'))\n-            header = image.header\n-\n-            header[str('CRPIX1')] = (cpdis.crpix[0], 'Coordinate system reference pixel')\n-            header[str('CRPIX2')] = (cpdis.crpix[1], 'Coordinate system reference pixel')\n-            header[str('CRVAL1')] = (cpdis.crval[0], 'Coordinate system value at reference pixel')\n-            header[str('CRVAL2')] = (cpdis.crval[1], 'Coordinate system value at reference pixel')\n-            header[str('CDELT1')] = (cpdis.cdelt[0], 'Coordinate increment along axis')\n-            header[str('CDELT2')] = (cpdis.cdelt[1], 'Coordinate increment along axis')\n-            image.ver = int(hdulist[0].header[str('{0}{1:d}.EXTVER').format(d_kw, num)])\n-            hdulist.append(image)\n-\n-        write_dist(1, self.cpdis1)\n-        write_dist(2, self.cpdis2)\n-\n-    def _remove_sip_kw(self, header):\n-        \"\"\"\n-        Remove SIP information from a header.\n-        \"\"\"\n-        # Never pass SIP coefficients to wcslib\n-        # CTYPE must be passed with -SIP to wcslib\n-        for key in (m.group() for m in map(SIP_KW.match, list(header))\n-                    if m is not None):\n-            del header[key]\n-\n-    def _read_sip_kw(self, header, wcskey=\"\"):\n-        \"\"\"\n-        Reads `SIP`_ header keywords and returns a `~astropy.wcs.Sip`\n-        object.\n-\n-        If no `SIP`_ header keywords are found, ``None`` is returned.\n-        \"\"\"\n-        if isinstance(header, (str, bytes)):\n-            # TODO: Parse SIP from a string without pyfits around\n-            return None\n-\n-        if str(\"A_ORDER\") in header and header[str('A_ORDER')] > 1:\n-            if str(\"B_ORDER\") not in header:\n-                raise ValueError(\n-                    \"A_ORDER provided without corresponding B_ORDER \"\n-                    \"keyword for SIP distortion\")\n-\n-            m = int(header[str(\"A_ORDER\")])\n-            a = np.zeros((m + 1, m + 1), np.double)\n-            for i in range(m + 1):\n-                for j in range(m - i + 1):\n-                    key = str(\"A_{0}_{1}\").format(i, j)\n-                    if key in header:\n-                        a[i, j] = header[key]\n-                        del header[key]\n-\n-            m = int(header[str(\"B_ORDER\")])\n-            if m > 1:\n-                b = np.zeros((m + 1, m + 1), np.double)\n-                for i in range(m + 1):\n-                    for j in range(m - i + 1):\n-                        key = str(\"B_{0}_{1}\").format(i, j)\n-                        if key in header:\n-                            b[i, j] = header[key]\n-                            del header[key]\n-            else:\n-                a = None\n-                b = None\n-\n-            del header[str('A_ORDER')]\n-            del header[str('B_ORDER')]\n-\n-            ctype = [header['CTYPE{0}{1}'.format(nax, wcskey)] for nax in range(1, self.naxis + 1)]\n-            if any(not ctyp.endswith('-SIP') for ctyp in ctype):\n-                message = \"\"\"\n-                Inconsistent SIP distortion information is present in the FITS header and the WCS object:\n-                SIP coefficients were detected, but CTYPE is missing a \"-SIP\" suffix.\n-                astropy.wcs is using the SIP distortion coefficients,\n-                therefore the coordinates calculated here might be incorrect.\n-\n-                If you do not want to apply the SIP distortion coefficients,\n-                please remove the SIP coefficients from the FITS header or the\n-                WCS object.  As an example, if the image is already distortion-corrected\n-                (e.g., drizzled) then distortion components should not apply and the SIP\n-                coefficients should be removed.\n-\n-                While the SIP distortion coefficients are being applied here, if that was indeed the intent,\n-                for consistency please append \"-SIP\" to the CTYPE in the FITS header or the WCS object.\n-\n-                \"\"\"\n-                log.info(message)\n-        elif str(\"B_ORDER\") in header and header[str('B_ORDER')] > 1:\n-            raise ValueError(\n-                \"B_ORDER provided without corresponding A_ORDER \" +\n-                \"keyword for SIP distortion\")\n-        else:\n-            a = None\n-            b = None\n-\n-        if str(\"AP_ORDER\") in header and header[str('AP_ORDER')] > 1:\n-            if str(\"BP_ORDER\") not in header:\n-                raise ValueError(\n-                    \"AP_ORDER provided without corresponding BP_ORDER \"\n-                    \"keyword for SIP distortion\")\n-\n-            m = int(header[str(\"AP_ORDER\")])\n-            ap = np.zeros((m + 1, m + 1), np.double)\n-            for i in range(m + 1):\n-                for j in range(m - i + 1):\n-                    key = str(\"AP_{0}_{1}\").format(i, j)\n-                    if key in header:\n-                        ap[i, j] = header[key]\n-                        del header[key]\n-\n-            m = int(header[str(\"BP_ORDER\")])\n-            if m > 1:\n-                bp = np.zeros((m + 1, m + 1), np.double)\n-                for i in range(m + 1):\n-                    for j in range(m - i + 1):\n-                        key = str(\"BP_{0}_{1}\").format(i, j)\n-                        if key in header:\n-                            bp[i, j] = header[key]\n-                            del header[key]\n-            else:\n-                ap = None\n-                bp = None\n-\n-            del header[str('AP_ORDER')]\n-            del header[str('BP_ORDER')]\n-        elif str(\"BP_ORDER\") in header and header[str('BP_ORDER')] > 1:\n-            raise ValueError(\n-                \"BP_ORDER provided without corresponding AP_ORDER \"\n-                \"keyword for SIP distortion\")\n-        else:\n-            ap = None\n-            bp = None\n-\n-        if a is None and b is None and ap is None and bp is None:\n-            return None\n-\n-        if str(\"CRPIX1{0}\".format(wcskey)) not in header or str(\"CRPIX2{0}\".format(wcskey)) not in header:\n-            raise ValueError(\n-                \"Header has SIP keywords without CRPIX keywords\")\n-\n-        crpix1 = header.get(\"CRPIX1{0}\".format(wcskey))\n-        crpix2 = header.get(\"CRPIX2{0}\".format(wcskey))\n-\n-        return Sip(a, b, ap, bp, (crpix1, crpix2))\n-\n-    def _write_sip_kw(self):\n-        \"\"\"\n-        Write out SIP keywords.  Returns a dictionary of key-value\n-        pairs.\n-        \"\"\"\n-        if self.sip is None:\n-            return {}\n-\n-        keywords = {}\n-\n-        def write_array(name, a):\n-            if a is None:\n-                return\n-            size = a.shape[0]\n-            keywords[str('{0}_ORDER').format(name)] = size - 1\n-            for i in range(size):\n-                for j in range(size - i):\n-                    if a[i, j] != 0.0:\n-                        keywords[\n-                            str('{0}_{1:d}_{2:d}').format(name, i, j)] = a[i, j]\n-\n-        write_array(str('A'), self.sip.a)\n-        write_array(str('B'), self.sip.b)\n-        write_array(str('AP'), self.sip.ap)\n-        write_array(str('BP'), self.sip.bp)\n-\n-        return keywords\n-\n-    def _denormalize_sky(self, sky):\n-        if self.wcs.lngtyp != 'RA':\n-            raise ValueError(\n-                \"WCS does not have longitude type of 'RA', therefore \" +\n-                \"(ra, dec) data can not be used as input\")\n-        if self.wcs.lattyp != 'DEC':\n-            raise ValueError(\n-                \"WCS does not have longitude type of 'DEC', therefore \" +\n-                \"(ra, dec) data can not be used as input\")\n-        if self.wcs.naxis == 2:\n-            if self.wcs.lng == 0 and self.wcs.lat == 1:\n-                return sky\n-            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n-                # Reverse the order of the columns\n-                return sky[:, ::-1]\n-            else:\n-                raise ValueError(\n-                    \"WCS does not have longitude and latitude celestial \" +\n-                    \"axes, therefore (ra, dec) data can not be used as input\")\n-        else:\n-            if self.wcs.lng < 0 or self.wcs.lat < 0:\n-                raise ValueError(\n-                    \"WCS does not have both longitude and latitude \"\n-                    \"celestial axes, therefore (ra, dec) data can not be \" +\n-                    \"used as input\")\n-            out = np.zeros((sky.shape[0], self.wcs.naxis))\n-            out[:, self.wcs.lng] = sky[:, 0]\n-            out[:, self.wcs.lat] = sky[:, 1]\n-            return out\n-\n-    def _normalize_sky(self, sky):\n-        if self.wcs.lngtyp != 'RA':\n-            raise ValueError(\n-                \"WCS does not have longitude type of 'RA', therefore \" +\n-                \"(ra, dec) data can not be returned\")\n-        if self.wcs.lattyp != 'DEC':\n-            raise ValueError(\n-                \"WCS does not have longitude type of 'DEC', therefore \" +\n-                \"(ra, dec) data can not be returned\")\n-        if self.wcs.naxis == 2:\n-            if self.wcs.lng == 0 and self.wcs.lat == 1:\n-                return sky\n-            elif self.wcs.lng == 1 and self.wcs.lat == 0:\n-                # Reverse the order of the columns\n-                return sky[:, ::-1]\n-            else:\n-                raise ValueError(\n-                    \"WCS does not have longitude and latitude celestial \"\n-                    \"axes, therefore (ra, dec) data can not be returned\")\n-        else:\n-            if self.wcs.lng < 0 or self.wcs.lat < 0:\n-                raise ValueError(\n-                    \"WCS does not have both longitude and latitude celestial \"\n-                    \"axes, therefore (ra, dec) data can not be returned\")\n-            out = np.empty((sky.shape[0], 2))\n-            out[:, 0] = sky[:, self.wcs.lng]\n-            out[:, 1] = sky[:, self.wcs.lat]\n-            return out\n-\n-    def _array_converter(self, func, sky, *args, ra_dec_order=False):\n-        \"\"\"\n-        A helper function to support reading either a pair of arrays\n-        or a single Nx2 array.\n-        \"\"\"\n-\n-        def _return_list_of_arrays(axes, origin):\n-            try:\n-                axes = np.broadcast_arrays(*axes)\n-            except ValueError:\n-                raise ValueError(\n-                    \"Coordinate arrays are not broadcastable to each other\")\n-\n-            xy = np.hstack([x.reshape((x.size, 1)) for x in axes])\n-\n-            if ra_dec_order and sky == 'input':\n-                xy = self._denormalize_sky(xy)\n-            output = func(xy, origin)\n-            if ra_dec_order and sky == 'output':\n-                output = self._normalize_sky(output)\n-                return (output[:, 0].reshape(axes[0].shape),\n-                        output[:, 1].reshape(axes[0].shape))\n-            return [output[:, i].reshape(axes[0].shape)\n-                    for i in range(output.shape[1])]\n-\n-        def _return_single_array(xy, origin):\n-            if xy.shape[-1] != self.naxis:\n-                raise ValueError(\n-                    \"When providing two arguments, the array must be \"\n-                    \"of shape (N, {0})\".format(self.naxis))\n-            if ra_dec_order and sky == 'input':\n-                xy = self._denormalize_sky(xy)\n-            result = func(xy, origin)\n-            if ra_dec_order and sky == 'output':\n-                result = self._normalize_sky(result)\n-            return result\n-\n-        if len(args) == 2:\n-            try:\n-                xy, origin = args\n-                xy = np.asarray(xy)\n-                origin = int(origin)\n-            except Exception:\n-                raise TypeError(\n-                    \"When providing two arguments, they must be \"\n-                    \"(coords[N][{0}], origin)\".format(self.naxis))\n-            if self.naxis == 1 and len(xy.shape) == 1:\n-                return _return_list_of_arrays([xy], origin)\n-            return _return_single_array(xy, origin)\n-\n-        elif len(args) == self.naxis + 1:\n-            axes = args[:-1]\n-            origin = args[-1]\n-            try:\n-                axes = [np.asarray(x) for x in axes]\n-                origin = int(origin)\n-            except Exception:\n-                raise TypeError(\n-                    \"When providing more than two arguments, they must be \" +\n-                    \"a 1-D array for each axis, followed by an origin.\")\n-\n-            return _return_list_of_arrays(axes, origin)\n-\n-        raise TypeError(\n-            \"WCS projection has {0} dimensions, so expected 2 (an Nx{0} array \"\n-            \"and the origin argument) or {1} arguments (the position in each \"\n-            \"dimension, and the origin argument). Instead, {2} arguments were \"\n-            \"given.\".format(\n-                self.naxis, self.naxis + 1, len(args)))\n-\n-    def all_pix2world(self, *args, **kwargs):\n-        return self._array_converter(\n-            self._all_pix2world, 'output', *args, **kwargs)\n-    all_pix2world.__doc__ = \"\"\"\n-        Transforms pixel coordinates to world coordinates.\n-\n-        Performs all of the following in series:\n-\n-            - Detector to image plane correction (if present in the\n-              FITS file)\n-\n-            - `SIP`_ distortion correction (if present in the FITS\n-              file)\n-\n-            - `distortion paper`_ table-lookup correction (if present\n-              in the FITS file)\n-\n-            - `wcslib`_ \"core\" WCS transformation\n-\n-        Parameters\n-        ----------\n-        {0}\n-\n-            For a transformation that is not two-dimensional, the\n-            two-argument form must be used.\n-\n-        {1}\n-\n-        Returns\n-        -------\n-\n-        {2}\n-\n-        Notes\n-        -----\n-        The order of the axes for the result is determined by the\n-        ``CTYPEia`` keywords in the FITS header, therefore it may not\n-        always be of the form (*ra*, *dec*).  The\n-        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n-        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n-        members can be used to determine the order of the axes.\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        SingularMatrixError\n-            Linear transformation matrix is singular.\n-\n-        InconsistentAxisTypesError\n-            Inconsistent or unrecognized coordinate axis types.\n-\n-        ValueError\n-            Invalid parameter value.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-\n-        ValueError\n-            x- and y-coordinate arrays are not the same size.\n-\n-        InvalidTransformError\n-            Invalid coordinate transformation parameters.\n-\n-        InvalidTransformError\n-            Ill-conditioned coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n-                   __.RA_DEC_ORDER(8),\n-                   __.RETURNS('sky coordinates, in degrees', 8))\n-\n-    def wcs_pix2world(self, *args, **kwargs):\n-        if self.wcs is None:\n-            raise ValueError(\"No basic WCS settings were created.\")\n-        return self._array_converter(\n-            lambda xy, o: self.wcs.p2s(xy, o)['world'],\n-            'output', *args, **kwargs)\n-    wcs_pix2world.__doc__ = \"\"\"\n-        Transforms pixel coordinates to world coordinates by doing\n-        only the basic `wcslib`_ transformation.\n-\n-        No `SIP`_ or `distortion paper`_ table lookup correction is\n-        applied.  To perform distortion correction, see\n-        `~astropy.wcs.WCS.all_pix2world`,\n-        `~astropy.wcs.WCS.sip_pix2foc`, `~astropy.wcs.WCS.p4_pix2foc`,\n-        or `~astropy.wcs.WCS.pix2foc`.\n-\n-        Parameters\n-        ----------\n-        {0}\n-\n-            For a transformation that is not two-dimensional, the\n-            two-argument form must be used.\n-\n-        {1}\n-\n-        Returns\n-        -------\n-\n-        {2}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        SingularMatrixError\n-            Linear transformation matrix is singular.\n-\n-        InconsistentAxisTypesError\n-            Inconsistent or unrecognized coordinate axis types.\n-\n-        ValueError\n-            Invalid parameter value.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-\n-        ValueError\n-            x- and y-coordinate arrays are not the same size.\n-\n-        InvalidTransformError\n-            Invalid coordinate transformation parameters.\n-\n-        InvalidTransformError\n-            Ill-conditioned coordinate transformation parameters.\n-\n-        Notes\n-        -----\n-        The order of the axes for the result is determined by the\n-        ``CTYPEia`` keywords in the FITS header, therefore it may not\n-        always be of the form (*ra*, *dec*).  The\n-        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n-        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n-        members can be used to determine the order of the axes.\n-\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n-                   __.RA_DEC_ORDER(8),\n-                   __.RETURNS('world coordinates, in degrees', 8))\n-\n-    def _all_world2pix(self, world, origin, tolerance, maxiter, adaptive,\n-                       detect_divergence, quiet):\n-        # ############################################################\n-        # #          DESCRIPTION OF THE NUMERICAL METHOD            ##\n-        # ############################################################\n-        # In this section I will outline the method of solving\n-        # the inverse problem of converting world coordinates to\n-        # pixel coordinates (*inverse* of the direct transformation\n-        # `all_pix2world`) and I will summarize some of the aspects\n-        # of the method proposed here and some of the issues of the\n-        # original `all_world2pix` (in relation to this method)\n-        # discussed in https://github.com/astropy/astropy/issues/1977\n-        # A more detailed discussion can be found here:\n-        # https://github.com/astropy/astropy/pull/2373\n-        #\n-        #\n-        #                  ### Background ###\n-        #\n-        #\n-        # I will refer here to the [SIP Paper]\n-        # (http://fits.gsfc.nasa.gov/registry/sip/SIP_distortion_v1_0.pdf).\n-        # According to this paper, the effect of distortions as\n-        # described in *their* equation (1) is:\n-        #\n-        # (1)   x = CD*(u+f(u)),\n-        #\n-        # where `x` is a *vector* of \"intermediate spherical\n-        # coordinates\" (equivalent to (x,y) in the paper) and `u`\n-        # is a *vector* of \"pixel coordinates\", and `f` is a vector\n-        # function describing geometrical distortions\n-        # (see equations 2 and 3 in SIP Paper.\n-        # However, I prefer to use `w` for \"intermediate world\n-        # coordinates\", `x` for pixel coordinates, and assume that\n-        # transformation `W` performs the **linear**\n-        # (CD matrix + projection onto celestial sphere) part of the\n-        # conversion from pixel coordinates to world coordinates.\n-        # Then we can re-write (1) as:\n-        #\n-        # (2)   w = W*(x+f(x)) = T(x)\n-        #\n-        # In `astropy.wcs.WCS` transformation `W` is represented by\n-        # the `wcs_pix2world` member, while the combined (\"total\")\n-        # transformation (linear part + distortions) is performed by\n-        # `all_pix2world`. Below I summarize the notations and their\n-        # equivalents in `astropy.wcs.WCS`:\n-        #\n-        # | Equation term | astropy.WCS/meaning          |\n-        # | ------------- | ---------------------------- |\n-        # | `x`           | pixel coordinates            |\n-        # | `w`           | world coordinates            |\n-        # | `W`           | `wcs_pix2world()`            |\n-        # | `W^{-1}`      | `wcs_world2pix()`            |\n-        # | `T`           | `all_pix2world()`            |\n-        # | `x+f(x)`      | `pix2foc()`                  |\n-        #\n-        #\n-        #      ### Direct Solving of Equation (2)  ###\n-        #\n-        #\n-        # In order to find the pixel coordinates that correspond to\n-        # given world coordinates `w`, it is necessary to invert\n-        # equation (2): `x=T^{-1}(w)`, or solve equation `w==T(x)`\n-        # for `x`. However, this approach has the following\n-        # disadvantages:\n-        #    1. It requires unnecessary transformations (see next\n-        #       section).\n-        #    2. It is prone to \"RA wrapping\" issues as described in\n-        # https://github.com/astropy/astropy/issues/1977\n-        # (essentially because `all_pix2world` may return points with\n-        # a different phase than user's input `w`).\n-        #\n-        #\n-        #      ### Description of the Method Used here ###\n-        #\n-        #\n-        # By applying inverse linear WCS transformation (`W^{-1}`)\n-        # to both sides of equation (2) and introducing notation `x'`\n-        # (prime) for the pixels coordinates obtained from the world\n-        # coordinates by applying inverse *linear* WCS transformation\n-        # (\"focal plane coordinates\"):\n-        #\n-        # (3)   x' = W^{-1}(w)\n-        #\n-        # we obtain the following equation:\n-        #\n-        # (4)   x' = x+f(x),\n-        #\n-        # or,\n-        #\n-        # (5)   x = x'-f(x)\n-        #\n-        # This equation is well suited for solving using the method\n-        # of fixed-point iterations\n-        # (http://en.wikipedia.org/wiki/Fixed-point_iteration):\n-        #\n-        # (6)   x_{i+1} = x'-f(x_i)\n-        #\n-        # As an initial value of the pixel coordinate `x_0` we take\n-        # \"focal plane coordinate\" `x'=W^{-1}(w)=wcs_world2pix(w)`.\n-        # We stop iterations when `|x_{i+1}-x_i|<tolerance`. We also\n-        # consider the process to be diverging if\n-        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|`\n-        # **when** `|x_{i+1}-x_i|>=tolerance` (when current\n-        # approximation is close to the true solution,\n-        # `|x_{i+1}-x_i|>|x_i-x_{i-1}|` may be due to rounding errors\n-        # and we ignore such \"divergences\" when\n-        # `|x_{i+1}-x_i|<tolerance`). It may appear that checking for\n-        # `|x_{i+1}-x_i|<tolerance` in order to ignore divergence is\n-        # unnecessary since the iterative process should stop anyway,\n-        # however, the proposed implementation of this iterative\n-        # process is completely vectorized and, therefore, we may\n-        # continue iterating over *some* points even though they have\n-        # converged to within a specified tolerance (while iterating\n-        # over other points that have not yet converged to\n-        # a solution).\n-        #\n-        # In order to efficiently implement iterative process (6)\n-        # using available methods in `astropy.wcs.WCS`, we add and\n-        # subtract `x_i` from the right side of equation (6):\n-        #\n-        # (7)   x_{i+1} = x'-(x_i+f(x_i))+x_i = x'-pix2foc(x_i)+x_i,\n-        #\n-        # where `x'=wcs_world2pix(w)` and it is computed only *once*\n-        # before the beginning of the iterative process (and we also\n-        # set `x_0=x'`). By using `pix2foc` at each iteration instead\n-        # of `all_pix2world` we get about 25% increase in performance\n-        # (by not performing the linear `W` transformation at each\n-        # step) and we also avoid the \"RA wrapping\" issue described\n-        # above (by working in focal plane coordinates and avoiding\n-        # pix->world transformations).\n-        #\n-        # As an added benefit, the process converges to the correct\n-        # solution in just one iteration when distortions are not\n-        # present (compare to\n-        # https://github.com/astropy/astropy/issues/1977 and\n-        # https://github.com/astropy/astropy/pull/2294): in this case\n-        # `pix2foc` is the identical transformation\n-        # `x_i=pix2foc(x_i)` and from equation (7) we get:\n-        #\n-        # x' = x_0 = wcs_world2pix(w)\n-        # x_1 = x' - pix2foc(x_0) + x_0 = x' - pix2foc(x') + x' = x'\n-        #     = wcs_world2pix(w) = x_0\n-        # =>\n-        # |x_1-x_0| = 0 < tolerance (with tolerance > 0)\n-        #\n-        # However, for performance reasons, it is still better to\n-        # avoid iterations altogether and return the exact linear\n-        # solution (`wcs_world2pix`) right-away when non-linear\n-        # distortions are not present by checking that attributes\n-        # `sip`, `cpdis1`, `cpdis2`, `det2im1`, and `det2im2` are\n-        # *all* `None`.\n-        #\n-        #\n-        #         ### Outline of the Algorithm ###\n-        #\n-        #\n-        # While the proposed code is relatively long (considering\n-        # the simplicity of the algorithm), this is due to: 1)\n-        # checking if iterative solution is necessary at all; 2)\n-        # checking for divergence; 3) re-implementation of the\n-        # completely vectorized algorithm as an \"adaptive\" vectorized\n-        # algorithm (for cases when some points diverge for which we\n-        # want to stop iterations). In my tests, the adaptive version\n-        # of the algorithm is about 50% slower than non-adaptive\n-        # version for all HST images.\n-        #\n-        # The essential part of the vectorized non-adaptive algorithm\n-        # (without divergence and other checks) can be described\n-        # as follows:\n-        #\n-        #     pix0 = self.wcs_world2pix(world, origin)\n-        #     pix  = pix0.copy() # 0-order solution\n-        #\n-        #     for k in range(maxiter):\n-        #         # find correction to the previous solution:\n-        #         dpix = self.pix2foc(pix, origin) - pix0\n-        #\n-        #         # compute norm (L2) of the correction:\n-        #         dn = np.linalg.norm(dpix, axis=1)\n-        #\n-        #         # apply correction:\n-        #         pix -= dpix\n-        #\n-        #         # check convergence:\n-        #         if np.max(dn) < tolerance:\n-        #             break\n-        #\n-        #    return pix\n-        #\n-        # Here, the input parameter `world` can be a `MxN` array\n-        # where `M` is the number of coordinate axes in WCS and `N`\n-        # is the number of points to be converted simultaneously to\n-        # image coordinates.\n-        #\n-        #\n-        #                ###  IMPORTANT NOTE:  ###\n-        #\n-        # If, in the future releases of the `~astropy.wcs`,\n-        # `pix2foc` will not apply all the required distortion\n-        # corrections then in the code below, calls to `pix2foc` will\n-        # have to be replaced with\n-        # wcs_world2pix(all_pix2world(pix_list, origin), origin)\n-        #\n-\n-        # ############################################################\n-        # #            INITIALIZE ITERATIVE PROCESS:                ##\n-        # ############################################################\n-\n-        # initial approximation (linear WCS based only)\n-        pix0 = self.wcs_world2pix(world, origin)\n-\n-        # Check that an iterative solution is required at all\n-        # (when any of the non-CD-matrix-based corrections are\n-        # present). If not required return the initial\n-        # approximation (pix0).\n-        if self.sip is None and \\\n-           self.cpdis1 is None and self.cpdis2 is None and \\\n-           self.det2im1 is None and self.det2im2 is None:\n-            # No non-WCS corrections detected so\n-            # simply return initial approximation:\n-            return pix0\n-\n-        pix = pix0.copy()  # 0-order solution\n-\n-        # initial correction:\n-        dpix = self.pix2foc(pix, origin) - pix0\n-\n-        # Update initial solution:\n-        pix -= dpix\n-\n-        # Norm (L2) squared of the correction:\n-        dn = np.sum(dpix*dpix, axis=1)\n-        dnprev = dn.copy()  # if adaptive else dn\n-        tol2 = tolerance**2\n-\n-        # Prepare for iterative process\n-        k = 1\n-        ind = None\n-        inddiv = None\n-\n-        # Turn off numpy runtime warnings for 'invalid' and 'over':\n-        old_invalid = np.geterr()['invalid']\n-        old_over = np.geterr()['over']\n-        np.seterr(invalid='ignore', over='ignore')\n-\n-        # ############################################################\n-        # #                NON-ADAPTIVE ITERATIONS:                 ##\n-        # ############################################################\n-        if not adaptive:\n-            # Fixed-point iterations:\n-            while (np.nanmax(dn) >= tol2 and k < maxiter):\n-                # Find correction to the previous solution:\n-                dpix = self.pix2foc(pix, origin) - pix0\n-\n-                # Compute norm (L2) squared of the correction:\n-                dn = np.sum(dpix*dpix, axis=1)\n-\n-                # Check for divergence (we do this in two stages\n-                # to optimize performance for the most common\n-                # scenario when successive approximations converge):\n-                if detect_divergence:\n-                    divergent = (dn >= dnprev)\n-                    if np.any(divergent):\n-                        # Find solutions that have not yet converged:\n-                        slowconv = (dn >= tol2)\n-                        inddiv, = np.where(divergent & slowconv)\n-\n-                        if inddiv.shape[0] > 0:\n-                            # Update indices of elements that\n-                            # still need correction:\n-                            conv = (dn < dnprev)\n-                            iconv = np.where(conv)\n-\n-                            # Apply correction:\n-                            dpixgood = dpix[iconv]\n-                            pix[iconv] -= dpixgood\n-                            dpix[iconv] = dpixgood\n-\n-                            # For the next iteration choose\n-                            # non-divergent points that have not yet\n-                            # converged to the requested accuracy:\n-                            ind, = np.where(slowconv & conv)\n-                            pix0 = pix0[ind]\n-                            dnprev[ind] = dn[ind]\n-                            k += 1\n-\n-                            # Switch to adaptive iterations:\n-                            adaptive = True\n-                            break\n-                    # Save current correction magnitudes for later:\n-                    dnprev = dn\n-\n-                # Apply correction:\n-                pix -= dpix\n-                k += 1\n-\n-        # ############################################################\n-        # #                  ADAPTIVE ITERATIONS:                   ##\n-        # ############################################################\n-        if adaptive:\n-            if ind is None:\n-                ind, = np.where(np.isfinite(pix).all(axis=1))\n-                pix0 = pix0[ind]\n-\n-            # \"Adaptive\" fixed-point iterations:\n-            while (ind.shape[0] > 0 and k < maxiter):\n-                # Find correction to the previous solution:\n-                dpixnew = self.pix2foc(pix[ind], origin) - pix0\n-\n-                # Compute norm (L2) of the correction:\n-                dnnew = np.sum(np.square(dpixnew), axis=1)\n-\n-                # Bookeeping of corrections:\n-                dnprev[ind] = dn[ind].copy()\n-                dn[ind] = dnnew\n-\n-                if detect_divergence:\n-                    # Find indices of pixels that are converging:\n-                    conv = (dnnew < dnprev[ind])\n-                    iconv = np.where(conv)\n-                    iiconv = ind[iconv]\n-\n-                    # Apply correction:\n-                    dpixgood = dpixnew[iconv]\n-                    pix[iiconv] -= dpixgood\n-                    dpix[iiconv] = dpixgood\n-\n-                    # Find indices of solutions that have not yet\n-                    # converged to the requested accuracy\n-                    # AND that do not diverge:\n-                    subind, = np.where((dnnew >= tol2) & conv)\n-\n-                else:\n-                    # Apply correction:\n-                    pix[ind] -= dpixnew\n-                    dpix[ind] = dpixnew\n-\n-                    # Find indices of solutions that have not yet\n-                    # converged to the requested accuracy:\n-                    subind, = np.where(dnnew >= tol2)\n-\n-                # Choose solutions that need more iterations:\n-                ind = ind[subind]\n-                pix0 = pix0[subind]\n-\n-                k += 1\n-\n-        # ############################################################\n-        # #         FINAL DETECTION OF INVALID, DIVERGING,          ##\n-        # #         AND FAILED-TO-CONVERGE POINTS                   ##\n-        # ############################################################\n-        # Identify diverging and/or invalid points:\n-        invalid = ((~np.all(np.isfinite(pix), axis=1)) &\n-                   (np.all(np.isfinite(world), axis=1)))\n-\n-        # When detect_divergence==False, dnprev is outdated\n-        # (it is the norm of the very first correction).\n-        # Still better than nothing...\n-        inddiv, = np.where(((dn >= tol2) & (dn >= dnprev)) | invalid)\n-        if inddiv.shape[0] == 0:\n-            inddiv = None\n-\n-        # Identify points that did not converge within 'maxiter'\n-        # iterations:\n-        if k >= maxiter:\n-            ind, = np.where((dn >= tol2) & (dn < dnprev) & (~invalid))\n-            if ind.shape[0] == 0:\n-                ind = None\n-        else:\n-            ind = None\n-\n-        # Restore previous numpy error settings:\n-        np.seterr(invalid=old_invalid, over=old_over)\n-\n-        # ############################################################\n-        # #  RAISE EXCEPTION IF DIVERGING OR TOO SLOWLY CONVERGING  ##\n-        # #  DATA POINTS HAVE BEEN DETECTED:                        ##\n-        # ############################################################\n-        if (ind is not None or inddiv is not None) and not quiet:\n-            if inddiv is None:\n-                raise NoConvergence(\n-                    \"'WCS.all_world2pix' failed to \"\n-                    \"converge to the requested accuracy after {:d} \"\n-                    \"iterations.\".format(k), best_solution=pix,\n-                    accuracy=np.abs(dpix), niter=k,\n-                    slow_conv=ind, divergent=None)\n-            else:\n-                raise NoConvergence(\n-                    \"'WCS.all_world2pix' failed to \"\n-                    \"converge to the requested accuracy.\\n\"\n-                    \"After {0:d} iterations, the solution is diverging \"\n-                    \"at least for one input point.\"\n-                    .format(k), best_solution=pix,\n-                    accuracy=np.abs(dpix), niter=k,\n-                    slow_conv=ind, divergent=inddiv)\n-\n-        return pix\n-\n-    def all_world2pix(self, *args, tolerance=1e-4, maxiter=20, adaptive=False,\n-                      detect_divergence=True, quiet=False, **kwargs):\n-        if self.wcs is None:\n-            raise ValueError(\"No basic WCS settings were created.\")\n-\n-        return self._array_converter(\n-            lambda *args, **kwargs:\n-            self._all_world2pix(\n-                *args, tolerance=tolerance, maxiter=maxiter,\n-                adaptive=adaptive, detect_divergence=detect_divergence,\n-                quiet=quiet),\n-            'input', *args, **kwargs\n-        )\n-\n-    all_world2pix.__doc__ = \"\"\"\n-        all_world2pix(*arg, accuracy=1.0e-4, maxiter=20,\n-        adaptive=False, detect_divergence=True, quiet=False)\n-\n-        Transforms world coordinates to pixel coordinates, using\n-        numerical iteration to invert the full forward transformation\n-        `~astropy.wcs.WCS.all_pix2world` with complete\n-        distortion model.\n-\n-\n-        Parameters\n-        ----------\n-        {0}\n-\n-            For a transformation that is not two-dimensional, the\n-            two-argument form must be used.\n-\n-        {1}\n-\n-        tolerance : float, optional (Default = 1.0e-4)\n-            Tolerance of solution. Iteration terminates when the\n-            iterative solver estimates that the \"true solution\" is\n-            within this many pixels current estimate, more\n-            specifically, when the correction to the solution found\n-            during the previous iteration is smaller\n-            (in the sense of the L2 norm) than ``tolerance``.\n-\n-        maxiter : int, optional (Default = 20)\n-            Maximum number of iterations allowed to reach a solution.\n-\n-        quiet : bool, optional (Default = False)\n-            Do not throw :py:class:`NoConvergence` exceptions when\n-            the method does not converge to a solution with the\n-            required accuracy within a specified number of maximum\n-            iterations set by ``maxiter`` parameter. Instead,\n-            simply return the found solution.\n-\n-        Other Parameters\n-        ----------------\n-        adaptive : bool, optional (Default = False)\n-            Specifies whether to adaptively select only points that\n-            did not converge to a solution within the required\n-            accuracy for the next iteration. Default is recommended\n-            for HST as well as most other instruments.\n-\n-            .. note::\n-               The :py:meth:`all_world2pix` uses a vectorized\n-               implementation of the method of consecutive\n-               approximations (see ``Notes`` section below) in which it\n-               iterates over *all* input points *regardless* until\n-               the required accuracy has been reached for *all* input\n-               points. In some cases it may be possible that\n-               *almost all* points have reached the required accuracy\n-               but there are only a few of input data points for\n-               which additional iterations may be needed (this\n-               depends mostly on the characteristics of the geometric\n-               distortions for a given instrument). In this situation\n-               it may be advantageous to set ``adaptive`` = `True` in\n-               which case :py:meth:`all_world2pix` will continue\n-               iterating *only* over the points that have not yet\n-               converged to the required accuracy. However, for the\n-               HST's ACS/WFC detector, which has the strongest\n-               distortions of all HST instruments, testing has\n-               shown that enabling this option would lead to a about\n-               50-100% penalty in computational time (depending on\n-               specifics of the image, geometric distortions, and\n-               number of input points to be converted). Therefore,\n-               for HST and possibly instruments, it is recommended\n-               to set ``adaptive`` = `False`. The only danger in\n-               getting this setting wrong will be a performance\n-               penalty.\n-\n-            .. note::\n-               When ``detect_divergence`` is `True`,\n-               :py:meth:`all_world2pix` will automatically switch\n-               to the adaptive algorithm once divergence has been\n-               detected.\n-\n-        detect_divergence : bool, optional (Default = True)\n-            Specifies whether to perform a more detailed analysis\n-            of the convergence to a solution. Normally\n-            :py:meth:`all_world2pix` may not achieve the required\n-            accuracy if either the ``tolerance`` or ``maxiter`` arguments\n-            are too low. However, it may happen that for some\n-            geometric distortions the conditions of convergence for\n-            the the method of consecutive approximations used by\n-            :py:meth:`all_world2pix` may not be satisfied, in which\n-            case consecutive approximations to the solution will\n-            diverge regardless of the ``tolerance`` or ``maxiter``\n-            settings.\n-\n-            When ``detect_divergence`` is `False`, these divergent\n-            points will be detected as not having achieved the\n-            required accuracy (without further details). In addition,\n-            if ``adaptive`` is `False` then the algorithm will not\n-            know that the solution (for specific points) is diverging\n-            and will continue iterating and trying to \"improve\"\n-            diverging solutions. This may result in ``NaN`` or\n-            ``Inf`` values in the return results (in addition to a\n-            performance penalties). Even when ``detect_divergence``\n-            is `False`, :py:meth:`all_world2pix`, at the end of the\n-            iterative process, will identify invalid results\n-            (``NaN`` or ``Inf``) as \"diverging\" solutions and will\n-            raise :py:class:`NoConvergence` unless the ``quiet``\n-            parameter is set to `True`.\n-\n-            When ``detect_divergence`` is `True`,\n-            :py:meth:`all_world2pix` will detect points for which\n-            current correction to the coordinates is larger than\n-            the correction applied during the previous iteration\n-            **if** the requested accuracy **has not yet been\n-            achieved**. In this case, if ``adaptive`` is `True`,\n-            these points will be excluded from further iterations and\n-            if ``adaptive`` is `False`, :py:meth:`all_world2pix` will\n-            automatically switch to the adaptive algorithm. Thus, the\n-            reported divergent solution will be the latest converging\n-            solution computed immediately *before* divergence\n-            has been detected.\n-\n-            .. note::\n-               When accuracy has been achieved, small increases in\n-               current corrections may be possible due to rounding\n-               errors (when ``adaptive`` is `False`) and such\n-               increases will be ignored.\n-\n-            .. note::\n-               Based on our testing using HST ACS/WFC images, setting\n-               ``detect_divergence`` to `True` will incur about 5-20%\n-               performance penalty with the larger penalty\n-               corresponding to ``adaptive`` set to `True`.\n-               Because the benefits of enabling this\n-               feature outweigh the small performance penalty,\n-               especially when ``adaptive`` = `False`, it is\n-               recommended to set ``detect_divergence`` to `True`,\n-               unless extensive testing of the distortion models for\n-               images from specific instruments show a good stability\n-               of the numerical method for a wide range of\n-               coordinates (even outside the image itself).\n-\n-            .. note::\n-               Indices of the diverging inverse solutions will be\n-               reported in the ``divergent`` attribute of the\n-               raised :py:class:`NoConvergence` exception object.\n-\n-        Returns\n-        -------\n-\n-        {2}\n-\n-        Notes\n-        -----\n-        The order of the axes for the input world array is determined by\n-        the ``CTYPEia`` keywords in the FITS header, therefore it may\n-        not always be of the form (*ra*, *dec*).  The\n-        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n-        `~astropy.wcs.Wcsprm.lattyp`, and\n-        `~astropy.wcs.Wcsprm.lngtyp`\n-        members can be used to determine the order of the axes.\n-\n-        Using the method of fixed-point iterations approximations we\n-        iterate starting with the initial approximation, which is\n-        computed using the non-distortion-aware\n-        :py:meth:`wcs_world2pix` (or equivalent).\n-\n-        The :py:meth:`all_world2pix` function uses a vectorized\n-        implementation of the method of consecutive approximations and\n-        therefore it is highly efficient (>30x) when *all* data points\n-        that need to be converted from sky coordinates to image\n-        coordinates are passed at *once*. Therefore, it is advisable,\n-        whenever possible, to pass as input a long array of all points\n-        that need to be converted to :py:meth:`all_world2pix` instead\n-        of calling :py:meth:`all_world2pix` for each data point. Also\n-        see the note to the ``adaptive`` parameter.\n-\n-        Raises\n-        ------\n-        NoConvergence\n-            The method did not converge to a\n-            solution to the required accuracy within a specified\n-            number of maximum iterations set by the ``maxiter``\n-            parameter. To turn off this exception, set ``quiet`` to\n-            `True`. Indices of the points for which the requested\n-            accuracy was not achieved (if any) will be listed in the\n-            ``slow_conv`` attribute of the\n-            raised :py:class:`NoConvergence` exception object.\n-\n-            See :py:class:`NoConvergence` documentation for\n-            more details.\n-\n-        MemoryError\n-            Memory allocation failed.\n-\n-        SingularMatrixError\n-            Linear transformation matrix is singular.\n-\n-        InconsistentAxisTypesError\n-            Inconsistent or unrecognized coordinate axis types.\n-\n-        ValueError\n-            Invalid parameter value.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-\n-        ValueError\n-            x- and y-coordinate arrays are not the same size.\n-\n-        InvalidTransformError\n-            Invalid coordinate transformation parameters.\n-\n-        InvalidTransformError\n-            Ill-conditioned coordinate transformation parameters.\n-\n-        Examples\n-        --------\n-        >>> import astropy.io.fits as fits\n-        >>> import astropy.wcs as wcs\n-        >>> import numpy as np\n-        >>> import os\n-\n-        >>> filename = os.path.join(wcs.__path__[0], 'tests/data/j94f05bgq_flt.fits')\n-        >>> hdulist = fits.open(filename)\n-        >>> w = wcs.WCS(hdulist[('sci',1)].header, hdulist)\n-        >>> hdulist.close()\n-\n-        >>> ra, dec = w.all_pix2world([1,2,3], [1,1,1], 1)\n-        >>> print(ra)  # doctest: +FLOAT_CMP\n-        [ 5.52645627  5.52649663  5.52653698]\n-        >>> print(dec)  # doctest: +FLOAT_CMP\n-        [-72.05171757 -72.05171276 -72.05170795]\n-        >>> radec = w.all_pix2world([[1,1], [2,1], [3,1]], 1)\n-        >>> print(radec)  # doctest: +FLOAT_CMP\n-        [[  5.52645627 -72.05171757]\n-         [  5.52649663 -72.05171276]\n-         [  5.52653698 -72.05170795]]\n-        >>> x, y = w.all_world2pix(ra, dec, 1)\n-        >>> print(x)  # doctest: +FLOAT_CMP\n-        [ 1.00000238  2.00000237  3.00000236]\n-        >>> print(y)  # doctest: +FLOAT_CMP\n-        [ 0.99999996  0.99999997  0.99999997]\n-        >>> xy = w.all_world2pix(radec, 1)\n-        >>> print(xy)  # doctest: +FLOAT_CMP\n-        [[ 1.00000238  0.99999996]\n-         [ 2.00000237  0.99999997]\n-         [ 3.00000236  0.99999997]]\n-        >>> xy = w.all_world2pix(radec, 1, maxiter=3,\n-        ...                      tolerance=1.0e-10, quiet=False)\n-        Traceback (most recent call last):\n-        ...\n-        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n-        requested accuracy. After 3 iterations, the solution is\n-        diverging at least for one input point.\n-\n-        >>> # Now try to use some diverging data:\n-        >>> divradec = w.all_pix2world([[1.0, 1.0],\n-        ...                             [10000.0, 50000.0],\n-        ...                             [3.0, 1.0]], 1)\n-        >>> print(divradec)  # doctest: +FLOAT_CMP\n-        [[  5.52645627 -72.05171757]\n-         [  7.15976932 -70.8140779 ]\n-         [  5.52653698 -72.05170795]]\n-\n-        >>> # First, turn detect_divergence on:\n-        >>> try:  # doctest: +FLOAT_CMP\n-        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n-        ...                        tolerance=1.0e-4, adaptive=False,\n-        ...                        detect_divergence=True,\n-        ...                        quiet=False)\n-        ... except wcs.wcs.NoConvergence as e:\n-        ...   print(\"Indices of diverging points: {{0}}\"\n-        ...         .format(e.divergent))\n-        ...   print(\"Indices of poorly converging points: {{0}}\"\n-        ...         .format(e.slow_conv))\n-        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n-        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n-        Indices of diverging points: [1]\n-        Indices of poorly converging points: None\n-        Best solution:\n-        [[  1.00000238e+00   9.99999965e-01]\n-         [ -1.99441636e+06   1.44309097e+06]\n-         [  3.00000236e+00   9.99999966e-01]]\n-        Achieved accuracy:\n-        [[  6.13968380e-05   8.59638593e-07]\n-         [  8.59526812e+11   6.61713548e+11]\n-         [  6.09398446e-05   8.38759724e-07]]\n-        >>> raise e\n-        Traceback (most recent call last):\n-        ...\n-        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n-        requested accuracy.  After 5 iterations, the solution is\n-        diverging at least for one input point.\n-\n-        >>> # This time turn detect_divergence off:\n-        >>> try:  # doctest: +FLOAT_CMP\n-        ...   xy = w.all_world2pix(divradec, 1, maxiter=20,\n-        ...                        tolerance=1.0e-4, adaptive=False,\n-        ...                        detect_divergence=False,\n-        ...                        quiet=False)\n-        ... except wcs.wcs.NoConvergence as e:\n-        ...   print(\"Indices of diverging points: {{0}}\"\n-        ...         .format(e.divergent))\n-        ...   print(\"Indices of poorly converging points: {{0}}\"\n-        ...         .format(e.slow_conv))\n-        ...   print(\"Best solution:\\\\n{{0}}\".format(e.best_solution))\n-        ...   print(\"Achieved accuracy:\\\\n{{0}}\".format(e.accuracy))\n-        Indices of diverging points: [1]\n-        Indices of poorly converging points: None\n-        Best solution:\n-        [[ 1.00000009  1.        ]\n-         [        nan         nan]\n-         [ 3.00000009  1.        ]]\n-        Achieved accuracy:\n-        [[  2.29417358e-06   3.21222995e-08]\n-         [             nan              nan]\n-         [  2.27407877e-06   3.13005639e-08]]\n-        >>> raise e\n-        Traceback (most recent call last):\n-        ...\n-        NoConvergence: 'WCS.all_world2pix' failed to converge to the\n-        requested accuracy.  After 6 iterations, the solution is\n-        diverging at least for one input point.\n-\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n-                   __.RA_DEC_ORDER(8),\n-                   __.RETURNS('pixel coordinates', 8))\n-\n-    def wcs_world2pix(self, *args, **kwargs):\n-        if self.wcs is None:\n-            raise ValueError(\"No basic WCS settings were created.\")\n-        return self._array_converter(\n-            lambda xy, o: self.wcs.s2p(xy, o)['pixcrd'],\n-            'input', *args, **kwargs)\n-    wcs_world2pix.__doc__ = \"\"\"\n-        Transforms world coordinates to pixel coordinates, using only\n-        the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n-        `distortion paper`_ table lookup transformation is applied.\n-\n-        Parameters\n-        ----------\n-        {0}\n-\n-            For a transformation that is not two-dimensional, the\n-            two-argument form must be used.\n-\n-        {1}\n-\n-        Returns\n-        -------\n-\n-        {2}\n-\n-        Notes\n-        -----\n-        The order of the axes for the input world array is determined by\n-        the ``CTYPEia`` keywords in the FITS header, therefore it may\n-        not always be of the form (*ra*, *dec*).  The\n-        `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n-        `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n-        members can be used to determine the order of the axes.\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        SingularMatrixError\n-            Linear transformation matrix is singular.\n-\n-        InconsistentAxisTypesError\n-            Inconsistent or unrecognized coordinate axis types.\n-\n-        ValueError\n-            Invalid parameter value.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-\n-        ValueError\n-            x- and y-coordinate arrays are not the same size.\n-\n-        InvalidTransformError\n-            Invalid coordinate transformation parameters.\n-\n-        InvalidTransformError\n-            Ill-conditioned coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),\n-                   __.RA_DEC_ORDER(8),\n-                   __.RETURNS('pixel coordinates', 8))\n-\n-    def pix2foc(self, *args):\n-        return self._array_converter(self._pix2foc, None, *args)\n-    pix2foc.__doc__ = \"\"\"\n-        Convert pixel coordinates to focal plane coordinates using the\n-        `SIP`_ polynomial distortion convention and `distortion\n-        paper`_ table-lookup correction.\n-\n-        The output is in absolute pixel coordinates, not relative to\n-        ``CRPIX``.\n-\n-        Parameters\n-        ----------\n-\n-        {0}\n-\n-        Returns\n-        -------\n-\n-        {1}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n-                   __.RETURNS('focal coordinates', 8))\n-\n-    def p4_pix2foc(self, *args):\n-        return self._array_converter(self._p4_pix2foc, None, *args)\n-    p4_pix2foc.__doc__ = \"\"\"\n-        Convert pixel coordinates to focal plane coordinates using\n-        `distortion paper`_ table-lookup correction.\n-\n-        The output is in absolute pixel coordinates, not relative to\n-        ``CRPIX``.\n-\n-        Parameters\n-        ----------\n-\n-        {0}\n-\n-        Returns\n-        -------\n-\n-        {1}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n-                   __.RETURNS('focal coordinates', 8))\n-\n-    def det2im(self, *args):\n-        return self._array_converter(self._det2im, None, *args)\n-    det2im.__doc__ = \"\"\"\n-        Convert detector coordinates to image plane coordinates using\n-        `distortion paper`_ table-lookup correction.\n-\n-        The output is in absolute pixel coordinates, not relative to\n-        ``CRPIX``.\n-\n-        Parameters\n-        ----------\n-\n-        {0}\n-\n-        Returns\n-        -------\n-\n-        {1}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n-                   __.RETURNS('pixel coordinates', 8))\n-\n-    def sip_pix2foc(self, *args):\n-        if self.sip is None:\n-            if len(args) == 2:\n-                return args[0]\n-            elif len(args) == 3:\n-                return args[:2]\n-            else:\n-                raise TypeError(\"Wrong number of arguments\")\n-        return self._array_converter(self.sip.pix2foc, None, *args)\n-    sip_pix2foc.__doc__ = \"\"\"\n-        Convert pixel coordinates to focal plane coordinates using the\n-        `SIP`_ polynomial distortion convention.\n-\n-        The output is in pixel coordinates, relative to ``CRPIX``.\n-\n-        FITS WCS `distortion paper`_ table lookup correction is not\n-        applied, even if that information existed in the FITS file\n-        that initialized this :class:`~astropy.wcs.WCS` object.  To\n-        correct for that, use `~astropy.wcs.WCS.pix2foc` or\n-        `~astropy.wcs.WCS.p4_pix2foc`.\n-\n-        Parameters\n-        ----------\n-\n-        {0}\n-\n-        Returns\n-        -------\n-\n-        {1}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n-                   __.RETURNS('focal coordinates', 8))\n-\n-    def sip_foc2pix(self, *args):\n-        if self.sip is None:\n-            if len(args) == 2:\n-                return args[0]\n-            elif len(args) == 3:\n-                return args[:2]\n-            else:\n-                raise TypeError(\"Wrong number of arguments\")\n-        return self._array_converter(self.sip.foc2pix, None, *args)\n-    sip_foc2pix.__doc__ = \"\"\"\n-        Convert focal plane coordinates to pixel coordinates using the\n-        `SIP`_ polynomial distortion convention.\n-\n-        FITS WCS `distortion paper`_ table lookup distortion\n-        correction is not applied, even if that information existed in\n-        the FITS file that initialized this `~astropy.wcs.WCS` object.\n-\n-        Parameters\n-        ----------\n-\n-        {0}\n-\n-        Returns\n-        -------\n-\n-        {1}\n-\n-        Raises\n-        ------\n-        MemoryError\n-            Memory allocation failed.\n-\n-        ValueError\n-            Invalid coordinate transformation parameters.\n-        \"\"\".format(__.TWO_OR_MORE_ARGS('2', 8),\n-                   __.RETURNS('pixel coordinates', 8))\n-\n-    def to_fits(self, relax=False, key=None):\n-        \"\"\"\n-        Generate an `astropy.io.fits.HDUList` object with all of the\n-        information stored in this object.  This should be logically identical\n-        to the input FITS file, but it will be normalized in a number of ways.\n-\n-        See `to_header` for some warnings about the output produced.\n-\n-        Parameters\n-        ----------\n-\n-        relax : bool or int, optional\n-            Degree of permissiveness:\n-\n-            - `False` (default): Write all extensions that are\n-              considered to be safe and recommended.\n-\n-            - `True`: Write all recognized informal extensions of the\n-              WCS standard.\n-\n-            - `int`: a bit field selecting specific extensions to\n-              write.  See :ref:`relaxwrite` for details.\n-\n-        key : str\n-            The name of a particular WCS transform to use.  This may be\n-            either ``' '`` or ``'A'``-``'Z'`` and corresponds to the ``\"a\"``\n-            part of the ``CTYPEia`` cards.\n-\n-        Returns\n-        -------\n-        hdulist : `astropy.io.fits.HDUList`\n-        \"\"\"\n-\n-        header = self.to_header(relax=relax, key=key)\n-\n-        hdu = fits.PrimaryHDU(header=header)\n-        hdulist = fits.HDUList(hdu)\n-\n-        self._write_det2im(hdulist)\n-        self._write_distortion_kw(hdulist)\n-\n-        return hdulist\n-\n-    def to_header(self, relax=None, key=None):\n-        \"\"\"Generate an `astropy.io.fits.Header` object with the basic WCS\n-        and SIP information stored in this object.  This should be\n-        logically identical to the input FITS file, but it will be\n-        normalized in a number of ways.\n-\n-        .. warning::\n-\n-          This function does not write out FITS WCS `distortion\n-          paper`_ information, since that requires multiple FITS\n-          header data units.  To get a full representation of\n-          everything in this object, use `to_fits`.\n-\n-        Parameters\n-        ----------\n-        relax : bool or int, optional\n-            Degree of permissiveness:\n-\n-            - `False` (default): Write all extensions that are\n-              considered to be safe and recommended.\n-\n-            - `True`: Write all recognized informal extensions of the\n-              WCS standard.\n-\n-            - `int`: a bit field selecting specific extensions to\n-              write.  See :ref:`relaxwrite` for details.\n-\n-            If the ``relax`` keyword argument is not given and any\n-            keywords were omitted from the output, an\n-            `~astropy.utils.exceptions.AstropyWarning` is displayed.\n-            To override this, explicitly pass a value to ``relax``.\n-\n-        key : str\n-            The name of a particular WCS transform to use.  This may be\n-            either ``' '`` or ``'A'``-``'Z'`` and corresponds to the ``\"a\"``\n-            part of the ``CTYPEia`` cards.\n-\n-        Returns\n-        -------\n-        header : `astropy.io.fits.Header`\n-\n-        Notes\n-        -----\n-        The output header will almost certainly differ from the input in a\n-        number of respects:\n-\n-          1. The output header only contains WCS-related keywords.  In\n-             particular, it does not contain syntactically-required\n-             keywords such as ``SIMPLE``, ``NAXIS``, ``BITPIX``, or\n-             ``END``.\n-\n-          2. Deprecated (e.g. ``CROTAn``) or non-standard usage will\n-             be translated to standard (this is partially dependent on\n-             whether ``fix`` was applied).\n-\n-          3. Quantities will be converted to the units used internally,\n-             basically SI with the addition of degrees.\n-\n-          4. Floating-point quantities may be given to a different decimal\n-             precision.\n-\n-          5. Elements of the ``PCi_j`` matrix will be written if and\n-             only if they differ from the unit matrix.  Thus, if the\n-             matrix is unity then no elements will be written.\n-\n-          6. Additional keywords such as ``WCSAXES``, ``CUNITia``,\n-             ``LONPOLEa`` and ``LATPOLEa`` may appear.\n-\n-          7. The original keycomments will be lost, although\n-             `to_header` tries hard to write meaningful comments.\n-\n-          8. Keyword order may be changed.\n-\n-        \"\"\"\n-        # default precision for numerical WCS keywords\n-        precision = WCSHDO_P14\n-        display_warning = False\n-        if relax is None:\n-            display_warning = True\n-            relax = False\n-\n-        if relax not in (True, False):\n-            do_sip = relax & WCSHDO_SIP\n-            relax &= ~WCSHDO_SIP\n-        else:\n-            do_sip = relax\n-            relax = WCSHDO_all if relax is True else WCSHDO_safe\n-\n-        relax = precision | relax\n-\n-        if self.wcs is not None:\n-            if key is not None:\n-                orig_key = self.wcs.alt\n-                self.wcs.alt = key\n-            header_string = self.wcs.to_header(relax)\n-            header = fits.Header.fromstring(header_string)\n-            keys_to_remove = [\"\", \" \", \"COMMENT\"]\n-            for kw in keys_to_remove:\n-                if kw in header:\n-                    del header[kw]\n-        else:\n-            header = fits.Header()\n-\n-        if do_sip and self.sip is not None:\n-            if self.wcs is not None and any(not ctyp.endswith('-SIP') for ctyp in self.wcs.ctype):\n-                self._fix_ctype(header, add_sip=True)\n-\n-            for kw, val in self._write_sip_kw().items():\n-                header[kw] = val\n-\n-        if not do_sip and self.wcs is not None and any(self.wcs.ctype) and self.sip is not None:\n-            # This is called when relax is not False or WCSHDO_SIP\n-            # The default case of ``relax=None`` is handled further in the code.\n-            header = self._fix_ctype(header, add_sip=False)\n-\n-        if display_warning:\n-            full_header = self.to_header(relax=True, key=key)\n-            missing_keys = []\n-            for kw, val in full_header.items():\n-                if kw not in header:\n-                    missing_keys.append(kw)\n-\n-            if len(missing_keys):\n-                warnings.warn(\n-                    \"Some non-standard WCS keywords were excluded: {0} \"\n-                    \"Use the ``relax`` kwarg to control this.\".format(\n-                        ', '.join(missing_keys)),\n-                    AstropyWarning)\n-            # called when ``relax=None``\n-            # This is different from the case of ``relax=False``.\n-            if any(self.wcs.ctype) and self.sip is not None:\n-                header = self._fix_ctype(header, add_sip=False, log_message=False)\n-        # Finally reset the key. This must be called after ``_fix_ctype``.\n-        if key is not None:\n-            self.wcs.alt = orig_key\n-        return header\n-\n-    def _fix_ctype(self, header, add_sip=True, log_message=True):\n-        \"\"\"\n-        Parameters\n-        ----------\n-        header : `~astropy.io.fits.Header`\n-            FITS header.\n-        add_sip : bool\n-            Flag indicating whether \"-SIP\" should be added or removed from CTYPE keywords.\n-\n-            Remove \"-SIP\" from CTYPE when writing out a header with relax=False.\n-            This needs to be done outside ``to_header`` because ``to_header`` runs\n-            twice when ``relax=False`` and the second time ``relax`` is set to ``True``\n-            to display the missing keywords.\n-\n-            If the user requested SIP distortion to be written out add \"-SIP\" to\n-            CTYPE if it is missing.\n-        \"\"\"\n-\n-        _add_sip_to_ctype = \"\"\"\n-        Inconsistent SIP distortion information is present in the current WCS:\n-        SIP coefficients were detected, but CTYPE is missing \"-SIP\" suffix,\n-        therefore the current WCS is internally inconsistent.\n-\n-        Because relax has been set to True, the resulting output WCS will have\n-        \"-SIP\" appended to CTYPE in order to make the header internally consistent.\n-\n-        However, this may produce incorrect astrometry in the output WCS, if\n-        in fact the current WCS is already distortion-corrected.\n-\n-        Therefore, if current WCS is already distortion-corrected (eg, drizzled)\n-        then SIP distortion components should not apply. In that case, for a WCS\n-        that is already distortion-corrected, please remove the SIP coefficients\n-        from the header.\n-\n-        \"\"\"\n-        if log_message:\n-            if add_sip:\n-                log.info(_add_sip_to_ctype)\n-        for i in range(1, self.naxis+1):\n-            # strip() must be called here to cover the case of alt key= \" \"\n-            kw = 'CTYPE{0}{1}'.format(i, self.wcs.alt).strip()\n-            if kw in header:\n-                if add_sip:\n-                    val = header[kw].strip(\"-SIP\") + \"-SIP\"\n-                else:\n-                    val = header[kw].strip(\"-SIP\")\n-                header[kw] = val\n-            else:\n-                continue\n-        return header\n-\n-    def to_header_string(self, relax=None):\n-        \"\"\"\n-        Identical to `to_header`, but returns a string containing the\n-        header cards.\n-        \"\"\"\n-        return str(self.to_header(relax))\n-\n-    def footprint_to_file(self, filename='footprint.reg', color='green',\n-                          width=2, coordsys=None):\n-        \"\"\"\n-        Writes out a `ds9`_ style regions file. It can be loaded\n-        directly by `ds9`_.\n-\n-        Parameters\n-        ----------\n-        filename : str, optional\n-            Output file name - default is ``'footprint.reg'``\n-\n-        color : str, optional\n-            Color to use when plotting the line.\n-\n-        width : int, optional\n-            Width of the region line.\n-\n-        coordsys : str, optional\n-            Coordinate system. If not specified (default), the ``radesys``\n-            value is used. For all possible values, see\n-            http://ds9.si.edu/doc/ref/region.html#RegionFileFormat\n-\n-        \"\"\"\n-        comments = ('# Region file format: DS9 version 4.0 \\n'\n-                    '# global color=green font=\"helvetica 12 bold '\n-                    'select=1 highlite=1 edit=1 move=1 delete=1 '\n-                    'include=1 fixed=0 source\\n')\n-\n-        coordsys = coordsys or self.wcs.radesys\n-\n-        if coordsys not in ('PHYSICAL', 'IMAGE', 'FK4', 'B1950', 'FK5',\n-                            'J2000', 'GALACTIC', 'ECLIPTIC', 'ICRS', 'LINEAR',\n-                            'AMPLIFIER', 'DETECTOR'):\n-            raise ValueError(\"Coordinate system '{}' is not supported. A valid\"\n-                             \" one can be given with the 'coordsys' argument.\"\n-                             .format(coordsys))\n-\n-        with open(filename, mode='w') as f:\n-            f.write(comments)\n-            f.write('{}\\n'.format(coordsys))\n-            f.write('polygon(')\n-            self.calc_footprint().tofile(f, sep=',')\n-            f.write(') # color={0}, width={1:d} \\n'.format(color, width))\n-\n-    @property\n-    def _naxis1(self):\n-        return self._naxis[0]\n-\n-    @_naxis1.setter\n-    def _naxis1(self, value):\n-        self._naxis[0] = value\n-\n-    @property\n-    def _naxis2(self):\n-        return self._naxis[1]\n-\n-    @_naxis2.setter\n-    def _naxis2(self, value):\n-        self._naxis[1] = value\n-\n-    def _get_naxis(self, header=None):\n-        _naxis = []\n-        if (header is not None and\n-                not isinstance(header, (str, bytes))):\n-            for naxis in itertools.count(1):\n-                try:\n-                    _naxis.append(header['NAXIS{}'.format(naxis)])\n-                except KeyError:\n-                    break\n-        if len(_naxis) == 0:\n-            _naxis = [0, 0]\n-        elif len(_naxis) == 1:\n-            _naxis.append(0)\n-        self._naxis = _naxis\n-\n-    def printwcs(self):\n-        print(repr(self))\n-\n-    def __repr__(self):\n-        '''\n-        Return a short description. Simply porting the behavior from\n-        the `printwcs()` method.\n-        '''\n-        description = [\"WCS Keywords\\n\",\n-                       \"Number of WCS axes: {0!r}\".format(self.naxis)]\n-        sfmt = ' : ' + \"\".join([\"{\"+\"{0}\".format(i)+\"!r}  \" for i in range(self.naxis)])\n-\n-        keywords = ['CTYPE', 'CRVAL', 'CRPIX']\n-        values = [self.wcs.ctype, self.wcs.crval, self.wcs.crpix]\n-        for keyword, value in zip(keywords, values):\n-            description.append(keyword+sfmt.format(*value))\n-\n-        if hasattr(self.wcs, 'pc'):\n-            for i in range(self.naxis):\n-                s = ''\n-                for j in range(self.naxis):\n-                    s += ''.join(['PC', str(i+1), '_', str(j+1), ' '])\n-                s += sfmt\n-                description.append(s.format(*self.wcs.pc[i]))\n-            s = 'CDELT' + sfmt\n-            description.append(s.format(*self.wcs.cdelt))\n-        elif hasattr(self.wcs, 'cd'):\n-            for i in range(self.naxis):\n-                s = ''\n-                for j in range(self.naxis):\n-                    s += \"\".join(['CD', str(i+1), '_', str(j+1), ' '])\n-                s += sfmt\n-                description.append(s.format(*self.wcs.cd[i]))\n-\n-        description.append('NAXIS : {}'.format('  '.join(map(str, self._naxis))))\n-        return '\\n'.join(description)\n-\n-    def get_axis_types(self):\n-        \"\"\"\n-        Similar to `self.wcsprm.axis_types <astropy.wcs.Wcsprm.axis_types>`\n-        but provides the information in a more Python-friendly format.\n-\n-        Returns\n-        -------\n-        result : list of dicts\n-\n-            Returns a list of dictionaries, one for each axis, each\n-            containing attributes about the type of that axis.\n-\n-            Each dictionary has the following keys:\n-\n-            - 'coordinate_type':\n-\n-              - None: Non-specific coordinate type.\n-\n-              - 'stokes': Stokes coordinate.\n-\n-              - 'celestial': Celestial coordinate (including ``CUBEFACE``).\n-\n-              - 'spectral': Spectral coordinate.\n-\n-            - 'scale':\n-\n-              - 'linear': Linear axis.\n-\n-              - 'quantized': Quantized axis (``STOKES``, ``CUBEFACE``).\n-\n-              - 'non-linear celestial': Non-linear celestial axis.\n-\n-              - 'non-linear spectral': Non-linear spectral axis.\n-\n-              - 'logarithmic': Logarithmic axis.\n-\n-              - 'tabular': Tabular axis.\n-\n-            - 'group'\n-\n-              - Group number, e.g. lookup table number\n-\n-            - 'number'\n-\n-              - For celestial axes:\n-\n-                - 0: Longitude coordinate.\n-\n-                - 1: Latitude coordinate.\n-\n-                - 2: ``CUBEFACE`` number.\n-\n-              - For lookup tables:\n-\n-                - the axis number in a multidimensional table.\n-\n-            ``CTYPEia`` in ``\"4-3\"`` form with unrecognized algorithm code will\n-            generate an error.\n-        \"\"\"\n-        if self.wcs is None:\n-            raise AttributeError(\n-                \"This WCS object does not have a wcsprm object.\")\n-\n-        coordinate_type_map = {\n-            0: None,\n-            1: 'stokes',\n-            2: 'celestial',\n-            3: 'spectral'}\n-\n-        scale_map = {\n-            0: 'linear',\n-            1: 'quantized',\n-            2: 'non-linear celestial',\n-            3: 'non-linear spectral',\n-            4: 'logarithmic',\n-            5: 'tabular'}\n-\n-        result = []\n-        for axis_type in self.wcs.axis_types:\n-            subresult = {}\n-\n-            coordinate_type = (axis_type // 1000) % 10\n-            subresult['coordinate_type'] = coordinate_type_map[coordinate_type]\n-\n-            scale = (axis_type // 100) % 10\n-            subresult['scale'] = scale_map[scale]\n-\n-            group = (axis_type // 10) % 10\n-            subresult['group'] = group\n-\n-            number = axis_type % 10\n-            subresult['number'] = number\n-\n-            result.append(subresult)\n-\n-        return result\n-\n-    def __reduce__(self):\n-        \"\"\"\n-        Support pickling of WCS objects.  This is done by serializing\n-        to an in-memory FITS file and dumping that as a string.\n-        \"\"\"\n-\n-        hdulist = self.to_fits(relax=True)\n-\n-        buffer = io.BytesIO()\n-        hdulist.writeto(buffer)\n-\n-        return (__WCS_unpickle__,\n-                (self.__class__, self.__dict__, buffer.getvalue(),))\n-\n-    def dropaxis(self, dropax):\n-        \"\"\"\n-        Remove an axis from the WCS.\n-\n-        Parameters\n-        ----------\n-        wcs : `~astropy.wcs.WCS`\n-            The WCS with naxis to be chopped to naxis-1\n-        dropax : int\n-            The index of the WCS to drop, counting from 0 (i.e., python convention,\n-            not FITS convention)\n-\n-        Returns\n-        -------\n-        A new `~astropy.wcs.WCS` instance with one axis fewer\n-        \"\"\"\n-        inds = list(range(self.wcs.naxis))\n-        inds.pop(dropax)\n-\n-        # axis 0 has special meaning to sub\n-        # if wcs.wcs.ctype == ['RA','DEC','VLSR'], you want\n-        # wcs.sub([1,2]) to get 'RA','DEC' back\n-        return self.sub([i+1 for i in inds])\n-\n-    def swapaxes(self, ax0, ax1):\n-        \"\"\"\n-        Swap axes in a WCS.\n-\n-        Parameters\n-        ----------\n-        wcs : `~astropy.wcs.WCS`\n-            The WCS to have its axes swapped\n-        ax0 : int\n-        ax1 : int\n-            The indices of the WCS to be swapped, counting from 0 (i.e., python\n-            convention, not FITS convention)\n-\n-        Returns\n-        -------\n-        A new `~astropy.wcs.WCS` instance with the same number of axes, but two\n-        swapped\n-        \"\"\"\n-        inds = list(range(self.wcs.naxis))\n-        inds[ax0], inds[ax1] = inds[ax1], inds[ax0]\n-\n-        return self.sub([i+1 for i in inds])\n-\n-    def reorient_celestial_first(self):\n-        \"\"\"\n-        Reorient the WCS such that the celestial axes are first, followed by\n-        the spectral axis, followed by any others.\n-        Assumes at least celestial axes are present.\n-        \"\"\"\n-        return self.sub([WCSSUB_CELESTIAL, WCSSUB_SPECTRAL, WCSSUB_STOKES])\n-\n-    def slice(self, view, numpy_order=True):\n-        \"\"\"\n-        Slice a WCS instance using a Numpy slice. The order of the slice should\n-        be reversed (as for the data) compared to the natural WCS order.\n-\n-        Parameters\n-        ----------\n-        view : tuple\n-            A tuple containing the same number of slices as the WCS system.\n-            The ``step`` method, the third argument to a slice, is not\n-            presently supported.\n-        numpy_order : bool\n-            Use numpy order, i.e. slice the WCS so that an identical slice\n-            applied to a numpy array will slice the array and WCS in the same\n-            way. If set to `False`, the WCS will be sliced in FITS order,\n-            meaning the first slice will be applied to the *last* numpy index\n-            but the *first* WCS axis.\n-\n-        Returns\n-        -------\n-        wcs_new : `~astropy.wcs.WCS`\n-            A new resampled WCS axis\n-        \"\"\"\n-        if hasattr(view, '__len__') and len(view) > self.wcs.naxis:\n-            raise ValueError(\"Must have # of slices <= # of WCS axes\")\n-        elif not hasattr(view, '__len__'):  # view MUST be an iterable\n-            view = [view]\n-\n-        if not all(isinstance(x, slice) for x in view):\n-            raise ValueError(\"Cannot downsample a WCS with indexing.  Use \"\n-                             \"wcs.sub or wcs.dropaxis if you want to remove \"\n-                             \"axes.\")\n-\n-        wcs_new = self.deepcopy()\n-        if wcs_new.sip is not None:\n-            sip_crpix = wcs_new.sip.crpix.tolist()\n-\n-        for i, iview in enumerate(view):\n-            if iview.step is not None and iview.step < 0:\n-                raise NotImplementedError(\"Reversing an axis is not \"\n-                                          \"implemented.\")\n-\n-            if numpy_order:\n-                wcs_index = self.wcs.naxis - 1 - i\n-            else:\n-                wcs_index = i\n-\n-            if iview.step is not None and iview.start is None:\n-                # Slice from \"None\" is equivalent to slice from 0 (but one\n-                # might want to downsample, so allow slices with\n-                # None,None,step or None,stop,step)\n-                iview = slice(0, iview.stop, iview.step)\n-\n-            if iview.start is not None:\n-                if iview.step not in (None, 1):\n-                    crpix = self.wcs.crpix[wcs_index]\n-                    cdelt = self.wcs.cdelt[wcs_index]\n-                    # equivalently (keep this comment so you can compare eqns):\n-                    # wcs_new.wcs.crpix[wcs_index] =\n-                    # (crpix - iview.start)*iview.step + 0.5 - iview.step/2.\n-                    crp = ((crpix - iview.start - 1.)/iview.step\n-                           + 0.5 + 1./iview.step/2.)\n-                    wcs_new.wcs.crpix[wcs_index] = crp\n-                    if wcs_new.sip is not None:\n-                        sip_crpix[wcs_index] = crp\n-                    wcs_new.wcs.cdelt[wcs_index] = cdelt * iview.step\n-                else:\n-                    wcs_new.wcs.crpix[wcs_index] -= iview.start\n-                    if wcs_new.sip is not None:\n-                        sip_crpix[wcs_index] -= iview.start\n-\n-            try:\n-                # range requires integers but the other attributes can also\n-                # handle arbitary values, so this needs to be in a try/except.\n-                nitems = len(builtins.range(self._naxis[wcs_index])[iview])\n-            except TypeError as exc:\n-                if 'indices must be integers' not in str(exc):\n-                    raise\n-                warnings.warn(\"NAXIS{0} attribute is not updated because at \"\n-                              \"least one indix ('{1}') is no integer.\"\n-                              \"\".format(wcs_index, iview), AstropyUserWarning)\n-            else:\n-                wcs_new._naxis[wcs_index] = nitems\n-\n-        if wcs_new.sip is not None:\n-            wcs_new.sip = Sip(self.sip.a, self.sip.b, self.sip.ap, self.sip.bp,\n-                              sip_crpix)\n-\n-        return wcs_new\n-\n-    def __getitem__(self, item):\n-        # \"getitem\" is a shortcut for self.slice; it is very limited\n-        # there is no obvious and unambiguous interpretation of wcs[1,2,3]\n-        # We COULD allow wcs[1] to link to wcs.sub([2])\n-        # (wcs[i] -> wcs.sub([i+1])\n-        return self.slice(item)\n-\n-    def __iter__(self):\n-        # Having __getitem__ makes Python think WCS is iterable. However,\n-        # Python first checks whether __iter__ is present, so we can raise an\n-        # exception here.\n-        raise TypeError(\"'{0}' object is not iterable\".format(self.__class__.__name__))\n-\n-    @property\n-    def axis_type_names(self):\n-        \"\"\"\n-        World names for each coordinate axis\n-\n-        Returns\n-        -------\n-        A list of names along each axis\n-        \"\"\"\n-        names = list(self.wcs.cname)\n-        types = self.wcs.ctype\n-        for i in range(len(names)):\n-            if len(names[i]) > 0:\n-                continue\n-            names[i] = types[i].split('-')[0]\n-        return names\n-\n-    @property\n-    def celestial(self):\n-        \"\"\"\n-        A copy of the current WCS with only the celestial axes included\n-        \"\"\"\n-        return self.sub([WCSSUB_CELESTIAL])\n-\n-    @property\n-    def is_celestial(self):\n-        return self.has_celestial and self.naxis == 2\n-\n-    @property\n-    def has_celestial(self):\n-        try:\n-            return self.celestial.naxis == 2\n-        except InconsistentAxisTypesError:\n-            return False\n-\n-    @property\n-    def pixel_scale_matrix(self):\n-\n-        try:\n-            cdelt = np.matrix(np.diag(self.wcs.get_cdelt()))\n-            pc = np.matrix(self.wcs.get_pc())\n-        except InconsistentAxisTypesError:\n-            try:\n-                # for non-celestial axes, get_cdelt doesn't work\n-                cdelt = np.matrix(self.wcs.cd) * np.matrix(np.diag(self.wcs.cdelt))\n-            except AttributeError:\n-                cdelt = np.matrix(np.diag(self.wcs.cdelt))\n-\n-            try:\n-                pc = np.matrix(self.wcs.pc)\n-            except AttributeError:\n-                pc = 1\n-\n-        pccd = np.array(cdelt * pc)\n-\n-        return pccd\n-\n-    def _as_mpl_axes(self):\n-        \"\"\"\n-        Compatibility hook for Matplotlib and WCSAxes.\n-\n-        With this method, one can do:\n-\n-            from astropy.wcs import WCS\n-            import matplotlib.pyplot as plt\n-\n-            wcs = WCS('filename.fits')\n-\n-            fig = plt.figure()\n-            ax = fig.add_axes([0.15, 0.1, 0.8, 0.8], projection=wcs)\n-            ...\n-\n-        and this will generate a plot with the correct WCS coordinates on the\n-        axes.\n-        \"\"\"\n-        from ..visualization.wcsaxes import WCSAxes\n-        return WCSAxes, {'wcs': self}\n-\n-\n-def __WCS_unpickle__(cls, dct, fits_data):\n-    \"\"\"\n-    Unpickles a WCS object from a serialized FITS string.\n-    \"\"\"\n-\n-    self = cls.__new__(cls)\n-    self.__dict__.update(dct)\n-\n-    buffer = io.BytesIO(fits_data)\n-    hdulist = fits.open(buffer)\n-\n-    WCS.__init__(self, hdulist[0].header, hdulist)\n-\n-    return self\n-\n-\n-def find_all_wcs(header, relax=True, keysel=None, fix=True,\n-                 translate_units='',\n-                 _do_set=True):\n-    \"\"\"\n-    Find all the WCS transformations in the given header.\n-\n-    Parameters\n-    ----------\n-    header : str or astropy.io.fits header object.\n-\n-    relax : bool or int, optional\n-        Degree of permissiveness:\n-\n-        - `True` (default): Admit all recognized informal extensions of the\n-          WCS standard.\n-\n-        - `False`: Recognize only FITS keywords defined by the\n-          published WCS standard.\n-\n-        - `int`: a bit field selecting specific extensions to accept.\n-          See :ref:`relaxread` for details.\n-\n-    keysel : sequence of flags, optional\n-        A list of flags used to select the keyword types considered by\n-        wcslib.  When ``None``, only the standard image header\n-        keywords are considered (and the underlying wcspih() C\n-        function is called).  To use binary table image array or pixel\n-        list keywords, *keysel* must be set.\n-\n-        Each element in the list should be one of the following strings:\n-\n-            - 'image': Image header keywords\n-\n-            - 'binary': Binary table image array keywords\n-\n-            - 'pixel': Pixel list keywords\n-\n-        Keywords such as ``EQUIna`` or ``RFRQna`` that are common to\n-        binary table image arrays and pixel lists (including\n-        ``WCSNna`` and ``TWCSna``) are selected by both 'binary' and\n-        'pixel'.\n-\n-    fix : bool, optional\n-        When `True` (default), call `~astropy.wcs.Wcsprm.fix` on\n-        the resulting objects to fix any non-standard uses in the\n-        header.  `FITSFixedWarning` warnings will be emitted if any\n-        changes were made.\n-\n-    translate_units : str, optional\n-        Specify which potentially unsafe translations of non-standard\n-        unit strings to perform.  By default, performs none.  See\n-        `WCS.fix` for more information about this parameter.  Only\n-        effective when ``fix`` is `True`.\n-\n-    Returns\n-    -------\n-    wcses : list of `WCS` objects\n-    \"\"\"\n-\n-    if isinstance(header, (str, bytes)):\n-        header_string = header\n-    elif isinstance(header, fits.Header):\n-        header_string = header.tostring()\n-    else:\n-        raise TypeError(\n-            \"header must be a string or astropy.io.fits.Header object\")\n-\n-    keysel_flags = _parse_keysel(keysel)\n-\n-    if isinstance(header_string, str):\n-        header_bytes = header_string.encode('ascii')\n-    else:\n-        header_bytes = header_string\n-\n-    wcsprms = _wcs.find_all_wcs(header_bytes, relax, keysel_flags)\n-\n-    result = []\n-    for wcsprm in wcsprms:\n-        subresult = WCS(fix=False, _do_set=False)\n-        subresult.wcs = wcsprm\n-        result.append(subresult)\n-\n-        if fix:\n-            subresult.fix(translate_units)\n-\n-        if _do_set:\n-            subresult.wcs.set()\n-\n-    return result\n-\n-\n-def validate(source):\n-    \"\"\"\n-    Prints a WCS validation report for the given FITS file.\n-\n-    Parameters\n-    ----------\n-    source : str path, readable file-like object or `astropy.io.fits.HDUList` object\n-        The FITS file to validate.\n-\n-    Returns\n-    -------\n-    results : WcsValidateResults instance\n-        The result is returned as nested lists.  The first level\n-        corresponds to the HDUs in the given file.  The next level has\n-        an entry for each WCS found in that header.  The special\n-        subclass of list will pretty-print the results as a table when\n-        printed.\n-    \"\"\"\n-    class _WcsValidateWcsResult(list):\n-        def __init__(self, key):\n-            self._key = key\n-\n-        def __repr__(self):\n-            result = [\"  WCS key '{0}':\".format(self._key or ' ')]\n-            if len(self):\n-                for entry in self:\n-                    for i, line in enumerate(entry.splitlines()):\n-                        if i == 0:\n-                            initial_indent = '    - '\n-                        else:\n-                            initial_indent = '      '\n-                        result.extend(\n-                            textwrap.wrap(\n-                                line,\n-                                initial_indent=initial_indent,\n-                                subsequent_indent='      '))\n-            else:\n-                result.append(\"    No issues.\")\n-            return '\\n'.join(result)\n-\n-    class _WcsValidateHduResult(list):\n-        def __init__(self, hdu_index, hdu_name):\n-            self._hdu_index = hdu_index\n-            self._hdu_name = hdu_name\n-            list.__init__(self)\n-\n-        def __repr__(self):\n-            if len(self):\n-                if self._hdu_name:\n-                    hdu_name = ' ({0})'.format(self._hdu_name)\n-                else:\n-                    hdu_name = ''\n-                result = ['HDU {0}{1}:'.format(self._hdu_index, hdu_name)]\n-                for wcs in self:\n-                    result.append(repr(wcs))\n-                return '\\n'.join(result)\n-            return ''\n-\n-    class _WcsValidateResults(list):\n-        def __repr__(self):\n-            result = []\n-            for hdu in self:\n-                content = repr(hdu)\n-                if len(content):\n-                    result.append(content)\n-            return '\\n\\n'.join(result)\n-\n-    global __warningregistry__\n-\n-    if isinstance(source, fits.HDUList):\n-        hdulist = source\n-    else:\n-        hdulist = fits.open(source)\n-\n-    results = _WcsValidateResults()\n-\n-    for i, hdu in enumerate(hdulist):\n-        hdu_results = _WcsValidateHduResult(i, hdu.name)\n-        results.append(hdu_results)\n-\n-        with warnings.catch_warnings(record=True) as warning_lines:\n-            wcses = find_all_wcs(\n-                hdu.header, relax=_wcs.WCSHDR_reject,\n-                fix=False, _do_set=False)\n-\n-        for wcs in wcses:\n-            wcs_results = _WcsValidateWcsResult(wcs.wcs.alt)\n-            hdu_results.append(wcs_results)\n-\n-            try:\n-                del __warningregistry__\n-            except NameError:\n-                pass\n-\n-            with warnings.catch_warnings(record=True) as warning_lines:\n-                warnings.resetwarnings()\n-                warnings.simplefilter(\n-                    \"always\", FITSFixedWarning, append=True)\n-\n-                try:\n-                    WCS(hdu.header,\n-                        key=wcs.wcs.alt or ' ',\n-                        relax=_wcs.WCSHDR_reject,\n-                        fix=True, _do_set=False)\n-                except WcsError as e:\n-                    wcs_results.append(str(e))\n-\n-                wcs_results.extend([str(x.message) for x in warning_lines])\n-\n-    return results\n+[...]\n+    def _array_converter(self, func, sky, ra_dec_order=False):\n+        \"\"\"\n+        A helper function to support reading either a pair of arrays\n+        or a single Nx2 array.\n+        \"\"\"\n+        def _return_list_of_arrays(axes, origin):\n+            # If any input array is empty, return empty output arrays\n+            if any(ax.size == 0 for ax in axes):\n+                return [np.array([], dtype=np.float64) for _ in range(len(axes[0].shape))]\n+            try:\n+                axes = np.broadcast_arrays(*axes)\n+            except ValueError:\n+                raise ValueError(\n+                    \"Coordinate arrays are not broadcastable to each other\")\n+            [... rest of the function remains unchanged ...]\n\\ No newline at end of file", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "django__django-10914", "model_patch": "--- django/conf/global_settings.py\n+++ django/conf/global_settings.py\n@@ -1,634 +1,13 @@\n-\"\"\"\n-Default Django settings. Override these with settings in the module pointed to\n-by the DJANGO_SETTINGS_MODULE environment variable.\n-\"\"\"\n+# ... (previous code remains the same)\n \n-\n-# This is defined here as a do-nothing function because we can't import\n-# django.utils.translation -- that module depends on the settings.\n-def gettext_noop(s):\n-    return s\n-\n-\n-####################\n-# CORE             #\n-####################\n-\n-DEBUG = False\n-\n-# Whether the framework should propagate raw exceptions rather than catching\n-# them. This is useful under some testing situations and should never be used\n-# on a live site.\n-DEBUG_PROPAGATE_EXCEPTIONS = False\n-\n-# People who get code error notifications.\n-# In the format [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]\n-ADMINS = []\n-\n-# List of IP addresses, as strings, that:\n-#   * See debug comments, when DEBUG is true\n-#   * Receive x-headers\n-INTERNAL_IPS = []\n-\n-# Hosts/domain names that are valid for this site.\n-# \"*\" matches anything, \".example.com\" matches example.com and all subdomains\n-ALLOWED_HOSTS = []\n-\n-# Local time zone for this installation. All choices can be found here:\n-# https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all\n-# systems may support all possibilities). When USE_TZ is True, this is\n-# interpreted as the default user time zone.\n-TIME_ZONE = 'America/Chicago'\n-\n-# If you set this to True, Django will use timezone-aware datetimes.\n-USE_TZ = False\n-\n-# Language code for this installation. All choices can be found here:\n-# http://www.i18nguy.com/unicode/language-identifiers.html\n-LANGUAGE_CODE = 'en-us'\n-\n-# Languages we provide translations for, out of the box.\n-LANGUAGES = [\n-    ('af', gettext_noop('Afrikaans')),\n-    ('ar', gettext_noop('Arabic')),\n-    ('ast', gettext_noop('Asturian')),\n-    ('az', gettext_noop('Azerbaijani')),\n-    ('bg', gettext_noop('Bulgarian')),\n-    ('be', gettext_noop('Belarusian')),\n-    ('bn', gettext_noop('Bengali')),\n-    ('br', gettext_noop('Breton')),\n-    ('bs', gettext_noop('Bosnian')),\n-    ('ca', gettext_noop('Catalan')),\n-    ('cs', gettext_noop('Czech')),\n-    ('cy', gettext_noop('Welsh')),\n-    ('da', gettext_noop('Danish')),\n-    ('de', gettext_noop('German')),\n-    ('dsb', gettext_noop('Lower Sorbian')),\n-    ('el', gettext_noop('Greek')),\n-    ('en', gettext_noop('English')),\n-    ('en-au', gettext_noop('Australian English')),\n-    ('en-gb', gettext_noop('British English')),\n-    ('eo', gettext_noop('Esperanto')),\n-    ('es', gettext_noop('Spanish')),\n-    ('es-ar', gettext_noop('Argentinian Spanish')),\n-    ('es-co', gettext_noop('Colombian Spanish')),\n-    ('es-mx', gettext_noop('Mexican Spanish')),\n-    ('es-ni', gettext_noop('Nicaraguan Spanish')),\n-    ('es-ve', gettext_noop('Venezuelan Spanish')),\n-    ('et', gettext_noop('Estonian')),\n-    ('eu', gettext_noop('Basque')),\n-    ('fa', gettext_noop('Persian')),\n-    ('fi', gettext_noop('Finnish')),\n-    ('fr', gettext_noop('French')),\n-    ('fy', gettext_noop('Frisian')),\n-    ('ga', gettext_noop('Irish')),\n-    ('gd', gettext_noop('Scottish Gaelic')),\n-    ('gl', gettext_noop('Galician')),\n-    ('he', gettext_noop('Hebrew')),\n-    ('hi', gettext_noop('Hindi')),\n-    ('hr', gettext_noop('Croatian')),\n-    ('hsb', gettext_noop('Upper Sorbian')),\n-    ('hu', gettext_noop('Hungarian')),\n-    ('hy', gettext_noop('Armenian')),\n-    ('ia', gettext_noop('Interlingua')),\n-    ('id', gettext_noop('Indonesian')),\n-    ('io', gettext_noop('Ido')),\n-    ('is', gettext_noop('Icelandic')),\n-    ('it', gettext_noop('Italian')),\n-    ('ja', gettext_noop('Japanese')),\n-    ('ka', gettext_noop('Georgian')),\n-    ('kab', gettext_noop('Kabyle')),\n-    ('kk', gettext_noop('Kazakh')),\n-    ('km', gettext_noop('Khmer')),\n-    ('kn', gettext_noop('Kannada')),\n-    ('ko', gettext_noop('Korean')),\n-    ('lb', gettext_noop('Luxembourgish')),\n-    ('lt', gettext_noop('Lithuanian')),\n-    ('lv', gettext_noop('Latvian')),\n-    ('mk', gettext_noop('Macedonian')),\n-    ('ml', gettext_noop('Malayalam')),\n-    ('mn', gettext_noop('Mongolian')),\n-    ('mr', gettext_noop('Marathi')),\n-    ('my', gettext_noop('Burmese')),\n-    ('nb', gettext_noop('Norwegian Bokm\u00e5l')),\n-    ('ne', gettext_noop('Nepali')),\n-    ('nl', gettext_noop('Dutch')),\n-    ('nn', gettext_noop('Norwegian Nynorsk')),\n-    ('os', gettext_noop('Ossetic')),\n-    ('pa', gettext_noop('Punjabi')),\n-    ('pl', gettext_noop('Polish')),\n-    ('pt', gettext_noop('Portuguese')),\n-    ('pt-br', gettext_noop('Brazilian Portuguese')),\n-    ('ro', gettext_noop('Romanian')),\n-    ('ru', gettext_noop('Russian')),\n-    ('sk', gettext_noop('Slovak')),\n-    ('sl', gettext_noop('Slovenian')),\n-    ('sq', gettext_noop('Albanian')),\n-    ('sr', gettext_noop('Serbian')),\n-    ('sr-latn', gettext_noop('Serbian Latin')),\n-    ('sv', gettext_noop('Swedish')),\n-    ('sw', gettext_noop('Swahili')),\n-    ('ta', gettext_noop('Tamil')),\n-    ('te', gettext_noop('Telugu')),\n-    ('th', gettext_noop('Thai')),\n-    ('tr', gettext_noop('Turkish')),\n-    ('tt', gettext_noop('Tatar')),\n-    ('udm', gettext_noop('Udmurt')),\n-    ('uk', gettext_noop('Ukrainian')),\n-    ('ur', gettext_noop('Urdu')),\n-    ('vi', gettext_noop('Vietnamese')),\n-    ('zh-hans', gettext_noop('Simplified Chinese')),\n-    ('zh-hant', gettext_noop('Traditional Chinese')),\n-]\n-\n-# Languages using BiDi (right-to-left) layout\n-LANGUAGES_BIDI = [\"he\", \"ar\", \"fa\", \"ur\"]\n-\n-# If you set this to False, Django will make some optimizations so as not\n-# to load the internationalization machinery.\n-USE_I18N = True\n-LOCALE_PATHS = []\n-\n-# Settings for language cookie\n-LANGUAGE_COOKIE_NAME = 'django_language'\n-LANGUAGE_COOKIE_AGE = None\n-LANGUAGE_COOKIE_DOMAIN = None\n-LANGUAGE_COOKIE_PATH = '/'\n-\n-\n-# If you set this to True, Django will format dates, numbers and calendars\n-# according to user current locale.\n-USE_L10N = False\n-\n-# Not-necessarily-technical managers of the site. They get broken link\n-# notifications and other various emails.\n-MANAGERS = ADMINS\n-\n-# Default charset to use for all HttpResponse objects, if a MIME type isn't\n-# manually specified. It's used to construct the Content-Type header.\n-DEFAULT_CHARSET = 'utf-8'\n-\n-# Encoding of files read from disk (template and initial SQL files).\n-FILE_CHARSET = 'utf-8'\n-\n-# Email address that error messages come from.\n-SERVER_EMAIL = 'root@localhost'\n-\n-# Database connection info. If left empty, will default to the dummy backend.\n-DATABASES = {}\n-\n-# Classes used to implement DB routing behavior.\n-DATABASE_ROUTERS = []\n-\n-# The email backend to use. For possible shortcuts see django.core.mail.\n-# The default is to use the SMTP backend.\n-# Third-party backends can be specified by providing a Python path\n-# to a module that defines an EmailBackend class.\n-EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'\n-\n-# Host for sending email.\n-EMAIL_HOST = 'localhost'\n-\n-# Port for sending email.\n-EMAIL_PORT = 25\n-\n-# Whether to send SMTP 'Date' header in the local time zone or in UTC.\n-EMAIL_USE_LOCALTIME = False\n-\n-# Optional SMTP authentication information for EMAIL_HOST.\n-EMAIL_HOST_USER = ''\n-EMAIL_HOST_PASSWORD = ''\n-EMAIL_USE_TLS = False\n-EMAIL_USE_SSL = False\n-EMAIL_SSL_CERTFILE = None\n-EMAIL_SSL_KEYFILE = None\n-EMAIL_TIMEOUT = None\n-\n-# List of strings representing installed apps.\n-INSTALLED_APPS = []\n-\n-TEMPLATES = []\n-\n-# Default form rendering class.\n-FORM_RENDERER = 'django.forms.renderers.DjangoTemplates'\n-\n-# Default email address to use for various automated correspondence from\n-# the site managers.\n-DEFAULT_FROM_EMAIL = 'webmaster@localhost'\n-\n-# Subject-line prefix for email messages send with django.core.mail.mail_admins\n-# or ...mail_managers.  Make sure to include the trailing space.\n-EMAIL_SUBJECT_PREFIX = '[Django] '\n-\n-# Whether to append trailing slashes to URLs.\n-APPEND_SLASH = True\n-\n-# Whether to prepend the \"www.\" subdomain to URLs that don't have it.\n-PREPEND_WWW = False\n-\n-# Override the server-derived value of SCRIPT_NAME\n-FORCE_SCRIPT_NAME = None\n-\n-# List of compiled regular expression objects representing User-Agent strings\n-# that are not allowed to visit any page, systemwide. Use this for bad\n-# robots/crawlers. Here are a few examples:\n-#     import re\n-#     DISALLOWED_USER_AGENTS = [\n-#         re.compile(r'^NaverBot.*'),\n-#         re.compile(r'^EmailSiphon.*'),\n-#         re.compile(r'^SiteSucker.*'),\n-#         re.compile(r'^sohu-search'),\n-#     ]\n-DISALLOWED_USER_AGENTS = []\n-\n-ABSOLUTE_URL_OVERRIDES = {}\n-\n-# List of compiled regular expression objects representing URLs that need not\n-# be reported by BrokenLinkEmailsMiddleware. Here are a few examples:\n-#    import re\n-#    IGNORABLE_404_URLS = [\n-#        re.compile(r'^/apple-touch-icon.*\\.png$'),\n-#        re.compile(r'^/favicon.ico$'),\n-#        re.compile(r'^/robots.txt$'),\n-#        re.compile(r'^/phpmyadmin/'),\n-#        re.compile(r'\\.(cgi|php|pl)$'),\n-#    ]\n-IGNORABLE_404_URLS = []\n-\n-# A secret key for this particular Django installation. Used in secret-key\n-# hashing algorithms. Set this in your settings, or Django will complain\n-# loudly.\n-SECRET_KEY = ''\n-\n-# Default file storage mechanism that holds media.\n-DEFAULT_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage'\n-\n-# Absolute filesystem path to the directory that will hold user-uploaded files.\n-# Example: \"/var/www/example.com/media/\"\n-MEDIA_ROOT = ''\n-\n-# URL that handles the media served from MEDIA_ROOT.\n-# Examples: \"http://example.com/media/\", \"http://media.example.com/\"\n-MEDIA_URL = ''\n-\n-# Absolute path to the directory static files should be collected to.\n-# Example: \"/var/www/example.com/static/\"\n-STATIC_ROOT = None\n-\n-# URL that handles the static files served from STATIC_ROOT.\n-# Example: \"http://example.com/static/\", \"http://static.example.com/\"\n-STATIC_URL = None\n-\n-# List of upload handler classes to be applied in order.\n-FILE_UPLOAD_HANDLERS = [\n-    'django.core.files.uploadhandler.MemoryFileUploadHandler',\n-    'django.core.files.uploadhandler.TemporaryFileUploadHandler',\n-]\n-\n-# Maximum size, in bytes, of a request before it will be streamed to the\n-# file system instead of into memory.\n-FILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n-\n-# Maximum size in bytes of request data (excluding file uploads) that will be\n-# read before a SuspiciousOperation (RequestDataTooBig) is raised.\n-DATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n-\n-# Maximum number of GET/POST parameters that will be read before a\n-# SuspiciousOperation (TooManyFieldsSent) is raised.\n-DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000\n-\n-# Directory in which upload streamed files will be temporarily saved. A value of\n-# `None` will make Django use the operating system's default temporary directory\n-# (i.e. \"/tmp\" on *nix systems).\n-FILE_UPLOAD_TEMP_DIR = None\n-\n # The numeric mode to set newly-uploaded files to. The value should be a mode\n # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\n-FILE_UPLOAD_PERMISSIONS = None\n+# Default is 0o644 for better security and consistent permissions.\n+FILE_UPLOAD_PERMISSIONS = 0o644\n \n # The numeric mode to assign to newly-created directories, when uploading files.\n # The value should be a mode as you'd pass to os.chmod;\n # see https://docs.python.org/library/os.html#files-and-directories.\n FILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n \n-# Python module path where user will place custom format definition.\n-# The directory where this setting is pointing should contain subdirectories\n-# named as the locales, containing a formats.py file\n-# (i.e. \"myproject.locale\" for myproject/locale/en/formats.py etc. use)\n-FORMAT_MODULE_PATH = None\n-\n-# Default formatting for date objects. See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-DATE_FORMAT = 'N j, Y'\n-\n-# Default formatting for datetime objects. See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-DATETIME_FORMAT = 'N j, Y, P'\n-\n-# Default formatting for time objects. See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-TIME_FORMAT = 'P'\n-\n-# Default formatting for date objects when only the year and month are relevant.\n-# See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-YEAR_MONTH_FORMAT = 'F Y'\n-\n-# Default formatting for date objects when only the month and day are relevant.\n-# See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-MONTH_DAY_FORMAT = 'F j'\n-\n-# Default short formatting for date objects. See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-SHORT_DATE_FORMAT = 'm/d/Y'\n-\n-# Default short formatting for datetime objects.\n-# See all available format strings here:\n-# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\n-SHORT_DATETIME_FORMAT = 'm/d/Y P'\n-\n-# Default formats to be used when parsing dates from input boxes, in order\n-# See all available format string here:\n-# https://docs.python.org/library/datetime.html#strftime-behavior\n-# * Note that these format strings are different from the ones to display dates\n-DATE_INPUT_FORMATS = [\n-    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',  # '2006-10-25', '10/25/2006', '10/25/06'\n-    '%b %d %Y', '%b %d, %Y',             # 'Oct 25 2006', 'Oct 25, 2006'\n-    '%d %b %Y', '%d %b, %Y',             # '25 Oct 2006', '25 Oct, 2006'\n-    '%B %d %Y', '%B %d, %Y',             # 'October 25 2006', 'October 25, 2006'\n-    '%d %B %Y', '%d %B, %Y',             # '25 October 2006', '25 October, 2006'\n-]\n-\n-# Default formats to be used when parsing times from input boxes, in order\n-# See all available format string here:\n-# https://docs.python.org/library/datetime.html#strftime-behavior\n-# * Note that these format strings are different from the ones to display dates\n-TIME_INPUT_FORMATS = [\n-    '%H:%M:%S',     # '14:30:59'\n-    '%H:%M:%S.%f',  # '14:30:59.000200'\n-    '%H:%M',        # '14:30'\n-]\n-\n-# Default formats to be used when parsing dates and times from input boxes,\n-# in order\n-# See all available format string here:\n-# https://docs.python.org/library/datetime.html#strftime-behavior\n-# * Note that these format strings are different from the ones to display dates\n-DATETIME_INPUT_FORMATS = [\n-    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'\n-    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'\n-    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'\n-    '%Y-%m-%d',              # '2006-10-25'\n-    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'\n-    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'\n-    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'\n-    '%m/%d/%Y',              # '10/25/2006'\n-    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'\n-    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'\n-    '%m/%d/%y %H:%M',        # '10/25/06 14:30'\n-    '%m/%d/%y',              # '10/25/06'\n-]\n-\n-# First day of week, to be used on calendars\n-# 0 means Sunday, 1 means Monday...\n-FIRST_DAY_OF_WEEK = 0\n-\n-# Decimal separator symbol\n-DECIMAL_SEPARATOR = '.'\n-\n-# Boolean that sets whether to add thousand separator when formatting numbers\n-USE_THOUSAND_SEPARATOR = False\n-\n-# Number of digits that will be together, when splitting them by\n-# THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...\n-NUMBER_GROUPING = 0\n-\n-# Thousand separator symbol\n-THOUSAND_SEPARATOR = ','\n-\n-# The tablespaces to use for each model when not specified otherwise.\n-DEFAULT_TABLESPACE = ''\n-DEFAULT_INDEX_TABLESPACE = ''\n-\n-# Default X-Frame-Options header value\n-X_FRAME_OPTIONS = 'SAMEORIGIN'\n-\n-USE_X_FORWARDED_HOST = False\n-USE_X_FORWARDED_PORT = False\n-\n-# The Python dotted path to the WSGI application that Django's internal server\n-# (runserver) will use. If `None`, the return value of\n-# 'django.core.wsgi.get_wsgi_application' is used, thus preserving the same\n-# behavior as previous versions of Django. Otherwise this should point to an\n-# actual WSGI application object.\n-WSGI_APPLICATION = None\n-\n-# If your Django app is behind a proxy that sets a header to specify secure\n-# connections, AND that proxy ensures that user-submitted headers with the\n-# same name are ignored (so that people can't spoof it), set this value to\n-# a tuple of (header_name, header_value). For any requests that come in with\n-# that header/value, request.is_secure() will return True.\n-# WARNING! Only set this if you fully understand what you're doing. Otherwise,\n-# you may be opening yourself up to a security risk.\n-SECURE_PROXY_SSL_HEADER = None\n-\n-##############\n-# MIDDLEWARE #\n-##############\n-\n-# List of middleware to use. Order is important; in the request phase, these\n-# middleware will be applied in the order given, and in the response\n-# phase the middleware will be applied in reverse order.\n-MIDDLEWARE = []\n-\n-############\n-# SESSIONS #\n-############\n-\n-# Cache to store session data if using the cache session backend.\n-SESSION_CACHE_ALIAS = 'default'\n-# Cookie name. This can be whatever you want.\n-SESSION_COOKIE_NAME = 'sessionid'\n-# Age of cookie, in seconds (default: 2 weeks).\n-SESSION_COOKIE_AGE = 60 * 60 * 24 * 7 * 2\n-# A string like \"example.com\", or None for standard domain cookie.\n-SESSION_COOKIE_DOMAIN = None\n-# Whether the session cookie should be secure (https:// only).\n-SESSION_COOKIE_SECURE = False\n-# The path of the session cookie.\n-SESSION_COOKIE_PATH = '/'\n-# Whether to use the non-RFC standard httpOnly flag (IE, FF3+, others)\n-SESSION_COOKIE_HTTPONLY = True\n-# Whether to set the flag restricting cookie leaks on cross-site requests.\n-# This can be 'Lax', 'Strict', or None to disable the flag.\n-SESSION_COOKIE_SAMESITE = 'Lax'\n-# Whether to save the session data on every request.\n-SESSION_SAVE_EVERY_REQUEST = False\n-# Whether a user's session cookie expires when the Web browser is closed.\n-SESSION_EXPIRE_AT_BROWSER_CLOSE = False\n-# The module to store session data\n-SESSION_ENGINE = 'django.contrib.sessions.backends.db'\n-# Directory to store session files if using the file session module. If None,\n-# the backend will use a sensible default.\n-SESSION_FILE_PATH = None\n-# class to serialize session data\n-SESSION_SERIALIZER = 'django.contrib.sessions.serializers.JSONSerializer'\n-\n-#########\n-# CACHE #\n-#########\n-\n-# The cache backends to use.\n-CACHES = {\n-    'default': {\n-        'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n-    }\n-}\n-CACHE_MIDDLEWARE_KEY_PREFIX = ''\n-CACHE_MIDDLEWARE_SECONDS = 600\n-CACHE_MIDDLEWARE_ALIAS = 'default'\n-\n-##################\n-# AUTHENTICATION #\n-##################\n-\n-AUTH_USER_MODEL = 'auth.User'\n-\n-AUTHENTICATION_BACKENDS = ['django.contrib.auth.backends.ModelBackend']\n-\n-LOGIN_URL = '/accounts/login/'\n-\n-LOGIN_REDIRECT_URL = '/accounts/profile/'\n-\n-LOGOUT_REDIRECT_URL = None\n-\n-# The number of days a password reset link is valid for\n-PASSWORD_RESET_TIMEOUT_DAYS = 3\n-\n-# the first hasher in this list is the preferred algorithm.  any\n-# password using different algorithms will be converted automatically\n-# upon login\n-PASSWORD_HASHERS = [\n-    'django.contrib.auth.hashers.PBKDF2PasswordHasher',\n-    'django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher',\n-    'django.contrib.auth.hashers.Argon2PasswordHasher',\n-    'django.contrib.auth.hashers.BCryptSHA256PasswordHasher',\n-]\n-\n-AUTH_PASSWORD_VALIDATORS = []\n-\n-###########\n-# SIGNING #\n-###########\n-\n-SIGNING_BACKEND = 'django.core.signing.TimestampSigner'\n-\n-########\n-# CSRF #\n-########\n-\n-# Dotted path to callable to be used as view when a request is\n-# rejected by the CSRF middleware.\n-CSRF_FAILURE_VIEW = 'django.views.csrf.csrf_failure'\n-\n-# Settings for CSRF cookie.\n-CSRF_COOKIE_NAME = 'csrftoken'\n-CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52\n-CSRF_COOKIE_DOMAIN = None\n-CSRF_COOKIE_PATH = '/'\n-CSRF_COOKIE_SECURE = False\n-CSRF_COOKIE_HTTPONLY = False\n-CSRF_COOKIE_SAMESITE = 'Lax'\n-CSRF_HEADER_NAME = 'HTTP_X_CSRFTOKEN'\n-CSRF_TRUSTED_ORIGINS = []\n-CSRF_USE_SESSIONS = False\n-\n-############\n-# MESSAGES #\n-############\n-\n-# Class to use as messages backend\n-MESSAGE_STORAGE = 'django.contrib.messages.storage.fallback.FallbackStorage'\n-\n-# Default values of MESSAGE_LEVEL and MESSAGE_TAGS are defined within\n-# django.contrib.messages to avoid imports in this settings file.\n-\n-###########\n-# LOGGING #\n-###########\n-\n-# The callable to use to configure logging\n-LOGGING_CONFIG = 'logging.config.dictConfig'\n-\n-# Custom logging configuration.\n-LOGGING = {}\n-\n-# Default exception reporter filter class used in case none has been\n-# specifically assigned to the HttpRequest instance.\n-DEFAULT_EXCEPTION_REPORTER_FILTER = 'django.views.debug.SafeExceptionReporterFilter'\n-\n-###########\n-# TESTING #\n-###########\n-\n-# The name of the class to use to run the test suite\n-TEST_RUNNER = 'django.test.runner.DiscoverRunner'\n-\n-# Apps that don't need to be serialized at test database creation time\n-# (only apps with migrations are to start with)\n-TEST_NON_SERIALIZED_APPS = []\n-\n-############\n-# FIXTURES #\n-############\n-\n-# The list of directories to search for fixtures\n-FIXTURE_DIRS = []\n-\n-###############\n-# STATICFILES #\n-###############\n-\n-# A list of locations of additional static files\n-STATICFILES_DIRS = []\n-\n-# The default file storage backend used during the build process\n-STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.StaticFilesStorage'\n-\n-# List of finder classes that know how to find static files in\n-# various locations.\n-STATICFILES_FINDERS = [\n-    'django.contrib.staticfiles.finders.FileSystemFinder',\n-    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n-    # 'django.contrib.staticfiles.finders.DefaultStorageFinder',\n-]\n-\n-##############\n-# MIGRATIONS #\n-##############\n-\n-# Migration module overrides for apps, by app label.\n-MIGRATION_MODULES = {}\n-\n-#################\n-# SYSTEM CHECKS #\n-#################\n-\n-# List of all issues generated by system checks that should be silenced. Light\n-# issues like warnings, infos or debugs will not generate a message. Silencing\n-# serious issues like errors and criticals does not result in hiding the\n-# message, but Django will not stop you from e.g. running server.\n-SILENCED_SYSTEM_CHECKS = []\n-\n-#######################\n-# SECURITY MIDDLEWARE #\n-#######################\n-SECURE_BROWSER_XSS_FILTER = False\n-SECURE_CONTENT_TYPE_NOSNIFF = False\n-SECURE_HSTS_INCLUDE_SUBDOMAINS = False\n-SECURE_HSTS_PRELOAD = False\n-SECURE_HSTS_SECONDS = 0\n-SECURE_REDIRECT_EXEMPT = []\n-SECURE_SSL_HOST = None\n-SECURE_SSL_REDIRECT = False\n+# ... (rest of the code remains the same)\n\\ No newline at end of file", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
{"instance_id": "django__django-10924", "model_patch": "", "model_name_or_path": "agent_programmer_deepseek/deepseek-r1-distill-qwen-32b"}
